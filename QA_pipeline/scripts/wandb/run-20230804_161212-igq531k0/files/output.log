  0%|                                                                                                                                                                              | 0/370 [00:00<?, ?it/s]/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▍                                                                                                                                                                   | 1/370 [00:25<2:34:43, 25.16s/it]

  1%|▉                                                                                                                                                                   | 2/370 [00:49<2:31:04, 24.63s/it]

  1%|█▎                                                                                                                                                                  | 3/370 [01:12<2:26:52, 24.01s/it]

  1%|█▊                                                                                                                                                                  | 4/370 [01:37<2:28:15, 24.30s/it]

  1%|██▏                                                                                                                                                                 | 5/370 [02:01<2:27:18, 24.21s/it]

  2%|██▋                                                                                                                                                                 | 6/370 [02:25<2:27:05, 24.25s/it]

  2%|███                                                                                                                                                                 | 7/370 [02:49<2:26:30, 24.22s/it]

  2%|███▌                                                                                                                                                                | 8/370 [03:13<2:25:32, 24.12s/it]

  2%|███▉                                                                                                                                                                | 9/370 [03:40<2:29:13, 24.80s/it]

  3%|████▍                                                                                                                                                              | 10/370 [04:05<2:29:45, 24.96s/it]

  3%|████▊                                                                                                                                                              | 11/370 [04:29<2:28:09, 24.76s/it]

  3%|█████▎                                                                                                                                                             | 12/370 [04:54<2:27:06, 24.65s/it]
{'loss': 2.4133, 'learning_rate': 0.00029964648475290094, 'epoch': 0.32}

  4%|█████▋                                                                                                                                                             | 13/370 [05:18<2:26:04, 24.55s/it]


  4%|██████▌                                                                                                                                                            | 15/370 [06:07<2:24:42, 24.46s/it]

  4%|███████                                                                                                                                                            | 16/370 [06:32<2:25:19, 24.63s/it]

  5%|███████▍                                                                                                                                                           | 17/370 [06:57<2:25:42, 24.77s/it]

  5%|███████▉                                                                                                                                                           | 18/370 [08:14<3:58:01, 40.57s/it]

  5%|████████▎                                                                                                                                                          | 19/370 [08:42<3:34:55, 36.74s/it]
{'loss': 2.3843, 'learning_rate': 0.0002987584011204152, 'epoch': 0.51}















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [02:32<00:09,  9.02s/it]


  5%|████████▎                                                                                                                                                          | 19/370 [11:46<3:34:55, 36.74s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-19)... Done. 2.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  5%|████████▋                                                                                                                                                        | 20/370 [14:24<12:28:33, 128.33s/it]

  6%|█████████▎                                                                                                                                                         | 21/370 [14:55<9:36:08, 99.05s/it]

  6%|█████████▋                                                                                                                                                         | 22/370 [15:23<7:31:25, 77.83s/it]
{'loss': 2.3291, 'learning_rate': 0.0002982131844136615, 'epoch': 0.59}

  6%|██████████▏                                                                                                                                                        | 23/370 [15:49<6:00:14, 62.29s/it]

  6%|██████████▌                                                                                                                                                        | 24/370 [16:18<5:00:53, 52.18s/it]


  7%|███████████▎                                                                                                                                                     | 26/370 [30:01<24:20:01, 254.66s/it]

  7%|███████████▋                                                                                                                                                     | 27/370 [30:25<17:40:30, 185.51s/it]

  8%|████████████▏                                                                                                                                                    | 28/370 [30:49<13:01:18, 137.07s/it]
{'loss': 2.204, 'learning_rate': 0.0002968283527643036, 'epoch': 0.76}

  8%|████████████▋                                                                                                                                                     | 29/370 [31:13<9:45:29, 103.02s/it]

  8%|█████████████▏                                                                                                                                                     | 30/370 [31:37<7:29:15, 79.28s/it]

  8%|█████████████▋                                                                                                                                                     | 31/370 [32:00<5:53:59, 62.65s/it]

  9%|██████████████                                                                                                                                                     | 32/370 [32:25<4:47:49, 51.09s/it]

  9%|██████████████▌                                                                                                                                                    | 33/370 [32:49<4:01:14, 42.95s/it]

  9%|██████████████▉                                                                                                                                                    | 34/370 [33:13<3:28:52, 37.30s/it]

  9%|███████████████▍                                                                                                                                                   | 35/370 [33:37<3:06:20, 33.37s/it]

 10%|███████████████▊                                                                                                                                                   | 36/370 [34:01<2:50:30, 30.63s/it]

 10%|████████████████▎                                                                                                                                                  | 37/370 [34:25<2:39:04, 28.66s/it]

 10%|████████████████▋                                                                                                                                                  | 38/370 [34:49<2:30:14, 27.15s/it]














 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:49<00:03,  3.52s/it]


 10%|████████████████▋                                                                                                                                                  | 38/370 [35:45<2:30:14, 27.15s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-38)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.314, 'learning_rate': 0.000295054158718698, 'epoch': 1.05}
 11%|█████████████████▏                                                                                                                                                 | 39/370 [36:10<3:58:46, 43.28s/it]

 11%|█████████████████▌                                                                                                                                                 | 40/370 [36:33<3:25:38, 37.39s/it]

 11%|██████████████████                                                                                                                                                 | 41/370 [36:57<3:03:10, 33.40s/it]

 11%|██████████████████▌                                                                                                                                                | 42/370 [37:22<2:47:18, 30.61s/it]

 12%|██████████████████▉                                                                                                                                                | 43/370 [37:50<2:42:39, 29.84s/it]

 12%|███████████████████▍                                                                                                                                               | 44/370 [38:23<2:48:41, 31.05s/it]

 12%|███████████████████▊                                                                                                                                               | 45/370 [38:58<2:54:09, 32.15s/it]

 12%|████████████████████▎                                                                                                                                              | 46/370 [39:24<2:43:00, 30.19s/it]

 13%|████████████████████▋                                                                                                                                              | 47/370 [39:48<2:33:09, 28.45s/it]

 13%|█████████████████████▏                                                                                                                                             | 48/370 [40:13<2:26:36, 27.32s/it]

 13%|█████████████████████▌                                                                                                                                             | 49/370 [40:38<2:21:52, 26.52s/it]

 14%|██████████████████████                                                                                                                                             | 50/370 [41:02<2:18:01, 25.88s/it]

 14%|██████████████████████▍                                                                                                                                            | 51/370 [41:26<2:14:48, 25.36s/it]

 14%|██████████████████████▉                                                                                                                                            | 52/370 [41:50<2:11:53, 24.88s/it]

 14%|███████████████████████▎                                                                                                                                           | 53/370 [42:14<2:10:24, 24.68s/it]

 15%|███████████████████████▊                                                                                                                                           | 54/370 [42:38<2:09:03, 24.51s/it]

 15%|████████████████████████▏                                                                                                                                          | 55/370 [43:11<2:21:16, 26.91s/it]

 15%|████████████████████████▋                                                                                                                                          | 56/370 [43:39<2:23:24, 27.40s/it]

 15%|█████████████████████████                                                                                                                                          | 57/370 [44:14<2:34:07, 29.55s/it]














 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:52<00:03,  3.83s/it]

{'eval_loss': 2.1019644737243652, 'eval_runtime': 60.8776, 'eval_samples_per_second': 4.189, 'eval_steps_per_second': 0.263, 'epoch': 1.54}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.1199, 'learning_rate': 0.0002869268789988203, 'epoch': 1.57}
 16%|█████████████████████████▌                                                                                                                                         | 58/370 [45:40<4:02:27, 46.63s/it]

 16%|█████████████████████████▉                                                                                                                                         | 59/370 [46:04<3:26:18, 39.80s/it]

 16%|██████████████████████████▍                                                                                                                                        | 60/370 [46:28<3:00:26, 34.92s/it]

 16%|██████████████████████████▊                                                                                                                                        | 61/370 [46:52<2:43:10, 31.68s/it]

 17%|███████████████████████████▎                                                                                                                                       | 62/370 [47:16<2:31:29, 29.51s/it]

 17%|███████████████████████████▊                                                                                                                                       | 63/370 [47:39<2:20:17, 27.42s/it]

 17%|████████████████████████████▏                                                                                                                                      | 64/370 [48:03<2:14:54, 26.45s/it]

 18%|████████████████████████████▋                                                                                                                                      | 65/370 [48:27<2:10:18, 25.63s/it]

 18%|█████████████████████████████                                                                                                                                      | 66/370 [48:51<2:08:01, 25.27s/it]

 18%|█████████████████████████████▌                                                                                                                                     | 67/370 [49:15<2:05:59, 24.95s/it]

 18%|█████████████████████████████▉                                                                                                                                     | 68/370 [49:39<2:03:54, 24.62s/it]

 19%|██████████████████████████████▍                                                                                                                                    | 69/370 [50:03<2:02:13, 24.36s/it]

 19%|██████████████████████████████▊                                                                                                                                    | 70/370 [50:27<2:01:52, 24.37s/it]


 19%|███████████████████████████████▋                                                                                                                                   | 72/370 [51:16<2:01:05, 24.38s/it]
{'loss': 2.1484, 'learning_rate': 0.0002785971942114498, 'epoch': 1.95}

 20%|████████████████████████████████▏                                                                                                                                  | 73/370 [51:41<2:00:57, 24.43s/it]

 20%|████████████████████████████████▌                                                                                                                                  | 74/370 [52:05<2:00:06, 24.35s/it]

 20%|█████████████████████████████████                                                                                                                                  | 75/370 [52:29<1:59:43, 24.35s/it]

 21%|█████████████████████████████████▍                                                                                                                                 | 76/370 [52:53<1:58:59, 24.28s/it]














 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:51<00:03,  3.97s/it]

{'eval_loss': 2.0952436923980713, 'eval_runtime': 58.6974, 'eval_samples_per_second': 4.344, 'eval_steps_per_second': 0.273, 'epoch': 2.05}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 21%|█████████████████████████████████▉                                                                                                                                 | 77/370 [54:16<3:24:42, 41.92s/it]
{'loss': 2.035, 'learning_rate': 0.0002751657185570771, 'epoch': 2.08}

 21%|██████████████████████████████████▎                                                                                                                                | 78/370 [54:40<2:57:23, 36.45s/it]

 21%|██████████████████████████████████▊                                                                                                                                | 79/370 [55:04<2:38:31, 32.69s/it]

 22%|███████████████████████████████████▏                                                                                                                               | 80/370 [55:28<2:25:07, 30.03s/it]


 22%|████████████████████████████████████                                                                                                                               | 82/370 [56:16<2:10:19, 27.15s/it]
{'loss': 2.0365, 'learning_rate': 0.0002715037294282162, 'epoch': 2.22}


 23%|█████████████████████████████████████                                                                                                                              | 84/370 [57:05<2:01:53, 25.57s/it]
{'loss': 1.8961, 'learning_rate': 0.0002699758867109579, 'epoch': 2.27}

 23%|█████████████████████████████████████▍                                                                                                                             | 85/370 [57:29<1:59:32, 25.17s/it]

 23%|█████████████████████████████████████▉                                                                                                                             | 86/370 [57:52<1:56:23, 24.59s/it]

 24%|██████████████████████████████████████▎                                                                                                                            | 87/370 [58:16<1:55:45, 24.54s/it]

 24%|██████████████████████████████████████▊                                                                                                                            | 88/370 [58:42<1:56:08, 24.71s/it]

 24%|███████████████████████████████████████▏                                                                                                                           | 89/370 [59:05<1:54:24, 24.43s/it]

 24%|███████████████████████████████████████▋                                                                                                                           | 90/370 [59:29<1:53:15, 24.27s/it]

 25%|████████████████████████████████████████                                                                                                                           | 91/370 [59:53<1:51:32, 23.99s/it]

 25%|████████████████████████████████████████                                                                                                                         | 92/370 [1:00:17<1:51:30, 24.07s/it]

 25%|████████████████████████████████████████▍                                                                                                                        | 93/370 [1:00:41<1:51:10, 24.08s/it]

 25%|████████████████████████████████████████▉                                                                                                                        | 94/370 [1:01:05<1:51:14, 24.18s/it]

 26%|█████████████████████████████████████████▎                                                                                                                       | 95/370 [1:01:29<1:50:24, 24.09s/it]














 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:49<00:03,  3.54s/it]

{'eval_loss': 2.104755401611328, 'eval_runtime': 56.3467, 'eval_samples_per_second': 4.526, 'eval_steps_per_second': 0.284, 'epoch': 2.57}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.1364, 'learning_rate': 0.00026008280496482984, 'epoch': 2.59}
 26%|█████████████████████████████████████████▊                                                                                                                       | 96/370 [1:02:49<3:06:08, 40.76s/it]

 26%|██████████████████████████████████████████▏                                                                                                                      | 97/370 [1:03:13<2:42:53, 35.80s/it]

 26%|██████████                            | 98/370 [1:03:37<2:26:38, 32.35s/it]

 27%|██████████▏                           | 99/370 [1:04:03<2:16:22, 30.19s/it]

 27%|██████████                           | 100/370 [1:04:27<2:08:18, 28.51s/it]

 27%|██████████                           | 101/370 [1:04:51<2:02:05, 27.23s/it]

 28%|██████████▏                          | 102/370 [1:05:15<1:57:01, 26.20s/it]

 28%|██████████▎                          | 103/370 [1:05:40<1:54:12, 25.66s/it]

 28%|██████████▍                          | 104/370 [1:06:04<1:51:53, 25.24s/it]

 28%|██████████▌                          | 105/370 [1:06:28<1:50:27, 25.01s/it]

 29%|██████████▌                          | 106/370 [1:06:52<1:48:48, 24.73s/it]

 29%|██████████▋                          | 107/370 [1:07:17<1:48:53, 24.84s/it]

 29%|██████████▊                          | 108/370 [1:07:42<1:48:03, 24.75s/it]

 29%|██████████▉                          | 109/370 [1:08:06<1:46:53, 24.57s/it]

 30%|███████████                          | 110/370 [1:08:30<1:45:41, 24.39s/it]

 30%|███████████                          | 111/370 [1:08:55<1:45:34, 24.46s/it]

 30%|███████████▏                         | 112/370 [1:09:19<1:45:08, 24.45s/it]

 31%|███████████▎                         | 113/370 [1:09:44<1:44:34, 24.41s/it]

 31%|███████████▍                         | 114/370 [1:10:08<1:44:09, 24.41s/it]














 94%|████████████████████████████████████████▎  | 15/16 [00:49<00:03,  3.60s/it]


 31%|███████████▍                         | 114/370 [1:11:05<1:44:09, 24.41s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-114)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.007, 'learning_rate': 0.00024207842127339178, 'epoch': 3.11}

 31%|███████████▌                         | 116/370 [1:11:56<2:35:36, 36.76s/it]
{'loss': 2.0509, 'learning_rate': 0.00024105863616436505, 'epoch': 3.14}

 32%|███████████▋                         | 117/370 [1:12:20<2:19:50, 33.17s/it]

 32%|███████████▊                         | 118/370 [1:12:44<2:07:27, 30.35s/it]

 32%|███████████▉                         | 119/370 [1:13:08<1:59:19, 28.52s/it]

 32%|████████████                         | 120/370 [1:13:32<1:52:55, 27.10s/it]

 33%|████████████                         | 121/370 [1:13:56<1:48:20, 26.10s/it]

 33%|████████████▏                        | 122/370 [1:14:20<1:45:38, 25.56s/it]

 33%|████████████▎                        | 123/370 [1:14:45<1:43:58, 25.26s/it]

 34%|████████████▍                        | 124/370 [1:15:09<1:42:00, 24.88s/it]

 34%|████████████▌                        | 125/370 [1:15:33<1:40:43, 24.67s/it]

 34%|████████████▌                        | 126/370 [1:15:57<1:39:24, 24.44s/it]

 34%|████████████▋                        | 127/370 [1:16:21<1:39:10, 24.49s/it]

 35%|████████████▊                        | 128/370 [1:16:46<1:39:00, 24.55s/it]

 35%|████████████▉                        | 129/370 [1:17:10<1:38:17, 24.47s/it]

 35%|█████████████                        | 130/370 [1:17:35<1:37:44, 24.44s/it]

 35%|█████████████                        | 131/370 [1:17:59<1:37:05, 24.37s/it]

 36%|█████████████▏                       | 132/370 [1:18:23<1:36:29, 24.33s/it]

 36%|█████████████▎                       | 133/370 [1:18:48<1:36:14, 24.36s/it]














 94%|████████████████████████████████████████▎  | 15/16 [00:54<00:03,  3.65s/it]

{'eval_loss': 2.1234943866729736, 'eval_runtime': 60.9375, 'eval_samples_per_second': 4.185, 'eval_steps_per_second': 0.263, 'epoch': 3.59}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0978, 'learning_rate': 0.00022163038296562322, 'epoch': 3.62}
 36%|█████████████▍                       | 134/370 [1:20:13<2:48:12, 42.76s/it]

 36%|█████████████▌                       | 135/370 [1:20:37<2:25:32, 37.16s/it]

 37%|█████████████▌                       | 136/370 [1:21:02<2:09:57, 33.32s/it]

 37%|█████████████▋                       | 137/370 [1:21:26<1:58:34, 30.53s/it]

 37%|█████████████▊                       | 138/370 [1:21:51<1:51:37, 28.87s/it]

 38%|█████████████▉                       | 139/370 [1:22:15<1:46:11, 27.58s/it]

 38%|██████████████                       | 140/370 [1:22:39<1:40:55, 26.33s/it]

 38%|██████████████                       | 141/370 [1:23:03<1:38:02, 25.69s/it]

 38%|██████████████▏                      | 142/370 [1:23:27<1:35:53, 25.24s/it]

 39%|██████████████▎                      | 143/370 [1:23:51<1:33:49, 24.80s/it]

 39%|██████████████▍                      | 144/370 [1:24:16<1:33:10, 24.74s/it]

 39%|██████████████▌                      | 145/370 [1:24:39<1:31:08, 24.30s/it]

 39%|██████████████▌                      | 146/370 [1:25:03<1:30:45, 24.31s/it]

 40%|██████████████▋                      | 147/370 [1:25:27<1:30:13, 24.28s/it]

 40%|██████████████▊                      | 148/370 [1:25:52<1:29:59, 24.32s/it]


 41%|███████████████                      | 150/370 [1:26:41<1:29:19, 24.36s/it]
{'loss': 1.9383, 'learning_rate': 0.00020291281298236423, 'epoch': 4.05}

 41%|███████████████                      | 151/370 [1:27:05<1:29:01, 24.39s/it]

 41%|███████████████▏                     | 152/370 [1:27:29<1:28:10, 24.27s/it]














 94%|████████████████████████████████████████▎  | 15/16 [00:49<00:03,  3.54s/it]

{'eval_loss': 2.132683038711548, 'eval_runtime': 56.3161, 'eval_samples_per_second': 4.528, 'eval_steps_per_second': 0.284, 'epoch': 4.11}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9619, 'learning_rate': 0.00019928135728662522, 'epoch': 4.14}
 41%|███████████████▎                     | 153/370 [1:28:50<2:29:06, 41.23s/it]

 42%|███████████████▍                     | 154/370 [1:29:15<2:10:56, 36.37s/it]

 42%|███████████████▌                     | 155/370 [1:29:39<1:57:33, 32.81s/it]

 42%|███████████████▌                     | 156/370 [1:30:04<1:48:02, 30.29s/it]

 42%|███████████████▋                     | 157/370 [1:30:28<1:40:39, 28.35s/it]

 43%|███████████████▊                     | 158/370 [1:30:51<1:35:16, 26.96s/it]

 43%|███████████████▉                     | 159/370 [1:31:15<1:31:37, 26.05s/it]

 43%|████████████████                     | 160/370 [1:31:40<1:29:33, 25.59s/it]

 44%|████████████████                     | 161/370 [1:32:04<1:27:53, 25.23s/it]

 44%|████████████████▏                    | 162/370 [1:32:28<1:26:25, 24.93s/it]

 44%|████████████████▎                    | 163/370 [1:32:52<1:24:53, 24.60s/it]

 44%|████████████████▍                    | 164/370 [1:33:16<1:24:00, 24.47s/it]

 45%|████████████████▌                    | 165/370 [1:33:40<1:22:57, 24.28s/it]

 45%|████████████████▌                    | 166/370 [1:34:04<1:22:23, 24.23s/it]

 45%|████████████████▋                    | 167/370 [1:34:29<1:22:20, 24.34s/it]

 45%|████████████████████████████████████████████████████████████████████████▋                                                                                       | 168/370 [1:34:53<1:21:33, 24.23s/it]

 46%|█████████████████████████████████████████████████████████████████████████                                                                                       | 169/370 [1:35:17<1:21:04, 24.20s/it]

 46%|█████████████████████████████████████████████████████████████████████████▌                                                                                      | 170/370 [1:35:41<1:20:53, 24.27s/it]

 46%|█████████████████████████████████████████████████████████████████████████▉                                                                                      | 171/370 [1:36:05<1:19:29, 23.97s/it]















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:51<00:03,  3.76s/it]

 46%|█████████████████████████████████████████████████████████████████████████▉                                                                                      | 171/370 [1:37:03<1:19:29, 23.97s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-171)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9377, 'learning_rate': 0.00017562446148380162, 'epoch': 4.65}
 46%|██████████████████████████████████████████████████████████████████████████▍                                                                                     | 172/370 [1:37:28<2:17:41, 41.73s/it]

 47%|██████████████████████████████████████████████████████████████████████████▊                                                                                     | 173/370 [1:37:52<1:59:47, 36.49s/it]

 47%|███████████████████████████████████████████████████████████████████████████▏                                                                                    | 174/370 [1:38:16<1:47:09, 32.80s/it]

 47%|███████████████████████████████████████████████████████████████████████████▋                                                                                    | 175/370 [1:38:41<1:38:29, 30.30s/it]

 48%|████████████████████████████████████████████████████████████████████████████                                                                                    | 176/370 [1:39:05<1:31:53, 28.42s/it]

 48%|████████████████████████████████████████████████████████████████████████████▌                                                                                   | 177/370 [1:39:28<1:26:36, 26.93s/it]

 48%|████████████████████████████████████████████████████████████████████████████▉                                                                                   | 178/370 [1:39:53<1:23:40, 26.15s/it]


 49%|█████████████████████████████████████████████████████████████████████████████▊                                                                                  | 180/370 [1:40:42<1:20:07, 25.30s/it]
{'loss': 1.8452, 'learning_rate': 0.00016542314957060405, 'epoch': 4.86}

 49%|██████████████████████████████████████████████████████████████████████████████▎                                                                                 | 181/370 [1:41:06<1:18:45, 25.00s/it]

 49%|██████████████████████████████████████████████████████████████████████████████▋                                                                                 | 182/370 [1:41:30<1:17:52, 24.85s/it]

 49%|███████████████████████████████████████████████████████████████████████████████▏                                                                                | 183/370 [1:41:55<1:16:45, 24.63s/it]

 50%|███████████████████████████████████████████████████████████████████████████████▌                                                                                | 184/370 [1:42:18<1:15:31, 24.36s/it]

 50%|████████████████████████████████████████████████████████████████████████████████                                                                                | 185/370 [1:42:42<1:14:53, 24.29s/it]

 50%|████████████████████████████████████████████████████████████████████████████████▍                                                                               | 186/370 [1:43:07<1:14:39, 24.35s/it]

 51%|████████████████████████████████████████████████████████████████████████████████▊                                                                               | 187/370 [1:43:31<1:13:43, 24.17s/it]

 51%|█████████████████████████████████████████████████████████████████████████████████▎                                                                              | 188/370 [1:43:55<1:13:21, 24.18s/it]

 51%|█████████████████████████████████████████████████████████████████████████████████▋                                                                              | 189/370 [1:44:20<1:13:30, 24.37s/it]

 51%|██████████████████████████████████████████████████████████████████████████████████▏                                                                             | 190/370 [1:44:44<1:13:32, 24.51s/it]















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:50<00:03,  3.55s/it]

 51%|██████████████████████████████████████████████████████████████████████████████████▏                                                                             | 190/370 [1:45:42<1:13:32, 24.51s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-190)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8454, 'learning_rate': 0.00015128752216231304, 'epoch': 5.16}
 52%|██████████████████████████████████████████████████████████████████████████████████▌                                                                             | 191/370 [1:46:07<2:04:51, 41.85s/it]

 52%|███████████████████████████████████████████████████████████████████████████████████                                                                             | 192/370 [1:46:31<1:48:09, 36.46s/it]

 52%|███████████████████████████████████████████████████████████████████████████████████▍                                                                            | 193/370 [1:46:54<1:36:18, 32.65s/it]

 52%|███████████████████████████████████████████████████████████████████████████████████▉                                                                            | 194/370 [1:47:17<1:27:16, 29.75s/it]

 53%|████████████████████████████████████████████████████████████████████████████████████▎                                                                           | 195/370 [1:47:42<1:21:59, 28.11s/it]

 53%|████████████████████████████████████████████████████████████████████████████████████▊                                                                           | 196/370 [1:48:05<1:17:22, 26.68s/it]

 53%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 197/370 [1:48:29<1:14:47, 25.94s/it]

 54%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                          | 198/370 [1:48:54<1:13:06, 25.50s/it]

 54%|██████████████████████████████████████████████████████████████████████████████████████                                                                          | 199/370 [1:49:18<1:11:54, 25.23s/it]

 54%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                         | 200/370 [1:49:43<1:11:03, 25.08s/it]

 54%|██████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 201/370 [1:50:08<1:10:08, 24.90s/it]

 55%|███████████████████████████████████████████████████████████████████████████████████████▎                                                                        | 202/370 [1:50:32<1:09:24, 24.79s/it]


 55%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                       | 204/370 [1:51:20<1:07:39, 24.46s/it]
{'loss': 1.9053, 'learning_rate': 0.00013457685042939592, 'epoch': 5.51}

 55%|████████████████████████████████████████████████████████████████████████████████████████▋                                                                       | 205/370 [1:51:45<1:07:04, 24.39s/it]

 56%|█████████████████████████████████████████████████████████████████████████████████████████                                                                       | 206/370 [1:52:09<1:07:03, 24.54s/it]

 56%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 207/370 [1:52:36<1:07:59, 25.02s/it]

 56%|█████████████████████████████████████████████████████████████████████████████████████████▉                                                                      | 208/370 [1:53:00<1:06:56, 24.80s/it]

 56%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 209/370 [1:53:24<1:06:04, 24.63s/it]















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:49<00:03,  3.54s/it]

 56%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                     | 209/370 [1:54:20<1:06:04, 24.63s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Networking_and_APIs-bs_16-lr_0.0003-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-209)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8389, 'learning_rate': 0.0001269164134943907, 'epoch': 5.68}
 57%|██████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 210/370 [1:54:46<1:51:46, 41.92s/it]

 57%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 211/370 [1:55:11<1:37:35, 36.83s/it]


 58%|████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 213/370 [1:56:00<1:19:56, 30.55s/it]

 58%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 214/370 [1:56:25<1:14:32, 28.67s/it]
{'loss': 1.8136, 'learning_rate': 0.00012184221518442023, 'epoch': 5.78}

 58%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                   | 215/370 [1:56:49<1:10:47, 27.40s/it]

 58%|█████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 216/370 [1:57:14<1:08:24, 26.65s/it]

 59%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                                  | 217/370 [1:57:39<1:06:59, 26.27s/it]

 59%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                 | 218/370 [1:58:04<1:05:26, 25.83s/it]

 59%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 219/370 [1:58:28<1:03:18, 25.15s/it]

 59%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 220/370 [1:58:55<1:04:46, 25.91s/it]

 60%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                | 221/370 [1:59:19<1:02:51, 25.31s/it]

 60%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                | 222/370 [1:59:44<1:02:09, 25.20s/it]
Aborted!
 60%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                | 222/370 [2:00:08<1:20:05, 32.47s/it]