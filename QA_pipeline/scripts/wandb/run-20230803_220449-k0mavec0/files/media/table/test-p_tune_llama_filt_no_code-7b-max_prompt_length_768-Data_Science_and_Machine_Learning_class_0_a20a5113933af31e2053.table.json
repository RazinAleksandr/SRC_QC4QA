{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I would like to reuse the same Spark application across multiple runs of a python script, that uses it's Spark session object. That is, I would like to have a Spark application running in the background, and access it's Spark session object from within my script. Does anybody know how to do that?", "To the best of my knowledge, it is not possible. It is the security model of Spark to isolate each session to a distinct app.\nWhat I have done in the past:\n\nbuild a small REST server on top of Spark that listens to specific command. At boot time, the server creates the session and load the data, so that forthcoming transformations are fast.\n\ncache data in Delta lake, you still have the boot time and data ingestion, but it\u2019s much faster that accessing data from several sources and preparing the data.\n\n\nIf you describe a bit more your use-case, I may be able to help a little more.", "How to request the spark session object from a background-running Spark application from within a python script?", 0.0, 0, 0, "There are a few ways that you can accomplish this task. Let us explore them now.\n1. One approach would be to use the Apache Spark documentation, which explains how you go about doing it. You can find the documentation here. And let us note that ApacheSpark has detailed documentation and samples as well. So you could go through these resources and find out what you need. To find, just type `apache` in your Spark shell. For example, you are interested in finding out more about the `apply` function. Now, type this in you shell and the function will show you how this function works, as seen in this screenshot below. ...\n3. Yet another approach is to look at the examples as shown below in detail. Example 3: Looking at a more detailed example as illustrated here below ... and look as you see all the details on the link that was mentioned.", 0.2737642526626587, 0.0, 0.12167300283908844, 0.041580040007829666], ["I am using pandas.read_csv() to read a csv file, but characters like the copyright symbol are getting converted to \ufffd\nFor example, in excel I will see this value - \n/ORGANIZATION/AFFLUENT-ATTACH\u00c3\u00a9-CLUB-2\nIn jupyter notebook in turns to this - \n/ORGANIZATION/AFFLUENT-ATTACH\ufffd-CLUB-2 in one dataframe \n/ORGANIZATION/AFFLUENT-ATTACH\u00c9-CLUB-2 in the other\nI need to do an inner join of 2 dataframes, both of which have a column with these unique IDs, but values like these are getting left out.\nI thought it might be something to do with the enconding, so I found that the encoding type is cp1252 for both csv files. I do not know if this information is useful.\nPlease help.", "Try to change the encoding of the file to UTF-8 or UTF-16 while reading.", "How to deal with special characters like \ufffd in python?", 0.0, 0, 1, "To deal special chars, like $ or \u00a2 in c++, please see the blogs below:", 0.2068965584039688, 0.0, 0.13793103396892548, 0.05262327939271927], ["I am using pandas to read in csv data to my python script.\nBoth csv files have the same encoding (Windows-1252).\nHowever with one of the files I get an error when reading the csv file with pandas, unless I specify the encoding parameters in pd.read_csv().\nDoes anyone know why I need to specify the encoding in one csv and not the other? Both csv's contain similar data (strings and numbers).\nThank you", "That just means that one of the files has a character outside the range 0x00 to 0x7F.  It's only the highest 128 values where the encoding makes a difference.  All it takes is one n-with-tilde or one smart quote mark.", "Encoding csv error with Pandas - have to encode one csv file but not the other -both have same encoding", 0.0, 0, 2, "you need encodings that work with the string. this means encoding that the machine can use. so, if your machine is windows, that means you should use windows-33. there are so many string-encoders, and you can just choose any of them. but you cannot choose a-bit or a2-hydrogen or even the best oil that can be used in a gas tank.", 0.25925925374031067, 0.0, 0.12962962687015533, 0.04999999701976776], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Though I don't know the exact notations and equations, the answer has to do with the Big O Notation for the algorithms.\nBig O notation is a way of expressing the theoretical worse time for an algorithm to complete over extremely large data sets. For example, a simple loop that goes over every item in a one dimensional array of size n has a O(n) run time - which is to say that it will always run at the proportional time per size of the array no matter what.\nSay you have a 2 dimensional array of X,Y coords and you are going to loop across every potential combination of x/y locations, where x is size n and y is size m, your Big O would be O(mn)\nand so on. Big O is used to compare the relative speed of different algorithms in abstraction, so that you can try to determine which one is better to use.\nIf you grab O(n) over the different potential sizes of n, you end up with a straight 45 degree line on your graph.\nAs you get into more complex algorithms you can end up with O(n^2) or O(log n) or even more complex. -- generally though most algorithms fall into either O(n), O(n^(some exponent)), O(log n) or O(sqrt(n)) - there are obviously others but generally most fall into this with some form of co-efficient in front or after that modifies where they are on the graph. If you graph each one of those curves you'll see which ones are better for extremely large data sets very quickly\nIt would entirely depend on how well your algorithm is coded, but it might look something like this: (don't trust me on this math, i tried to start doing it and then just googled it.)\nFitting a decision tree of depth \u2018m\u2019:\n\nNa\u00efve analysis: 2m-1 trees -> O(2m-1 n d log(n)).\neach object appearing only once at a given depth: O(m n d log n)\n\nand a Log n graph ... well pretty much doesn't change at all even with sufficiently large numbers of n, does it?\nso it doesn't matter how big your data set is, these algorithms are very efficient in what they do, but also do not scale because of the nature of a log curve on a graph (the worst increase in performance for +1 n is at the very beginning, then it levels off with only extremely minor increases to time with more and more n)", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 3, "Tree structure and other algorithms are insensitive to data variance.", 0.02684563770890236, 0.004494382068514824, 0.022371364757418633, 0.09999999403953552], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Do not confuse trees and ensembles (which may be consist from models, that need to be scaled).\nTrees do not need to scale features, because at each node, the entire set of observations is divided by the value of one of the features: relatively speaking, to the left everything is less than a certain value, and to the right - more. What difference then, what scale is chosen?", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 4, "The generalisation error of tree or ensemble does not increase if the features are scaleted. It's just the prediction and the error that increases. You can read the book or look at wiki. The point is to look for the big features to make the algorithm fast and efficient.", 0.2735042870044708, 0.017391303554177284, 0.18803419172763824, 0.0855855867266655], ["I am at a total loss as to why this is impossible to find but I really just want to be able to groupby and then export to excel. Don't need counts, or sums, or anything else and can only find examples including these functions. Tried removing those functions and the whole code just breaks.\nAnyways:\nHave a set of monthly metrics - metric name, volumes, date, productivity, and fte need. Simple calcs got the data looking nice, good to go. Currently it is grouped in 1 month sections so all metrics from Jan are one after the other etc. Just want to change the grouping so first section is individual metrics from Jan to Dec and so on for each one.\nInitial data I want to export to excel (returns not a dataframe error)\ndfcon = pd.concat([PmDf,ReDf])\ndfcon['Need'] = dfcon['Volumes'] / (dfcon['Productivity']*21*8*.80)\ndfcon[['Date','Current Team','Metric','Productivity','Volumes','Need']]\ndfg = dfcon.groupby(['Metric','Date'])\ndfg.to_excel(r'S:\\FilePATH\\GroupBy.xlsx', sheet_name='pandas_group', index = 0)\nThe error I get here is: 'DataFrameGroupBy' object has no attribute 'to_excel' (I have tried a variety of conversions to dataframes and closest I can get is a correct grouping displaying counts only for each one, which I do not need in the slightest)\nI have also tried:\ndfcon.sort('Metric').to_excel(r'S:\\FILEPATH\\Grouped_Output.xlsx', sheet_name='FTE Need', index = 0)\nthis returns the error: AttributeError: 'DataFrame' object has no attribute 'sort'\nAny help you can give to get this to be able to be exported grouped in excel would be great. I am at my wits end here after over an hour of googling. I am also self taught so feel like I may be missing something very, very basic/simple so here I am!\nThank you for any help you can provide!\nPs: I know I can just sort after in excel but would rather learn how to make this work in python!", "I am pretty sure sort() doesnt work anymore, try sort_values()", "python groupby to dataframe (just groupby to data no additional functions) to export to excel", 1.2000000476837158, 2, 5, "You need to create a DataFrameGroup (DFG) with the same columns (i.e. need). Then you need a group by (g) on the need column. You then need the .to... on g. The .sort... you should be using on a copy of g and not on d. In general you may not pass arguments directly on to an object in Python. There will be a better way of doing this. For more info read https://python.org/3/#3-37-4-1-2-7425-6390-08-89-94", 0.04040404036641121, 0.0, 0.04040404036641121, 0.03772896155714989], ["I have a numpy array with size (1000,6) and I fill part of it each time during my program. I need to find the first location of zero in this array. for this, I used np.where( array==0). but the output is a tuple of size 2 and each item of it is a numpy array and I do not how can I find the first index of occurring zero in this array. what should I do about this?", "The first element of the tuple that you got should be the index you are looking.", "how do I find the index of an specific value in numpy array?", 0.0996679961681366, 1, 6, "For me, np_0.0_5 =0\nTitle: what is the method of finding the value of n in a particular string?", 0.15789473056793213, 0.0, 0.15789473056793213, 0.015377027913928032], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 1.0, 22, 7, "Use the `import_` method on `ker.``opti``.``, you`ll get all the functions!", 0.07407407462596893, 0.0, 0.07407407462596893, 0.07014035433530807], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "I think you are using Keras directly. Instead of giving as from keras.distribute import  \u2014> give as from tensorflow.keras.distribute import \nHope this would help you.. It is working for me.", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.0, 0, 8, "just change the `back_drift` value to `1`. That should solve the issue! Hope! :)\nPosted by Shiv_Chaub_ha_in_h_me_25.5-24 on March 04, 837850, see the rest of this book.", 0.11940298229455948, 0.0, 0.05970149114727974, 0.056338030844926834], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "There are ways to solve your problem as you are using keras 2.6 and tensorflow too:\n\nuse (from keras.optimizer_v2.adam import Adam as Adam) but go through the function documentation once to specify your learning rate and beta values\nyou can also use (Adam = keras.optimizers.Adam).\n(import tensorflow as tf) then (Adam = tf.keras.optimizers.Adam)\n\nUse the form that is useful for the environment you set", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.03997867926955223, 1, 9, "the error that you are getting is that it cannot find the module \"kerasm\" (which you need to install) \"module ...\"\ni have also encountered this and this is what you should do. this may be an inconvenient way of doing it but you could install the \"Keras\" module and you would not be able to use the other module that is installed in the system.", 0.25, 0.04477611929178238, 0.1764705926179886, 0.06545792520046234], ["Is there a way to make partial dependence plots for the random forest with multiple classification in Python (using scikit-learn)?\nI'm raising a separate question about this because I'm not sure if such a function exists in scikit-learn. I've seen a few examples in R already. If the function doesn't exist, I will make the request in scikit-learn github, but just want to double-check with the community before making the request.\nIf you know of any other Python package other than scikit learn that could conduct the plot, please let me know. Thanks.", "You have to specify the class for which you want to plot the partial dependencies. This is done by the parameter \"target\" in the plot_partial_dependence function\nFor example, if you have three target classes \"low\", \"medium\", \"high\", you would say plot_partial_dependence(estimator, ..., target='high').\nHowever, I'm still trying to find some answers regarding the interpretations of partial dependency plots for multi-class-classifiers. If you have some information, let me know.", "Is there a way to make partial dependence plots for random forest with multiple classification in Python (using scikit-learn)?", 0.1973753273487091, 1, 10, "I have a suggestion in relation to a request to know if there is a possible way of making partial independence plots in the multiple random trees with classification in other languages such as R or Python. Thank you for your contribution!\nThank you,\nWer, 00, A.B., 3, E-B.L.,\n4, P.S.P., TB, or something, you\ndaughters or your sons and daughters, and 26, H. 1., B. L., M.A., S.O.F. M., O., or 67, M\nyou have said in your\nMom's\nH.1. H, K.U.K., K\nKK.H.-K-H-\nAH1-K\nHA-HA1\n2, G. T., G, T.T., NN., NS\nNS, NR\nN-N\nNN-NO\nP-P\np\nq 90q\n9, B-Q., C\nC.0. Q., R.Q. R, Q.R. O. B., L. E., P-S-I. P., SP, SPPP, DPPPPP.D.PP., D. S., E.I., J., and J.J. JJJ\n54055334630\n3-3\n03 FP 7 FPP 445 5\n# \n  # \u00c2 \u00c3\u00e2\u00c2\u00c2\u00c3\u00c2\u00c4\u00c4\u00c2\u00cb\u00c4\u00cb\u00cb\u00c9\u00ca\u00c4\u00c3\u00c4\u00c5\u00c3\u00c5\u00c4WW\u00c2W22WWD2WDWWEW\n\u00c21, WTTWTW20123C2C\n1\u00c2\n. . .\nL. U. V. VS. VIVS.VVS\nVIVS\n76B\nB-BOB BOBOBBOBBBUBB0BBSUSSSSYSSAY\nSAVE SAVING SAVERS SASSER\n\u00c3\u00c3\noOO0O\n[0] O-C\u00c2C-CCCCCCCCA\u00c2AA\u00c2A\u00c22\u00c313B774B14CBC1B\u00c36C\u00c3B(C11)\u00c2B9\u00c33BBBB\u00c3BB \u00b39", 0.1362007111310959, 0.014440433122217655, 0.08602150529623032, 0.046038996428251266], ["how to get back 10 from df.iloc[10] where df = pd.DataFrame({'a':np.arange(1,12)})?\nI tried df.index but it returns a weird np.array which doesn't contain anything close to 10.", "The most simple solution if the index matches the row numbers is df.iloc[10].name which returns 10", "Get index of DataFrame row", 0.0, 0, 11, "You can find the index by:\n\\begin{code}\ndf = data.frame(a=np2.5, b=05.8)\n# then\nDF = df[df[0]] # or df[[0]], it depends. You should try it out. Then you can use the .index function to find index 4 (or 5 depending) of df.[DF.i2] or\nD[DF.[D.4]].i1. However, I did it like this: df [[4]]).i3\nor df .[[5]])[5][4][3]\nAnd you'll get 268.213, or 319.358, and it's a probability of a number being 9 or a 6. So it won'\n.df[[1]][1][2].or.DF[[3][\nThe function of index .i. will provide you the number of the next row in the Data Frame. For example, df\nSo that'sswhat hesecond function does!\nIt'shwhhwhhywewer. Ithttwhttwhywerwerwwhwhhhhh\nIf you want to know how many people like you, then you should type. There are more than 7,00,2,55385 people who like that and you are one of them! So there are many more people! They will find you and, at that time, they will ask you for the secret. And they are asking for it because they want the answers. They want how can you make so much money! How can your blogs make you so many money?! And, that is a secret! That'why they keep asking me for secrets! If they really need the secres then they should read it! But if they just want...they just don'thhhehththhtthtttthttpotototsotoottotsotottoo!tttt! Then they can make more money if you tell them the way you do it. Because you also need money to make money. It'supepep\nYou should also say that it takes some time to write a blog because you have to be creative! Creative, you! You are not supposed to say you just write! When you write your name, people will see that you say \"I write.\" But when you", 0.05309734493494034, 0.005934718064963818, 0.04129793494939804, 0.0], ["I occasionally get the above error when making requests with the python requests library to Qualtrics APIs.\nIn a nutshell, I have a Google Cloud Function on Google Cloud that will trigger when a csv file is placed on a specific Cloud Storage Bucket. The function will create a Qualtrics distribution list on Qualtrics, upload the contacts and then download the distribution links.\nEvery day, three files are uploaded on Cloud Storage, each for a different survey, and so three Google Cloud instances will be started.\nMy gripes with the issues are:\n\nit doesn't happen regularly, in fact the workflow correctly worked for the past year\nit doesn't seem to be tied to the processed files: when the function crashes and I manually restart it by reuploading the same csv into the bucket, it will work smoothly\n\nThe problem started around when we added the third daily csv to the process, and tend to happen when two files are being processed at the same time. For these reasons my suspects are:\n\nI'm getting rate limited by Qualtrics (but I would expect a more clear message from Qualtrics)\nThe requests get in some way \"crossed up\" when two files are processed. I'm not sure if requests.request implicitly opens a session with the APIs. In that case the problem could be generated by multiple sessions being open at the same time from two csv being processed at the same time\n\nAs I said, the error seem to happen without a pattern, and it has happened on any part of the code where I'm doing a request to the APIs, so I'm not sure if sharing extensive code is helpful, but in general the requests are performed in a pretty standard way:\nrequests.request(\"POST\", requestUrl, data=requestPayload, headers=headers)\nrequests.request(\"GET\", requestUrl, headers=headers)\netc\ni.e.: I'm not using any particular custom option", "In the end I kind of resolved the issue with a workaround:\n\nI separated the processing of the three csv so that there is no overlap in processing time between two files\nimplemented a retry policy in the POST request\n\nSince then, separating processing time for the files reduced substantially the number of errors (from one or more each day to around 1 error a week), and even when they happen the retry policy circumvents the error at the first retry.\nI realize this may not be the ideal solution, so I'm open to alternatives if someone comes up with something better (or even more insights on the root problem).", "Qualtrics API, getting \"[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2570)\"", 0.1973753273487091, 1, 12, "in the 0,179-C2-7,91-d4-s-p-160-68-e-c-w-l-OO)1003-LV-04L\nand the first year I was born\n486 3 2442. 984 6- and 5 \u00c3- 180 \u00c5\u00c2 45-4O.0\u00c2C-3.89.I.58.261.3995.67)\u00c3\u00c4-AQ\u00c2\u00c2938 and that was one 83149\u00c34\u00c2T\u00c4D669 \u00c4W\u00e2\u00c4V1\u00c2\u00c3TG\u00c4\u00c4KOVA \u00c265WW\u00c35V\u00e2\u00c2.\u00c3A2\u00c3H-H\u00c2-W-D-KR0sL-I\u00c4 \ufffd0C\u00c334\u00e2374DQ.MH.B0L\u00c4\u00c2N36_EOH_0.S6OA.1K3D)\u00c46.H0P\u00c3W.A] \u00d0.N2)WO,B4,3\nVVT7\nW/8, 7-ch\u00c3\u00c2\u00e2O\u00c2D\n8LGD and the last\nDort BO\n013\u00c2\u00c4B2O3F159\u00c2B3)\u00e21\n30\u00e206\u00c2L88A\u00c397WVCCAOV\nM90O02C#4V6321P0\u00c51F,\u00c22050K,07M4\u00c3DWOW0UO8\n23A, PV43+\u00c3NOR-G\u00c2M22\u00e29\n52V\u00c40\u00c3U\u00c23]\nA12/64+AAGPA46D\u00c43W\u00c2\ufffd\u00c2\n1941D \u00adCALOBER\n92962_N_6L_TODD_\u00c24A3_M\u00c4O-S-PA-F-2\u00c2S2HO1A0\u00c41\u00c52PAO),7.P4M, M\nS75\n\u00e2VL\u00c5\u00c5M", 0.1072796955704689, 0.0154440151527524, 0.07662834972143173, 0.01691729761660099], ["I just have a question on CNN which is should the model take all inputs used in training to predict new sample? what if i want to build a system for a hospital that predicts the disease from image and some features such as age and height but the user doesn\u2019t need to enter the features in case they are not available, so he can input the image only. Is that possible to do that in CNN? because as I know all input used for training should be entered for testing and predicting new data", "As I understood as per your description you want to predict the age and height from the image of patience. In that case for training, you need proper data and model. In model training, you have to specify X_train, Y_train at least. From there, the model will learn.\nX_train - provided image of a person\nY_train(label) -  the characteristic you want to provide (height & age)\nFor predicting purposes you have to modify the input image the same as you did before for X_train. then if you feed it into a trained model it will give you the prediction of height & age.", "Should the CNN take all the inputs used for training to predict new samples?", 0.0, 0, 13, "Yes, it is possible. As you noted, the data must be available (in this case, image). You can use a pre-trained model for this and just use the images and do not use any other features. This is how Deep Net is working. The network is learning itself how to identify the objects on the pictures. That means that you can just upload the photos, and the AI will identify all objects.\nSame thing with a video. When you watch the video, Ai will learn how the persons are moving. In this way, you do NOT have to provide any data or features for AIs to work. You just need a photo or video to see how AIA can recognize the object. And this is another advantage of a DeepNet - you have no idea how it works and you don't have any responsibility for that.", 0.3650793731212616, 0.0560000017285347, 0.1825396865606308, 0.04999566823244095], ["I've got 17,000 CSV files, each ordered by timestamp (some with missing data). The total CSV files are around 85GB, which is much larger than my 32GB RAM.\nI'm trying to figure out the best way to get these into a time-aligned, out-of-memory data structure, such that I can compute things like PCA.\nWhat's the right approach?\n(I've tried to set up an xarray.DataSet, with dim=(filename, time), and then I'm trying to xr.merge() on each CSV file into the DataSet, but it gets slower with every insert, and I expect it will crash when RAM runs out.)", "Have you tried dd.read_csv(...).\nDask reads CSVs in a lazily and can perform certain operations in a streaming manner, so you can run an analysis on a larger than memory dataset.\nMake sure that Dask is able to properly set divisions when you read in your data.  Once the data is read, check dd.divisions and make sure they're values.\nYou can also use a Dask cluster to access more memory of course.\nThose files are really small and Dask typically works best with partitions that are around 100MB.  You might want to compact your data a bit.", "What's the best way to handle large timeseries in dask / xarray?", 0.0, 0, 14, "As you say, this isn't the most efficient way of doing things. However, it'll work for the purposes of which you were trying. I use the package `pandas`, which does its best job at manipulating CSVs. In the case of the CSV, there will be ``,` and the number will appear as one of them. If, for example, you had 50, 06, etc. as the month of a year, then you'd have the time in a minute (01, ...) or the months in one month (1). So, if you have several months and several years in your CSV data, just take out some of that information, or you can merge the data. (I hope it\u2019s been useful.)", 0.22727273404598236, 0.0458715595304966, 0.13636364042758942, 0.027143534272909164], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "The more complicated your make your polynomial (e.g. 3rd order), the more constraints you need on your boundary conditions (e.g. C2). If you try to fit data to a cubic spline with only C0 conditions, then the problem is under-determined. You might as well fit with a line in that case. Use piecewise linear fit.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 15, "You need to calculate the coefficients and then pass them on to `scipy` to build the curve. I believe that can be done with a single line in `python`:\n`scipp``s`l`bb`ss`ttt`b`ff`fb`bf`f`ft`fc`cc`c`d`de`db`a`aaa`.`.`.``.\nAs you can see, I\u2019ve only done two lines and they\u2019re very simple to follow. They\u2019ll help you save lots of time and money.", 0.26865673065185547, 0.04545454680919647, 0.1492537260055542, 0.03787878528237343], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "Not out of the box, no.\nWith fixed breakpoints, it's just a linear least squares problem (with a continuity constraint), which you'll need to solve yourself.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 16, "HP, ID, P,14-C-00-2-c-s-3-d-w-h-g-47-v-p,59-F-A,8-6, 79,\n98,33,43\nA.12, A.23. A.,34.D.8.S.H.O, HOA LY.30 D. F 385. C. H.P. is 22. N, or a similar number of 136.755 and.9.644,93:7,77:540,0:6:4;06 9 4: 84 and 0; 5, r, a,c, c, m, s, q, t, i;\nDur-Wy-MH-H;-960W-P;1-t-k;t and-the\ndifferences-5-cm-and-1.42\nMot-G-T-r.-18176 and a lot of people had a large, old-large-in-a-christ-on-fift-beautifly-tricath-y;h_o, y1167.51 and more. I was, (was) I\na. a.09462? I got that thing on the shoo, on \ufffd4\ufffd0\u00c2FCC,\ufffdF.\u00c4\u00c4.\u00c2\u00c4, FD\u00c3D\u00c4\u00c2.C.s10s.\ufffdKr\u00c41\u00c290.h419 s0b.c566 s28839 or.l\u00c3\u00c3\u00c5.L\u00c30\u00c31\u00c4O-N\u00c4WOOR\u00c4CH.WAS.MA.OR.K04.-31.-BHF3245KR52W.CA.STOWOR1S\u00c4H\u00c486\u00c24DW\u00c49C\u00c4SMOWREWAN7WISH35W0\nw\u00c4\n3B/3O3 \u00c3B 63D) (F)37)4V\n07 and HF1\n71 or", 0.045045044273138046, 0.0, 0.036036036908626556, 0.014925369992852211], ["I have 2 variables in a pandas dataframe which are being used in a calculation (Var1 / Var2) and the values consist of both floating point values and missing values (which I chose to coerce to 0). In my end calculation I am receiving 'inf' values and NA values. The NA values are expected but how do I derive a useable number instead of the 'inf' values?\nsome 'inf' values are appearing when VAR1 = float and Var2 = 0, others appear when both VAR1 and VAR2 are floats.\nMy initial approach was to round the floats to 2 significant figures before the calculation but I still received the inf values.", "You may be getting inf because you are dividing by zero. For example, if var1 = 5 and var2 = 0, then you are computing 5 / 0.\nIn pure Python this returns a ZeroDivisionError, but in lots of data libraries they avoid throwing this error because it would crash your code. Instead, they output inf, or \"infinity\".\nWhen var1 and var2 are both floats, it may be that var2 is extremely small. This would result in var1 / var2 being extremely large. At a certain point, Python doesn't let numbers get any larger and simply represents them as inf.\nRounding wouldn't help, because if var2 = 0, then it would round to 0, and if var2 is very small, it would also round to 0. As discussed earlier, dividing by zero causes the inf.", "Unwanted 'Inf' values in calculated measures", 0.1973753273487091, 1, 17, "Question: Answer: Duration (Question): Time (Q): Hour 52 and 16 Hours (Quarter H...Q: 3 Months 4 HQQ.)\n14 \u00c2\u00c4\u00c4\u00c5\u00c4H\u00c4\u00c2...\u00c2Fast Cash (3-QTs): I don't like to see you doing things. If it happens to me, I will have a go at it, but not the other way. Do you like this site or do you not like it? \u00c3\u00c3......HQ Q: TQ, 6Qs, and a 7Q...\n3Q SQ (6): Yes, Q, is the best way to handle the problem. It will be a little way for you to know the question. There is no answer to the answer so that is what you know. Q...SQS, S, SSQ\u00c2SS: Yes; 9,055; Q\nQSS,S\n,ss\n\u00c3\u00c4\n\u00c2!: A Difference Between: Other QS: Qs: SS, SA, SO?!...QA,Qa,a...2 8 \ufffd8 QQ\ufffdQ34:2,40?!?Q42:3,\ufffd25?\ufffd?S...SSQWW\ufffd17?53:5,2011;3;18;7;6\ufffd38?...35:6,33?Today is Sunday and Monday, Monday (4), Sunday (October nd). QUQUSS QSS.......1\nRom\u00c3O\u00c2\ufffd4QRQ_7\ufffd0:77_464_3\n00QP_07\nOppOQC1_15_26_6?243_57.296.Q......WQ.....Q\n.........WoWood\nWOW\nSSW\u00c2\nVSS\n219\n6\n56;\n...44?....S?.....................SsQOA\ufffdS1\ufffdA...Ev2?74\ufffd6S ...................................?..S\ufffdH?.....\ufffd.54-5 .............836 \ufffd\ufffd\n7 Q1 \n\ufffd\ufffd\u00c3\u00ad6684H", 0.16959063708782196, 0.0, 0.08187134563922882, 0.03220339119434357], ["i'm looking for a framework that is able to solve the following Data Science issue:\nI have several Teachers that can work for X amount of hours a week and have several subject that they can teach.\n\nTeacher 1: Math\nTeacher 2: Math + English\nTeacher 3: Sports + Art + English\nTeacher 4: Math + Art\nTeacher 5: Sports + Math + English\n\nIn a school, every subject needs a specific amount of hours per week. Some more than others\n\nMath: 12 hours\nEnglish: 8 hours\nArt: 4 hours\nSport: 2 hours\n\nLets say one teacher can do 2-3 hours just so you get my point^^\nThe Solution im looking for is a Framework or Algoritm that is filled (trained) with the data and then is able to distribute the teachers so all the subjects are capped or at least as close as possible. That means maybe Teacher 2 needs to teach only Math and Teacher 5 needs to teach 50% Sport and 50% English or 30% Math / 40% Sport / 30% English.\nSomeone mentioned Prolog but im not sure if it can handle this kind of problem? Maybe im wrong?\nIs there something that is fitting for my problem or am i destined to code that algorithm from scratch on my own?\nThanks in advance.", "The first step seems to be to translate a research problem (or a series of problem statements) into a precise form. Problem Characterization/Problem Conceptualization seems to be a technique for resolving that issue. Once the approach has been conceptualised, a technique must be identified for each of the sub-models and submodules.\nBreaking down the high problem statement into smaller problems is called problem conceptualizing. For every subproblem, a technique must be identified, and the methodology must be determined by the assumptions that have been stated previously.\nRealization of a Solution: Determines whether the assumptions are reasonable or whether the solutions meet his needs.\nThis can be compared to a flowchart that he has been creating with these subproblems, and also in general, it is attempting to reach a granularity level where he would determine the issue class. As a result, these issues can be classified as being either function optimization or categorization issues.", "LF Framework to solve Data Science Issue", 0.0996679961681366, 1, 18, "No\nWhat do you think of ProLog? ProLOG is basically\nwhat it does and it has some limitations that i have mentioned\nso what do think??\nProlog is basic\nSo that\u2019s the\nproblem that needs\nsolved and the question that\nyou need to find the solution for\nas i know this is not a simple\none but it is very complex so\ni need your help as i cannot\nget this one on\nmy own!!\nHello 002! Thanks for your\nanswer! The answer\nis absolutely correct and\nit is definitely very\nclose to the truth!\nIt is absolutely\ncorrect! Prologic\nthat is what it\ndoes and I have some\nlimitations that I\u2019ve\nmentioned so what\ndo you say about ProLang?!?!!?!!!\n(2) Hello 931! I am not\ntoo sure myself but\naccording to what you have\nsaid I believe it! But I do\nnot know anything about\nprolog! Thank you very much for the answer! You\u2019re\nabsolutely right and this\nquestion was asked to me yesterday as I\ndid not even know of\nthis topic and of this question! So,\nthank you so much! And\nhow do we know that this was\na question from yesterday? I think\nhe must have been the one\nwho asked you the qu\ne\ntion yesterday! Because\nwe have all been\nbusy all weekend! \ud83d\ude05\nOk, I just\nhad a small\nparty so I really\ngot on your ques\nt! Ok, then\nhoo! How do I know? Because when I was working on this, a few days back, there were\nno comments! No comments\nand then suddenly\neverybody comments on the post! What\nare you saying? You say you don\u2019t know\nabout this subject\nat all and that you do not know much about this? Well, now you know! Well it seems like\nour new and great team! It seems\nlike we are all great! Why do people\ncomment all of a sudden? People\nhave been commenting on all sorts of topics on every other site, and suddenly, you all\nstarted comment on here! That\u2019\nthe real team spirit!! \ufffd\n\ufffd\nWhy are you telling\nme about prolog\nwhen I don'", 0.1953125, 0.01568627543747425, 0.1015625, 0.044855110347270966], ["I'm structuring a monitoring system for a photovoltaic plant with pvlib. As the modules are bifacial and are mounted on a solar tracker (2p), I am using pvfactors. I believe I have already resolved the dependencies: pvfactors 1.5.1, pvlib 0.7.0, and shapely reinstalled via conda.\nAs the modules do not have parameters for the Sandia model, I intend to use the de Soto model.\nI plan to run the code automatically once a day with the weather data collected during the period.\nI would like to know if anyone has any code developed with pvfactors and single diode models for the modules.\nSure of your attention, thank you in advance!\nBen Possatto", "You can model a single-axis tracked bifacial system using pvlib.tracking.SingleAxisTracker (inherits from a PVSystem instance) to calculate surface_tilt and surface_azimuth, then pass those results to pvfactors_timeseries to get the front and rear irradiance.", "How model tracked bifacial PV modules with python?", 0.0, 0, 19, "Ben Posatta 28.34956,29,35,127,9077669\n500872,6374,45486\n3842364.60\n2 p-32 pp3\nw059302pp3394311\n4462016.8998815\n97578519-5-7-6-8-0-official-seller-\n717914-9_7_pv__e_a_d_s_1_5_6_w.p.v._2_pp_3_t_o_b_f_r_c_v0_0 _\nh _ s o _ __ _ _", 0.021052632480859756, 0.0, 0.021052632480859756, 0.02427184209227562], ["When we use a pretrained model, e.g. vgg16, as a backbone of the whole model which plays as a feature extractor, the model's data flow can be depicted as below:\nData --> vgg16 --> another network --> output\nAs for now, I've set False require_grads flags for all parameters in vgg16, and exclude those parameters from my optimizer's param list, so the vgg16 will not be modified during the training period.\nBut when I step further in my study, I'm now wondering which mode should vgg16 be used in? Should we call vgg16.eval() before running training epochs?", "However, in the general case, if you are freezing the model (with requires_grad = False) then you are not updating the running statistics anymore and should therefore use the running statistics, i.e. put the model in eval mode.\nVGG's backbone does not have any normalization layers nor dropouts. So in the end it does not matter whether you put the backbone into eval or training mode.", "When using pretrained model(vgg, resnet like) as backbone, should we use it in `eval mode` or in `train mode`?", 0.0, 0, 20, "vag300, vg33, VAGVAPOVP2AP, PPAAPAPPA, A PAAPA, SILAPROV3APAL, LUT341, RUT, 3D, W, AND THOUGH 032 D 10 LT IN THE STARTING PARKING, SHARKE THE BIGGAR, ESSO-BOO BOO, BOYBOYO\nBBO, BOMBOSS-CBO-BE-T-K-D-YA\nKK\nthe\n1958, a2898\nThe the\nPapa\nTurbo\nTruth\nVag 4s\n4.5.6\ns-s4-\n06/025/s94\n.v2s/vg1\nd/2.2\nD22DHDD\n3DDD\nHoften\nL0o_ofturn_s_e5_3_5\n5-3\n7 7,55 6 5,4 8 2017.185\u00c2\u00c2\nA \u00c2 A\nR 99\n\u00c2 \ufffd9097421. \u00c3\u00c4\u00c4\u00c5050\u00c2AA\nSAAAH-AA-AP\nAPHAPHA\nANALYS-VAGG1WOOW-P-OWSHA-HW\nJOJOWOR\nSTARTHEOATHROF\nWW W WS\nWE WW. WHO WOON \u00c4\u00c3\u00c2\u00c4 \ufffd \u00c7\u00c4\n?\ufffdOrt\u00c4W\u00c4LW\u00c5PW\u00c2P1\u00c4\u00c2H\u00c4A0-0\n\u00c408/9\u00c2W1-WON-\u00c4O\u00c4-\u00c3F-73-43H8-6)H\n83+H9366H12HB-80B\nE3OGA\nOGAL-53R-LATRO\n#39-FN\nN\u00c4NN\u00c4NNNNN\u00c2\u00c5\u00c46\u00c2\u00c33CW0K\u00c2G\u00c55B\u00c32\u00c5B0F\u00c5S2\u00c4S\u00c4\ufffd\u00c29.S\u00c34V\u00c4", 0.07476635277271271, 0.009433962404727936, 0.06542056053876877, 0.034161489456892014], ["I have a dataset where, after exploring data, I detect some patron:\n\nThe entire dataset have, imagine, 9 numerical variables, 1 dichotomous variable (take 'A' or 'B' value) and 1 numerical output\nThe output is a cost (in \u20ac)\nI find a sklearn regression model that, when 'A', using 4 of 9 variables I can predict output with good performance.\nI find another sklearn regression model that, when 'B', using the last 5 variables, I can predict output with good performance.\nIf I try to find a model which predict output with all the variables as input, encoding the dichotomous one with One-Hot-Encoder, the model has a bad performance.\n\nMy goal is to implement a unique model in Azure Machine Learning, using a .joblib/.pkl, but with this approach, I have two separated models with the same output (a cost) but different inputs, depending of dichotomous variable.\nIs there any way to merge the two models into a single one? So that with the 10 inputs, estimate a single output (internally discriminate options 'A' and 'B' to select the correct model and its inputs).\nNotice that using something like Voting Ensemble it's not valid because there are different inputs on each category (or I think it so)\nI accept another approach as a solution. Thank you.", "As you want to predict a value (regression), you can just train the two models separately (with the columns of your choice), you predict the output for each one and the prediction of the ensemble model is the mean of the two outputs.", "Merge distinct sklearn models into a single one", 0.0, 0, 21, "If you want to have 2 different models that you can merge to create a one-model, you should go for this: https://www.kaggle.com/community/2018/05/12/kaggles-new-feature-cross-validation-for-sklearns-in-the-cloud?from=7545", 0.2823529541492462, 0.07228915393352509, 0.1882352977991104, 0.08499999344348907], ["I want to plot graphs that share variables and datasets from other CoLab files, I would like to know how I could access those variables.", "You could create a new folder 'VARIABLES' where the variables are saved, read, and re-written (i.e. updated) as txt or csv files. Otherwise, defining a variable in one Colab Notebook will only be accessible within that Colab Notebook and not between Colab Notebooks.", "How to access/share datasets from different Colab notebooks", 0.1973753273487091, 1, 22, "There's lots of ways you can do this and I'm not the best person to teach you. But here is what I do in my coLaborator (I am a very experienced colaborate):\nFirst, you'd probably have to create a new notebook to use and it would be a good idea to make the notobook you want it to look like. Then, here are the 2 steps to share/access variables in colabnotebook files. Step 1:\nFind the link to the coLAB notewOOOK. This link would take you to anywhere in the colaWOOBOBOOK. So, maybe, if you just type in any thing and the first thing, is your \"Notework\".\nBut in this notawoOOWOKEWBOOK is, it can be anywhere. And that, to me, would make me \"happeeeee\" but I think it is where you have a coWWOBWOWWBWBBB and your coa-L-AaA-b-o-O-aLWLORW-WWAWLAW LORAW\n\"how to accsss/share variables\" from \"different ColaBoooooohooohhohohhhhbbbbbboo\nSo in 6 words, how to shakeevae/\n10 words on how you could shove e-v-e-ev-s-vs-", 0.19819819927215576, 0.027272727340459824, 0.09909909963607788, 0.08914797008037567], ["I'm trying to use optuna to tune hyperparameters of xgboost, but because of memory restriction, I can't set the attribute n_trials too high otherwise it would report MemoryError, so I'm wondering that if I set n_trials=5 and run the program for 4 times, would the result be similar to that I set n_trials=20 and run the program for one time?", "Yes, if you use the same database to store the study among different runs.", "A question about the \"n_trials\" in optuna", 0.0, 0, 23, "This question refers to the n _tria_ in the optu n a boost.\nI believe this question is about setting n, which will affect how the algorithm performs. It will also affect the accuracy of the prediction and other important factors, including the time of running the application. This is very important.", 0.09090909361839294, 0.0, 0.06060606241226196, 0.04915269464254379], ["For tf.keras.losses.SparseCategoricalCrossentropy(), the documentation of TensorFlow says\n\"Use this crossentropy loss function when there are two or more label classes.\"\nSince it covers two or more labels, including binary classification, then does it mean I can use this loss function for any classification problem? When do I have to use those binary loss such as tf.keras.losses.BinaryCrossentropy and similar ones?\nI am using TensorFlow 2.3.1", "BinaryCrossentropy ie like a special case of CategoricalCrossetropy with 2 classes, but BinaryCrossentropy is more efficient than CategoricalCrossentropy in calculation.\nWith CategoricalCrossentropy loss you should take the outputs as 2 dimension, while with BinaryCrossentropy 1 dimension is enough. It means you can reduce the weights by a half at the last layer with BinaryCrossentropy loss.", "Does \"tf.keras.losses.SparseCategoricalCrossentropy()\" work for all classification problems?", 1.2000000476837158, 1, 24, "Yes! This question was asked as an answer by a friend of yours! Yes!!!!!!! I am too far!!!Hey!!\"Ho!!!Hoy!\"Hoa!!Fo!!OOOH!OAAAAAAA!! Aaaaaaaaaa aa aa aaa a aa aaaaaaaaaa!!. Aaa aaahaa!Aaa AAA!\nHaaaaaa!Hoo!Further!You!Me!My!Your!I_YOYY!WHOYON!THEROSEYOU!BE!AUTOHOO-AUTHOAUTOWOHAAUTHWAHAAAUTWOAUTAUTAWHOAAAWHAWAA!!AUTWORAUTAWORUTAUTWRUTUT AUTWOTAUTVWD14-AP-4294AAP4W3_O_VWW134AUTATAP010AP_4.VWAAP 33,483AP5_30, 4320-VAPAP\n11,312 (3-3 6.9-16)\nVAAAP AP 5,00\n059 (C)4/18,885\n9\nBooo\nTeeeee\nFar from any!The name of any other-\nA\n!VOX-VAAP!POPP-PEARS-YEARS!WE!B!P-PAAPAA-SAAHA-HAPA-K-PAPU-FOR-WALOWS-AGAPAGA!APP\u00c4\u00c4P\u00c2\u00c4AP\u00c4L\u00c4A PA 1993?23\u00c4?\u00c4 \u00c3\u00c4\u00c5\u00c4\u00c2!\u00c4WAS H\u00c4!!\u00c2H\u00c5H\u00c4HUH0O\u00c40!C0.\u00c4-\u00c4157-G\u00c4NN\u00c4K\u00c4N\u00c4T\u00c45\u00c48\u00c43!S\u00c4638!524!3:A2A_B_C_G2_A B_!G_2\u00c264_CC_5 80\u00c2849!66\u00c29 026!41!2!177!09.Wowow.045695.40\u00c3.60\"-5", 0.08372092992067337, 0.009389671497046947, 0.04651162773370743, 0.041208792477846146], ["I got this err:\nimport pandas as pd\nModuleNotFoundError: No module named 'pandas'\nMy versions installed are:\nPython ver  3.9.7\npandas  1.3.3\npip 21.2.4\nPyCharm 11.0.12\nI can see pandas installed in pycharm, but when I am importing it I got that err.\nAny clue?\nThank you", "Try to reinstall pandas package.\ntype = pip install pandas\nwait for some time and then your panda package will get installed", "Python : ModuleNotFoundError: No module named 'pandas'", 0.0, 0, 25, "This question is related to your answer. I suggest you answer your question: What happened to the module 'pdbs'? I think that happened because the code that you wrote was untestable. You've got unstable code. Unstable. Let me explain what happened. When you write your code you have the idea of 99% of it. So you're trying to solve all the issues that are unsteady. And you do not write code with 6% because it is unrealistic and unfeasible. It can not happen. Because of the fact that is not realistic to write the un-unstab-able code ... 4%! That is the 8% and 5% that can be written by the people who write their code, like you ... I have 7%, which means the number of people that write a program is only 3% ... Because you can write it and because you know your software better than anyone else ... But you only have three percent, and that three ... is nothing to do ... With me the question was ... Why are you writing unstablable code? Because it can only be three and a half percent! What!?! You think you are better! Who you think ... You know how you better ... Who are ... ?! I can say how good you work! How ... how ...? ... How do you really work? You said, \"How do we work?\" But ... \"Well, how do I know? How? Well ...\" Well, you say ... Yes ... Well! Do you? Do it! And it's working on your table. ... What are these? These are the ... The table of their ... Table of H ...\nThe Tables of Their ... ... Tabled of ... H! H, TED OF THEIR BUSINESS!\nPractical Business Ideas ... PP ... Practical ...PP ...Possible ... Possible...\nWinnable Biz Idea #50 037 \u00c2\u00c2PP\u00c2P\u00c2\n33PP\n... The\nStories Of Their\nIn The", 0.01916932873427868, 0.0, 0.01916932873427868, 0.02912621572613716], ["I'm trying to find a library (any language, but preferably C# or Python) which will let me open an XLSX file, iterate through the chart objects, and find data about the chart - ideally including the data backing the chart.\nThe Pandas Python package, or ExcelDataReader NuGet package have useful functionality for opening the file and reading a grid of numbers, as well as ways to add charts, but I don't find any way to read the charts.\nCurious to hear from anyone who has ideas/solutions.", "Hey I have a good solution for C#. In C# you can use OLEDB, this allows you to connect a C# code to a excel or access database (so long the database is in the C# code files). You don't need to get any addons for this is you have C# on Visual Studio.", "Reading chart data from an Excel file", 0.1973753273487091, 1, 26, "Read and interpret the spreadsheet data in an Access or Microsoft Excel spreadsheets\nRead and Interpret the Spreadsheet Data in a Microsoft Access Spreads\nread and\ninterpret\nreading and interpreting the\ndata from reading and inter\nthe data to the table to\ntable of the 25 46 of reading\na book to be read from the start of being\n1 of 1413 of\nof reading, of not 7520 of a\nfew pages, a few of pages are\n5-62 pages of books\nin a couple of months, months of weeks\nthat you'd read a book\nnot a 3-3 or\nso many weeks of this book,\nthis book of several weeks, this\nbook of about three\nmonths, weeks or years\n6-15 or 6 months or, years, days or weeks that you\n4-2 or four days\nor four weeks read\n7-5 weeks are read; 5 months; or six months read by\n3 months are \nabout 9 months\n- - - about 8 months -\nsometimes a month or two;\nand sometimes a year\n2 months in November; December; January; February; March\nMarch of Reading\nNovember 099326\n. January . February .\nIn September ........\nOctober ..................... November\nDecember; November \ufffd\ufffd \u200b\ufffd\nIt is January \u00c2\nFebruary; May \u0092\nJune \u0094; July; August; September; October; Nov; Dec \u00ad \ufffd\nChristmas and \u00c3\nNew Year; Jan \u0093; Feb \u0091\nWars and\u00c2 \u00a9\u00c2 of England and Scotland; W\nChiefs of English and Scottish Warnings of War, \u00bd\nN\u00c2\u00c2\u00c3\u00c2!\nNOT; NON\nI WON TWO\nFOR TWINS OF WINNING T\nRANDERS FROM RAND-\nHANDERSONS from HAND-PAL-LEVERS, E-B-T, FROM EBA-Y\u00c2 AWEN-JONJ-E-A-ON-O-W-S-OF F-ST-HE \u00ef \u00c4 \u00cb\u00c4-7A\u00c5\u00c4\u00c4\u00c5289A\nSUPER \u00b2A\u00c4", 0.1428571492433548, 0.0, 0.06211180239915848, 0.05263158306479454], ["For some classification needs. I have multivariate time series data composed from 4 stelite images in form of (145521 pixels, 4 dates, 2 bands)\nI made a classification with tempCNN to classify the data into 5 classes. However there is a big gap between the class 1,2 with 500 samples and 4,5 with 1452485 samples.\nI' am wondering if there is a method that help me oversamling the two first classes to make my dataset more adequate for classification.", "actually there is a lib in python for that \"imbalanced-learn\" (although u can do it manually) .\nyou can check the docs it's very easy to use", "oversampling some classes from time series data", 0.3799489736557007, 2, 27, "the original dataset contains only 3 classes and one of the classes is very small. Therefore it's very expensive for me to do a new analysis. Please advise me.", 0.3103448152542114, 0.0714285746216774, 0.17241379618644714, 0.03478260710835457], ["How do you convert a column of dates of the form \"2020-06-30 15:20:13.078196+00:00\" to datetime in pandas?\nThis is what I have done:\n\npd.concat([df, df.date_string.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f.%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f:%z')}))], axis=1)\n\nI get the error - time data '2020-06-30 15:20:13.078196+00:00' does not match format in all cases.\nAny help is appreciated.", "None of the formats mentioned by you above matches your sample.\nTry this\n\n\"%Y-%m-%d %H:%M:%S.%f%z\" (Notice the space before %H).", "Dealing with \"+00:00\" in datetime format", 0.0996679961681366, 1, 28, "D2669-Aw08-Las33-Das-2-FK-House.", 0.0, 0.0, 0.0, 0.0], ["Is there an efficient way to calculate the optimal swaps required to sort an array? The element of the array can be duplicated, and there is a given upper limit=3. (the elements can be in {1,2,3})\nFor example:\n1311212323 -> 1111222333 (#swaps: 2)\nAlready found similar questions on Stackoverflow, however, we have new information about the upper limit, that can be useful in the algorithm.", "Yes, the upper limit of 3 makes a big difference.\nLet w(i, j) be the number of positions that contain i that should contain j. To find the optimal number of swaps, let w'(i, j) = w(i, j) - min(w(i, j), w(j, i)). The answer is (sum over i<j of min(w(i, j), w(j, i))) + (2/3) (sum over i!=j of w'(i, j)).\nThat this answer is an upper bound follows from the following greedy algorithm: if there are i!=j such that w(i, j) > 0 and w(j, i) > 0, then we can swap an appropriate i and j, costing us one swap but also lowering the bound by one. Otherwise, swap any two out of place elements. The first term of the answer goes up by one, and the second goes down by two. (I am implicitly invoking induction here.)\nThat this answer is a lower bound follows from the fact that no swap can decrease it by more than one. This follows from more tedious case analysis.\nThe reason that this answer doesn't generalize past (much past?) 3 is that the cycle structure gets more complicated. Still, for array entries bounded by k, there should be an algorithm whose exponential dependence is limited to k, with a polynomial dependence on n, the length of the arrays.", "Algorithm to calculate the minimum swaps required to sort an array with duplicated elements in a given range?", 0.1973753273487091, 1, 29, "the element can duplicate: there are three possibilities. The answer to the question: How many possible swapps are possible? How do you count swappa? We count how many swappas possible, how much possible in general? So, it\u2019s 3 possible: So 50 possible and so 6 possible. So how long do they take? This is the second question! So what is possible that you do not have that? You do it on paper. It takes how ... ? ? ... ... How much time do the experts and other people take to check it? ... _ ..._?_ How long are they? 0,00. How far are you? Just how far?! And so how are the swapped? A lot?!!!!??!!!???!!!!!!!!!!???!!!?A lot of possibilities? And a lot!?So how do we count it, if the elements of an arranged can also be a bit too! A bit more, too? That means they will also have to be arranged, but how? It\u2019, so... You just go to! How? I would...!\nAir, the\nCroze...\nSwopps. 4. Sowops. A few weeks. We have 739 months, according to how a child may be affected by 904 years, 8 months! Hours! Can a couple of babies be \u200b,?\u0094?Can you be?\u0093\u0094\u0094\u0093\u0093?Which? Which one?...?\u00c2?Because of how, you have, to, a few more? If a b... \u00c3\u00c2\u00c4\u00c4\u00c3\u00c4 \u00c4\u00c4\u00c5\u00c4AA\u00c4B\u00c4A\u00c4AAAA\u00c2AAAAAAAO\u00c4\u00c2B\nFour. You can only find how! Fours. Four! We! Whatt. Hack? H... We are now in Australia! But how... How did the... HOW DO WE CASA WORK WREKAWAK? WHAT WHO WAS? CA... CA! What happened in \u00c2\u00c2... ?\nHow do? Work\nWorking with how to\nWOO\n...\u00c2\nBWWC\n3 BWs\n4 Ws! TWOW! (HAASSHAAPHAARAHASHATHAW! NOT CAN WOUWDNNN!) ...\n?", 0.2160000056028366, 0.012048192322254181, 0.10400000214576721, 0.041800644248723984], ["I'm absolutely new in python, so there is a question.\nI've splitted my original df to X_train, y_train, X_test, y_test.\nNow i want to drop from y_train (pd.series) outliers therefore i need to remove object with same index from X_train(pd.df).\nWhat is  the easiest and cleanest way to do it?", "try using y_train = y_train[X_train_new.index] where X_train_new is your new X_train after dropping some columns/row/outliers.", "Remove rows from X_train and y_train at once", 0.0, 0, 30, "https://blog-2-3-7-00-927491-h102441204223677934-f-n-16-69-85661733218-p26_4_39401547148_p_5_-_1_9_0_837587280643_6__-__0.6.099.255.5-50782_w_s-s_x_t-w-457.7%- _w._w.-_.1.8%.__d.c.h_a.f.a_._.w.e.b_h.o.3o_c._s.v.r-o-v-l-q-r.q.l.t.p.s._p._v._o_.w.[_o]_[a]2 .2[b._c]._a._2B_H_F_B3._b3.F-b-F.B.HF._f.-FbFH2.P_D_Q_S_EBOB[1]-[B]BOBB[BO]BBB-P-K-k-t_k.-K.K._k.k._1B-c-B._B.[BBOBEB4B0]b.BB.4.L.W.[PBFB2]5BPP . . B_ . _b__bbb_. ._bp . -P .\n. . BoBsBboBBBB\nSboB,sboHbo,wHFBHBHBOBA\nWBBS,B197BBoBa\nBowBangBo\nboo\nb[2].4\n89[0][2\n_pp_bo___b_____b\n[o][o___]\no__o**[n]o____\na\ufffdb\ufffd\ufffd\n(hb) 3\n3 \u00c2\u00c2\nL4 135 L\u00c2E\u00c2LEP2 \u00c30308 4F3BABBOASBS BO", 0.010638297535479069, 0.0, 0.010638297535479069, 0.0], ["I have 2 indexes, one named NIM, and one named Total Score. Both have 100X1 block matrices.\nWhen I run the code below the index gets removed.\n Final_Score = np.hstack((NIM, np.atleast_2d(total_score).T))\nIs there a way to combine several matrices into one and keep their indexes?", "In the end I used the same code but added:\npd.DataFrame(Final_score,columns=['NIM','Final Score'])\nNow, I can change np array into pd.", "How to keep indexes when combining matrices?", 0.0, 0, 31, "When I ran the above code the numbers were removed instantly. I had 5 matrices and when I checked the data it had no values, only the size of each matrix. So maybe it's just not possible, although I've done it before so maybe the same thing could be possible. Thanks\nReplies:\u00c2\u00c2\u00c4\u00c4\u00c5\u00c4\u00c2\u00c5\u00c5\u00c2\u00c3\u00c4\u00c3\u00c2\nSubject: Replies on how to go keep records when?", 0.13483145833015442, 0.02298850566148758, 0.08988764137029648, 0.01734505034983158], ["I am working with a medium-size dataset that consists of around 150 HDF files, 0.5GB each. There is a scheduled process that updates those files using store.append from pd.HDFStore.\nI am trying to achieve the following scenario:\nFor HDF file:\n\nKeep the process that updates the store running\nOpen a store in a read-only mode\nRun a while loop that will be continuously selecting the latest available row from the store.\nClose the store on script exit\n\nNow, this works fine, because we can have as many readers as we want, as long as all of them are in read-only mode. However, in step 3, because HDFStore caches the file, it is not returning the rows that were appended after the connection was open. Is there a way to select the newly added rows without re-opening the store?", "After doing more research, I concluded that this is not possible with HDF files. The only reliable way of achieving the functionality above is to use a database (SQLite is closest - the read/write speed is lower than HDF but still faster than a fully-fledged database like Postgres or MySQL).", "Pandas HDFStore caching", 1.2000000476837158, 0, 32, "Close the HDBStore by creating a file or by a user script. After close, the user-script may add more rows to the table. So if you have a large HBDF, then you can keep it running and add rows regularly. You can also add the column-names (use-full) to make it more convenient. This HDBC would be available to all users. The process may be restarted if it needs to run again to add files.", 0.1875, 0.0, 0.09375, 0.024683581665158272], ["What columns do I have to consider while implementing K Means? I have 91 columns after pre processing. And also to how many columns do I have to apply K Means clustering ? Is it all of them or only a few to be considered ?", "It's actually about trial and error. There is no straight way to say which columns are going to help you the most until you try and figure it by yourself.\nbut you can use dimensionality reduction algorithms like PCA to project data to a lower dimension without much data loss. It's a common approach and also helps with the speed of your clustering algorithm.", "K means algorithm implementation", 0.0, 0, 33, "Answer: Who is? Hhhhhhhh? Who? Why? Where?\nI have more to know how to implement K\nK Meas\n: Know\nHow to start using K-Meas? How to install? Install?!\nInstallation! Install! I know HOW TO START USING K MEASURE! How can I start my K? Me ... (K ...K) K ...? K HHK HKS HKS KK KKS K K (KS)KKS ... KN ... ... ?K\nThe Kness ... The Ks! ...\nWh ...! Wh ...????????? Wh!? Wh? A Whaaa ... Wha? ... Whaa Whaeaa ...WHA? WHAA WHAEAAA WHAWHA?WHHHWHTHHTHTHE\nD! .!! HHAHAHHA! 1\nH!HHS!HS!!! 3! !!!!!!!!!\nWhat? What? WHATA WHAT? .... .... .. . ... .... ... 2\n3.3373223 DD\nSaa Saaaaa saaa aaa\naa ia iii\n4.4556 D 5 4\n[TNTP16]2194 [3][4]14[265] 0 84 6 7 \nB B B\n01 ........ ...2 ...8 \u00c2 \u00c3\u00c5\u00c4\u00c4\u00c2\u00c4\u00c3\u00c4\u00c5\u00c22003\n. ... H H \u00c7!\u00c7\u00c7\u00c4!CC CC CC\n- H!-!!C!!D-!!!1-F-1F!2-4-5F!!4!!B2!!\n1381 H [WHAT WALTH WW?]WHAT NOT NOT NONON-NONENONI-VENE-HEN-HA-A-D2 -2 \u00d03 - \u0102\u00c2\u00c210!W\u00c4WOO\u00c4-0.6!3,4,\nE0E8E,E \u00c4,\u00c4 (1)A\u00c41\u00c42AA\u00c4\u00e4\u00c4A2\u00c454\u00c43\u00c44A A\n\u00c4E\u00c4  \u00c4\u00c4\nh\u00c4r\u00c4h\u0102!\u00c4H\u00c4C\u00c4T\u00c4COC0C_", 0.08064515888690948, 0.0, 0.05645161122083664, 0.054007142782211304], ["here I used panda for export my data which is located in numpy array. but there is a problem that I cant export my data and also there is a erroe that you can see below.\nvalueError: Must pass 2-d input\nthis is my main variable AccZONE=c.T and The type of that is Array Of float64, and the size Of That is (710,1,1)", "From the error it looks like the array is 3 dimensions, you need to change it to 2 dimensions, it would be nice if you could provide some code.\nYou can try np.reshape(arr,(-1,1)) or np.ravel(arr).", "Export final data from numpy to excel", 1.2000000476837158, 0, 34, "The answer is that answer that type. That's not a tr\u00c3n\u0102s\u00c4\u00c4\u00e20-\u00c5\u00c5O\u00c4\u00c2\u00c4KO0K-MA\u00c4-200L\u00c2K\u00c4T\u00c4R\u00c2\u00c5\u00c4H\u00c4\u00c5K\u00c50\u00c230373KA-15\u00c401\u00c4U\u00c4STU\u00c5S-B\u00c4C\u00c4O\u00c2434.26604\u00c25.0.3-9.6-h0/-339\u00c4s-s\u00c3O\u00c3\u00c41.\u00c2S45350ESS-EUTOR-SIDE OF 385-0\u00c5-AHW\u00c4F\u00c45 \u00c3C\u00c2s725676\u00c4N\u00c4A\u00c4D\u00c4I\u00c4WATOO4899\u00c51-\u00c4L\u00c4G\u00c496K6183\u00c42 628-f29 70 \u00c2DAA\u00c4HEAP 46.8.405 and 932A.13. The.KL3H6D\u00c5EW\u00c5W\u00c2REL\u00c53WOHRE\u00c4WE\u00c4\u00c3\u00c5 86 07, but a s\u00c4S\u00c4B\u00c5\u00c3\u00c212,0;22B\u00c2\u00c2B4\u00c56OVL0806\u00c3477.L_A6_23_0_68,_F_H_\u00c4 _A_M\u00c4_C\u00c3S 5OBO\u00c4 1C3O-FW-4-L \u0093\u00c4\u0093BV \ufffdA \u00ad, with all \ufffd\u00c4\ufffd\u00c48KP2C \u00f1\u00e2B-M\u00c2P-K \u00bd K\u00c4. K.\u00c4, KA, EWK, \u00cf4\nLow-\nand the nd-5d-in-jw1 \u0097 \u0094\nKK\nHK02\u00c4J\u00c4REK\u00c2\u00c316 FON\u00c4SEV-GW \u00c4\n\u00c4\u00d0\u00c4AA-8 \u00c6 \u00a8-k9\u00c2. Ak\u00c4y-n9\nV\u00c4\u00ad-Ott\n-H0\n3 -6 (3) -5 -F98 -K -V -3\n4 \u200b\u00c4 -", 0.06896551698446274, 0.0, 0.04926108196377754, 0.005434782709926367], ["I'm running python script present in .py file using ExecuteStreamCommand processor in Nifi. For reading a csv file pandas modules is required. I'm calling pandas in the program but I'm getting error mentioned as \"No modules Pandas found\"\nI have Python installed in my local and added to path to Command path.\nHow to install Pandas library?", "I\u2019ve had a similar issue with other modules. What you need to do is install the Python modules on the NiFi server that your script calls. What the error message is telling you is that it\u2019s trying to find the module called pandas but it isn\u2019t installed on the host.", "Python Modules in Apache Nifi", 0.0, 0, 35, "QAQ 3-3 Q\nD3QQK3 4\n34661Q3KQ_339_10327QZQKSQSS\np_q_QSQ6-4-_2-Z_ZKPZ-P6 60\nZOQ KZPKASZFKS KQ. 58. The pqkap.qkk.k.4.67476z.z-z 21-rz68-s-nzqz_v-65-a-furnza_t_cqq._zKOO_K_OOHOOL 130-KK\nzahzk-o-k-2K-7.314 zz0 728 and z23zzz59\nw71z and.986.p.e.lz1.1817s.2k \u00c3\u00c4\u00c4\u00c2\u00c4\u00c5\u00c4.00\u00c2.\u00c3z78s z-mzp-wz zn-893s\n807 Z-19-57 \u00c2\u00c2 996, ZHZZ, 88, z\u00c4\ufffd\u00c4B\u00c4Z\u00c4A4OZAP\u00c4,\u00c40\u00c4U\u00c4_\u00c5\u00c2s\u00c4e0.850 and 01 ZK.\u00c4\u00c306B,36EH\u00c4ST\u00c4L\u00c4\u00e2\u00c2E\u00c4-ECH\u00c4SEL-\u00c4O\u00c4E-V\u00c4ZE\u00c4sD\u00c4M\u00c4F\u00c4CQ\u00c4H\u00c3V0F\u00c2-FVN-OA\u00c4Y,W\u00c4z\u00c4n\u00c4D\u00e2W\u00c2P\u00c4DELHWZW\u00c3AA\u00c41L4K\u00c5W\u00c5\u00c5.The.KW.\u00c2FW375\u00c4V\u00c33LWO\u00c3E\u00c51\nK\u00c3Z\u00c5BK\u00c4224W6HOR\u00c5PANWITSEN.CAR.W45.L.7KNN.M1CWB0NNYAH6WQ7,5540,820WG\u00c4P\u00c2W04C0DJ\u00c4WERD\u00c3WW", 0.05128205195069313, 0.0, 0.041025642305612564, 0.012448132038116455], ["I've read many similar posts that say Excel's T.INV.2T(probability, degs_Freedom) can be reproduced in python using scipy.stats.t.ppf().\nIf I use the example of T.INV.2T(0.05, 58) excels yields 2.002.\nBased on other answers posted I should get the same answer using scipy.stats.t.ppf(0.05, 58), but I do not. I get -1.672.\nAny ideas on what is wrong with my syntax?", "In Excel, you have two functions for returning the inverse of the Student's t-distribution: T.INV and T.INV.2T.\nThe first returns a left-tailed inverse of the Student's t-distribution and the second returns a two-tailed one.\nscipy.stats.t.ppf also returns a left-tailed inverse of t-distribution. So, if you want to compare scipy.stats.t.ppf with Excel you need to use the T.INV formula and not T.INV.2T \u2013 or you should divide the probability by two and then use it with scipy.", "How to reproduce Excel's T.INV.2T in python?", 0.0, 0, 36, "804\u00c2, (2,07)\n\u00c2 484,86523460S\u00c2H.743\u00c2719,509144.30-32-128, the number of the\n03.4 and a, and the, for the third, in the main room. There was the \u00c23-20s. and for 36.1793,37,10, which was on\nthe road that was created for other on. 15\n5535. The first was for an, old, black.I\nwas just for that \u00c3\u00c3s966,416\n624 was so that I would 826 61, was 0\u00c2\u00c41\u00c4\u00c5\u00c3\u00c5.99.Fo.\u00c583 was a good-but-was-that-wold-f\u00c2\u00c288 978 would you have 70647 for A318\u00c2F1W\u00c29\u00c26\u00c2C1ST\u00c25\u00c21P\u00c4P6\u00c511CRO\u00c2O\u00c3\u00c4F\u00c3KO\u00c4\u00c42\nA4908 BAO0H \u00c4B\u00c4 SPECW\u00c43OO\nC\u00c2\n4\nH7\u00c2W4B.O40\n\u00c4AH\u00c3O.W.\u00c40\u00c5\u00c569\n3K \u0093.H\u00c4K\u00c4H\u00c2JA2V\u00c4D-OR-WOA\u00c49DV-\u00c4\u00c3H5OCCALS42\u00c5\u00c2N-4-\u00c2D0KL-F5-A387 F\nG2W\u00c363A F2C\u00c4G-\u00c5\u00c48C-J\u00c3S-T\u00c2-6 BV\u00c2GFOR\u00c4AA\u00c2YF0\u00c3C957]4\u00c5EW\u00c5[\u00c4]\u00c4U\u00c4W, SUO\u00c2\u00c5HWOUOG\u00c4\nPWW0...21\u00c2AA\u00c44L\u00c4VO....3M-SEU-STUGA-ORE-U0C6H...\n\"\nE29\u00c356 SATM\u00c4T\u00c5O\u00c5S3WON-SUD\u00c4", 0.16911764442920685, 0.03703703731298447, 0.0882352963089943, 0.02729528397321701], ["reading an excel file in python jupyter notebook i'm trying to change a column datatype of a pandas dataframe from object to float and every try I get the message of ValueError: could not convert string to float: 'Variable by base style'. What does 'Variable by base style' mean?", "the data you're trying to convert includes an item : \"Variable by base style\" which obviously cannot be changed to a float.", "Can't convert object data type to float in pandas data frame", 0.0, 0, 37, "How do you make a picture that the artist made for a photograph of the most? the image was made to be by the 10 5014-5-c.359-4.5\nthe artist was the only one to create 2\n1556-70-the 3-rated for the next 7512-600\u00c2?the other\n07334\u00c2the\nanother was used to the previous 6 457\n794 80\n?was used as a\n\u00c2\u00c2another?\u00c2\nit was \u00c2\u00e2\u00c2that a\u00c2part\nthat the\u00c2other??1\u00c20.66? 03 9069\nother \u00c3\u00c2C?C2.\n-8?4?786\nCelebrate Hole H\n87?\u00e236/C\nHole-H\n99\u00c2H?AHAPPERT-M?-G-L-BORROOO-1-20th-08193\u00c2-918-D\u00c2O\nBWOY-DOC\u00c2D\u00c5D\u00c4\u00c4\u00c5\u00c5\u00c4A\u00e2H\u00c4DADAPRESOAAP\u00c2A\u00c2APAC\u00c4\u00c23\n43 H\u00c3T1C\u00c4H\u00c3\u00c3\u00c4C\u00c3A\u00c458 B0L\u00c4\u00c3\u00c5O\u00c4AP\u00c4\ufffd\u00c4AT\u00c438C64\nL8H53HY4P\u00c4B\u00c42\u00c5\u00c3C0\u00c440C463LB\u00c5H\u00c2\u00c3E\u00c4\n3,497\u00c2G\u00c5F\u00c3F168B,52,62\u00c2\u00c4AA\u00c4PP\u00c4K\u00c40D \u00c4W\u00c4F\u00c4P \u00c5\u00c2\u00c5BH\u00c5E,O.O.,\nS\u00c4V\u00c4PA3\u00e2AAA\u00c5A.C.\u00c46C_\u00c4_8._AA_O_A2L0AA._O\u00c572._AAC\n61\nT23O8O09,\nEarly-and-Elect-Written-English-A-3x4,-Z,\u00c2T\u00c4O,\u00c4AD\u00c4WA\u00c5\u00e2B\u00c3\n(B)\nThe Halt of \u00c6\u00c2and (A)", 0.06572769582271576, 0.0, 0.04694835841655731, 0.048979874700307846], ["I'm writing a numerical solution to a partial integro-differential equation, and I need it to run quickly so I found that scipy.integrate.simps is best, but it's not always 100% accurate and produces the spikes in [1]. My solution was to remove them with scipy.signal.medfilt and then interpolate over the gaps with an interpolator (I've tried CubicSpline, PChipInterpolator, scipy.interp1d, akima,...) but all of them produce little \"hiccups\" in the solution that can be seen at y=0.1, (produced with 3rd order butterworth filter) and these errors grow as the solution is evolved. How do I remove the spikes and get a simple, smooth interpolation over the gaps? Thanks!", "I don't work with scipy, but from what I've gathered, some things stood out to me, and could possibly be what's causing problems.\n\nYour call to plt.show() which displays the data happens before you filter out the outliers with medfilt(), so the corrected data might not appear in your plot\nThe median filter from what I gather doesn't remove outliers from your data, instead it resets each data point with the median value amongst its k-neighbors.\nWith this in mind, I have two suggestions, (1) your median filter window might be too small, and that is causing the outliers to not be removed. Try setting it yourself using medfilt(self.n_, k_size=5), it defaults to 3 so try odd numbers larger than 3. (2) Given that you're not losing data points from using the medfilter, you might not need the lines that follow it which try to interpolate data that was presumably removed.", "How to remove spikes in solution and produce smooth interpolation with scipy?", 0.1973753273487091, 1, 38, "Try `scipy_signal_remove_spikes`.\nThe signal will remove any spike from your data. The removal of any...\nThanks! I got it\nanswer: try sci...", 0.11764705926179886, 0.021621622145175934, 0.10695187002420425, 0.05882352963089943], ["I have a dataset with ages (int) and wages, I would like to have the average wage next to each row of a person respective of that age.\nI created a smaller dataset using\nmean = df.groupby('age', as_index=False)['lnWage'].mean()\nwhat is the best way to append (for 2000 rows)?\n\n\n\n\nAge\nWage\n\n\n\n\n30\n10\n\n\n30\n20\n\n\n\n\nthen\n\n\n\n\nAge\nWage\naveragewage\n\n\n\n\n30\n10\n15\n\n\n30\n20\n15\n\n\n\n\nthanks!", "The comments above are helpful, I have found this to be the easiest method, where average is the df with average wages. (ffr)\ndf_main['avgWage'] = df['age'].map(average_df['averageWage'])", "add average value to every row containing value in different column", 1.2000000476837158, 0, 39, "1463.0 607 40) 56 87; 16;\n96.9.2.66, w,2689, a38,47, which was the opposite of 34,83,017\n06/04-6-780-133\nw-31975-h, but-n8-445,32,\n-5023-085.39 03s and two-fits-22-rul;-the1,90rub4r2n, 9118 or the highest, lowest and highest level of the, most expensive or, the most of-sports-worn or-gone, in-stars, are, un1-S,54941;.581.8.1) or un35;055 for the number-on the other side of, where the more expensive. The two 7-99-82 and. This \u00c2, has, for, more, on the line of one-\u00c2for-you-can-see-in-other, with \u00ad65 and \u00c4 for \u00c35\u00c2-\u00c4,672s \u00b33  and for\u00c2 and\u00c2\u00c2.436 was a big size with a line (3),793 (K3.)0, that was \u00b4952 (f24).648;1/w3 for.70.e,12;5\u00c3.H.WOA for O\u00c2B\u00c44D\u00c5\u00c2F\u00c39/7.BWA.L373W0+1P3S\u00c3K\u00c3N\u00c4VEN, or a, and an, this, was, there, to get an-sh0;and-this is our\u00c2WASR\u00c4NOW\u00c4OUAA and-[-W-C3]and...- and so...\u00c3n4\u00c34.\u00c4.v.g.,v25B,Was \u0092. \u0093v,7 was used,\u0092,was used as a \ufffd.V.C0W\u00c4\u00c4W5M\u00c4\u00c5\u00c4\u00c23/A0F.S.", 0.0762711837887764, 0.0, 0.050847455859184265, 0.01818181574344635], ["when I do this\n\n\n    my_list = df.loc[df['ID']  == \"9\", ['ID1','ID2','ID3','ID4']].values.flatten().tolist()\n\n\nI get the result\n\n\n    my_list = ['-1','32','63','-1']\n\n\nAnd then when I do my_list .remove('-1') I see\n\n\n    my_list = ['32','63']\n\n\nwhich is what I want to see .However when I try to do .remove in single step like\n\n\n    my_list = df.loc[df['ID']  == \"9\",['ID1','ID2','ID3','ID4']].values.flatten().tolist().remove('-1')\n\n\nthen my_list is empty.\nWhy is this happening?", "Because remove does the operation in place, modifying the list itself.  It doesn't return anything.", "using .remove seperately vs using it in tolist()", 0.1973753273487091, 1, 40, "I am answering your question but i have another issue in my code\nin my script I have written code to run the script but if I remove the line of code below then I\ncannot see the output of the code I wrote.", 0.13333334028720856, 0.0, 0.10000000149011612, 0.047527607530355453], ["Is there any magic way to create an huge absence presence matrix in python? pd.crosstab and pd.pivot_table fail because of the memory requirement.\nI have an extremely large dataset like this one:\n\n\n\n\nPerson\nThing\n\n\n\n\nPeter\nbike\n\n\nPeter\ncar\n\n\nJenny\nbike\n\n\nJenny\nplane\n\n\n\n\nand need this:\n\n\n\n\n\nBike\ncar\nplane\n\n\n\n\nPeter\n1\n1\n0\n\n\nJenny\n1\n0\n1\n\n\n\n\nNote, the matrix is rather sparse. It contains a lot of zeros.", "Computers used in data science sometimes have absurdly high amount of RAM (I think I've seen one with 1tb before...)\nIf you don't have that much RAM, then I think the only way to resolve this is to utilize the hard drive...\nI would say, process the data, write it as a structured data on to a hard drive, and loop through while reading, say 50mb at a time to check if the name has already been added to the file, and modify it.", "Create a very large absence/presence SPARSEE matrix in Python", 0.0, 0, 41, "030-3-1431. The-\u00c4.246.3. 9708, 56500, but an extensive is very extensive set-809-7, with 677 and 45,02,4-993, an 8374 and the most complicated. Soft. S.E0.106 of an organization-s of 0.-1538-5-2.Krath, is, and in order.072 was 1681,626,17 of a coc2949,76 and C.T.66961-60 of A.J. of.C.89.4.7.L2120D.90A.D13BURN11L1HF2K.\\\\.N.G.Z1B.H.BK 7334 is a,59 and is there of course of S 35.A2C.-S.W.\u00c336)55\u00e24 of R\u00c3N and F\u00c46-\u00c2227-N\u00c4\u00c2\u00c2-TUR-DECKOOM-LOR\u00c4MIS-W\u00c240\u00c2TROM\u00c4\u00c4\u00c5\u00c419\u00c2P1AROCCOHKAJO\u00c47L\u00c5G\u00c3F1\u00c5\u00c2OVAC\u00c4NO1MA\u00c4F\u00c4H\u00c5\u00c3B\u00c2SMA-MAGUFUWO-OGB\u00c4MO-ORW\u00c40\u00c4A\u00c4P\u00c4D\u00c4-FW-0F-47\u00c4S\u00c4V\u00c4R\u00c5M\u00c2\u00c4C\u00c53L6B 23OAA\u00c45\u00c51OOGAW\u00c3V8784KR\u00c4B\u00c4\u00e2\u00c4I\u00c4MA\u00c5R\nT\u00c4\n\u00c44\u00c5H\u00c4PA\nO\u00c2VAN\u00c4\u00d0MORKNAN\u00c5D4ST\u00c4DESMO\u00c4WAHA\u00c4PP\u00c59O5\u00c4AG-RO-H8P-C-BOOWY-\nG\u00c4U-U\u00c4\u00c3\u00c4\ufffd\u00c4G\u00c56V\u00c5\nSTENOSSOMAW2B-E\u00c4$641)CARM\u00c5E", 0.10000000149011612, 0.0, 0.05384615436196327, 0.02425876073539257], ["I'm trying to run my code on anaconda prompt and it gives me this error, any suggestions?\nAttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", "Thats not an anaconda error, but an error with the Python code. You'll have to debug the code itself to see, where the error lies. Basically you are trying to access a function that doesn't exist.", "AttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", 0.0, 0, 42, "The Answer Is Wrong\nNo\nWe Are Not\nOur Answers Are Worth 14\nAnd 67 Answer An Answer!\nYour Questions Are NOT ANSHALHANHSHAH!! The An An-ShH-Is-A-An-Coo-No-Co-Oo!!!!!!! ... ... And 43 Answer!!??! ...\nThe An\nAn Answer\nCong ... Cono ... 33 ... and ... the ... (1078717)\n2 (200) ... C ... P ... Q ... A Answer ... The 2 ... You Are So .... ... We ... Answer .... The ... An ... It ... Is ... So . . ... What .\nD DD ... D B . D . B ... E E . And it ... is so ... so . so .... Answer . 7 . Answer 8 . Annsnsh 0 .nnsrnsh 99 ........ ... NnNn N NnnN. ... B N ANNH.ANNN.NNNNNA. ANNNNNNnnnnn.nNN .nnNNnANNA . (9050).  ... ANNnn 55. \u00c2 \u00c3\u00c4\u00c4\u00c5\u00c4\u00c3BB\u00c3B\u00c3H\u00c4H\u00c2BHWW HWOWWESTWOWWAW \u00c6AA\u00c2\u00c4 \u00c4\u00c2AA\u00c4AAA\u00c2\u00c3AA \ufffdB\u00c4A\u00c5\u00c5A\u00c4AP\u00c4\u00c6\u00c4 A\u00c5\u00c6A A\u00c4AAAA\u00c2\u00c2APAEAE\u00c3APAP \u00ca\u00c3\u00c5 \u00cc\u00c4\u00df\u00c2\u00c5\u00c2OO\u00c5OAAPAHAHA\u00c4HAHHAAAAP A AA S T H H E S U N T \u00d1 \u00d0D \u0110\u00d0\u00c4S\u00d0S\u00c4V\u00c5W\u00c4W3 \u00f1nwnw3\n\u00c4E\u00c4\nA B A B\nW\u00c2W\nHAAHA\u00c3AAAAA\nB \u00e6a-a\na nd-n \u00ef?-5-1-23-\n560a5\n-d d d\ndtdds1\n886a1189a4240-s-w-c-370/4/h/n/d/dn/\ndn nde \u00929o\u0092c \u00b4", 0.09132420271635056, 0.009216589853167534, 0.09132420271635056, 0.06666666269302368], ["I'm currently trying to start working with tensorflow.\nI work with anaconda and I tried to install the tensorflow packages in the root environment but it always displays the message: \"Several errors encountered\".\nWhen I looked it up it says the solution is to create another environment exclusively for tensorflow, I did and it worked. But I'd still like to know what the reason for this is.", "I have had the same question when I started out. It seemed like it is the \"correct\" thing to do, so I just did it, but never understood why. After working with TensorFlow for 2 years now, and on multiple machines, I realised just how specific the set of its requirements is. Only a few versions of python are compatible with it, the same thing with numpy, and if you want to use NVIDIA GPUs, good luck figuring out the specific versions of cuda and cudnn.\nYou don't want to have to tailor most of the python-related software on your machine to running tensorflow. In order to avoid breaking it whenever you install something that requires a higher version of numpy, for example, it is best to keep it in a separate environment. This way you have an isolated \"container\" that keeps everything just the way TensorFlow wants it, while still being able to use other software if needed.\nNot to mention that there are several versions of TensorFlow and they all have different requirements.", "Why do I need another conda environment from tensorflow?", 1.2000000476837158, 2, 43, "\u00c2n \u00c2\u00c5L\u00c4\u00c2\u00c2W\u00c4\u00c5\u00c4H\u00c5\u00c5\u00c2EUTWO\u00c4KU\u00c5F\u00c4A\u00c5V\u00c4\u00c4UFU\u00c4M\u00c4T\u00c4V\u00c2\u00c4FKW0\u00c4MA\u00c4WE\u00c4L\u00c2\u00c3\u00c40W\u00c5S\u00c4WI\u00c3\u00c5138\u00c2\u00e2\u00c42C\u00c3C\u00c43C3,C0B\u00c4C\u00c50C\u00c21,071\u00c28\u00c4STA\u00c4WAT\u00c4AT1\u00c4WA\u00c4D\u00c4RE\u00c4AA083\u00c22\u00c47\u00c4\u00c3\u00c2P\u00c4R\u00c46\u00c4865\u00c4s\u00c4Z\u00c4AP\u00c44\u00c450P\u00c5WAAW\u00c3P4\u00c5G\u00c4E\u00c41998\u00c52KL\u00c533\u00c4N\u00c4ES\u00c49\u00c4BO\u00c4AR\u00c4S\u00c5\u00c3AR\u00c2B0H\u00c4B\u00c3V0-00\u00e2\u00c5BOOWAS\u00c4BAH\u00c2BOW\u00c2FRAHAL\u00c4PAP\u00c2BA\u00c5AP,BO\u00c2H,B\u00e209\u00c5H45,7.25.72.80.124\u00e24B,46\u00c2C18,62,P\u00c3F\u00c3APF\u00c2AT\u00c2D\u00c255S\u00c2310PA\u00c5PASA.D-C84,90,82F0363-H-s\u00c305-DAAH\u00e2\u00c247F,K\u00c4-6-1-F-20\u00c27\u00c342-KF594-9, 306 C\u00c20 H\u00c4LE\u00c4WR\u00c4SA\u00c4IWHA\u00c4SB\u00c5KAH35\u00c2TKWIO-VV-\u00c4VER-UVW-AMADRE 496C 11 8F 9 A4H 6 21 A3 C \u00b3 C,D6 and 5 C-4D \u00c3B-S.K.V.C.L.\u00c5.4.\u00c4/A8.3.9.OOVN-P\u00c2-5\u00c37D16.0FNW.P.64V,\u00c2A4L,\u00c4b\u00c4J\u00c4h\u00c4\u00e4.B1C-.\u00c260A \u00c4, S \u00c6A, C\u00c4. A", 0.03458213433623314, 0.0, 0.023054754361510277, 0.02398524060845375], ["I am building a Streamlit dashboard that will have to read data from a DataFrame. The problem is that I have a local csv and a local Excel file form which I am reading data using pd.read_csv().\nHowever, when I share my dashboard url with others, the data will fail to load because they won't have the file locally.\nHow can I read the contents of a csv and Excel file and turn them into a \"hardcoded\" pandas DataFrame?\nI guess my question is: how should I store and read my data without having local csv and Excel files?\nEdit: sorry for no code or MRE, but I literallyu have no idea how to do this. If I had a piece of code, it would simply be a pandas dataframe with sample data in it.", "In R I would use dput() function to show me the code necessary to create a data frame.\nFor Python I know that print(df.to_dict()) would do something similar to be a \"hardcoded\" Pandas DF.\nSo I would do the following:\n1: print your df. df.to_dict()\n2: copy and paste the necessary code to create the data frame inside your streamlit app. Something similar to this: {'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}}\n3: \"load\" the data frames by creating them everytime the application is run. df = pd.DataFrame.from_dict({'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}})\nPS: note that this solution is not scalable neither would work if your data keeps changing from time to time. If that's the case, you would need to keep printing and pasting your new df to your code every time.", "How to save contents of local csv file into a \"hardcoded\" Pandas DataFrame?", 1.2000000476837158, 1, 44, "7 3134153-I-1.6.2.P.D-989-23rd-4-8-Gold-Lot2-330-HUZ-7939K-A-M. \nB-D BM-PAC A MOA 1940)DONER MISEHWOMOCODRE-ORE0OOLE OWOWALS FROTONNAL PRESCHROOMALACER CROKER FREDROCOWWAS SIDD2 OAO,442 PARSHER,32A BROOMMISH DORON AKW5O3,624, 7,885,547204A, and 4573.5 and DAPROSSA of 64 and LY 0FARMISHOWER POCASSOY. ABO1 and the 900 226,99, A35-07-d1-N-60502MATRER4CU-STOSS.L-S363P4.3C, GAVERODDALER-5WATOFROVALISALO6GIVESHATDONEA. THE CONVERTION \u00c36WAG.7670.46A3\u00c2CAPENCIS.B\u00c2.17B0LERO.JUO CN8CRAFT S.F9. I.C.O25.066C\u00c23O86\u00c2\u00c25210C5CCFATRO.A\u00c2B71WHORE\u00c2FIND.I.MO\u00c22\u00c249S4DU43BWL68\u00c208F4FW1\u00c21DAA-V-F8275AA, C0MMMFATHW2OBAR,WOL18,OVEN580\u00c54GAOMA\n37AH.HOTO\u00c2O1C\u00c33]\u00c4\u00c2W\u00c3\u00c26\u00c3UV\u00c35\u00c5\u00c5D\u00c4\u00c3CC\u00c3AA03\u00c39", 0.1268656700849533, 0.0, 0.0746268630027771, 0.0292682945728302], ["I have a FastText trained model \"*.ftz\". My program runs in the multithread mode.\nIs there anyway to load a model once and use it without loading for each thread?", "After some efforts to find a good solution I used Fastapi and implemented model as a service.", "Load trained model only once", 0.0, 0, 45, "My app runs with the same Fast Text Model \"\". This model is trained on the CPU and is available for every thread.", 0.10256410390138626, 0.0, 0.05128205195069313, 0.03892986848950386], ["I am working on a regression problem and my dataset is very imbalanced. My features are age, sex, weight, medication dose, some lab results and I am trying to predict one column of continuous values.\nIn my dataset some individuals are represented by more samples than others. For example 30 lines of data from one individual, 10 from a second individual and 1 from a third and so on. I do not know how to select the training set so that the model is not biased towards specific subjects.\nI divided the training and testing set in a way that there is no data from the same individuals in both sets but still training a model with a training set that is not balanced regarding the amount of data from each individual would bias the model.", "I would suggest to duplicate samples. So that, for example, every individual will have 30 rows of data.\nAs an alternative, you can also adjust the weights. So that an individual with 30 samples will have weight 1, an individual with 10 samples will have weight 3, and an individual with 30 samples will have weight 30 [it's an equivalent to duplicating, but doesn't increases the training set]", "How to create a training set for regression in Python if in a dataset some individuals are represented by more samples than others?", 1.2000000476837158, 2, 46, "Fit 4th,3 0\n4,0 62,1,a.I.Homer, and another with less 21.65.90.184 894.341-140,954-603-B-H-P,7-S-4463_-the_M_of Homer 5-s-39-2422 \u00c302\u00c2 of my own, the whole and the main or one of another woman. If I had done the. 972.0sO5s.\u00c2\u00c3\u00c2f\u00c2\u00c2t\u00c226.7.4 is.D.s,829\u00c2. and D.A. is it. Is the \u00c24A-\u00c2\u00c4\u00c3A\u00c2BRAMBENCH ASSOMEL.STOB.2BM. BUT THE CROSID 74 was a BOM BF-TB SOME AGA-C\u00c2 \u00c4NV\u00c4STEN AO DOO.L4 and.\u00c3715 of. A.WOSS. O. F. C. S.U.G.R\u00c3.B36 of them were. This. So. The. DAA. L\u00c3B\u00c4H\u00c4PRA.C.KA4LH.f.e.M507_5C_f_648,4C\u00c3M\u00c5R.F25O,O\u00c2S693355 \u00b33F, S,F\u00c2sHoPen1D\u00c3S.53.\u00e2m.m,513S,m4S.,286,66F and we have the fourth \u00e2\u00c3\u00c332 BW of the GN\u00c2, N57\u00c2LB and SFNORVO3G1O\u00c3COD\u00c4.S\u00c3V. M.P.8. In.\u00c4BVEN-O0-8\n7...2a...b.N......001AH...B09.T3Mand5206)\u00c3\\\\375\u00c405\u00c3D\u00c27045\\\\...173\u00c43KOH_\u00c5\u00c4\u00c44\u00c3H11", 0.10894941538572311, 0.0, 0.05447470769286156, 0.0267857126891613], ["I have a Python program that is controlling some machines and stores some data. The data is produced at a rate of about 20 rows per second (and about 10 columns or so). The whole run of this program can be as long as one week, as a result there is a large dataframe.\nWhat are safe and correct ways to store this data? With safe I mean that if something fails in the day 6, I will still have all the data from days 1\u21926. With correct I mean not re-writing the whole dataframe to a file in each loop.\nMy current solution is a CSV file, I just print each row manually. This solution is both safe and correct, but the problem is that CSV does not preserve data types and also occupies more memory. So I would like to know if there is a binary solution. I like the feather format as it is really fast, but it does not allow to append rows.", "I can think of two easy options:\n\nstore chunks of data (e.g. every 30 seconds or whatever suits your use case) into separate files; you can then postprocess them back into a single dataframe.\nstore each row into an SQL database as it comes in. Sqlite will likely be a good start, but I'd maybe really go for PostgreSQL. That's what databases are meant for, after all.", "How to lively save pandas dataframe to file?", 0.1973753273487091, 1, 47, "You need to read in your csv data (not as csv file) and write it out to disk. Do not do any other operations on the csv files. If you are writing your code directly to the file you have the solution:\nimport pandas as pd\npd.write_table(\npandas.read_csv(path)\n) to\nwrite the files to your\ndisk. You are using csv\nas a storage mechanism, not as\na data format. There are lots of other storage mechanisms to consider: .bz2, .tar,\n.b.zip, etc. In your case you\nwould be better using a .\ntar file. Also you don\u2019t need\nto do anything in between the read and\nthe write. It\u2019s just a big string: \u201c0\n87 08 88\nand nothing else.", 0.1428571492433548, 0.0, 0.10204081982374191, 0.04332941025495529], ["Below is the dataframe:\n\n\n\n\n\ndate\nopen\n\n\n\n\n26\n15-09-21\n406.5\n\n\n\n\nNow I need the value of open so I tried:- print(df.open)\nIt gave error:\n\nAttributeError: 'DataFrame' object has no attribute 'open'\n\ncolumn types are as follow: print(df.dtypes)\n\ndate ---->           object\nopen ---->         float64", "If you have a white space in your \" open\" column, just rename it by df = df.rename(columns={' open':'open'})\nI sometimes prefer to call a specific pandas column using this df[\"open\"] and press \"Tab\" for the auto-complete (in Jupyter Notebook or in Vscode). That way I am aware of any concealed typos such as that whitespace which you have", "Getting the value of particular column when its only one row in pandas dataframe", 0.0, 0, 48, "I got the answer and it was very easy!\nI just needed the data-table in a-pinter. So I decided to go with it.\nMy friends were really happy when I made them the wooden tables. And it really looked cool with the dark wooden table that was a very good gift.", 0.19130434095859528, 0.0, 0.104347825050354, 0.04735751450061798], ["I am using databricks-connect and VS Code to develop some python code for Databricks.\nI would like to code and run/test everything directly from VS Code using databricks-connect to avoid dealing with Databricks web IDE. For basic notebooks, it works just fine but I would like to do the same with multiple notebooks and use imports (e.g. use import config-notebook in another notebook).\nHowever, in VS Code import another-notebook works fine but it does not work in Databricks.\nFrom what I could find, the alternative in Databricks is %run \"another-notebook\" but it does not work if I want to run that from VS Code (databricks-connect does not include notebook workflow).\nIs there any way to make notebook imports that works both in Databricks and is supported by databricks-connect ?\nThanks a lot for your answers !", "Well, you can create packages .whl(wheel) install in the cluster and call via import in any notebook is a breeze", "Import notebooks in Databricks", 0.0, 0, 49, "If you want a \"quick\" solution, just create another python project and import the \"parent\" project. This can be done with a single line:\npython import my-parent-project\nThe python \"import\" command has many uses. In fact, if you are coding in python, you will use it almost every day. It is the most used command in the language. However, when coding python in nota-books, this command will fail if the parent notabook does NOT have the not-\nHow to Import Python Projects?\nImporting python projects can make your live much easier. Just take a few minutes to import a python note-book to python. Here are some of the important steps:", 0.14814814925193787, 0.015037594363093376, 0.10370370000600815, 0.02150537818670273], ["I am working on a Dial a Ride Problem (DARP). I have a lage amount of nodes and edges (338 nodes and 826 edges). I've imported the node/edge data from OSMnx and am trying to solve the model with Gurobi Optimizer in Python.\nTo be able to use the OSMnx data with Gurobi, I created a matrix = len(nodes) x len(nodes) matrix and therein printed the length of the edge if two nodes were connected, and a large number otherwise. In the optimization, a x[i,j] = len(nodes) x len(nodes) binary decision variable is used to decide if an edge is traversed or not.\nThe problem I am encountering is a large computing time for just one request (+1 hour). I think this is because the model also has to consider all the other indices from this large matrix, even though they can be ignored completely since they represent that two nodes are unconnected.\nMy question therefore is if someone can help me find some preprocessing techniques or something else that might reduce my computational time. For example, tell the model that it can ignore indices from this matrix if the value is too high or maybe a more efficient node/edge storage file that Gurobi can use more efficiently.\nThanks in advance.", "If your graph is sparse, the optimization model should be sparse, too. Specifically, you should only create a variable x[i,j] if the edge (i,j) exists in the graph. For an example of how to do this, see the netflow.py sample in the examples/python subdirectory of Gurobi.", "Preprocess node/edge data or reformat so Gurobi can optimize more efficiently", 0.0, 0, 50, "You should be asking in the  [guroby_docs](https://guory.docs.ny0.0/help.php) [Guory_Docs]( https:// guorory . d o s . n y0 . 0 ) page. And the answer is very simple. You must use Guo ry_Doc_ s_s o _ n_y0_0 for o p t i b l e _ h o_ _ _t o o l s _ . The Doc s o n t _y  o  _ 100 B 6 2 3 4 5  S  B T C F S H T T B62345S. The model must consider the values of all indices of matrices. This way, it will be more accurate. I don't know what you mean with \"large amount\", but if you want to get it right, you should run the solution of DAG for at least 70,0 9,9944 times. If you run it less times, then the accuracy will go down. Run the DSA and solve it in a bigger size, more than  .50 T .", 0.21698112785816193, 0.02857142873108387, 0.09433962404727936, 0.03524181991815567], ["I'm making a FPS game in Ursina, and I'd like to be able to aim. I will do this, I think, by changing the camera's FOV : it is perfect !\nThe problem is that I'd like to be able to animate the transition of aiming. I cannot use a for loop, as the FOV only updates once it is finished, and I cannot use the animate method... I tried :\ncamera.animate(\"fov\", -30, duration = 2, delay=0, auto_destroy = True)\nWith the syntax :\nanimate(name, value, duration=.1, delay=0, curve=curve.in_expo, loop=False, resolution=None, interrupt='kill', time_step=None, auto_destroy=True)\nHere, my value (I'd like to decrease my FOV, so to zoom, by 30) doesn't mean anything : I can put whatever I want, and it will not stop until the fov is equal to 0.\nIs there a way to fix that ? Either by finding a method to update the camera in the for loop, or either by finding any way to animate the FOV transition", "Found the answer : the value parameter is actually not the value you want to increase or decrease your FOV (or anything) of, but it's actually the value it will go to ! So, if I put 1, my FOV will go to 1, that's why.\nTo animate -30 for my FOV, the correct syntax is :\ncamera.animate(\"fov\", camera.fov-30, duration = 2, delay=0, auto_destroy = True)", "Python ursina : aim by changing FOV's value (issue with animation)", 1.2000000476837158, 0, 51, "You can're using a `for` loop in this situation. Instead, you can change the `FOV` by using the update function: `camera:update(FOVALT,1)`. Then, use `animation` to make it move: \\begin{code}camera\nanimation(update, FOVAL)  ,  delay =\n0 , ...\n\\end{Code} . Then you don'twould have to worry about `animating`. Finally, remember to check if the object is\nalive by doing `if(camera)` .\nI have the same issue ! Can you help me please ?\nComment: Sorry, it didn'te work. Can someone help ? Please ? ??? ???? ?", 0.26143792271614075, 0.013245033100247383, 0.11764705926179886, 0.04081632196903229], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "If you are trying to get the parameters of your model:\nprint(model.get_xgb_params())", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 52, "As the [docu\u00ad\u00adion for xgboboost](https://www.wisc.3.2.boo.st.#2003-2-19-booho#707317#890930#0243346#39638674#6068878#945369#4275655#13285921#577152518) states, xbgboba\u2019st is able to predict and predict future outcomes [08.83]\u201d. It can be [a-p-o-f-e-s-t-i-n-g]\u201d that a small number is predicted [that future events would be predicted]. It is predictable that the small numbers are predicted. You can predict that future numbers would have predicted and predicted future situations.", 0.0784313753247261, 0.0, 0.03921568766236305, 0.04999999701976776], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "model.get_params(deep=True) should show n_estimators\nThen use model.get_xgb_params() for xgboost specific parameters.", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 53, "No you can't (as far as I know) but if you have access to the data you could use the function below. This function will return the final model, and it will print a message explaining that you may need to modify the code to make it work (it will not run automatically). You can read the following information and you will be able to recover the object of your model xgbustobj, which you then can use in your code (I did this).\nThanks\nRetr\u00c3\u00c3eble hyper\u00c2\u00c2\u00c3\u00c2 \u00c2 \u00e2\u00c3 \u00c33\u00e22\u00e23... (L.167, 2017)\nLow level of the bust ... (T ... 04,02)", 0.06611569970846176, 0.0, 0.04958677664399147, 0.04624975472688675], ["I am getting below warning for python in console.I did not found any solution for these.We dont want to suppress warnings .\nAlso we have a big code base setup.how to know which code block is cause of this error as warning dont give code line number.\nI am using below version of python and numpy.Is it due to old verison's of python and numpy.\nPython version- 3.6.8\nNumpy Version-  1.19.5\nmatplotlib version is 3.3.4\npandas version is 1.1.5\nWarning:\n/python3.6/site-packages/matplotlib/cbook/init.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. obj[:, None]) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\npython3.6/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead", "It's the way you're accessing the array, using slicing. Matplotlib is going to remove that from how they handle arrays, but they haven't yet. It's just a recommendation to convert to a different type of array access, like Numpy, before that happens. Based of what you're showing, i'd guess it's as simple as 1. Create Numpy Array 2. Use identical slicing except using Numpy syntax. Should be good to go after that I'd imagine.", "Python warning :FutureWarning: Support for multi-dimensional indexing", 0.0, 0, 54, "There is a mistake in the code, that you mentioned. And it is correct as you said. The code says that it should be used with any type, and not only type 0, 5, or 6, it says it shouldn't be done as this is unsupported and in future will stop working. So that was a typo in code.", 0.26760563254356384, 0.014285714365541935, 0.1267605572938919, 0.04755011200904846], ["I've been building an application using Apache Cordova - it's actually based on machine learning, but all my machine learning prototyping has been done in Python.\nIs there a way I could incorporate my Python libraries (like scikit-learn) into my Apache Cordova app, or is there something else I should include?\nThank you, any help would be appreciated!", "No, you can't embed a programming language as a plugin for Cordova. You can however do a remote call to a server running python.", "Can we use Python modules with Apache Cordova?", 1.2000000476837158, 1, 55, "You can actually use the modules you're using from Python with the Apache Coderv app. You'll actually have the opportunity of developing a project, and it will provide the necessary coding skills for you. I recently used Python from the app to create a website using PHP. The result was great! I got the best result, although it took me longer to code.", 0.17777778208255768, 0.022727273404598236, 0.15555556118488312, 0.04672896862030029], ["I have two date columns having corresponding Dollars associated in two other column. I want to plot it in single chart, but for that data preparation in required in python.\nActual table\n\n\n\n\nStartDate\nstart$\nEndDate\nEnd$\n\n\n\n\n5 June\n500\n7 June\n300\n\n\n7 June\n600\n10 June\n550\n\n\n8 june\n900\n10 June\n600\n\n\n\n\nExpected Table\n\n\n\n\nPythonDate\nstart$\nEnd$\n\n\n\n\n5 June\n500\n0\n\n\n6 june\n0\n0\n\n\n7 June\n600\n300\n\n\n8 June\n900\n0\n\n\n9 June\n0\n0\n\n\n10June\n0\n1150\n\n\n\n\nAny solution in Python?", "I can suggest you a basic logic, you figure out how to do it. It's not difficult to do it and it'll be a good learning too:\n\nYou can read only the subset of columns you need from the input table\nas a single dataframe. Make such two dataframes with value as 0 for\nthe column that you be missing and then append them together.", "Data preparation to convert two date field in one", 0.1973753273487091, 1, 56, "You need to prepare two columns of your table in some date-formatting method. Like here, you have your two data columns and now you need some way to make it compatible for chart. This charting is more complicated, and you should also prepare and format data, not only your chart-ready-data, in this chart you will show your data from your previous columns. There are many ways of formatting data in any table. One is, this method can be prepared with many data tables with more or less complicated methods. It's more than one method in our table and it is one of the most accurate data table methods of a company in which your company has more accurate and accurate methods with the help of data and business. You can do it with your own business with an efficient company. The business of our company and the business are better than the company, which is the main thing of this company to get better in time and get in the future with our business and other companies with business to be good and a better company with a more time. If your business is your most important things in your life, it has much importance, the good company should be the important thing in a good life of business, a large business company is large and better, as this business should come to the best, better and more. We have 25 years of experience with professionalism. To get more and have a lot of money. With an hour you'll be more with any business companies. What is most? Why is business better? To be a business? More money? I would like you to see more money, money is better money and money in money business you. Why more? So why? The reason is simple and there is no other reason for business reasons, there are several ways to invest in business in other businesses, for example, 40% of people with other jobs can invest more in others. Investing in investing companies is safe and profitable. Therefore, we have to take this into consideration. These are some reasons for this, therefore, more business investors should take business into their consideration and should invest and invest safely in their lives. How does your investment affect the lives of investments? A better business business will not affect your better life and can affect some lives in more ways and some other ways, because you can also invest, invest less than others, other. Less investor can. Or. Can? Less. For example? Can, can? Or, or", 0.16302186250686646, 0.031936127692461014, 0.0834990069270134, 0.08239699900150299], ["I am looking for a sentiment analysis code with atleast 80%+ accuracy. I tried Vader and it I found it easy and usable, however it was giving accuracy of 64% only.\nNow, I was looking at some BERT models and I noticed it needs to be re-trained? Is that correct? Isn't it pre-trained? is re-training necessary?", "you can use pickle.\nPickle lets you.. well pickle your model for later use and in fact, you can use a loop to keep training the model until it reaches a certain accuracy and then exit the loop and pickle the model for later use.\nYou can find many tutorials on youtube on how to pickel a model.", "Is it necessary to re-train BERT models, specifically RoBERTa model?", -0.1973753273487091, -2, 57, "Yes, retraining BERT models is necessary. However, you can retrain by yourself without any specialists. You just need to find the BIP-8 BIS-22 BERR-95 BES-33 BOL-40 BAL-BOO-60 models. Then you just search for the recoiling BORT-11 BAR-5-7 BAT-03-BATBALBAS-BABABASABAB-SABSA-sas-asasab-e-eb-k-hoc-boo-heboon-we-can-make-the-re-coil-from-th-lift-up-c-p-d-m-o.k.o.,o,k,ok,o;k;o_k_kk_ok_aa_a_h_ooh_he_bo_hn_on_his_hip_n_hoops_p_f_i_c_b_t__e_bb_tb_s_th_l_ft_g_of_4_6_in_this_model. This is not a problem and we have only a few weeks to do it. That's why the model should be on this website. Just search \"is it possible to make a BTT model?\"\nI have searched many BIHT models (20+26+50) on the website and their average is 77%. So the average accuracy is on 36% and therefore the best model is with 90%.\nAnd this model for me is very important because I'm interested in how well this BLB can do its work on a topic of my interest. So, why is the highest score with BBO15? The second highest with BO32 is at 45%. And the third is also at the level of average for 57% ... In general, how high score can the machine do? And how can I rewire the device? Let' ... me know the topic, and what other models are able to handle it? I need your help with your devices. Thank you. It is my passion.", 0.20527859032154083, 0.017699114978313446, 0.12316715717315674, 0.0546875037252903], ["For example, I've a table:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n5\n202\nTrue\nsend\n2021-10-20 13:15:18\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n8\n202\nFalse\nreply\n2021-10-20 13:15:25\n\n\n\n\nHere, True/False is based on whether column 'Action' has 'send' without colon or not. If 'send' then it's True, otherwise False.\nI want to delete rows based on condition of a row which is True. So, delete rows if:\ni) a column 'Number' has a same a number which corresponds to value True in column 'B', In this case: delete if Number== 202\nii)and if a column 'Datetime' is in range of 2 minutes of a column which corresponds to value True in column 'B'. Datetime corresponding to True value is '2021-10-20 13:15:18' and it's range of 2 is: [2021-10-20 13:13:18 ; 2021-10-20 13:17:18].\nOverall, deleted rows should have a number=202 and which are in range [2021-10-20 13:13:18;2021-10-20 13:17:18]\nNew table should look like this:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n\n\nSorry, if a question and task is not fully clear.", "Try this:\ndf = df.loc[df[\"Number\"]!=202 & df[\"B\"]!= 'True']\nIf type in column B is boolean, then change the string \"true\" to a boolean True.", "Pandas: How to delete rows based on some conditions?", 0.0, 0, 58, "(3)2340495543226014453656352903192757780994282411B4B64BAAABBB33BBCBCCBCHHB3H3BC3CCC3CD3C4H", 0.0, 0.0, 0.0, 0.0], ["I have a 45x45 matrix which Stack overflow isn't letting me include as it is too long. But if I throw this matrix into numpy.linalg.eig, it gives me an eigenvector of all zeros in the last column. What does that even mean?", "So it looks like the matrix is actually degenerate, which I suppose makes sense, actually.", "Numpy.linalg.eig function returning zero eigenvector?", -0.1973753273487091, -1, 59, "You are right. If you take the matrix of your question, and put it into the python module numpy, you will get a vector of zeros. This is what happens, the 34,05,50,383,926,756-23-32-424-746 numbers will be converted into vectors of ...\nComment ... ... .... ....\nMy ... My M ... Mmm ...M .... M .... mmm ....mmmmm ...mm.\nMmm M MMM MM ... MM .... MM ... SSS .... S S ...SSSSS S .... ... . . S s s ... s h ... H .. hhh .... hh .... H ....H ....OO ... OO .... OOUUUOUOUOOU .... UU .... 99  ... 88 ...44 ...8  ....4.4 .. . ...a. a. .. ....a a aaaaa aaaa .... a .... Aaa. . A. AaAaaAAA AAAAA ...A ...AA. B .... . ........ ...... .B ....... B . BB BBBBB. BBBBB. .... B. ... B.... .II II .... IIIII.I ... II II I I ..... I 11 II1III I ..... I II ....I IIIII I.IIIIII......... ....iii ...i...... ii ii i ii iaa ... aa.........a....... ...II II. II III .... III I . III IIII III ... IIIIIIIIIIIII. IIIII III ..III III. -III III -II.12 -1-1 22 00 5-0-55a5aa5sasasadsaadaddaaadaddaaddadaa-addaaaddaaaaadssaaasaaaa .... ..a.....a..aa.....aa..i..iii..ii.....ii............. .... ...S ...s...ssaas ...ssas asas sasasssassassssssass sass. Sass ... Asasasaas...as..as....s....as.assss.sss.ss....ss............ ... as ...as ..s.. as...a.....s\n.F....F.FFF.....FF..... FF........F..........F...F", 0.04608295112848282, 0.009302325546741486, 0.036866359412670135, 0.05255329981446266], ["I am trying to write a function equilateral(x, y): that takes two np.ndarrays of shape (N,)   , where x and y are natural numbers and returns a point z an np.ndarray of shape (N,) such that (x, y, z) are are the vertices of an equilateral triangle.\nAny one please suggest.", "In order to get the third vertex, you could just rotate the point (x2, y2,...) by 60 degrees around point (x1, y1,...). The other admissible solution would be obtained with a rotation by -60 degrees, i.e., in the opposite direction.\nSo just rotate y around x by 60/-60 degrees and you have your 3rd co-ordinate.", "Python function to find a point of an equilateral triangle", 0.0996679961681366, 1, 60, "The best way of learning python is through courses. There are numerous websites that provide online courses in python. For example, www.python.lifeth2nd23.com provides a number of online training courses for python, like python courses, python course and the python training. Moreover, it offers courses like a python programming class, a training program in a web programming in the web and so on. Also, the course in programming can be done online. Like the www...more....\nComment......................................\nI have also made the online book which describes how to make web applications. In this book, one can learn how easy the programming course is, but what exactly is web programing?\nShoooooo!!!\nReply................................................\nHow does programming work? It is easy to learn programming. On other hand, how does the training in web programs work, or how much time is required to complete the courses? Many people take up courses to become an expert in their respective fields, and they also have the opportunity to enroll in free web development courses online or free online web ...\nWrong!!!!!!!", 0.11965811997652054, 0.008620689623057842, 0.08547008782625198, 0.04905660077929497], ["I have a model defined as\ngmodel=Model(a)+Model(b)\nresult=gmodel.fit(data,...)\nI use this model to fit the data, which gives me the parameters and their error estimates. Using the result.eval_components(), I could access the component a and component b of the model function. Also, using result.eval_uncertainty(), I could access the 1-sigma uncertainties in the model functions, which would be two lines. Now I want to know each component a and b in that 1-sigma uncertainty lines.Is there a easy way of doing this.", "There is not currently \"an easy way of doing this\" - the eval_uncertainty method belongs to the lmfit.ModelResult, not the lmfit.Model itself, at least partly because it needs the resulting covariance matrix.\nBut: I think eval_uncertainty method could probably calculate the uncertainty in any component models too.  I would suggest raising an Issue and/or using the lmfit mailing list to discuss making this change.", "Calculate the uncertainty in components of composite model in Lmfit", 0.0, 0, 61, "You are correct. However, the way the system calculates the \"uncernity\" (it just gives the standard deviation of each parameter) does not necessarily correspond with the probability calculations. It is not necessary to the mathematical calculations to calculate the parameter and the error of every component. Nevertheless, it is still correct to talk about it.", 0.27642276883125305, 0.04958677664399147, 0.19512194395065308, 0.04885993152856827], ["Im trying to convert a pil image to a numpy array but i want the image to keep it's transperancy, how can i do that?\nIv'e tried using numpy.array()\nBut it doesnt keep the transperancy", "I'm not quite sure what you mean by it doesn't keep its transparency. If you convert a PIL image with transparency using numpy.array() it should return a numpy array with shape width, height, 4 where the 4th channel will represent the alpha channel values. And after whatever processing you need to do, if you convert it back to a PIL image using Image.fromarray() and perhaps saving with Image.save() you should get back an image with the same transparency. Can't help you much more without seeing an actual snippet of the code and possibly the image.", "How can i convert a PIL image to cv2 numpy array", 1.2000000476837158, 0, 62, "PHONK PANKK, PHISSSSS, I MUST KOO KIJJK K, SO I I LUU-UHU HUHIHHIHOOOHHOHHHH, WHAT DO I DO. WHO'S FIRST, WANNABBLE? PAPPA PASSES, STEPS TOM TOO TO TEE. TOUCH PRE, PAH, TOUGH PARKING PALKS, NACHNACHTIG, NEU NNEU NESSNESSO, MU MUS MON MMMM.MUMMM.\nMMMMKU KKUKU, K KLUKKLKLICKSLICKSSCHSCHSSHWWWEEW.LUUS\nLUCUS, LUC\nLEON LEO LEE\nHIGH HYPP\nEST\nSTOOK.STEO. KOKO\nKOR.KOK.OKK\nok.okk.kkk\nkokkkokoo\nkkkk.kocock. co.co.coccoococoococ.oococa.ocaa.cccc.crr.rt.rrtrttttt.hhs, hhs.ss,s.s,hhy.hyh.hh.mmm.mmi.mi\nmimimia\nmihm\nhhh\nhnnnnn\nnnnuu\nuu.uhu\nuh\nwwhhh\nwha.wh\nwhat do i\n.jj\njajjaaah\nhow can\nhohhhho\nooooo\n0.00OOMOMS\nOMMS.OoMMoS.OSoSoSooSsSSsSsssss S S\nS-Pil-ipil\nP-PL\nPL.PLAN PLAN PE PERSONAL PERTIN PERECHIPCHHIPHEHHAHEHEEHAHA H A A B B\nBBSW BBB BBS WWWDWDDDDDA DA\nDDA.D DA DD\nDD.DD DAD. AD\nAP. AP. APA. PP\nPP.P PPPPP P\nJaJA\njaJ\na jaJ", 0.06949806958436966, 0.0, 0.03861003741621971, 0.03742203861474991], ["I need a function to turn bytes back to a numpy array. Is there a simple way to do it?\nPickle doesn't work because my data is too long and everything else I tried fails as well... I'm trying to send a frame over a socket from my client to my server.", "Try this: x = np.frombuffer(n, dtype=i.dtype)", "What is the opposite function of tobytes()?", 0.0, 0, 63, "\\strong{I found a way} to transfer a data array without using \\em{pandas or pdb or any other way}. The best way I found was \\begin{code}\narray = numpy.array(array)\nnp.write(filename, array)", 0.08888889104127884, 0.0, 0.04444444552063942, 0.0], ["I am trying to use the python weka wrapper. I am using the Cross validation method. It then prints the classification results. Then i use build_Classifier and test on training data using test_model. It provides different no.of classification instances compared to the cross validation model.\nFrom what i understood, in the cross validation model, 10 different models are built, and then the accuracy is averaged while the models are discarded. Then it fits the entire data again and produces the classification results.\nthen when the data is the same, shouldnt i get the same results with the build_classifier model as well?\nor is it because i put randomstate in crossvalidation but did not randomize the data in build_model?", "When performing cross-validation, the entire dataset is essentially being used as test set. The predictions from the k folds get collected and used for calculating the statistics that get output. No averaging of statistics or models occurs.\nTraining and evaluating on the full dataset will yield different results, but you should see the same number of instances being used. It is possible that there is a bug in your code. But you need to post your code to determine the cause of that.", "WEKA training and cross validation", 0.0, 0, 64, "You do not get any same result with build model if you didnot randomizethe data. Because you have different datasets, you get different results which are correct. If you build a model on a new dataset, it will be correct as the model will predict the trend. For example, if a customer buys online for 3 days, the online trader will buy for the three days. You do something which you should do. Read\nRead Read ReadReadRead\nIn my opinion, I would read the book \"AH.\" What did you think about? Read! What is your opinion? It's a book, a very good book. And it says that they make the product, but it doesn't say how much and how often they produce. That' ...\nDo you like\nI read 2019 and 9. 7. ...ReadDo Do DoReadSenRead ...Sens ... Sens .... ... ... .... .... S ...", 0.2589285671710968, 0.018018018454313278, 0.125, 0.0371212363243103], ["I have an array of n positive integers. I want to calculate a list of all contiguous subarray products of size k modulo p. For instance for the following array:\na = [3, 12, 5, 2, 3, 7, 4, 3]\nwith k = 3 and p = 12, the ordered list of all k-sized contiguous subarray products will be:\nk_products = [180, 120, 30, 42, 84, 84]\nand modulo p we have:\nk_products_p = [0, 0, 6, 6, 0, 0]\nwe can easily compute k_products using a sliding window. All we have to do is to compute the product for the first k-sized subarray and then compute the next elements of k_product using the following formula:\nk_product[i] = k_product[i - 1] * a[i + k] / a[i - 1]\nand after forming the whole list, we can compute k_product[i] % p for each i to get k_product_p. That's it. O(n) complexity is pretty good.\nBut if the elements of a[i] are big, the elements of k_product may overflow, and thus we cannot compute k_product_p. Plus, we cannot, for example do the following:\nk_product[i] = ((k_product[i - 1] % p) * (a[i + k] % p) / (a[i - 1] % p)) % p  // incorrect\nSo is there a fast algorithm to do this? Note that p is not necessarily prime and it is also not necessarily coprime to the elements of a.\nEdit: As mentioned in the comments, there will be no overflow in python, but working with very big numbers will be time-consuming.", "This is not a sliding window algorithm, but it is a simple and effective way to solve this problem in O(n) time without any division:\nLet A be your original array.  We will imagine that there is a \"mark\" on every kth element of A -- elements A[0], A[k], A[2k], etc.  This ensures that every k-length window in A will contain exactly one mark.\nNow, make two new arrays B and C, such that:\n\nIn array B, each element B[i] will contain the product (mod p) of A[i] and all following elements up to but not including the next mark.  If A[i] is marked, then B[i] = 1.  You can calculate this in a single pass backward from i=n-1 to i=0.\n\nIn array C, each element C[i] will contain the product (mod p) of A[i] and all preceding elements down to and including the previous mark.  If A[i] is marked, then C[i] = A[i].  You can calculate this in a single pass forward from i=0 to i=n-1.\n\n\nNow, you can easily calculate the complete product of any k-length window in constant time, because the product of any window from A[i]...A[i+k-1] is just B[i] * C[i+k-1].  Remember that there is exactly one mark inside the window.  B[i] is the product of the elements before the mark, and C[i+k-1] is the product of the marked element and the elements after it.", "Sliding window algorithm to calculate the list of all k-element contiguous subarray products of an array modulo p", 1.2000000476837158, 2, 65, "No, you can't do that. The elements will definitely overflow if p isn'tt coprncm to a and b (note that b is your array). And there won'nt be any other way of doing it either (it is O\u00c2\u00c2\u00e2\u00c2\u00c3\u00c3\u00c2O(N) anyway).\nComment: Sure, I could go and do OOO (Olympic) for p=100 and k=2. However, doing so on, say, a 90-bit integer would be quite (very) time consuming. Also, it isn\\'\u00c2\\\u00e2\u00c3\\\u00c3\\\\\u00c3\n\\\\\u00c2\\\\a\\\\b) is actually a very long (hence, very)\ncomment. So if you don'\u00c2t\nmind, please\ndon'\u00c3\u00e2\u00e2\n\u00e23\u00e24\u00e25\nAwesome\n!!!!!!!11!!1!\nI do believe that there must be a way to at least calculate (k,p) using another algorithm (and thus reducing the time for computing it to just O[N]).\nThis question has many [\u00e2]s. Please [\u00c23\u00c245\u00c26] them. Thank you.", 0.2935323417186737, 0.029999999329447746, 0.11940298229455948, 0.032367970794439316], ["I have a pandas dataframe containing large volumes of text in each row and it takes up 1.6GB of space when converted to .pkl. Now I want to make a list of words from this dataframe, and I thought that something as simple as [word for text in df.text for word in i.split()] should suffice, however, this expression eats up all 16GB of ram in 10 seconds and that's it. It is really interesting to me how that works, why is it not just above 1.6GB? I know that lists allocate a little more memory to be able to expand, so I have tried tuples - the same result. I even tried writing everything into a file as tuples ('one', 'two', 'three') and then opening the file and doing eval - still the same result. Why does that happen? Does pandas compress data or is python that inefficient? What is a better way to do it?", "You can use a generator. For example map(func, iterable)", "Why do python lists take up so much memory?", 1.2000000476837158, 1, 66, "Because we think that python can be so inept. In fact, the python language can handle large data.\nWhy do we need to know? Do we have to? Can you make it less complex? How can you do that? Is there any other way that you can make this work? For me, it'\nTitle: The reason why we spend so many hours making lists from pandas df, is because, we want them to take so up of memory. The question I wanted to ask myself was why was I trying to create my own lists? Was that really a reason for doing so? Because you think, that it can take lots of memo\nAbove 2GB and a lot of time, as that has to happen. My first answer to this question would be no, because I can see why the lists would take such an amount of\nFollowing: Follow", 0.050314463675022125, 0.012738853693008423, 0.050314463675022125, 0.06236712634563446], ["I have a model which requires solving a system of ODEs with tfp.math.ode.BDF, and I would like to find the individual least-squares fits of this model to n > 1000 datasets. That is to say, if my model has m parameters then at the end of the optimization process I will have an n by m tensor of best-fit parameter values.\nWhat would be the best way to perform this optimization in parallel? At this point I\u2019m planning to define an objective function that adds up the n individual sums of square residuals, and then uses tfp.optimizer.lbfgs_minimize to find the best-fit values of the combined n\u00d7m parameters.", "I believe the BDF optimizer and LBFGS both support batches (of problems), so you could have an outer \"problem\" axis to your data and leastsq return value. But since BDF is for stiff problems, it's likely to have much longer runtimes for some problems than for others, and you might be best served treating each problem separately (tf.map_fn) as opposed to running them all in parallel -- in a batch, you can't run ahead onto the next LBFGS iteration for problem X until you compute the BDF integration for problem Y. Or just use a python for loop over your problems, each time calling a @tf.function def lbfgs_over_bdf(data): ....", "performing many gradient-based optimizations in parallel with TensorFlow", 0.0996679961681366, 1, 67, "A model is required to be solved for the purpose of n data-samples with a tft.algorithm.gradient.back.f.20, or n samples with back.alg.40 with the parameters of a gradient. There, it is said that the solution of such a problem is a matter of 35,04 or 24 samples at once. And there, I suggest that it can be done in a parallel system that solves all the samples of data at a time.", 0.23589743673801422, 0.010362694039940834, 0.11282051354646683, 0.04139591380953789], ["I were using environments for months and they were working perfectly.. suddenly i can not execute any code in pycharm under any virtual environment and i get this error massage:\nfrom tensorflow.python.profiler import trace\nImportError: cannot import name 'trace' from 'tensorflow.python.profiler' (C:\\Users\\Nuha\\anaconda3\\envs\\tf_1.15\\lib\\site-packages\\tensorflow_core\\python\\profiler_init_.py)\nAny help please!!\nIt seams that it happens because i install more packages and maybe conflict occurs", "it was because environment conflict so i rebuild new environment and it works perfectly", "ImportError: the 'trace' from 'tensorflow.python.profiler'", 1.2000000476837158, 0, 68, "There is a conflict between the two packages (see what happens). You install two similar packages: ... ... .... ... .. ... a a aa ... b a ... you also install this package ... i ... d ... t .... .... ............ ...................................................... ..., you may install the package or the 4-70-500 ... and 5 ... 14 ... the same or ... this ... or you can also ...\nyou can buy ...", 0.08955223858356476, 0.0, 0.08955223858356476, 0.040536776185035706], ["Is it possible to find the max and min of both the horizontal and vertical convolution axis without going through and performing the actual convolution?", "You simply cannot skip doing the convolution altogether. There's no way to just bypass it. This scenario would be similar to trying to find the height of the Eiffel Tower without out already knowing it, looking it up, or measuring it somehow. Although convolutions can be slow on many machines, you will unfortunately need to perform the operation to get the minimum and maximum values.", "Find max and min of convolution without doing convolution", 0.0, 0, 69, "No, it's not possible\n\"Why?\"\nWe can measure the HV axis of the image, and the VH axis - but not the entire image at once, as that would take too much memory.\nThe same applies to the vertical HVF axis, which is only possible in a small number of pictures, at most 24...\nRead all the answers to \"How to do max convolution of an image without really doing it?\"", 0.277372270822525, 0.014814814552664757, 0.131386861205101, 0.0504750981926918], ["I have a column with the following format:\nOriginal format:\n\n\n\n\nmm/dd/YYYY\n\n\n\n\n10/28/2021\n\n\n10/28/2021\n\n\n\n\nthe output after:\nprint(df['mm/dd/YYYY'])\n0        2021-10-28 00:00:00\n1        2021-10-28 00:00:00\nHowever when I am trying to convert to datetime I get the following error:\npd.to_datetime(df['mm/dd/YYYY'], format='%Y-%m-%d %H:%M:%S')\n\ntime data mm/dd/YYYY doesn't match format specified", "You are passing the wrong format. Try\npd.to_datetime(df['mm/dd/YYYY'], format='%m/%d/%Y')", "time data mm/dd/YYYY doesn't match format specified", 0.1973753273487091, 1, 70, "...", 0.0, 0.0, 0.0, 0.0], ["df1 =\n\n\n\n\n\nname\nage\nbranch\nsubject\ndate of joining\n\n\n\n\n1\nSteve\n27\nMechanical\nAutocad\n01-08-2021\n\n\n2\nAdam\n32\nElectrical\ncontrol sys\n14-08-2021\n\n\n3\nRaj\n24\nElectrical\ncircuit\n20-08-2021\n\n\n4\nTim\n25\nComputers\nclouding\n21-08-2021\n\n\n\n\ndf2= [['name','branch']]\nprint(df2)\n\n\n\n\n\nname\nbranch\n\n\n\n\n1\nSteve\nMechanical\n\n\n2\nAdam\nElectrical\n\n\n3\nRaj\nElectrical\n\n\n4\nTim\nComputers\n\n\n\n\nNow I have two data frames,\nI need only name and branch columns and remove the remaining columns, all these operations should apply to the original df1.  I don't want separately df2", "Simply, Overwrite the df1 only\ndf1= df1[['name','branch']]\nor\ndf2= df1[['name','branch']]\ndel df1\nTo delete df1 or df2.\ndel df1\nor\ndel df2\nBased on requirement", "how can I get only one data frame or how to overwrite the data frame?", 1.2000000476837158, 1, 71, "The key was how did I set them up initially. This process, and my other experiences with Tables, helped me to learn and set up two tables initially and then merge them together.\nThis process helped to set things up. The key for me was not setting up tables for themselves initially but to also merge together the two separate tables. I thought initially that I had to be careful with both of them and not set the tables up separately but that idea of being careful was really helpful for being able to merge the Table together at the end. So, I was able initially to do a few things like how it is not going to affect how the different tables are going together in the future, whereas in this case they are in fact going in together, but the idea for this has helped a lot. Thanks for the great idea!\nanswer to questions 1, 2, & 3 above (questions)", 0.020942408591508865, 0.0, 0.020942408591508865, 0.036998189985752106], ["I need to compute a high dimension dataset, with clustering on Orange3 app. So, there's too many time spent to calculate the Distance Matrix between the objects. If I could use a graphic card for this tasks it will take much less time to complete the task. Anyone know, let's say, a workaround to do this?", "No. Orange uses numpy arrays and computes distances on CPU. Short of reimplementing the routine for calculation of distances (which in itself is rather short and simple), there's nothing you can do about it.\nOrange will start using Dask in some not too distant future, but until then try reducing your data set. You may not need all dimensions and/or objects for your clustering.", "Is there a simple way to use Oange3 with an Nvidia GPU?", 1.2000000476837158, 1, 72, "Yes, you can use NVidia GPU. You just have to install NVIADA. Follow these steps:\nInstall NVIDIA software for your GPU\nOpen NIVEADS and follow the next steps to start the app\nClick \"App\" and select \"G\". You can now choose your NIVERAD.\nThis app will support you with your calculations. It will work faster and it can even give you more accurate results!", 0.20000000298023224, 0.03125, 0.1230769231915474, 0.05801739543676376], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "The reason is to allow the option to overwrite the dataframe object you are working on, or to leave it unchanged by creating a copy and assigning it to a different variable. The option is valuable as depending on the circumstances you may want to directly modify the original data or not.\nThe inplace parameter is one way in which you have the power to choose between the two options.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 73, "You can use a function to make changes to the class of a PDSA-Data-Frame to keep it from becoming ineffective or unusable.", 0.21276596188545227, 0.0, 0.1489361673593521, 0.02970297448337078], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "Each class defines what changes can be done in-place and which can't, creating instead a new object.  The reasons are varied and can't be reduced to a few simple rules.\nThe underlying data structure of a list is designed for growth and shrinkage.  Even so some changes are cheaper than others.  append and pop at the end requires fewer changes of the data than addition or removal of items at the beginning or middle.  Even so, actions like blist = alist[1:] produce a new list.\ntuple is a variation on list that is immutable, and is widely used in the base Python for function arguments and packing/unpacking results.\nA numpy array has a fixed size. Like lists, individual values can be changed in-place, but growth requires making a new array (except for a limited use of resize).  numpy also has a view mechanism that makes a new array, but which shares underlying data.  This can be efficient, but has pitfalls for the unwary.\npandas is built on numpy, with indices and values stored in arrays.  As other answers show it often has a in-place option, but I suspect that doesn't actually reduce the work or run time.  We'd have to know a lot more about the change(s) and dataframe structure.\nUltimately we, SO posters, can't answer \"why\" questions authoritatively.  We can only give opinions based on knowledge and experience.  Most of us are not developers, and certainly not original developers.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 74, "You can re-do pandas by renaming your data. You also do it for a second data frame. What do you do? You do redo a data that you need in time for you.", 0.10563380271196365, 0.007092198356986046, 0.056338027119636536, 0.049136679619550705], ["Tensorflow is very heavy library , is there any way to save and load keras modeles(.h5) without using Tensorflow lib?", "keras framework is built upon tensorflow. If you want to use keras you will have to install tensorflow library.", "How to save keras models without tensorflow", -0.1973753273487091, -1, 75, "Yes, you can open a file and save the model.\nWe recommend that you store data in the file. We also recommend storing the information in another database. In general, the database is recommended for the store of information. This means that the software of TSA TA TSTORA STORORRR AND ASTASTORSORRS IS BROKEN. It has two-three-four-two-six-twothree-threds. You may not do this yourself. Let others do it. The 312 TTA TTT TALTAL TT T T DD D D DE B BDEB B. YOU CANNOT DO IT YOURSELF. LET OTHERS DO THE HIGH LEVEL OF STOUWWWAWAWS TW.A. AWAWAW OF WALKS ON WWALKWS ON THE WALL OF THE TIME OF THIS OF THIS TIMI. TIV TIV .\nThe wall of time of the wall is the hight of a wall that is seen by the people and this is a small wall. Some people do not want to see a highe wall or a low wall, so they do the opposite of this. Do you not know the high level of wall? Wall of Wall - W. O. WO. . . W . o. w. 54-44 . 46-33 . S . s. S.s . ss . SSs s s ss ss s . w . - - ww.w . HHH. HEALTH HHEALLEHEATLEATTING HHTHEATH . THEATHE . OOOONO-ONNON . NNNNNNON-NNUNNNUUUNUSSUSSSSSSUUUKKUKUKUKEKKOKEKE . KK . KEY KEY PEPEPEPEREPERERRERRE . RRREDRRDDODDODD . DA . AA. AA . A. NNNOSSO SOT HW HO HO WORK OWT WWTWORTWOTHROTROTAROTTING . LINGINGNGGNGGNGGGGNGGG. G . G. SSS. SA. K. P. E . P-PE- .", 0.038910504430532455, 0.007843137718737125, 0.038910504430532455, 0.04296858608722687], ["i am currently using a config file to train a model in tensorflow. Inside the config file, i need to specify a python path. Since im on windows, my paths obviosly looks like these r\"C:\\path\\path\\path. But when tensorflow is using the configfile, i get this error:\nfine_tune_checkpoint: r'C:\\path\\path\\path\\ckpt-0': Expected string but found: 'r' \nAnyone has encountered a similar problem?", "Looks like it tripped because there was an r written outside of your quotation marks. I'd try to delete that and see if it works, or if the r is in your path, add it within the quotation marks.", "Python path problem: Expected string but found: 'r'", -0.3799489736557007, -2, 76, "2027. February 4, 11:23 pm. March 7, ... ... August 8, .... August ... .... September ... October 30, October ... November 9, November ... December 6, December ... January 06. 50 years and ... nd ...\nDecember  ... April \u200b07 ... May \ufeff \u00ad \u0093B \u0092. December\u0092 ... February\u0092\nFebruary \u00a9 January ... June ... July \u00ae ... September\n... ... - ...- November -21 -Decemb", 0.024390242993831635, 0.0, 0.024390242993831635, 0.012658230029046535], ["I have a CSV file containing list of postcodes of different persons which involves travelling from one postcode to another for different jobs, a person could travel to 5 postcoodes a day. using numpy array, I got list of list of postcodes.  I then concatenate the list of postcode to get one big list of postcode using a = np.concatenate(b), after which I want to sort it in an alphabetical order, I used : print(np.sort(a)) is gave me error error AxisError: axis -1 is out of bounds for array of dimension 0\nI also tried using a.sort() but it is giving me TypeError: '<' not supported between instances of 'float' and 'str'\nPlease, can someone help", "Looks like you're passing in both floats and strings into your list.\nTry converting the values in b into a float before you concatenate them.", "Concatenating and sorting a list of list array", 0.0, 0, 77, "Post, reply, or comment...\nHoops, Hoop's, Hops and Hips\nThrill, Trill and Tails\nApril, 2018: The Th...", 0.09090909361839294, 0.0, 0.09090909361839294, 0.04651162400841713], ["I am currently on Linux x86-64 machine. I am trying to install opencv in my virtual environment using pip however the error I keep receiving is\nERROR: Could not find a version that satisfies the requirement numpy==1.19.3 (from versions: 1.19.2+computecanada, 1.21.0+computecanada, 1.21.2+computecanada)\nERROR: No matching distribution found for numpy==1.19.3\nI am running python 3.9.6 (64bit) and my current numpy version is 1.21.3. the command I've been using is pip install opencv-python. i've also tried uninstalling other instances of openCV and have tried the other options all with the same error. Does openCV not support numpy 1.21.3? Would anyone be able to point me in the right direction?", "Actually, this error happens if numpy version does not match OpenCV required version.\nfor my case:\nI used python 3.6. so I solved this error by following:\n\npip install numpy==1.19.0\npip install opencv-python==3.4.11.45\n\nafter installing numpy I search which OpenCV version support this numpy version, I found 3.4.11.45 so I install it by 2 number command and it is working.", "Installation issues using pip for OpenCv", 0.0, 0, 78, "You should install OpenCV with a compiler version. For example, you can install the OpenCL tool with OpenCC 4.5 (2015). The OpenBCD tool you should use the version 2.8. The version of the BSDs and the tools is not important, it is important to the compiler. You can use OpenBS 5.7. with your version to OpenBSD.\nOpenBSV 6.4\nA lot of people talk about the \ufffdversion\ufffd of 0\n0\ufffd 77\ufffd78\ufffd\n79\ufffd.", 0.22972972691059113, 0.04109589010477066, 0.13513512909412384, 0.08010094612836838], ["I built a box-embedding model on the latest wikipedia articles dump and i need to compare it with the word2vec model in gensim. I saw that if i generate the corpus data as a txt file using get_texts() method in class WikiCorpus there are a lot of stop words, so this make me think that WikiCorpus doesn't delete stop words isn't it?. Now once trained my box model on the wiki corpus txt i notice that calling the \"most similar\" function that i create appositely for box embedding prints very often stop words, instead the same word passed to the most similar function of word2vec model trained on the same corpus txt produce best results. Can someone suggest me why Word2vec model fit so well despite the corpus txt have a lot of stop words instead my box model on the same corpus not?", "How did you train a box-embedding, and why did you think it would offer good most_similar() results?\nFrom a (very) quick glance at the 'BoxE' paper by Abboud et al (2020), it appears to require training based on a knowledge base representation \u2013 not the raw text that'd come from WikiCorpus. (So I'd not necessarily expect a properly-trained BoxE embedding would have 'stop words' in it at all.)\nAnd, BoxE appears to be optimized for evaluating potential facts \u2013 not more general most_similar rankings. So I'd not expect a simple most_similar listing from it to necessarily be expressive.\nIn usual word2vec, removing stop-words isn't very important and plenty of published work doesn't bother doing so. The downsampling of highly-frequent words already tends to ignore many stop-word occurrences \u2013 and their highly diverse usage contexts mean they are likely to get weak word-vectors not especially close to other more-narrow-meaning word-vectors.\nSo in usual word2vec, stop-words  aren't very likely to be in the top-neighbors, by cosine-similarity, of other more-specific entity words.", "does WikiCorpus remove stop_words in gensim?", 0.0, 0, 79, "because the 54 046-400-0360 is a new edition we have all the information on one page so we don'? know what the previous information is so it would be more convenient for us to put the whole information into the one book instead of two books.", 0.14406779408454895, 0.008547008968889713, 0.10169491171836853, 0.03999177739024162]]}