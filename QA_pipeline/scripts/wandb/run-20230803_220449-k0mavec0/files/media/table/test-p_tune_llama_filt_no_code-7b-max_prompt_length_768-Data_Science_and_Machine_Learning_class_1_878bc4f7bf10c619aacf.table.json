{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have a dataframe where the indexes are not numbers but strings (specifically, name of countries) and they are all unique. Given the name of a country, how do I find its row number (the 'number' value of the index)?\nI tried df[df.index == 'country_name'].index but this doesn't work.", "Why you don make the index to be created with numbers instead of text? Because your df can be sorted in many ways beyond the alphabetical, and you can lose the rows count.\nWith numbered index this wouldn't be a problem.", "How do I find the row # of a string index?", 0.0, 0, 80, "Hey! Good news! The row numbers for each column are stored in a separate table. The rownumber in the table is the number of columns for the given row!\nAnd good news: We don'tdoanythingswith thesocialmedial. No data. That'sdangerous. It'sdangeroustable! So, yeah! Let me go!", 0.20224718749523163, 0.0, 0.11235955357551575, 0.08791209012269974], ["I have a pandas Timestamp column that looks like this:\n2021.11.04_23.03.33\nHow do I convert this in a single liner to be able to look like this:\n2021-11-04 23:03:33", "use a regular expression by looking for the hour and minute second  pattern (\\d{4})-\\d{2}-(\\d{2})\\s+(\\d{2})_(\\d{2}).(\\d{2}).(\\d{2}) and use re.findall then get each group part then reassemble the datetime stringthen convert to a datetime", "Pandas Timestamp reformatting", 0.0, 0, 81, "1 answer:1 question:2 questions:4 questions with an answer or a question? 3 questions?\nWhat will you do if you can? You have some time to do this so...\nSo what can you say? So you should think about doing this, I think it makes sense to make the process\nMake the\nprocess timeless and so timeliness\nTimeless\nAnd so you might think\n... about you. So what?...? (\nPost: (the Post) )\nTop Post: )...?)\n( ) Post......)\nOOSS\nSSSS", 0.13008129596710205, 0.0, 0.08130080997943878, 0.029040956869721413], ["I'm doing PID gain tuning for a DC motor\nI gathered real data from the motor which involve the position according to time.\nAnd i want to calculate the rise time, overshoot, and settling time from the data.\nIs there any function in matlab or python which can do this?\nThank you!!", "In the cases that you use the step command to extract the step-response characteristics of the system, the stepinfo command calculates the rise time, overshoot, and settling time, and so on. I don't know whether it is applicable in the data case or not but you can test it?", "Is there any function that calculates the rise time, overshoot, and settling time?", 0.0, 0, 82, "ANSWERS WORK BOOOCHOH!?!???!?????\nRepublic of India!Born in India on November 22, 1930!Must have worked on that!\n... MUST HAVE WOOKKKNNN!HAAA!L...LASU...SSSSUSUUSUSVIVIVVIWW...HAPHALTHAPHAHHAAPAAPAPAC...", 0.10389610379934311, 0.0, 0.051948051899671555, 0.03804348036646843], ["On google colab I installed conda and then cudf through conda. However now i need to reinstall all packages like sklearn etc which I am using in my code. Is there some way to install cudf without conda ? pip no more works with cudf. Also if there is some other similar gpu dataframe which can be installed using pip, it will be of great help.", "No, cudf is not available as a pip package.\nYou don't say why you need cudf, but I would try pandas, dask or vaex, surely one of those will do what you need.", "Install cudf without conda", 0.1973753273487091, 1, 83, "The 5 most asked questions on Quiet Quo\n5 questions you should have answered! The...\n7 questions of the 2013s! How would you...?", 0.14035087823867798, 0.0, 0.10526315867900848, 0.01886792480945587], ["While writing in CSV file, automatically folder is created and then csv file with cryptic name is created, how to create this CSV with any specific name but without creating folder in pyspark not in pandas.", "That's just the way Spark works with the parallelizing mechanism. Spark application meant to have one or more workers to read your data and to write into a location. When you write a CSV file, having a directory with multiple files is the way multiple workers can write at the same time.\nIf you're using HDFS, you can consider writing another bash script to move or reorganize files the way you want\nIf you're using Databricks, you can use dbutils.ls to interact with DBFS files in the same way.", "How to write in CSV file without creating folder in pyspark?", 0.0, 0, 84, "This example was created for pynsparks (PYS_580_14_S4).\npyyspark_s4-2989_2018-12-5-49-97-s-63-77_-__433_35_f-c-f06_-.f_00-_.f.1.b.o.a.y.p.s.5.0.f-.p-o-.1-.3.34.c.t.h.n.d.4-.a-.o-n-.d-.4_.\n3\n4\n5\n6\n7\n8\n9\n10\n.F0:.B1:B3:02\n2", 0.03726708143949509, 0.0, 0.03726708143949509, 0.014778322540223598], ["For an assignment I have to write some data to a .csv file. I have an implementation that works using Python's csv module, but apparently I am not supposed to use any imported libraries...\nSo, my question is how I could go about doing so? I am no expert when it comes to these things, so I am finding it difficult to find a solution online; everywhere I look import csv is being used.", "I guess that the point of your assignment is not to have some else to do it for you online. So a few hints:\n\norganise your data per row.\niterates through the rows\nlook at concatenating strings\ndo all above while iterating to a text file per line", "How to write to a .csv file without \"import csv\"", 0.06656807661056519, 1, 85, "Import .scvi is not a requirement for any programming language. It'l possible that the code doesn't use an import for import or anything like that, it could also be possible the application didn' t use the import. Importing the module to be used is, however, a need for the programming... Read more...Read moreReadReadMoreRead More Read MoreRead Read ReadMore Read LessRead LessLess Read... Less Read Less...LSSOYRead... More LSSORadSORSIDIDSIdiIdididIdId2IdIsIsisisaaasa4sa5s4sss55ss5ReadsRead....Read S.......S....S........S.....S...OO....O...0O0.........Reading... Reading........READING\nReadING...READING....\nWORODWORT...WORRDWORTWORWWWA....A....WAaWa....\ufffdV... VW.... WW...VW.\ufffdWV....V\ufffdC\ufffd\n.....\ufffdCC........C\n........CCCCA............CC..C..CC.CC...CCC.....CCC...\ufffdP....PPP...PPPP\n..........................................\ufffdPP\ufffd\ufffd\u00c2AP\u00c2\nPP \ufffdAAP\n. . . ........A .\n.... 00 2\n...TREADS\n0323385TADADAPAAARA\n18847811AAAR \n-808 3948305\n40973643763 45 54\n504 61277\ufffd16929\ufffd-342025\ufffd262\ufffd.282.890\n213158\nL. S\nread more read less 1 less read \u00c2\u00c2 \u00920C \u00c3C0 C C\u00c3 C0CC CC CC CCC C S S SS S SA S0 SO \u00ad\u00ad 7 8 A A H H \ufffd 9 \u0093 \u0094\n9 HHHHHH \u0091\u0092 \u00b4 \u201a \u00c4\u00c4\u00d6\u00c4\u00c5\u00d6WOWOWSOWASAWAS\n\u00c4AWA\u00c4W\u00c5O\u00c4WA\u00c5\u00c5\u00c4O\u00c5\nA B\nB", 0.09909909963607788, 0.0181818176060915, 0.06306306272745132, 0.029556650668382645], ["I have a time series in which i am trying to detect anomalies. The thing is that with those anomalies i want to have a range for which the data points should lie to avoid being the anomaly point. I am using the ML .Net algorithm to detect anomalies and I have done that part but how to get range?\nIf by some way I can get the range for the points in time series I can plot them and show that the points outside this range are anomalies.\nI have tried to calculate the range using prediction interval calculation but that doesn't work for all the data points in the time series.\nLike, assume I have 100 points, I take 100/4, i.e 25 as the sliding window to calculate the prediction interval for the next point, i.e 26th point but the problem then arises is that how to calculate the prediction interval for the first 25 points?", "A method operating on a fixed-length sliding window generally needs that entire window to be filled, in order to make an output. In that case you must pad the input sequence in the beginning if you want to get predictions (and thus anomaly scores) for the first datapoints. It can be hard to make that padded data realistic, however, which can lead to poor predictions.\nA nifty technique is to compute anomaly scores with two different models, one  going in the forward direction, the other in the reverse direction, to get scores everywhere. However now you must decide how to handle the ares where you have two sets of predictions - to use min/max/average anomaly score.\nThere are some models that can operate well on variable-length inputs, like sequence to sequence models made with Recurrent Neural Networks.", "Interval Prediction for a Time Series | Anomaly in Time Series", 1.2000000476837158, 0, 86, "2068 and two to the BUTCHOWN. It is the 9th series that BS 830 or other BN 3404 and 4-6467.6605 in 03-8-4 6-9628 that was the HS-PORTSCHOLF 75-2,72-55,1, that is, if 50,533, or \ufffd238, for these series, and for some other. If the series can be measured. However the CMAs for this type of a particular group have been given to a considerable degree of the type. \u00c309, \u00c2, can and other have not been measured for years for them. They were built in a series of several, all, the, but in all of, there is no other type or type made by the two-year-and-s-v-0.39363.186.297-3135\u00c2s. For this I saw a long. He saw. This type had the c15 and c278\u00c2AMS3 of.4.A3748.and A nd of2454 with \u00945\u0094 of that type and was. See \u0093Fur.\u0094 I was also with the three-living type that. In the third series (12 \u00b3of \u00ad\u00c232)443A \ufffd\u00c2a\u0092of this series was made for two of those. And \u00914 of more.\u00922 and all kinds of this will be discussed in chapter four \u0092. You could have made a cococount for yourself. We could not have seen an invisible tratha and that could be a good and long type \u00ef\u00c4\u0092, you could make an \u00c401\u00c2\u0094.\u00e29.\u00c2\u00c22A82.73 type was \u00c6142 type\u00c2\u00c4. But the size of it is \u00a221.0 \u00b8\u00c4C.CORRODAS CROMMFLED AND THE COWOOCRO.F.56DOTORA58 BOCA.M.L.MA.D4192\u00c2779 and the other is in. A.\u00c457", 0.2216748744249344, 0.019801979884505272, 0.10837438702583313, 0.038453053683042526], ["I have a list of different expressions. It looks like this:\nmy_list = [[1 ,2 ,'M' ,2], [1 ,2 ,'A' , 1], [1 ,2 ,'g' ,3], [1 ,2 ,'o' ,4]]\nI want to sort the list. The key should always be the first entry in the list, in this case the book positions A, M, g, o. However, upper and lower case should be ignored.\nIn python I used:\nmy_list.sort(key = itemgetter (3))\nOutput is:\n[[1, 2, 'A', 1], [1, 2, 'M', 2], [1, 2, 'g', 3], [1, 2, 'o', 4]]\nThe problem is that in my result the uppercase letters are sorted first and then the lowercase letters. How can I make lower and upper case letters sort together? The result should look like this:\n[[1 ,2 ,'A' ,1], [1 ,2 ,'g' ,3], [1 ,2 ,'M' ,2], [1 ,2 ,'o' ,4]]", "Use key=lambda lst: lst[2].lower().", "python sort multi-dimensional lists CASE INSENSITIVE", 1.2000000476837158, 1, 87, "you have 8 different characters in your list so just: `mylist=sorted(mylists)` `and you will have them sorted too.", 0.0, 0.0, 0.0, 0.0], ["I'm building a model to identify a subset of features to classify an object belong which group. In detail, I have a dataset of 11 objects in which 5 belong to group A and 6 belong to group B, each object has been characterized with a mutation status of 19,000 genes and the values are binary, mutation or no-mutation. My aim is to identify a group of genes among those 19,000 genes so I can predict the object belongs to group A or B. For example, if the object has gene A, B, C mutation and D, E gene with no mutation, it belongs to group A, if not it belongs to group B.\nSince I have a large number of features (19,000), I will need to perform feature selection. I'm thinking maybe I can remove features with low variance first as a primary step and then apply the recursive feature elimination with cross-validation to select optimal features. And also don't know yet which model I should use to do the classification, SVM or random forest.\nCan you give me some advice? Thank you so much.", "Obviously in a first step you can delete all features with zero variance. Also, with 11 observations against the remaining features you will not be able to \"find the truth\" but maybe \"find some good candidates\". Whether you'll want to set a lower limit of the variance above zero depends on whether you have additional information or theory. If not, why not leave feature selection in the hands of the algorithm?", "Is it reasonable to use 2 feature selection steps?", 0.0, 0, 88, "04132 is a DL model.150.923060DJU 3, 440,6,5,3/189 and KKV\u00c45.D0-FU1\u00c443.293\u00c3\u00c3, the first of a c.c. 99/39-4.4-6.37-38-10/46-20 and a 790124C\u00c55\u00c2\u00c5C\u00c2.B\u00c5.C.\u00c26953566\u00c2C\u00c4.68.E8, A-GD, and all other.7.8742,2D\u00c4\u00c4\u00c2\u00c4, with the.Opp.t731.W.G.A.V2A6AH651\u00c2\u00c2O.L.\u00c4STM.Mort.\u00c35\u00c3P47P0T. AH26/\u00c4P\u00c2L\u00c4\u00c3\u00c4WKA-\u00c42.\u00c5K.5L2-Aw\u00c3-.RM-545 and AMPA02/278\u00c2F\u00c21G\u00c4C14OV\u00c3G\u00c23-V-091\u00c32C-N\u00c3.Y.K\u00c3362M\u00c4A DW3RK\u00c4F. F\u00c3CMA\u00c43\u00c40703V.CCOU33F\u00c49\u00c31,S\u00c4KKO\u00c5\u00c4M,\u00c4175KD\u00c5-R\u00c5,1\n3\u00c276\n\u00c50\u00c22KL\u00c5\n4G\n\u00c4LHK\u00c5T\u00c4UV\nCANE\u00c4R\u00c4D\nD\u00c3T\u00c5L\u00c3LOR\u00c4OMAF6K\u00c2K, WORKERS,4\u00c2D2\u00c2PACWO\u00c4\nH34 and\n0A3AOODDOOR,L1C4/F\u00c5\u00c2,O\u00c2B\u00c4\u00c5\\\\C3M1\u00e2\u00c5O4\\\\\nKP\u00c4849\u00c4ZSURKID, F, H, MAO3OE\nL,A8)C6Ho\u00c4#,f6\u00c4s\u00c4BW\u00c47\u00c4EW\u00c3A\u00c4V\u00c54\n1", 0.06956521421670914, 0.0, 0.052173912525177, 0.026109661906957626], ["I deployed Apache Spark 3.2.0 using this script run from a distribution folder for Python:\n./bin/docker-image-tool.sh -r <repo> -t my-tag -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build\nI can create a container under K8s using Spark-Submit just fine.  My goal is to run spark-submit configured for client mode vs. local mode and expect additional containers will be created for the executors.\nDoes the image I created allow for this, or do I need to create a second image (without the -p option) using the docker-image tool and configure within a different container ?", "It turns out that only one image is needed if you're running PySpark.  Using Client-mode, the code spawns the executors and workers for you and they run once you create a spark-submit command.  Big improvement from Spark version 2.4!", "Two separate images to run spark in client-mode using Kubernetes, Python with Apache-Spark 3.2.0?", 0.0, 0, 89, "the main script-script-sc-n-p-85-the-latter-portion-of-th-d-o-1663-00-k5.k.o.n.w.a.s.t.m.h.u.d.1.b.3124-f.5.-.c-27.the.f-3506-a-b-4.4.-495/the.-.--c.-spark-i-w-6.-a.-h-5617-s-parki-v-h0s6.8.p.9.7./-[.sp2.-.]soul-stang-spark-99-m/p/.-q-for-less-than-in.6./q.-6458/hq.and.rqw.-1840. . .-. .5w [.q]a [] [o .s\u00c2\u00c2 [\u00c2. [ .c2b .\u00c2\u00c43 .H-L-A-B-C-\u00c2-\u00c42\u00c2B .L\u00c4-\u00e2-K2 B-V-O -\u00c4V \u00c2\u00c3 1 .-P-PA-BOO-FU-T-DONT 2-W\u00c2ST\u00c2CRED]\u00c209[B_C_B]OO[L255]1/w23w\n347\u00c2s\n5W-\na\u00c2a\u00e2 4\n\u00c24 .1 8 0 7 6/ 5 9/6 [.] \u00ads\u00adp\u00ad\u00adC\u00adO_\u00adH3_5_W_H_2_P_PA_BO0_E\u00c2U_V_O \ufffd\u00c2\u00c5B\u00c2AA-H\u00c2 \ufffd\nS-\u00ad\nw_S\n[P\u00c3\u00c2]410\n\u00e2w \u00c3\u00c454\u00c2HU\u00c4\n8W\nW\u00c4\u00c4H\n0 . \tH [].\nJrJ [J\u00c21]\nO33 [P]H \u009330 [3, \u0091H\u0092\ufffdA [1-\u0092-\u00915 [43] of the of \u0092S15, the BS5", 0.07894736528396606, 0.0, 0.061403509229421616, 0.008658009581267834], ["I have an array\narr = np.array([[1,1,2], [1,2,3]]).\nI want to get amount of unique element for each row and count mean\nI can do this np.array([len(np.unique(row)) for row in arr]).mean().\nBut seems, that it's a slow way. Is there another faster approach?", "set(arr.flatten()) will create your desired result. Not sure about how fast it is though.\nOutput:\n{1, 2, 3}\nEdit:\nYou wanted the number of unique elements, so you wrap the whole thing in len()", "Get amount of unique elements in numpy", 0.0, 0, 90, "https://python.t-25.2.95?#2?2#3?3#7#6#5\nPost:\nA lot of work going into creating new and unique designs and creating the design that speaks out and out loud for those who really care about their work, which is something.\nThe last series, of which the last, and the series was the most of the beautiful, beautiful. The collection was named after the two-year series: A series of designs is a series that is very beautiful and very long. A long collection will be the collection of very unique and beautiful designs. With your own hand and pen, the hand of wood and paint is ready to put the designs that are very unusual and interesting, so that the long and unusual collection can be a collection that can change, be, or be ... or the bea ... ... 800 Series and 50 ... Series of ... series and more. 2018 collections series more more ... collection more about the special and creative collections more special collections collections and collections from the designer collection. We hope to be able to give you some of these special-collection designs from some ... of a more-special-collaborative-design-from-a-more-unique-and-fashionable-long-hollow-collections-the-shock-of-new-stuff-in-these-s- ... S S C C CO C CC COC ... C ... DD ...DDD... DDA D D A D AD A ...\n\u00c4\u00c4\u00c5\u00c4\nS\u00c5\u00c5S\u00c4SJ\u00c5J\u00c4H\u00c4HH\u00c4PPP\u00c4POPP\u00c4PR\u00c4SA\u00c4\u00c2\u00c4OO\u00c3\u00c4\u00c3O\u00c4OF\u00c4A\u00c4V\u00c4VS\u00c4WE\u00c4W\u00c4VERSE OF INTERSESS\u00c4N\u00c4SSSS\u00c2SATA\u00c4ST\u00c22-OA\u00c2O\u00c2OR\u00c23-S\u00c268-6-\u00c2B-R-5-N\u00c2NN\u00c4NNNN-NNWW\u00c2W_\nW W\u00c4UWG\nNONNN\nNNNO\nEER\u00c4K\u00c4KS\u00c456\nH\u00c50H4H3\u00c54\u00c5\n\u00c55 14\nL\u00c4LLE\u00c4 \u00c2T\u00c430 35\u00c53H\n\u00c3\u00c528\u00c378899323\n33 \u00c3", 0.07831325381994247, 0.0060606058686971664, 0.05421686917543411, 0.030674846842885017], ["For example, how many Mb will be required for EfficientNetB3? On drive model weights require 187 Mb of memory, does it mean that when the model will be loaded on GPU, it will use 187 Mb of GPU memory?", "It's difficult to calculate total, but you can estimate a minimum to just load a model, which would be roughly the model size. Tensorflow, for example, defaults to reserving 100% of the GPU memory. You can set limits, but the amount of memory to be used is based on many things, such as number of layers, input image size, batch size, etc.", "How to evaluate the required GPU memory for running neural network models?", 0.0, 0, 91, "Yes! The \u201cNe\u201d model requires at least 32 GPS!\nRead more! \u2026 \u2026\nWhat is the \u2018B\u2019 neural \u2026 what does \u2018Ne\u2019 \u2026 read \u2026 more \u2026 of course! What are the \u201cB\u201d models!?! ... How many MBs do we need to run? How much \u2026? ... ...", 0.15841583907604218, 0.0, 0.09900990128517151, 0.046159226447343826], ["How to get pandas dataframe when select only one column? In R there is drop = False for that.\nWe can use pd.DataFrame(df['breakfast']) or df[['breakfast']], but do we have smth like drop = False as it in R?\nPS: press F for breakfast)", "I think you are looking for something like index=df.index.\nThe question is a bit broad", "How to get pandas dataframe when select only one column? In R there is drop = False for that", 0.0, 0, 92, "yes, we can do this\nanswer\nComment\nHow to use this as the main column, say? It\u2019s all about one day\nThat\u2019d be all right\nYes, how to go? (He went so!)\nBachachchach, (2014) the big chach\nChachhachcheche? I\u2019m gonna try that! (The 2-3!) that will get on his\nTo tell the\nStory, to tell a lot of a\nTototheory of how we tell? Tell a story, and how you tell it?!\nI got to try to\nY\u2019know how he tells it\nThe story of\nWhattheheck? Is that how? Let me tell you that I told? That\u2019\nIt was so great to have him, that he told it. So I am telling it! That was good, let me see it again. That\nWas wonderful! Wonderful! Let\nThem talk about that? So let\u2019th\nAbout it that, okay? He was great! It was wonderful, it was a great story to make it so wonderful. The\nPeople talk\nSometimes you have\nLost, sometimes you can\nWhat about the other times? Can you\nCan you talk to people? Sometimes you\u2019ll talk. Okay? Okay, but can you make the times so? The story\nof a man. I got\nHo how does it work? Does it ever work again? Because I love him. Because\nHe\u2019re a little bit of the story. He\u2019rs\nA little story about how it works? No, I think it will ever be\nWorks? Work! How many\nDreams do you? You can dream of all the stories. You say,\nWhy can\u2019t\nThey do it, like they did it for the movie? They did that for\nMore than 30! More than\n3! Tell about more than three\nStories! Yes, tell about, more stories, than, three stories! Three stories is not enough! You\u2019ve\nDo you know that they do that now? Do you think that about three? How about\nThree stories are not\nEnough for three! I thought three was enough. And three is a number. It's a whole number! But it doesn't work for you! So\nWhen you got the numbers, you had", 0.054200541228055954, 0.01089918240904808, 0.0379403792321682, 0.04159734398126602], ["I'm new to OpenCV and trying to use SIFT and SURF for a project.\nOn my laptop I have OpenCV version= 4.5.1.48 and also added OpenCV-contrib-python of version 4.5.1.48\ncurrently the problem I'm facing is the error I'm getting after following the documentation SIFT works perfectly after following documentation but SURF isn't working and giving me error for following codes\ncode 1\nsurf = cv.xfeatures2d.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'xfeatures2d'\ncode 2\nsurf = cv2.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'SURF_create'\nAfter reading many answers on Stack overflow I changed version of OpenCV and did many things but nothing is working for me\nI'm new to this please someone guide me through this\nI read about the patent expiring too but nothing is working in my case pls tell me if im wrong somewhere\nThanks in advance", "For patent reasons, opencv 4.5.1.48 does not include the whole algorithm\nYou can use Python3.6 (or Python3.7 maybe OK) and install opencv-pyhton==3.4.2.16 and opencv-contrib-python==3.4.2.16, then you can use the function that:\nsurf = cv2.xfeatures2d.SURF_create()\nor\nsift = cv2.xfeatures2d.SIFT_create()", "AttributeError: module 'cv2.cv2' has no attribute 'SURF_create' , 2. module 'cv2.cv2' has no attribute 'xfeatures2d'", -0.0996679961681366, -1, 93, "These two Open CV modules have the same functionalities and the other modules (which include the following: the optical, the face, etc) but they are different. The first module contains two modules; Open and cv:2cv. Opencv is a module of software that is designed to be used to manage images; the cvmodule is an application program that helps to run images by using all the features of images, and;\nThe second module is used by a number of specialists who help to determine the image; it is also a software application module that can be designed as an image management tool for open source code. We therefore conclude that both modules are the most important and efficient software applications available today. In your case, it can help you to achieve your goal. You therefore do not have any problem.", 0.13131313025951385, 0.010204081423580647, 0.07070706784725189, 0.0411377027630806], ["w2v = gensim.models.Word2Vec.load(\"w2v.pkl\")\nI am using this method to load pickle file through gensim but encountering an error.\nAttributeError: 'dict' object has no attribute '_load_specials'", "If you saved the model using pickle, you should be using some form of unpickle to re-load it. (Gensim has a utility method for unpickling a file containing a single pickled object at [gensim.utils.unpickle][1].)\nGensim's per-instance .save() and per-class .load() methods are a custom save/load protocol that internally makes use of pickle but does other things (& perhaps spreads the model over multiple files) as well. You should only Word2Vec.load(\u2026) a filename that was previously created by code like w2v_model.save(\u2026).", "Load pickle file in gensim", 0.1973753273487091, 1, 94, "15\n29\n30\n18\n53\n44\n66\n77\n979988900", 0.0, 0.0, 0.0, 0.0], ["I am working in Google Cloud Platform and I am trying to use Pyspark to convert a csv file into an avro file. I have seen a lot of websites but I haven't been able to implment the solution. Thank you in advance. :)", "You can read the csv file into a dataset/dataframe using spark and use databricks library to write it as avro. Something like:\ndataset.write.format(\"com.databricks.spark.avro\").save(\"your output path\")", "How to convert a csv file to an avro file using PySpark?", 0.0, 0, 95, "Thank you! You are really helping me. The issue that I face is that the program is not compatible with Linux. As such, you should not work with Google.\nYou should work in Hadoop and HADOO! Thanks!\nSarah is the best! I love the HAAP!HAH!HOOHAHA! HOW ARE YOU SOOAHAOOH! HOOHOHOHAHOH!!!!!!!\nI love you and your work! Good! And good! That! What!?!????!!? What? Why? This question is so good, I think that this should have answered in a simple question and answer! Yes! Thank me! Answer me on! Which? Which question do you have? I asked \"How do I?\" How do the questions? \"\nHow to get answers on qu\nwhich is\nquizzzzzzzzzezezzzezhzhaa\nhow to\nanswer the question\nquestion how to make money\nthat how can\nmake money using\npyswawawwwswwwwnpwnlwdwnpdwdwowownwownwnwocococcoccocacachacasachasheasathheathatheatr\nthe the theater plays\nthey\nwhat about the money? Yes!! You! \"!\"\"!\"!\"\n\"how can I make\nmoney using pysaw\nsawwawsawwrwrwwrwpwn\nwowsawwnsowwnwnow\na 2019\n#11335\nfurniture 100% furniture166995658902\nFurniture 0 509 96061\n2983 4 8\n0-0\nThe Birth Of\nNew! PYSWAS!WW!! WW!!!PWSWSS!!!W...WOWEW0W30...40!5!2!3!4...6...7\n855771804!8!9!1!05 6-6\n67-970863-46!-7!6!!!3!!4!!5!!6!!8!!\n...\n54512534?5?6?8?9?1", 0.07531380653381348, 0.0, 0.04184100404381752, 0.050695907324552536], ["I'm trying to extract project relevant information via web scraping using Python+ Spacy and then building a table of projects with few attributes , example phrases that are of interest for me are:\n\nThe last is the 300-MW Hardin Solar III Energy Center in Roundhead, Marion, and McDonald townships in Hardin County.\nIn July, OPSB approved the 577-MW Fox Squirrel Solar Farm in Madison County.\nSan Diego agency seeking developers for pumped storage energy project.\nThe $52.5m royalty revenue-based royalty investment includes the 151MW Old Settler wind farm\n\nHere I have highlighted different types of information that I'm interested in , I need to end up with a table with following columns :\n{project name} , {Location} ,{company}, {Capacity} , {start date} , {end Date} , {$investment} , {fuelType}\nI'm using Spacy , but looking at the dependency tree I couldn't find any common rule , so if I use matchers I will end up with 10's of them , and they will not capture every possible information in text, is there a systematic approach that can help me achieve even a part of this task (EX: Extract capacity and assign it to the proper project name)", "You should be able to handle this with spaCy. You'll want a different strategy depending on what label you're using.\n\nLocation, dates, dollars: You should be able to use the default NER pipeline to get these.\nCapacity, fuel type: You can write a simple Matcher (not DependencyMatcher) for these.\nCompany: You can use the default NER or train a custom one for this.\nProject Name: I don't understand this from your examples. \"pumped storage energy project\" could be found using a Matcher or DependencyMatcher, I guess, but is hard. What are other project name examples?\n\nA bigger problem you have is that it sounds like you want a nice neat table, but there's no guarantee your information is structured like that. What if an article mentions that a company is building two plants in the same sentence? How do you deal with multiple values? That's not a problem a library can solve for you - you have to look at your data and decide whether that doesn't happen, so you can ignore it, or what you'll do when it does happen.", "Information extraction with Spacy with context awareness", 0.0, 0, 96, "4376-44-1-Y-Roft_Hrt_Ro_Brown _Dod_67\u00c2D\u00c2\u00c2K4239_Dave_R_Wo_\u00c4_V_A_299\u00c2\u00c4\u00c4V\u00c4\u00c2PV1\u00c280\u00c3\u00c49\u00c3A HR\u00c4T\u00c4\u00c5245\u00c2FHRTS\u00c42\u00c2-3-202-S4\u00c22,12 and33\n1,038,94, 226\n-VW\n#C 08\n8-C\u00c240AHAOO-K\u00c4SHER\u00c4116 H\u00c4B\u00c4CH\u00c4L\u00c4-\u00c4R\u00c5-CHOOT\u00c4KO\u00c4WOON \u00c4CAR-JOUNN\u00c4OVOL\u00c4F\u00c4E-MAD\u00c4A\u00c2C\u00c4\nW\u00c4 \u00cb \u00c6\u00c4\ufffdFREK\u00c5O\u00cbS0OSE2C-5-s\u00c44+\ns=w+t-0_\nw\u00c431 H\u00c3M\u00c4w835 \u00c3E\n3 9/3O \u00c2.WV605L093_5\n0\u00c4U\u00c46ODD-\nU-OE2POWERFACOMFEN48+-H0S\u00c536A\n54.41.1706/07.90\u00c2LH_P_S2_fE3L\nVit8W/P5O46,8_049\u00e2\u00c48A.63.8O\u00c234_9AZ_YWL_CA\u00c26\u00e2\u00c2AWB\u00c2O\u00c3F\u00c2_KL\u00c3_J3G\u00c213\u00c2\u00c31OIHYO186\u00c2HKY0F2W\u00e2TCC\u00c4N-UW-B-FWUB4YE\u00c3CC\u00c27/HU3/JOU\u00c2W\u00c2R\u00c2WE\u00c2PA\u00c4J\u00c4Z\u00c4PARO-MKA-W\u00c5TW0V5W_Q-CO_C4WEN_1_GAA_PA_COO_CH\u00c2M_TOLG\u00c4\ufffd\u00c2S\u00c2UV\u00c2Z", 0.04117647185921669, 0.0, 0.04117647185921669, 0.020408160984516144], ["Suppose you have a pandas.DataFrame like so:\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID1\n14.5\n0\n0.32\n...\n\n\nID2\n322.12\n1\n0.94\n...\n\n\nID3\n27.08\n0\n1.47\n...\n\n\n\n\nMy question is simple: how would one select rows from this dataframe based on the maximum combined values from two or more columns. For example:\n\nI want to select rows where the columns Feat1and Feat3 have their maximum value together, returning:\n\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID2\n322.12\n1\n0.94\n...\n\n\n\n\nI am certain a good old for loop can take care of the problem given a little time, but I believe there must be a Pandas function for that, hope someone point me in the right direction.", "You can play arround with:\ndf.sum(axis=1)\ndf['row_sum'] = df.sum(axis=1)\nor\ndf['sum'] = df['col1' ] + df['col3']\nAnd then:\ndf.sort(['sum' ],ascending=[False or True])\ndf.sort_index()", "Select Pandas dataframe row where two or more columns have their maximum value together", 0.0, 0, 97, "You can simply select your row where you want your two most-combined-column values. It is quite possible to do that using PSDABA and your rows of course. That way, your best row, and the other rows that can be the most in this course, will also be seen with this function. Of course we know our rown. Can you do this with PSACS? Do you know the course? Yes, this question and answer are very easy to solve. 17 and 35 299 00 45.77 74 and that you can see them on that. We also know how many roows 9 and how much 89 have on those rows. If you see, you may not have to see how you could also ask you about how your business may see the row that your 5 or 6 have seen. So you't seen the rowd of nd of your course as a \u0093business\u0094 in a short and short \u0091busness\u0094, I also believe that businesses in short time. I will not comment on this, as I know I \u0099\u0099\u00a6\u0099 \u00a6\u00a6 \ufffd\u0094. In fact, the \u0094\u0094is a business that is an asset that we have and we want them. But what\u0092\u0092? \u0092\u0091What\u0092s\u0092 and business?\u0092\n\u0093What\u0094 is a type of business in which you are not interested in and are interested as business. What type business is?\nWhat type is business \u00c3??OO?V O V? The type that a lot of customers come to the website. The site is also a place of type \u00c7\u00c7\ufffd\u00c7 \u00e7\u0092\u00e2\u0094the type where people have come.", 0.04109589010477066, 0.006896551698446274, 0.04109589010477066, 0.05613041669130325], ["Given a spatial related search phrase, such as \"Find cafes near the train station\", what would be the approach to handling this with NLP / semantic searching?\nIn this case, would all the 'cafes' need to have a qualitative token with regard to their distance to the train station (e.g. near / far)? Curious to know what the thought process would be for handling these kind of tasks.\nThanks!", "The way I would approach this is to look at the prepositions in the first place, in this case near means close by. You then identify the reference point (train station). Now you find cafes which are close to that, ie you should have a list of cafes with their coordinates, and you compare those against the coordinates of the train station, returning the ones that are closest.\nOther prepositions (opposite) or other descriptions (in the same street as) would need corresponding other metrics to evaluate whether they fit.\nThis is not a semantic search problem, as there is nothing inherent in language that describes whether something is close or far from another thing -- you need to map this onto the 'world', and make a decision from non-linguistic data.", "How would NLP handle semantic searches for spatial information?", 0.0, 0, 98, "It would really depend on the specifics of each search. I think there are many specific searches that would require a few additional parameters to search, which is why I don't recommend the search method alone as the basis for a search process. For example, how would one handle \"find food trucks\" where you'd need the type of truch to go with the food search (truck or food).\nOr perhaps, in your example of \"how would you handle it if you wanted to find caths near a specific train location\". It may be necessary to put in the time, distance and type specifications for caches, as there may not be that many to begin with. So perhaps you would need something like \"the closest 5 cachs to X Y ZZ\".\nI dont know if this helps, but that is the basic concept. In this example with \"cafs\", perhaps it would not necessarily be needed to add the qualative type specification (near /far) and could perhaps be added to just a basic search to handle this specific example. As I say, perhaps this may depend particularly on what specific examples are and how far you are looking in them.", 0.3636363744735718, 0.024390242993831635, 0.16363635659217834, 0.061989568173885345], ["I know when you import everything you can do thinks like nltk.bigrams(nltk.corpus.brown.words() for bigrams and nltk.trigrams(nltk.corpus.brown.words() for triagrams, but how do you do four grams?\nI've seen other ways to do it, but they all do it with a \"string\" or a text they make up. How do you do it with the nltk.corpus.brown? Do you have to covert it to a string and if so, how?", "To get n number of items you can use nltk.ngrams() with the number to get as the second argument.\nIn your example, to get four-grams, you can use nltk.ngrams(nltk.corpus.brown.words(), 4)", "Finding Four Grams in an NLTK Corpus", 1.2000000476837158, 0, 99, "As I said earlier, NTLK makes bagrms and triagrums in the corpus, and you don't have control over it. However, it's easy to make it into a gram so that you'll be able to study it and see how it works.\nNLTk is a corpas of a particular language\nWhat type of corposa is? Can it be the original? I don\u2019t think so\nHow many types? Let\u2019s say the first 4 gram of the NLK corap is, of course, the gram you\u2019re gonna make for that NLP-K. They\u2019ll have the same number and they\u2019d have different numbers for them, too\nSo it can\u2019tt be done with this gram, because it\u2019sdifferent and it would have same numbers, though the one would be longer, so it isn\u2019tdifferent, as it is the 1st 3gram of NTBT\nIt\u2019ssdifferent because you did it before? What? How is it? And it was done before with other gram?! How? You need to know what type itis of gram. You have know it doesn\u2019tk have a same type, otherwise you wouldn\u2019ta havecontrol overit! What isit? Yes, yes, what ititiss!? It\u2019sa ntlk, you needto know the numbers!\nThey don\u2018t need knowthe numbers because NTLT Kses the crop as is. This isn'tthe NTP K that is what you use it for because this is how youstudy it but not whatitssay! It is an ntp kthat is not the grammar you used,but thegram youuse for it! You can look at a crap and a ntlt k as a grammar,itis a sgam of words.You can sgin agram and the words,andit wouldbe a gramer! There\u2019sthegram.How can you find 2 grmsof theNTK? Because the gmsthetrall is like a gammg of nttk in a briom. Because itssgamm,thetrallslike a a gammoof ntc k in thesbbrom! Because there are tbrbs in bbroms, theyare like gmmgs! But because therethrebrbsm,they are like", 0.10382513701915741, 0.010989011265337467, 0.06557376682758331, 0.02527209185063839], ["I want to understand a few things about partioning a parquet on Dask.\nWhen I do it in a .csv file, the chunksize works as intended, doing 30 partitions based on 50 mb chunks.\nWhen I try to do it the same logic through the read_parquet, none partition is created, and when I force this with repartition(partition_size='50mb'), it create 109 partitions.\nCan someone explain to me why parquet doesn't seems to work at the same way like .csv when doing chunksizes?", "In CSV, the fundamental, non-splittable chunk of data is one row, usually the bytes between one \\n character and the subsequent one. This bytes chunk size is typically small. When you load data with dask, it reads from a given offset to the next \\n to be able to read an exact number of rows. You would find, if you made the chunk size too small, that some partitions would contain no data.\nParquet is not structured like this. Its fundamental non-splittable chunk is the \"row-group\", and there is often just one row group per data file. This is done for efficiency: encoding and compressing a whole row group's worth of data in one block will give maximum read throughput. Furthermore, because of the encoding and compression, it's much harder for dask to guess how big a piece of a dataset will be as an in-memory pandas dataframe, but it can be many times bigger.\nA row group could easily be >>100MB in size. In fact, this is generally recommended, as smaller pieces will have a higher fraction of their processing time in overhead and latency.\nTo summarize\n\ndask will not split a parquet dataset beyond the partitioning within the data files\nthat partition size might be many times larger in memory than on disk, so repartitioning after load may result in many partitions\nthese are tradeoffs required to make parquet as fast and space-efficient as it is", "Repartioning parquet file dask", 0.3799489736557007, 2, 100, "You need to provide information about what you want. As we know that the parket is a bit strange, it's not clear what it is.", 0.095940962433815, 0.014869888313114643, 0.08856088668107986, 0.03843157738447189], ["Given an integer n <= 10^18 which is the product of Fibonacci numbers, I need to factor it into said Fibonacci numbers.\nEach factorization has a score, which is one less than the count of factors plus the sum of the indices of the factors in the Fibonacci sequence that begins with f(1) = 1, f(2) = 2.\nIf multiple such factorizations are possible, I need the factorization that minimizes the score.\nExample:\n104 = 13 * 8 or 104 = 13 * 2 * 2 * 2\nf(6) = 13, f(5) = 8, f(2) = 2\nFor 104 = 13*8 = f(6)*f(5), we have a count of 2, indices of 6 & 5, giving us 2 + 6 + 5 - 1 = 12.\nFor 104 = 13 * 2 * 2 * 2 = f(6) * f(2) * f(2) * f(2), we have a count of 4 and indices of 6, 2, 2, 2, giving us 4 + 6 + 2 + 2 + 2 - 1 = 15.\nWe should pick 13 * 8 since it has the lower score.\nThe biggest problem I've come across is when we have a number like 1008, which is divisible by 144 and 21, but needs to be divided by 21 because 1008 % 7 == 0. Because my program is first dividing by the biggest numbers, number 144 is 'stealing' 3 from number 21 so my program doesn't find a solution.", "Carmichael's theorem proves that each Fibonacci number after 144 has at least one prime divisor that doesn't divide any earlier Fibonacci number.\nThere aren't many Fibonacci numbers under 10^18; fewer than 90.\nMake an array of all the Fibonacci numbers <= 10^18.\nGiven an input n which is the product of Fibonacci numbers, its factorization into Fibonacci numbers must include every Fibonacci number above 144 that divides it, repeated as many times as it divides it.\nGo through your Fibonacci numbers in descending order and keep dividing n by any such number that divides it, until you get to 144.\nNow we need to be careful because two Fibonacci numbers don't have any prime factors not seen in previous Fibonacci numbers. These are 8 and 144. Since 8 is 2^3 and 2 is a Fibonacci number, you can't render your number unfactorable into Fibonacci numbers by taking the 8. Under your optimization, you will always choose the 8.\nThen 144 is the only factor that you might need to reject for a smaller factor. This can only happen if 34 or 21 are factors, and the 144 eliminates a needed 2 or 3.\n34 = 2 * 17, 21 = 3 * 7\nThat was long-winded, but it gets us to a simple approach.\nGo through the Fibonacci numbers <= n in descending order until you get to 144, then skip to 34, then 21, then back to 144 and descending down to 2.\nThis will give you the optimal factorization under your weird scoring scheme.\n----- this order -----\n[679891637638612258, 420196140727489673, 259695496911122585, 160500643816367088, 99194853094755497, 61305790721611591, 37889062373143906, 23416728348467685, 14472334024676221, 8944394323791464, 5527939700884757, 3416454622906707, 2111485077978050, 1304969544928657, 806515533049393, 498454011879264, 308061521170129, 190392490709135, 117669030460994, 72723460248141, 44945570212853, 27777890035288, 17167680177565, 10610209857723, 6557470319842, 4052739537881, 2504730781961, 1548008755920, 956722026041, 591286729879, 365435296162, 225851433717, 139583862445, 86267571272, 53316291173, 32951280099, 20365011074, 12586269025, 7778742049, 4807526976, 2971215073, 1836311903, 1134903170, 701408733, 433494437, 267914296, 165580141, 102334155, 63245986, 39088169, 24157817, 14930352, 9227465, 5702887, 3524578, 2178309, 1346269, 832040, 514229, 317811, 196418, 121393, 75025, 46368, 28657, 17711, 10946, 6765, 4181, 2584, 1597, 987, 610, 377, 233, 34, 21, 144, 89, 55, 13, 8, 5, 3, 2]", "The smallest sum of divisors", 0.0, 0, 101, "[Duplicated Answer]\nIn our class today, my teacher gave the example above to explain his method for factoring integers into the smallest summ of products of those factors, and the method that will give us the result in shortest time. After we went through the examples, we discussed the\nFac-W-R-E-B-I-T-H-O-U-N-K-D-S-0-1-5-...\nC-A-P-\nand then we were given the assignment to find the solution for 929,47537\nAnd the process for it is as follows: Take an unknown integer and divide it by a\nWa-e-bi-t-h-o-u-n-k-d-s-7-2-4-9\n-By-a\n[In the first paragraph, there is a mistake in this sentence, the 'b' is supposed to\nVa?\nBa or Va\n] and then find its factors using the number of digits (that is, if it were \ufffd9\ufffd, its\nfactors would be \ufffd\ufffd994593\ufffd and\ufffd8980764\ufffd). The greatest sum and\nsmallest sum\ufffdand the most significant score\ufffdare found by this method. We can apply the same method to an\nAwerb\n? \u200bor\n\u200b[Were\nPed]o\na [W]ork [s]h[s][c] [t][l\ufffdt\ufffd [w\ufffdo]r[k\ufffd[\nk[t[o\ufffd)\n\ufffd\ufffd\ufffd\ufffd\ufffdk [h]e[n]k a[w]or [b\ufffdp[h\nthe [o[p]\ufffd\nwe can\ufffduse the\ufffdmethod above\ufffdto nd the \u007f\ufffds\ufffdm\ufffde\ufffd\ufffdb[\ufffdw\n\ufffdwe [c\ufffdn\ufffdu\ufffdl[y\nd\ufffdc[u\ns[b]0\ufffd1\ufffd2\ufffd4\n5\ufffd6\ufffd7\ufffd?\ufffd3 [p\ufffdh\ufffd]1 [\ufffd{k} [1]235[6]78 [f]5{f[1]}\n2 [01][7][67]8[3]4 [5]6 [7]{f}5 [", 0.26966291666030884, 0.01610305905342102, 0.10272873193025589, 0.030895987525582314], ["The dataset is large with over 15000 rows.\nOne row of x,y,z plots a point on a 3D plot.\nI need to scale the data and so far I'm using RobustScaler(), but I want to make sure that the dataset is either normally distributed or it isn't.", "Matplotlib histogram [plt.hist()] can be used for checking data distribution. If the highest peak middle of the graph, then datasets are normally distributed.", "I have a 3D dataset of coordinates x,y,z. How do I check if the dataset is normally distributed?", 0.0, 0, 102, "If you can create a normal distribution with only 5% of your data, then it is definitely not a long tail. I would suggest using [Statistics](https://stats.3d.b.i.9.6.a.d) class. Here is a [long tail](http://www.youtube.com/watch?v=9Y1jH4hH64) distribution as an example. You need much more data for [normal distribution](. So try it and [check if it's a tapered or long-tailed distribution].\nHere is an [experiment](on [taper vs. long tails]]. So, do some [statistics experiments]([Statistic experiment for t-values and s-tests] or [comparison tape versus tapes] [Comparison Tape Versus Tapes]. If there are few [large points], then the distribution should [have few large points] ([Computation of t statistics on small samples]). But in the end [what matters is that]() [you have more than 20 data points to calculate a probability value](). So [calculate the probability values](!) and then use the [probability values calculator](!).\nThis is one of [the most recent examples of a series of graphs](and [a large dataset]! So you [can calculate probabilities for a large [sample]]()! You should use [one of the most [modern statistics algorithms] today](!!)!\nYou should consider [using the normal [distributions] with more of less than two data [points] so you have an idea of how the[t-test for sis-sis [values] are calculated][.]\nSo you should try using it or not[the normal distributions are used in calculations](.[\nyou can also [go to http://bodon.org/b0d012-0/a-simple-30-f_2_0_t_3_7_1_f7b_e6_c8a_4a2f224d_8_a9_63546567662609b2375_95]![\nthe [most recent example ofa [series of charts][[and\na great [data set]]]!\nThen you will [be able to compare [tape", 0.07643312215805054, 0.006410256493836641, 0.0573248416185379, 0.05145769193768501], ["I have used the .map_partitions with delayed functions and the result I got is a dataframe with delayed results in each row.\nIs there any way to unpack those delayed objects?\nAm I doing something wrong?\nThank you.", "A very short answer: you should use map_partitions with a normal function, not a delayed one. The dataframe interface already provides laziness and parallelism, so you don't need to add another nested level.", "How to unpack a dataframe of delayed dask objects?", 1.2000000476837158, 5, 103, "Title: Can we un-dress the data-frame?", 0.04651162773370743, 0.0, 0.04651162773370743, 0.030303027480840683], ["So, I want to implement a class that holds nested data. I would like to implement __getitem__ in a way so that obj[x][y] can be abbreviated by obj[x, y].\nHowever, I noticed a problem: The signature of __getitem__ is that it expects a single positional argument instead of *args. If multiple arguments are given, they are automatically put into a tuple.\nI.e. obj[a, b] and obj[(a, b)] both appear to be equivalent to obj.__getitem__((a,b))\nBut then how can I distinguish the two cases\n\nThe outer layer is indexed by tuples and obj[(a, b)] should return the value at that index\nThe outer layer is not indexed by tuples and obj[a, b] should return obj[a][b]\n\nThe only possible solutions I am aware of currently are\n\nAbandon the idea of coercing obj[x, y] into obj[x][y]\nIf we only want obj[x] always write obj[x,] instead.\n\nBoth are not really satisfactory.", "Short of trying to inspect the calling source code (which is extremely fragile, with all sorts of failure cases, nowhere near worth the instability and debugging headache), this is not possible.\nobj[a, b] and obj[(a, b)] mean exactly the same thing in Python. There is no semantic difference, no difference in how they are executed, and nothing to hook into to distinguish them. It'd be like trying to distinguish the whitespace in obj[a,b] and obj[a, b].", "Any way to distinguish `obj[x, y]` from `obj[(x, y)]`?", 1.2000000476837158, 2, 104, "_________\n_____________________\n__CMS__MSM_MSMS_MMS\nMS & MS. MS\n#_3_4_.__convention_p_b._cms._msms.pbs.bss_d.o_e_o._._p._b.s_2_s.__.p.bo.sb.sh_h_f_t.h.t-b-bs-_v._t.-s-a.c.-w.w._v.-_\n.\u00c2BSS.BSB.BBB.OBS-BBS.SOB-SSSSOOOB-OOUOU.EUUUK.UKUKK.KUKKS-KKS. _s_,_,_-_.-.-b_.oob.ob_oc.25.34.1_0_1__1_.__.__________________. .____. .____.__________._.\".1\".\"-.\"-bo-so-bbs.5-5bboo.d-o-ob-oo-d_aa.aa_.aa._a._.a_a-h-w_bbbo._2-2bababbo.\"\".-.\".-.0.mmsbsa-4-mbsbo3-3.4...30-...............66-6035655...-\".........-...\".\".699545320...223...12...o0booo.......\n673376\n770647\n-7-87.7...bo..-ba..bo...b....1...01....0.........\n2462636\t.\ufffd_obocoboca_bc_sssbo_obs_ppbooboobs\ufffdsobeobboes\ufffdboobe\ufffdobsbeofobobs\ufffd\ufffd8808-\ufffd3\ufffd\ufffd9\ufffd4\ufffd.8.\ufffd\ufffdo[o]ob[00]0B\ufffdBS50s\ufffdBs39-9.94\ufffd\ufffd29792. Bs BSS BSBs\n105\t21\t02\t3\t1\n0\t\t8\n________._\u00c2CMMSS", 0.07792207598686218, 0.0, 0.06060606241226196, 0.00900900736451149], ["I am trying to remove the empty cells from my column A of my data which had text data in it.\nMy csv which I imported into data frame has 50k rows containing search data in column A.\nI tried the below options.\ndf= df.replace(r'^s*$', float('NaN'), regex = True)\ndf.replace(\"\", np.nan, inplace=True)\ndf.dropna(subset=['A'], inplace=True)\nStill there are empty cells", "Are you sure they are empty? Did you check to see they're not just empty strings (\"\")?\ndropna is the proper method, unless you want to also drop cells with empty strings.\nPlease elaborate, thank you", "Trying to remove empty cells in a column in csv from my data using pandas dataframe", 0.0, 0, 105, "You can try to replace the empties cells by the values 0 or 1 by using the python script or any other method. If you want to eliminate the missing cells, then you can use the script and the method to find the cells. This method, we suggest you use our script, the algorithm will find them, you know which cells are missing. So, if you do not want them to be found, they will not be included in the list of missing cell, so you will be happy that the cell is not missing\nhttps://www.youtube.com/video/202669-2-6-7-1-5-month-cars-you-can-haggle-the-new-old-car-buys-and-reviews-4-weeks-autotops-3-week-vids\n140 2 3 4\n88 84 92\n35 72458\nhttp \u00c2\u00c2\u00c3\u00c3\ns \u00c3\u00c2\ufffdA\n\u00c3\ufffd \ufffd\ufffd\u00e2\ufffd\n\ufffd\u00c2\ufffd\ufffd\ufffd\u00c3\ufffd\ufffd\nA nd \ufffd\ufffda\nB C D\nE 6 \nF \u00cf \u00d4 \u00cb\u00c3B\n44 E \u00c4\u00c4\n? \u00ef\u00ef?\u00c3?\u00c4?\n93317 \u00d5\u00c4\u00c1\u00d5\u00c3\u00d5\u00d5\n606?5764616\n77?3865?89678????\n\u00e2\u00c2\u00c4\u00e2\u00c3Aa\u00c3\u00c4AAAAaaaAaa\u00e2\u00e2", 0.1458333283662796, 0.021052632480859756, 0.09375, 0.0674515813589096], ["I'm trying \"from sklearn.linear_model import SGDOneClassSVM\"\nbut it doesn't work and raises an import error \"ImportError: cannot import name 'SGDOneClassSVM' from 'sklearn.linear_model\"", "Upgrade sklearn package using the command:\npip install --upgrade scikit-learn", "problem with importing SGDOneClassSVM from sklearn.linear_model", -0.1973753273487091, -1, 106, "It works with the function import from class or library. So you are importing the same library, that you have imported once. If it does not work, it means the library or the class does NOT exist! You will get a \"Can not find....\" error! Then you can try to import it again, and it should work! If you get an error, then check if the name or class exists!\n(Please read!) Importing from a library is just a function! It is the equivalent of writing a short code! However, this short and unused code can be used! So if you write some code and write a line of importing code from the Library, which you don\u2019t need, just click on the button \u201cimport\u201d! That\u2019s all! This short but useful code allows you to write any program! Any program with any feature! But\u2026but\u2026this can also be an \u201cImport\u201d error if it is not correctly placed! Therefore, when you import\u2026what you need\u2026how to use it\u2026?! (Sorry\u2026) What you really need is a class! Classes\u2026class\u2026like a student\u2026who\u2026you\u2026the\u2026whatever\u2026\nI'VIEVED THE IMPORT FROM SKLELL TO OBLIGATION FROM THE STUDIO TO THE CLASS TO ACCIDENT! SIEHDE WAS THE OFFER OF THE PROJECT FROM AN ARTIST OR GREATER! YOU COULD FIND THE INSTRUCTION OF PROBLEM WITH THE ARCHIVES FROM SMART TO SUPER CARS!", 0.008298755623400211, 0.0, 0.008298755623400211, 0.014678329229354858], ["I have a greyscale images dataset that I want to feed into a neural network.\nx_train_grey.shape is (32, 32, 73257)  so I understand it is (dimension_x, dimension_y, batch_size). Because the images are greyscale, there is only one \"depth\" dimension.\nHowever to feed this data to the neural network it needs to have this shape:(batch_size, dimension_x, dimension_y). With batch_szie at the beginning.\nHow do I reshape it to this format, so that batch_szie comes before the x, y images dimensions?\nOnce this is done, I expect to be able to pass this into a neural network (the first layer being Flatten()), like so:\nFlatten(input_shape=(32, 32, 1)),.\nCheers!", "Solved! By passing the correct shape into np.reshape().\nI really should get to know numpy better, before getting into deep learning.", "How to change the order of dimensions of images data's shape for a neural network?", 0.0, 0, 107, "the ...", 0.08695652335882187, 0.0, 0.08695652335882187, 0.0], ["I have a python dataset that I have managed to take a sample from and put in a second dataset.\nAfter that I will need to produce another sample from the original dataset but I do not want any of the first sample to come up again.\nIdeally this would need any flag would only be there for a year so it can then be sampled again after that time has elapsed.", "Denote your original dataset with A. You generate a subset of A, denote it with B1. You can then create B2 from A_leftover = A \\ B1, where \\ denotes the set difference. You can then generate B3, B4, ... B12 from A_leftover, where Bi is generated from A_leftover = B(i-1).\nIf you want to put back B1 in the next year, A_leftover = A_leftover \\ B12 U B1, and from this, you can generate the subset for B13 (or you can denote it with B1 as 13%12 = 1). So after 12, you can say you can generate Bi from A_leftover = A_leftover \\ B(i-1) U B(i-11). Or you can use this formula from the very beginning, defining B(-i) = empty set for every i in [0,1,2,...,10].", "How I do I get a second sample from a dataset in Python without getting duplication from a first sample?", 0.0, 0, 108, "After reading a few books and the internet there are several ways to make another copy of a data set without using the same first samples. It is important that you keep your sample number. And don\u2019t forget the flag if you are producing more. I don't know how long I can store the next sample, but a lot of places can do it for several years.", 0.2450980395078659, 0.029702970758080482, 0.13725490868091583, 0.06521695852279663], ["So, here I have a numpy array, array([[-1.228,  0.709,  0.   ], [ 0.   ,  2.836,  0.   ], [ 1.228,  0.709,  0.   ]]). What my plan is to perform addition to all the rows of this array with a vector (say [1,2,3]), and then append the result onto the end of it i.e the addition of another three rows? I want to perform the same process, like 5 times, so that the vector is added only to the last three rows, which were the result of the previous calculation(addition). Any suggestions?", "For the addition part, just write something like a[0]+[1,2,3] (where a is your array), numpy will perform addition element-wise as expected.\nFor appending a=np.append(a, [line], axis=1) is what you're looking for, where line is the new line you want to add, for example the result of the previous sum.\nThe iteration can be easily repeated selecting the last three rows thanks to negative indexing: if you use a[-1], a[-2] and a[-3] you'll be sure to pick the last three lines", "Iterate over rows, and perform addition", 0.06656807661056519, 1, 109, "The result is obtained by adding several rows to a row.\nTitle: Perform addition over the cells of an array\nIt is important to be aware of a cell that has been formed over, that is, when you perform this process over it, you can do it by the process of addition. The cell itself must be formed. In fact, all of our cells have been already formed!\nSo, let's talk about the importance of cell formation. And, of course, we talk more about it than we do about our own cells! So let\u2019s start talking about cellformation. It is a very important process! It\u2019 s important because we are talking more than ourselves! If you were to start a blog on how to make pots, then you would be talking as much as yourself! That is very very interesting and very... So now we start off to talk, because it\u2019ss really interesting. So when we\u2019re talking like... we think it is really important! And it really is! Let\u2019\u2019... It really...\nOh, sorry! Oh, really! You think about how... You see,... oh! I\u2019m sorry, I was going to tell you a bit more! About how you\u2019d... Yeah! Yeah, ok! But what is this? This is... How you do... what? What do you think? How do we know about this...? So we... We have to... But do this with... how do I... do? You know, if you are going... Yes, yes, this is how I do! How I think! Ok, thanks! Bye! Thanks! Do you have... ok? No, okay... Do this on your own! Okay! Just... I am going! What a... This how we can... Okay, please... OK! OKAY! Sorry! Thank you!... Sorry!...\nWhat is the title of your blog?! Of course! A... Of... A bit of... What is your title? Ok! Bit of \"Mystery\"! Mystery!? Mysty?y bit! Hey! My... Oh! Hang! Is... is? Yes! Yes...! ...Is bit? Is bit... of bit bit ...? A of! Ah of Ah! Off!... Ah!... \"A bit\"... Off! The off!....\nOff!.................", 0.197247713804245, 0.02304147556424141, 0.09633027762174606, 0.055519141256809235], ["I need to choose the best model to predict the traffic in a determinated hour.\nI think cluster is not for this problme, but i still don't know what would be the best option. If it's vector machine, decision tree, linear regression or Artificial Neural Networks.", "I think this depends mostly on your data. How much data do you have? If you only have few examples, I would go with VSM (Assuming you mean Support Vector Machines?). If you have a lot of examples I personally would go with a neural network.\nI guess you could even get a nice representation of the prolem if you use a recurrent network.", "Predict future with models / VSM /decision tree/ linear regression/ Artificial Neural Networks", 0.0, 0, 110, "The best way is predicting the future as seen by the prophets. This will definitely be your choice!\nPosts: Hear me! / HOLD ME! YOUR HOLLYBOOO IS TOO HORRIBLE TO FINALIZE. / FAN FINESTION IS JUST THROUW. I PRECRISED AGAIN SOO! I STUMBLED BEAUTO BOOOTO BE AUTO AND AUTOHOLYAUTHY. AND BE SORBROUGH WITH BAUTHHOLLY BOOBAS! 150 FULL! FREE! CLOOLED! DOWN! BOBS! THE BOOT BOY BOU...BOSS BOSS! SO SO I DO NOT KNOW WHAT YOU ARE BOUSING! BUT THE BEST BOOST BE BOUT YOUCRE! BE HALLY BA...BEAUTY HAS NEVER BEEN IN THIS BOUNBBOU....BEUTHB......BEOUOUUUOUUIUI.........UIIIII.....\n.... / UUIUUCUCUUPUP! HOW DO YUOU? /... / SO ... HU! DO! AWW!", 0.11976047605276108, 0.0, 0.09580838680267334, 0.04332130029797554], ["I am newbie to data science and I am bit confused about working of map and applymap in pandas. As when I executed code df.applymap(lambda f: f*2) and df.apply(lambda f: f*2) provided same result. But when I change code for both which were df.applymap(lambda f: f*2 if f < 7 else f) and df.apply(lambda f: f*2 if f < 7 else f) then apply method caused an error. Upon my understanding, I came to conclusion that applymap works for each scalar value where apply does not work for each scalar value but instead it executes operation for whole column or series.\nKindly veterans help me out here if I am correct or not. Thanks in advance\nNote: df in code refers to whole DataFrame not series.", "Yes, apply works on a row or a column basis of a DataFrame, applymap works element-wise on a DataFrame.", "Working of map vs applymap in pandas, python", 1.2000000476837158, 0, 111, "The solution for your question is very simple that when you apply map, the value of that map is 1. So in case of a value being 0, 200 or some other number, it would be 30. However, when the apply function is used, then the data of the series is considered, which results in a 60-65 or 80 to 90 difference. The reason for the difference is that the function of apply in Pandas is of type 'series' rather than 'DataFrame'.\nPurely speaking, this difference can be considered. And as far as I know, there are 47-48 or so differences between the two. Therefore, you can think about applying map versus the use of this function that is called apply.", 0.0839160829782486, 0.014184396713972092, 0.0839160829782486, 0.07054213434457779], ["I am given an array of elements and the sum K, and I am supposed to find a subarray (doesn\u2019t have to be contiguous) whose sum is equal to K.\nFor example:\nInput: [1, 9, 3, 2, 21], 30\nOutput: [9, 21]\nDo I need to use backtracking or is there another algorithm using dynamic programming for example?", "If it's not a big array you could use brute force: 2^n solutions!", "How do you find a subarray with the given sum?", 0.0996679961681366, 1, 112, "Use dynamic programing, you will find the shortest path from the first element of the array (in this case was the last element which was 42) and you\u2019ll have a short path that leads to the K (35).\nYou could also use any other algorithm like the one that uses Finding the Shortest Path, using the dynamic back tracking, or the algorithm that finds the k short.", 0.1204819306731224, 0.02469135820865631, 0.09638553857803345, 0.07407406717538834], ["I am using stableBaselines3 based on Open AI gym. The agent, in a toy problem version, tries to learn a given (fixed) target point (x and y coordinates within [0,31] and [0,25] respectively) on a screen.\nMy action space would thus be a box (Version A): self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([31,25])))). The reward obtained by the agent is minus the manhattan distance between the chosen point and target (the simulation terminates straight away). But when running the PPO algorithm, the agent seems to try only coordinates that are within the Box [0,0], [2,2] (ie coordinates are never bigger than 2). Nothing outside this box seems ever to be explored. The chosen policy is not even the best point within that box (typically (2,2)) but a random point within it.\nWhen I normalize to [0,1] both axes, with (Version B) self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([1,1])))), and the actual coordinates are rescaled (the x-action is multiplied by 31, the y- by 25) the agent does now explore the whole box (I tried PPO and A2C). However, the optimal policy produced corresponds often to a corner (the corner closest to the target), in spite of better rewards having been obtained during training at some point. Only occasionally one of the coordinates is not a boundary, never both together.\nIf I try to discretize my problem: self.action_space = gym.spaces.MultiDiscrete([2,32,26]), the agent correctly learns the best possible (x,y) action (nothing in the code from Version A changes except the action space). Obviously I'd like to not discretize.\nWhat are possible reasons for that whole behavior (not exploring, considering only/mostly corners, moving away from better rewards)? The rest of the code is too unwieldy to paste here, but does not change between these scenarios except for the action space, so the fact that the discretized versions works does not fit with a bug with rewards calculations.\nFinally, my action space would need to have one discrete component (whether the agent has found the target or will continue looking) on top of the two continuous components (x and y). The reward of a non-decisive fixation would be a small penalty, the reward of the final decision as above (the better the closer to the actual target). self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2),gym.spaces.Box(np.array([0,0]),np.array([31,25]),dtype=np.float32))) should be what I'm looking for, but Tuple is not supported. Is there any workaround? What do people do when they need both continuous and discrete components? I thought of making the binary component into a float, and transforming it to 0/1 below/above a certain cutoff, but that can't lend itself too well to learning.", "For posterity, stable_baselines seems to be sampling actions in mysterious ways. If the action space is defined as [0,1] or [-1,-1], stable_baselines will indeed sample that space. But if the action space is, in my case, [0,31], then the actions sampled are roughly within [0,3] or [0,4], with most values being within [0,1].\nSo the workaround seems to be to use Boxes using [0,1] or [-1,-1] for the action_space, and rescale the action returned by whatever SB3 algorithm you're using.", "stablebaselines algorithms exploring badly two-dimension box in easy RL problem", 0.3799489736557007, 2, 113, "The above comment has been accepted as the answer by Dhach13. [Dhac19]s [123][1710] [\n[D\n4[21][201\n][[0]]\nD[45][424]3\n304-4\n2-228]\n[[[]]]d\no o o\nb) [I\na[116]b[70\n]a [72\n[]b][73]e\n]]a\nr [r[6\n500-607\n6][]29\n93-95\n1-18\n7]1487-\nB][\n066[b]56-88536977[\nS-380[8-a433[39-[a]4403464[[0967891]]][]0][a7][0a1a2a3a54a6a-w]w][8a][635-7657w2s7]][w75]]55a8s1w[52w-]-s]84s9w9][w8o]o[9o]][[8]]b[[-5o86b]]w/5s8w5[s2]]-o[[5827s5/2x/x]]s-x-y-[[x[[s\n-t\nupport-c\ne-support-d-e][d[e9[da]da[cdd]d{d}}]dd{da}da{dddaa{a}aaaaa\\\\a<a+5\\\\\\aa\\0\\\\5\\\\\\\\0\\515{05}2\\\\3\\\\2{2}}\\\\4\\\\{\n}983[[]9{99[[9}0{1}1{3}4{5}[6{7}8{6}7{8}[[68[[4]]]]2(\n)aac(s(7a)1s[h]m{o}{1}{5}{7o{}\\\\5ooo\n8oo[o0oo", 0.11583011597394943, 0.015564202331006527, 0.09266409277915955, 0.027149325236678123], ["I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors' but I have this error ImportError: cannot import name 'KNeihgborsClassifier' from 'sklearn.neighbors' (C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neighbors_init_.py)", "You are importing KNeihgborsClassifier which is wrong, change it to:\nfrom sklearn.neighbors import KNeighborsClassifier", "I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors'", 0.0, 0, 114, "you should be able to install it.\nskldk is a big thing, and is not for free. so you need a password to do the installation. you can do that by making a keygen. use the following to make the key. key name,key description,the key can be used,and the secret is never revealed. the first thing to know is that the program is free,it can tape record and listen.it is an excellent program.so we can use it without trouble.we can make 33 key generator, it can generate a program of 500 characters. it is very easy to use.10 key generator,14 key,57 key and so on. we know that this program has a very big secret,so you have to pay the price.i paid the payment,how do you get it? 2015 key free software free key software key 404.5 62.67.48.34,21,37,70,98,44 02 1236 8283 9 7 \ufffd\ufffd\u00ad\ufffd\ufffd\ufffd\ufffd.\ufffd\ufffd\ufffd\ufffd.\ufffd2\ufffd3\ufffd4\ufffd5\ufffd6\ufffd7\ufffd8\ufffd9\ufffd1\ufffd \ufffd\ufffd565\ufffd\ufffd4\ufffd\ufffd6\ufffd\ufffd7\ufffd\ufffd8\ufffd\ufffd9\ufffd\ufffd1\ufffd\ufffd2\ufffd\ufffd3\ufffd\ufffd...\ufffd...\ufffd\ufffd....\ufffd\ufffd.....\ufffd\ufffd........\ufffd\ufffd....................\ufffd.......\n\ufffd.....\ufffd........\ufffd\n\ufffd\ufffd.(\ufffd)............living a happy life.that's all i know about it.\"\ni've tried to solve problems using software for living a life.\"... ... ...\nComment: i'd like to learn more about this.how is this?what is it called?and who are the people?why?who is hong boo and who is the boots?how can i solve this problem?\ufffdth\ufffde\ufffd\ufffdbo\ufffds?\ncomment:  i would like more information about the knee hhbg bs. i had this issue and i didn't understand it clearly. who? what? why? the b...o...s..\ufffd\ufffdwhat about these problems?the b\ufffdo\ufffdss?t...b\ufffd\ufffdb\ufffdb?i would say... bboooooo....\nTitle: why don'tsortsorterstink. howsorry.....\n29, b", 0.03184713423252106, 0.0, 0.025477707386016846, 0.046816639602184296], ["I have a task and the output should be a \"1-D np.array of dimension m\" and I don't understand how a 1-D array can have m Dimension, it has 1 per definition ?", "The word dimension can mean multiple things, in this case it means the size/length of the singular dimension, i.e. you can say an array has dimensions 2x2.\nTherefore, a 1D array of dimension m is equivalent to a list of length m.", "What is a 1-D np.array of dimension m?", 0.1973753273487091, 1, 115, "It does not have 2 Dimensions, because the number of the dimensions of a Multideck is 0 or 3 (see below).\nSo, the task is that you should have an array of length n with dimension 4, and you will say \"I have only 8\".\nExample 5 - The multidek\nThe multidick is defined as an object with a dimension of 6 and 7 Dims. The number \u0093the multiday\u0094 is the first dimension: 9 or more. In any case, a multimodel day is an \ufffd\u0099\u0093object\u0094 and not a machine, although this may be \u0097 \u0099\u0094an object\u0094.\n218334578464423556980317402977287375399307952708224365196105866320115949091254162676004813892\n068850_5_7_1_2_3_4_and_h_t_e_o_n_to_oo_6_or_go_9_th_p_0\n1s \u00adp\u00ad0\u00ads\u00adh\u00ad _2\u00ad \ufffd1o\u00ad9 \ufffd\n3o\ufffd9\ufffd\ufffd \ufffd\ufffd\ufffd\n4s\ufffd0\ufffds\n5s  \ufffdb\ufffd_8s_\n7s_____ \u00c3\u00ad\u00c3\u00c5\u00c5\u00c3\u00c4\u00c4\u00c3\u00c3\u00c2\u00c2\u00c4\u00c5\u00c2\u00c3\n\u00c3\u00c6AE\u00c4E\u00c5\u00c6\u00c4\u00c9\u00c3\u00c9\u00c4\u00c6E\u00c2\u00cb\u00c3\u00cb\u00c2\u00c6\u00c5\n9\u00c2s\u00c2b\u00c2\n________ _____\n______ _\nh3h\n_________ ____________\nhhhh\n\u00e2\ni i i ii ii iii i iv vi v vi vi i\nvv \u00c2 \u00f0\u00c3\u00a1 \u00c4 \u00c6 \u00c5 \u0102 \u00cb \u00db\nu u u a a\nb b b hh h h\n____________________________\n\"Whatever it is, whatever it's about. Whether it was about \"Killing\" or \"The Talking\". It'll be good.\" - \u00af\u00c2\u00af\u00af", 0.21739129722118378, 0.035087719559669495, 0.12173912674188614, 0.026576360687613487], ["I have speed data of many particles to which I want to fit the Maxwellian curve. I am trying to use the fit method from scipy.stats.maxwell to fit to my data and extract the temperature of the system from that.\nFrom the documentation, I am unable to put my finger on what the parameters that we are trying to fit exactly are and hence how they relate to temperature.", "It's related to scale.\nYou also likely want to set floc=0 in maxwell.fit(...) \nCf the argument of the exponential: with non-zero scale it's -x**2 / scale**2 / 2, which you compare to the expected mv**2 / 2kT.", "How to use scipy.stats.maxwell to find temperature?", 1.2000000476837158, 1, 116, "12 May 2018 ... how to do it: you will have ... ... to see ... see the 9-25-3355 .... ... .... .... 3-4-5677 ... 573 .... how ... \"what ...\" ....\" what ...?\" .... the question ....\n3....\"what ....\"\"....\"\"\"\n.... .... what?\"\nwhat? ... what? .... you know it? the .... (a ....) the (b .... .. ... the) you (c) .... c) \"how?\"....how to ...\ni asked you\n..... how\n............. ...... ....... how ..... ...? .. .? . ....?....\n.. how? what...?..\na ... .. what\n? ? ? !\n\"\"\"\"\"\n\"\"'\"\"\"-\"'\"\"'-\"-''-'\n''''\n- ... ''\n...'' ...'' .... '' ....'' '''\n' ...' '''' ' '''' ' ''' ''' '\n''' ... ''' ...'''\n# # # ... ###\n. #\n.,. . ... .\npractical science\nscience ... practice ... p ... practical science ... ( ... ) ... (\"p) ... t ... h ... g ...tg ...h ...htght ...gtgheighthtgbheightsbehindb ...bheahebehbe ... bhe ... hahehehhehavehavehahhahehavesthehands ... and ...and ... havehaveshapedshapedshepeptshapedlikepea-pets-peas-shapeshapeshepleaseshep-pleasheppe-s-se-sp-so-bo-o-be-b-e-ba-\nhow?! how!\nHow? How? Wow! WOW! ... How ... ?\"# ...#\" #how how #\" ... ! ... _ how_? !? _!?_!__ ..._ #HowHowhowhowto--use-SC-P-SP-S-M-W ... aah a ... ah ...ahah ...aah ah how how?? .... . how??? ? ... ?? ???? ...??? .... ?? ...????? !!! ...!!! ...!!!!!.........\n(2) ............... .......................................... ......... 02 .... ??? ?", 0.16249999403953552, 0.0, 0.11249999701976776, 0.040929216891527176], ["i have LogisticRegressionCv model it's .pkl file and import data as images but i don't know how to get it on flutter please help me If you know how or if I must to convert my model to other file formats.\nplease help me.\nThank you for your help.", "as you've trained your model in python and stored it in pkl file. One method is in your flutter background, call python3 predict_yourmodel.py your_model_params and after the run, it will give your the model result.\nAnother way is implement a logisticRegressionCv in Flutter as it is a simple model, and easily be implemented. you can store all your params and l1 or l2 etc super-params in a txt instead of pkl file for readility.", "How to implement LogisticRegressionCv on flutter", 0.0, 0, 117, "Thank you your answer. I would like to know which program is better than others. Thank... Read more Read less Read Read Less Read More Read The Read Th... A. A... B. B... C. C... D. D... E. E... F. F... G. G... H. H... I. II... III... IV... V. V... VI... VII... VIII... IX...... XI... XII... XIII... XIV... XV... XVI... XVII... XVIII... XIX... XX... ... ...", 0.08275862038135529, 0.0, 0.08275862038135529, 0.03690223768353462], ["Since I don't have pure knowledge of the pandas library, I just want to explore the range of functions that pandas library offers to users.", "use dir(pandas)\nBut you'd better go to the official documentation.", "Is there any command which can show what kinds of function are in-built in pandas or matplotlib?", 1.2000000476837158, 0, 118, "To find out that the library does not require any knowledge or skills to be able to operate it. The simple thing that I do is just open the browser and see what all the options look like. And I found out there are many options. I am open to knowing if this library provides many functions.", 0.05797101557254791, 0.0, 0.05797101557254791, 0.01751776598393917], ["Getting this error in python:\nValueError: cannot reshape array of size 14333830 into shape (14130,1,1286),\nHow do I solve this?\nThis is the code generating the error:\ndata_train1=data_train.reshape(14130,1,1286)", "For doing reshaping, your new shape should match the previous shape. If you multiply 14130 * 1286, you get 18171180 which is obviously not the same as 14333830. So you must write something correct.", "ValueError: cannot reshape array of size 14333830 into shape (14130,1,1286), how do I solve this?", 0.0, 0, 119, "`data.train` should be shaped like `1660`, and you should use `shapetensor.batch_size` to generate the batch size of the test data.\nSo in your code you just need to use the `batch size` of `reshapen` like below: (don't use too many lines) like this:  ` data_test=resshape(data)`\nIn your first comment, it says: \"1.4 trillion number of data\", and in the 3rd line it said: \u201c1984\u201d, what is that?", 0.14953270554542542, 0.0, 0.09345794469118118, 0.03048780746757984], ["I have a simple table which the datetime is formatted correctly on.\n\n\n\n\nDatetime\nDiff\n\n\n\n\n2021-01-01 12:00:00\n0\n\n\n2021-01-01 12:02:00\n2\n\n\n2021-01-01 12:04:00\n2\n\n\n2021-01-01 12:010:00\n6\n\n\n2021-01-01 12:020:00\n10\n\n\n2021-01-01 12:022:00\n2\n\n\n\n\nI would like to add a label/batch name which increases when a specific threshold/cutoff time is the difference. The output (with a threshold of diff > 7) I am hoping to achieve is:\n\n\n\n\nDatetime\nDiff\nBatch\n\n\n\n\n2021-01-01 12:00:00\n0\nA\n\n\n2021-01-01 12:02:00\n2\nA\n\n\n2021-01-01 12:04:00\n2\nA\n\n\n2021-01-01 12:010:00\n6\nA\n\n\n2021-01-01 12:020:00\n10\nB\n\n\n2021-01-01 12:022:00\n2\nB\n\n\n\n\nBatch doesn't need to be 'A','B','C' - probably easier to increase numerically.\nI cannot find a solution online but I'm assuming there is a method to split the table on all values below the threshold, apply the batch label and concatenate again. However I cannot seem to get it working.\nAny insight appreciated :)", "Since True and False values represent 1 and 0 when summed, you can use this to create a cumulative sum on a boolean column made by df.Diff > 7:\ndf['Batch'] = (df.Diff > 7).cumsum()", "Pandas create a column iteratively - increasing after specific threshold", 0.1973753273487091, 3, 120, "\\n\u00c2\u00c2\u00c3\u00c2B\u00c2\u00c5\u00c2A\u00c2OO\u00c2OPPPPPPPEPEPTESPESPESSSSPESES S S B S BOOT BOO BOOB BOOH B OO OOB B AOOB AAAAAAAAA AAAA AAABB B T B B P B M PB M M BP BMPBMP BMMBMMMM BM B MB BMB MBMBMBBMBMPMBPMBMMPMMB PPMSMSMMSOOMOYO Y YYY Y S YYYYY Y\nY A A\nAA A B C C B\nC CCC\nCCCC CC\nCA\nAB AB AB ABS ASS AS AAsasas\nSs\n33415165662552450886878AAABAABBBBABAA BBBB BBBB\n5\n\\n\\nnnnsnsnseaseaseaseseasees esesees\nese\nes s ee e eses eesessess s s\ns s ss\nb b bb\nbb\nss sss\na a a\np p p pp p\nap ap\n4438593772846989\n7114\n\\\\\\\\23\ni i\nnni i ii\nw w s w ss p a p b p s p ss w\nwp w pps psa\nsa sa\nf 27 92 42 sa sa.saas.sasasa.as sas a aa a.a s.o oo.oo ooa.oaooa oaa.aaasaaaas sa ssaa.seesa.es.2.1.3.5.6.7.8.9.0.4.h.", 0.04926108196377754, 0.0, 0.029556650668382645, 0.054420504719018936], ["Consider a vector [0 1 2] and a matrix of size 3 x n. How can I multiply each element of the vector with the corresoponding row of the matrix. Each element of row 0 should be multiplied with 0, each element of row 1 should be multiplied with 1 and so on?", "I assume you're using numpy. You can use matrix *= vector.reshape(-1, 1). This will convert the vector to a column, then multiply the rows.", "element wise multiplication vector with rows of matrix", 1.2000000476837158, 1, 121, "It looks like this: [...] the method of multiplying an element with other one element.\nHere is an example of how to multiply the elements. 734-48-7-6477441-12-5-30-2495575283-03806-9-A23.97858.596.4536154391467.099.669887940.372.705.2084.171.811332219004262763189293510256501607....\nWe can also multiply by using the multiplier formula: a*b. The formula is applied to different sizes of vector and matrices. In order to get the correct result you need to find the right multiplication formula for these sizes. For example, 9*9 = 5*1 or 4*3 =. I believe you can use different formulas for different size matrices, for example 8*7 = b*2 or you will need 6*6 = a*.\nThank you for answering! Please don't worry, I'm going to learn about math!\nHi, Thank you! please don\u2019t fret, i\u2019m likely to be learning maths!", 0.17341040074825287, 0.035087719559669495, 0.06936416029930115, 0.05171627178788185], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "If the data is small enough that you can use pandas to process it, then you likely don't need pyspark. Spark is useful when you have such large data sizes that it doesn't fit into memory in one machine since it can perform distributed computation. That being said, if the computation is complex enough that it could benefit from a lot of parallelization, then you could see an efficiency boost using pyspark. I'm more comfortable with pyspark's APIs than pandas, so I might end up using pyspark anyways, but whether you'll see an efficiency boost depends a lot on the problem.", "Databricks - Pyspark vs Pandas", 1.2000000476837158, 2, 122, "If you are working in databrick, you don't need to make any changes. You can use whichever library you want and keep the same code you have.\nDataback is an environment with lots of data where you get to see more cases. It also has its own server where people can share their data with others. People also share data in a separate way. Most people also find it difficult to share the data they have on another website where they can create their own accounts and make more money.", 0.29441624879837036, 0.0615384615957737, 0.17258882522583008, 0.04314366728067398], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "Pandas run operations on a single machine whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark is the best fit which could process operations many times(100x) faster than Pandas.\nPySpark is very efficient for processing large datasets. But you can convert spark dataframe to Pandas dataframe after preprocessing and data exploration to train machine learning models using sklearn.", "Databricks - Pyspark vs Pandas", 0.0, 0, 123, "If you're using the pandas from the package, you can just import the library and use it instead.\nI have just started using databrick and have no experience with python, but just tried out this and it worked!\nHere are the two blocks from my code that I was using with pandas:\n`````from import import pandas as pnd````\n''''` import pandas as `pnd.pand.pandas.pd`{2}``\n\\begin{code}\nimport import [...\n]\n{1} [...]\n{} {2} {3} [4] [5] [...] [...] [6] {4} pym ... {5} ... p ...\ns {6} and s {7}.......  {8} ........... and... {9} s ...s ... s s ss ... ... .... .. .... s\nss s p p ss s . .\n. . ....... . \n\u00c2\u00c2 ...\u00c2 .... .\u00c2 . s ps p s. ss ss . ... . ss ps.s ss. ... ss\u00c2.\u00c2 s pp\u00c2p\u00c2s\u00c2ss\u00c2ps\u00c2a ss \u00c2as 001 10 20...\u00c2\no pps.o... ...oopppsoppssoppssps... pssssp s a .s a s 7 8 a a .... .... ...o. 4 .4 5. ....\n435 .5o pp s opp sso ppspppsp\nspssspspapaspppa\nsa .as ss\npppp\n...o\n..... .saa a\n......\nso p soo oppspp\noppo ooo\nb b bb...b\nbbbb...bbbbb....bbs...spspssp....\nps p 6552112232...h.b.bsb..bs.........b.....booopppsooopsosspso................b.....s\n..................s....s.....sss........s ................  .. ... so oopsopsas\nas a as a .....ss....ss...sa....sb....aas........bs\nbsbbbs..bb..b.............bs.....as.....bb.....sa..a....sa.a.saas.as..asbbbo....bo", 0.10526315867900848, 0.01413427572697401, 0.05614035204052925, 0.02968144603073597], ["I have a scenario in which I have a peptide frame having 9 AA. I want to generate all possible peptides by replacing a maximum of 3 AA on this frame ie by replacing only 1 or 2 or 3 AA.\nThe frame is CKASGFTFS and I want to see all the mutants by replacing a maximum of 3 AA from the pool of 20 AA.\nwe have a pool of 20 different AA (A,R,N,D,E,G,C,Q,H,I,L,K,M,F,P,S,T,W,Y,V).\nI am new to coding so Can someone help me out with how to code for this in Python or Biopython.\noutput is supposed to be a list of unique sequences like below:\nCKASGFTFT, CTTSGFTFS, CTASGKTFS, CTASAFTWS, CTRSGFTFS, CKASEFTFS ....so on so forth getting 1, 2, or 3 substitutions from the pool of AA without changing the existing frame.", "Let's compute the total number of mutations that you are looking for.\nSay you want to replace a single AA. Firstly, there are 9 AAs in your frame, each of which can be changed into one of 19 other AA. That's 9 * 19 = 171\nIf you want to change two AA, there are 9c2 = 36 combinations of AA in your frame, and 19^2 permutations of two of the pool. That gives us 36 * 19^2 = 12996\nFinally, if you want to change three, there are 9c3 = 84 combinations and 19^3 permutations of three of the pool. That gives us 84 * 19^3 = 576156\nPut it all together and you get 171 + 12996 + 576156 = 589323 possible mutations. Hopefully, this helps illustrate the scale of the task you are trying to accomplish!", "Generate the all possible unique peptides (permutants) in Python/Biopython", 0.0996679961681366, 1, 124, "you need to define a frame and generate pepples. so you can do a quick search like the below\nd=pepdb.db(0)\nf=d.frame(3,3)", 0.1358024626970291, 0.012500000186264515, 0.09876543283462524, 0.03960396349430084], ["When I try to install sklearn-crfsuite, I get the following error:\n\nfatal error C1083: Cannot open include file: 'basetsd.h': No such file\nor directory\n\ntrying this command pip install sklearn-crfsuite, also installed Microsoft visual C++ 2019 and the required libraries.\nPlease let me know if there is any solution to this, do I need to set any variable in the system path?", "If I understand your problem correctly, add the path to your header file in your project using Property->C/C++->General->Additional Include Directories.\nIf you want this to apply to all your projects use the Property manager to add this path.\nOf course make sure the header exists.", "fatal error C1083: Cannot open include file: 'basetsd.h': No such file or directory", 0.0, 0, 125, "The error message is completely wrong. You can open an include (a .h) file with the `include` command. The `.h` files can be in any location.", 0.18666666746139526, 0.0, 0.1066666692495346, 0.049116767942905426], ["ctx=con.cursor()\nctx.execute(select col1 from table1)\nresult=ctx.fetchall()\ndata=pd.DataFrame(result)\ndata.columns['field']\nfor index,row in data:\nupdate table2 set col2='some value' where col1=str(row['field'])", "Solution to this is:\nInsert the data into some transient table and then then use that table for update.\nFor insert :\ndata = panda.DataFrame(result)\njust use data.to_csv('file complete path',index=False,Header=True)\nusing put command place the file in internal stage and from there use Copy command to copy data into transient table.\nlater on you can use this table to update your target table.", "Updating snowflake table row by row using panda dataframe (iterrows()) taking lot of time .Can some one give better approach to speed up updates?", 1.2000000476837158, 0, 126, "I don't see the \"wow\" factor in this answer. I've been through the process, it takes time. It's not a one-liner.\nIt takes a few minutes to update the data. However, you need to do the research, find out how to go about it and then do it. So I think, I just have to admit that I didn'\u00c3\u00c3\u00a1n t\u00c3\u00a2\u00c3\u00a5h\u00c3\u00c2nh t\u00c4V\u00c32.366 and he was justified, that he had to think out a bit. \u00c2.\u00c2\u00c2\nWhat kind of thoughts? I can'\u00c2\u00c3\u00e2h3.68.2 and I was \u00c3Viet Nh2341386,690. That'S\u00c3B3,96.0, the only thing that comes to mind is the number 2,352 that they had received from him. What was it? It was something like an antic, a big antique that was given to him from the president. A lot. Th\nAnswA\nwEk A wokAwK and wOkA and a lot and lots of things came to her mind. Like the other things she used to look out in the future, she was very confident that she would be able to achieve things like those. Because of the fact that the people she worked with were very happy. She was happy with the idea of getting those kinds of stuffs and the work she could achieve that would have a positive impact on the society. The fact of that being able of such kinda stuff and of having the ability to be happy that those kindas things would bring happiness to the families of her country. And the country would also benefit from those things. There would, therefore, be lots and other such things that came into her thoughts.", 0.12535612285137177, 0.017191976308822632, 0.07407407462596893, 0.030168769881129265], ["after model training, spacy has generated model\\model-best and model\\model-last folders. What's the difference between the two models and which one should be used for predictions?", "model-best is the model that got the highest score on the dev set. It is usually the model you would want to use.\nmodel-last is the model trained in the last iteration. You might want to use it if you resume training.", "difference between model-best and model-last in spacy", 1.2000000476837158, 1, 127, "this is a common question. here are some tips to help you answer it.\nfirst: it'll help a lot if you show your spaci\nshow your predicting model\nsecond: as spocs know, the last model, or model best, is actually the prediction model that spoca makes during training. the best model will usually be a very good prediction that can do well. now, let me tell you this: you should only make one prediction. one single prediction with a single data set. this way, you can test your predictions and also test if they are making accurate predictions. that'\nthird: if your prediction data, like a file, then you'd test it with all the data. all data sets are the same. and if it does a great prediction, it will be able to do a better prediction in the future. also, if the file you test has some data in it, and you want to predict it too, maybe the predictions won't be too great because you have less data to work with. so, test how accurate the model predicts. if predictable, great. but if not, get the whole file and test that way. i know that there are three data files in different formats. there're also three files with different data and different models. try testing how far the new prediction will predict the old file. a long way predict and predict. because this prediction is the predicted best and most accurate, so the third file will make more accurate and predicted predictions in future as the other two files will do. you will test the real and the fake as well as. make sure your data is good and that your new data makes a real prediction as much as you make a good one with your real data with the false prediction you made. it makes more predicted and accurate as future, that prediction was false, but real. with this, i make it clear to you that it should never predict two times and it doesn'twalk the wookkah wokk that the tes is not the first time as it is so many times. (3) it would be very stupid and not so smart to make several predictions because if a new and false data would make better predictions than a false file would, in theory it couldn'tkwalk woks on the second prediction time. although the process of making predictions makes it seem logical, one time makes the forecasting in a few seconds and a whole new time would take hours or even days to be", 0.13389120995998383, 0.04621848836541176, 0.0920502096414566, 0.039772726595401764], ["I am using compute_face_descriptor function in dlib which is the function of dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat').\nThere is an option to set \"num_jitters\". I set \"num_jitters\"=10, but the output embedding I am getting different on subsequent runs. I have tried setting seed using np.random.seed(43), but still, the output changes on subsequent runs\nIs there a way to set seed in this function using \"num_jitters\"=10 so that the output embedding doesn't change on subsequent runs?", "\"num_jitters\". means how many times dlib will re-sample your face image each time randomly moving it a little bit. That's why you are getting different embeddings.", "How to set seed to compute_face_descriptor function in dlib?", 0.0, 0, 128, "You should use dlt_fit_dt_desc_from_d_lt.\nIt's just 6 lines!\nAnd there are a lot of people using it! It'll work out! Trust me! And it'sa bitof theft! So, let the\nthe the the! The other thing! Is! You know! That'so\nhow do you set the seed of the \ufffdh\ufffd?!\ufffd\ufffd\nset! set! it works! Okay! Go with it!...\nOkay! Let\ufffdem! Set the\ufffdset the set\ufffdthat! They'\nokay\ufffdoky! okay! go with! Ok! Right! Come\ncome! come! that! I'm\ufffdcoming! OK! We\nwe we! wee! thee\nee! you know that\n\ufffdyou know\nyou should! should\nuse! use! uu! 11\n17! (I was trying\nI could!)\nwhat! what! w\ufffdhow to\nW\ufffdH! W\ufffdHH! WHAT! TO\nSET! SET! IT WORKS! AH\nTHE! THE! UU!UU\nU\ufffdUUB\nA\ufffdA\ufffd\ufffdA AAAAAAA\nH\ufffdY\nYOO! YO\nOOF OFF\nOFF O\nOTTTTTTTT!!O\ufffdOOL\ufffdOL\nLILI\ufffdLILLILILIIIIIIIIIIII!!!!HIII!!!\ufffdIII\ufffdIi\ufffd\ufffdi\ufffdiI\nP\ufffdP\nPA\ufffdPAA PA\n\ufffd\ufffd\ufffdah\ufffd! ah! a\na\ufffdaahh! aaah! af\nAFARFARAPAP! PA! PAPP! THPP! TESS! T\nTT\ufffdTT\u00c2\u00c2\n\u00c3\u00c3\u00c2AAAAA\u00c2A\u00c2AP\nPOPE! POO PO PO P\ufffdPO\ufffd PO\n!\u00c2!!AA! AA! AB\nabba abba\nbboo! bb! be be b\ufffdb\np\ufffd\ufffdpppp\ufffd\ufffd\ufffd\ufffdp\ufffdp\nAA\ufffdAAAAAAAAA\nS\u00c2S\u00c3\nSA! SAA!!!!!!!!\nSEESSSSS\n(Ho\ns.s\n.)\n)\ufffd) (\n,bBb!)b,!bBBB!!BBBB!!B(\n).(b)b(", 0.055776894092559814, 0.0, 0.03187251091003418, 0.06518891453742981], ["during i trained my own model, i have a simple question.\norigianl input image shape is (height : 434, width : 636), and i used resized image(416 x 416) for my train model(Unet++).\nI wonder if it is right to resize the test image when inference step, How can I resize the model output to the original image size when comparing test output with original test image.\n---------process\noriginal input size : (434, 636)\ntrain input size: (416, 416)\n\ninference\ntest img -> resize (416, 416) -> test model -> test output(416,416) -> comparing test output with test img", "It's absolutely right to resize the input image to the model input size else, it will generate an error if you feed an image of different size to the model. Coming towards your question, you can solve this either by rescaling the model output to the original size of your input images. A simple technique can be resizing the masks but there can be better ways. OR\nYou can resize your input images and their Ground Truths (masks) to the model size, and so you won't need to rescale the model's output. I hope that answers the question !!!", "Question about Inference for Image segmentation", 0.0, 0, 129, "Test Image is a 2D shape. So, test input could be resizied to an image of any shape, size, and dimension.", 0.1803278625011444, 0.05000000074505806, 0.1147540956735611, 0.03488132730126381], ["I want to get the molecules from the SMILES using rdkit in python. The SMILES I used was downloaded from the drugbank.\nHowever, when I using the function Chem.MolFromSmiles, some SMILES would report but some wouldn't:\nExplicit valence for atom # 0 N, 4, is greater than permitted.\nI found some explanation about this problem: it is because the SMILES generated a invalid molecule that doesn't exist in real world.  But I am not a chemistry student.... So anyone know how to fix this?", "Your SMILES string would appear to have a neutral 4-co-ordinate nitrogen atom in it, which doesn't exist in real molecules.  4-co-ordinate nitrogen atoms have a positive charge, eg [N+] in a SMILES string.", "Problems encountered when using RDKIT to convert SMILES to mol", 1.2000000476837158, 2, 130, "The valance for an atom is 1, this can be done in the RDMKIT. However, if you do an explicit valense for more than 3, it won'...", 0.0923076942563057, 0.0, 0.0923076942563057, 0.04667463153600693], ["I am trying to write several Panda Dataframes into a SQL database. The dataframes are generated in different processes using the multiprocessing library.\nEach dataframe should get its own trial number when it is written into the database. Can I solve this using SQL autoincrement or do I have to create a counter variable in the Python code.\nIf I use the function pandas.DataFrame.to_sql and set an index as autoincrement, I get a consecutive index for each row.\nHere is an example how it should look like\n\n\n\n\ntrial number\ntimestamp\nvalue\n\n\n\n\n1\ntime1\nvalue1\n\n\n1\ntime2\nvalue2\n\n\n1\ntime_n\nvalue_n\n\n\n2\ntime1\nvalue1\n\n\n2\ntime2\nvalue2\n\n\n2\ntime3\nvalue3\n\n\n2\ntime_n\nvalue_n\n\n\n\n\nI use Python 3.9 and MariaDb as Database. I hope for help. Thanks", "You should have a separate trials table in your database where you cspture the details of each trial. The trials table will have an auto incremented id field.\nBefore writing your dataframes to your values table, each process inserts a record into the trials table and get the generated auto increment value.\nThen use this value to set the trial number column when you dump the frame to your table.", "Write Panda Dataframes to SQL. Each data frame must be identifiable by a trial number", 0.0, 0, 131, "answer. how do you ...? Do you think ... Do it and you'll be very successful ...\"", 0.0941176488995552, 0.0, 0.0941176488995552, 0.014918448403477669], ["df['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : x.replace('.', ',',1).replace('.', '').replace(',', '.',1)).astype(float)\nSlowly learning lambda command, my understanding of this line of code is:\n\nChange dataframe type to str\nApply lambda with one perimeter x\nReplace all the string format . to , , (I don't understand what does 1 stands for, have done research prior asking, didn't find clue)\nReplace all the string format . to null value\nReplace all the string format , to . , (again still have no clue what does 1 stands for in this case)\nChange dataframe type to float\n\nPlease help me better understanding this line of code, thank you", "This replaces the first . in the string with a ,, removes the remaining periods, changes the first , back to a ., then converts the result to a float for the 'Current Ver' column in the dataframe.", "How to understand this lambda with 3 .replace() line of code", 1.2000000476837158, 2, 132, "... ... how to underst...\nUnderstand how the code works!\nThank you for reading and commenting! I appreciate your kind comments! And ... .... .... ... How To Understand This ... The ...\nThe ... \u200b ...  \u200b\u200b \ufffd \ufffd\ufffd! How 2 understand! 0!", 0.21875, 0.0, 0.15625, 0.04999999701976776], ["Initially, my dataframe had a Month column containing numbers representing the months.\n\n\n\n\nMonth\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\nI typed df[\"Month\"] = pd.to_datetime(df[\"Month\"]) and I get this...\n\n\n\n\nMonth\n\n\n\n\n970-01-01 00:00:00.0000000001\n\n\n1970-01-01 00:00:00.000000002\n\n\n1970-01-01 00:00:00.000000003\n\n\n1970-01-01 00:00:00.000000004\n\n\n\n\nI would like to just retain just the dates and not the time. Any solutions?", "get the date from the column using df['Month'].dt.date", "Date and Time Format Conversion in Pandas, Python", 0.0, 0, 133, "Solve the Problem!\nP...Pp...p\n6\n0\n? 65376178834779987\n738448\nd1d3d4 756 107 25 360585093 9452335\n...?26728646963?1243229?314?42?5?6?c?a??4a4f9a54-495-9-559.5.13.9#6. 82.4.???.7-8??? 4???11?0???1????.7.8-?? 57,9,8. B920,0890 -9 -?7?9!4!5!8!!!5!!!!!0!!!!066!!6!6a6aa91,a2a,3,4 and \u00c3\u00c4\u00c4\u00c3\u00c5\u00c5\u00c4!!4!!!6#?8??9!!!1#2!3!D3D!!?dD,D_D!!?w??!bwawawasbwerwerwasbwr2-3w2...2???3...3-2!!!3#!!3!!WWWA2!!1...  W  AW...W  H2  D W D D....W H30...OO!...(W)!!o!c3s4s3sb2s1b21bo1816b3bo7!7!!...715...!W2[...]...H2W[]2D[DW]C3[!]3B3S4S3_S_SA_A_B2B_W_E_2_w-o-w!w!!-W!!w...!!......!!H3C!!F3O!!2??3H6!!!1.....7.....................8.....1.........2....H4.......9...1H!!C2H!C...C[2]H#H?....3....4h5h1!!h....2,B1_4,27 and the", 0.025477707386016846, 0.0, 0.025477707386016846, 0.021271640434861183], ["I've installed the native version of python3 through miniforge and the intel version of Spyder through homebrew. Everything is working fine with one exception, my plots seem to work with the \"graphics backend\" \"inline\" option. When I try to select the \"automatic\" option, or any other option rather than inline, the IPython doesn't initiate. Has anyone had the same problem?\nKind regards,", "(Spyder maintainer here) This problem is fixed in our 5.2.0 version, released in November 2021.", "Spyder \"Graphics backend\" \"automatic\" option not working on M1 macbook", 0.0, 0, 134, "... ... .... ...\nOk so ... let's do this ...", 0.08695652335882187, 0.0, 0.08695652335882187, 0.052294693887233734], ["I have df = pd.concat(dict_of_df, axis=0) and sometimes [rarely] it might be the case that all of the df in the dictionary are empty in which case I would like Pandas to cheerfully return an empty dataframe. But instead I get a ValueError.\nI can write a loop to check for the length of each df before calling concat, but would prefer to not always do that, so at the moment I just embed the concat into a try/except... which doesnt make be really happy either because if there was a \"true\" ValueError I would like to have know it. So then I could do a try/except loop and if exception is thrown then do a count of all the dicts and ... ugh. This is getting crazy.\nIs there something more clean? Thanks.", "Sorry, I am going to withdraw the question. I now realize that\npd.concat([None,None]) produces the ValueError, whereas as noted above pd.concat(pd.DataFrame(),pd.DataFrame()) does exactly what you would hope. Also pd.concat([None,pd.DataFrame()]) is fine too. So it's not really fair of me to complain about concat. I need to stop feeding my routine non-existent datasets !\nThanks for feedback", "Can I avoid a ValueError concatenating empty dataframes?", 0.0, 0, 135, "Are you sure that there isn't always a value error? I know I got a bit angry with my mother on a Friday, because it's Saturday, and Friday you can, you know, there'sa value error at 12 o'clock on Friday. And they know that if Friday comes to 3.3 or 4.0 they won'e even have had a shake, they would have the shaken. I think you need to take a long route before you do anything, like I do all week before Saturday. Saturday is a wonderful day. You can'have your day without any wor, without a...\"...\nAnd I certainly don'\nt know\nthat there certainly isn\n...t there\nanyway\nSo when Saturday comes you\ncant have anything else\nwithout doing the right thing\nthe right way\n\"That\"\nHey,\nHi!\nSha! So, we can do something! We can!We can!!!!!!!We Can!!We 20! WOW! 0OO!HANNE 7\" 99!OOH!NICONCATCHING OOLLY HANNNNNNNN...NORNN!NNNRNNG!WE CONNICATED!!\"\nwe can\n# 833\nCan you please avoiding a\nvalue error\nwould like\nPotah\n(Pahah)\nPosted!", 0.12851405143737793, 0.016194332391023636, 0.06425702571868896, 0.032445937395095825], ["in pandas the inplace parameter make modification on the reference but I know in python data are sent by value not by reference i want to know how this is implemented or how this work", "Python\u2019s argument passing model is neither \u201cPass by Value\u201d nor \u201cPass by Reference\u201d but it is \u201cPass by Object Reference\u201d\n\nWhen you pass a dictionary to a function and modify that dictionary inside the function, the changes will reflect on the dictionary everywhere.\nHowever, here we are dealing with something even less ambiguous. When passing inplace=True to a method call on a pandas object (be it a Series or a DataFrame), we are simply saying: change the current object instead of getting me a new one. Method calls can modify variables of the instances on which they were called - this is independent of whether a language is \"call by value\" or \"call by reference\". The only case in which this would get tricky is if a language only had constants (think val) and no variables (think var) - think purely functional languages. Then, it's true - you can only return new objects and can't modify any old ones. In practice, though, even in purest of languages you can find ways to update records in-place.", "the inplace parameter in pandas how it works?", 1.2000000476837158, 1, 136, "this can be answered by this answer however it is not accepted to this blog to be a blogger you have to put your real name or a nick name on this site however if you are a novel or not a novelist you can create blog sites however for more information on how to create a website for a book and how the novel can work this\nanswer: I don't know I am a fictionist however a\nfictionist\nhow to work a site?...how does it work?....how do you work the site......\ncan you make an answer?.....can' you?.............\nis this a question? .........is it a.....? ...............\nHow can you answer a? I? Don'?t? Know?!\n....How Can You Work A? ...How Does It Work? ......\n..........How Do You? Work The Site?........................................\n.the inpac? o? p?a?p a a ...how?.. How? Does it Work????..... ....\n? the i?nplace? t?h?o?k? ? how?does it?work????.... .... ...\nthe?inplace p h a o k h ...h o w d s it w rk k?!!?? ....... ...... ...t h e in place? P H?O?K?H?W??D?S?it?w?rkk!???..... ... ... t h?i np p o ke? h o ? w ?h how s t works????? .... .... ? ...theinplacet h hok hw d?s itw rks???!!!??!??!!!!!.....w ??the i nplace t H H O K H W ? ?D ? S IT W RKS?? ???.... ? t ? h i t w rs k s ???? .s i thwrsks? s i ts t whs ks???s s s ss ss sssss sssss ? ssth w s k ss w?tt w hs on dsa onsaonss on a ons a nsnsnns n nsn onsds soons on s a as a aa as aa ss ? as asss as ?s as s as ssas? as?as asasas sas ss?ss s?f 66s o o", 0.20876826345920563, 0.025157231837511063, 0.1085594967007637, 0.036186475306749344], ["I have 2 columns of data with the first column being product codes (all filled out) and the second column with product description.\nThe first column has all the product codes filled out but there are some rows where the product description (second column) is missing.\nFor example row 200 has a product code of 145 but the description on that row is empty (NaN). However, there are other rows with product code 145 where the description exists, which is \"laptop\". I would like to have the description of row 200 to be filled with \"laptop\" because that's the description for that product code.\nI want to find a solution where I can fill out all NaN values in the second column (product description) based on the first column (product code).", "First, decide on a function that takes descriptions and picks out one of them. You could use min, max, mode, define you own get_desc, etc. Then you can separate the dataframe by product code with groupby and apply whatever function you decided on: df.groupby('product code').apply(get_desc) or df.groupby('product code')['product description'].apply(get_desc) depending on whether get_desc takes a dataframe or column as input. Then you can merge the resulting dataframe with your original dataframe. You can either replace the entire original product description column with the product description column of the groupby output, or have merge create a new column, then fillna the old product description with the new product description.", "Fill NaN in column 2 with median string based on value in column 1 in Python", 0.0, 0, 137, "...answer 4 months ...\nhttps://hyp.h.z2.3.0.1.9.c.p.2607.s.t.b.6.5.8.778875767 7 (6 6) 8-8 9-11 383664.q33q.o.a.n.r.46155932394737402434412\n3-270-3 0 5 \n. \u00c2\u00c2. H\u00c4\u00c4.\u00c4\u00cb\u00c4V\u00c4F\u00c40\u00c41\u00c42\u00c43\u00c5\u00c4H\u00c4\u00c5\u00c5BOH!\nhttp3,0,2,6\n8,10 and \u0093\n13and \u00948\n4 \u00b4\u00b45`\u00b4\n``6`B7`\n6-4-W\nw \u00c3\u00c5-\u00c4\n\u00c4\u00e2\u00c5\u00c3\u00c4ST\u00c4\u00c2STB\u00c5B\u00c4WW-\u00c2W\u00c4B\u00c3BWA-B\n-N\u00c4N\n\u00c3NANN\u00c5NVANA NAAANNN\u00c2\u00c4\u00c3L\u00c2S\u00c4EW\u00c2E\u00c2C\u00c2F\u00c3\u00c3AO\u00c4K\u00c4A\u00c2KA-OO\u00c3K-OPEN-\u00c3-JA\u00c4O19,99\nJ\u00c4AA\u00c5\u00c2A2\u00c5F\nD\u00c4NN\u00c4TT\u00c4L\u00c4D\u00c3\u00c2\u00c3ALSASA\nOOC-\n\u00c2J\u00c5H\u00c2ALAAL\nCOC\u00c4C\u00c3NA-C\u00c52O\u00c5C\u00e2\u00c4T-A\u00e2A\u00c530AAFFF\u00c5AA-E\u00c4-T\u00c4R\u00c4S-KOCK\u00c4NA\u00c4DEM\u00c4RE-M-PORTE\u00c4CH-PA-RO\u00c4LE25-WAH-STAAWA\u00c4HO\u00c4TW\u00c4HA\u00c4MAT\u00c45\u00c48\u00c33\nP\u00c47\u00c49\u00c4P\u00c2\u00c5L9 \u00c4\u00c4\ufffd\u00c4\u00d0\u00c4AS\u00c4ATHRE\u00c4Y\u00c46\u00c2AA\u00c4UT\u00c4MO\u00c5T\u00c59\u00c5A0\nST\u00c21S2\u00c2P\n#7\nFRE\u00c5WATER\nSELL\u00c4PA\u00c4CA\n7\u00c5AP-L\u00c5084C-6\u00c4", 0.03412969410419464, 0.0, 0.027303753420710564, 0.008960573934018612], ["I want to horizontally cut the spectrogram of a wav file into 24 pieces,and after measuring the power of each piece, and finally rank the pieces by power orders what should I do please", "Could you show some code that you have written to try out the same? It would be easier to help if we have something to build upon and rectify issues, if any.\nAdditionally please try basic image manipulation to do the same. Instead of cutting you could divide the image into N (here 24) regions and analyze them in parallel using multiprocessing.", "cut the spectrogram of a wav file", 0.0, 0, 138, "to measure the wavelength of the radio wave file, it takes a large amount of time\nto measure each wave by turning off the device, the time for a month, two months, three months or for any period\nthat you do not forget the file. the only way to make the wave is a short cut and the\nspectrum of this wave has a wide range, where it ranges from 0-190 hz.\nhow can you make\nthe wave? how do you cut it? you can cut your phone and you will get\na large piece of your voice and some of it. what is the length of\nyour voice?\ncutting the volume of cutting your volume in the second piece. how can it be done?!", 0.21276596188545227, 0.02150537632405758, 0.12765957415103912, 0.06334707140922546], ["I am trying to understand why one or two parameters in my Pytorch neural network occasionally become nan after calling optimizer.step().\nI have already checked the gradients after calling .backward() and just before calling the optimizer, and they neither contain nans nor are very large. I am doing gradient clipping, but I don't think that this can be responsible since the gradients still look fine after clipping. I am using single-precision floats everywhere.\nThis behavior happens randomly every hundred thousand epochs or so, and is proving very difficult to debug. Unfortunately the code is too long to reproduce here and I haven't been able to replicate the problem in a smaller example.\nIf anyone can suggest possible issues I haven't mentioned above, that would be super helpful.\nThanks!", "This ended up being ignorance on my part: there were Infs in the gradients that were evading my diagnostic code, as I didn't realize Pytorch's .isnan() method doesn't detect them.", "What are the main reasons why some network parameters might become nan after calling optimizer.step in Pytorch?", 1.2000000476837158, 0, 139, "It turns out that there was a bug in the network that was causing the problems above. The network had been overloaded by an excess of variables, so the optimization step could not produce a result of any type. This is the cause for the failure of the neural net that I describe below. So far we have not seen failure in single, or, two-part neural nets. However, one single or 2-neural net had failed. It had become ana, which indicates failure. In other cases, however, the netz became anan, indicating a high frequency. Now I know that the first reasons are that it is due to the excess, of other, reasons that cause the failures of netic networks. 18) In netc ... HAH! ... 6! --! 9! YOU WOULD NEVER REBOUT HENCE! -! P YTORCH NETWORKS! .... HAAA! H ... ! ...\nBur ... ... of ... the 3rd one! I ... me?! BUROBO! (Burn of Hahah)\nMy name is Peter. ... I just started studying PYTAHROCH! and you? ... and?!!Hahhaaa! \"\nThe P. T. H. R. C. N. was in ...! D-! F\n2 ...22! C\n.C! AHHHA! OH HOW LONDE HYOU!! LADY! WHAT WOOHAHYU!? HYYYYYYYHHYY\nY YOU\nRW YYWW00W\nWO Y\nOW\n0. W.O! WONT0! SCHAHH!H\nTOP BOO\nD!D D!W!O0O 0\n5,6\n4!60667!0D 58!YAYAAYHH\nAHAHA?HHHH?O?PETER BUSCH\nTHE 4\n70 74 83\n84 HHHHHAAAAAAAA\nAAADDADDDAADDAADDAAADDDDAEDEEEE\nSUSUSUUS! SS!SS!S!SOO OOOH! SOOHNONONNYNNY NNNNNN", 0.0749063640832901, 0.007547169923782349, 0.06741572916507721, 0.05998768284916878], ["Can someone help me with transforming the following table using a PYTHON function?\nI need 2 new columns: A \"follower Type\" which will have entries as organic or paid and a \"Follower count\" which has the values corresponding to the type of follower.\nCurrent Table -\n\n\n\n\norg\norganic follower\npaid follower\nstart date\nstop date\n\n\n\n\nOne\n2\n0\n1634169600000\n1634256000000\n\n\nOne\n-1\n0\n1634256000000\n1634342400000\n\n\n\n\nDesired Table -\n\n\n\n\norg\nstart date\nstop date\nFollower Type\nFollower Count\n\n\n\n\nOne\n1634169600000\n1634256000000\nOrganic\n2\n\n\nOne\n1634169600000\n1634256000000\nPaid\n0\n\n\nOne\n1634256000000\n1634342400000\nOrganic\n-1\n\n\nOne\n1634256000000\n1634342400000\nPaid\n0\n\n\n\n\nIf anybody knows how to do this, please do let me know.\nThanks and Cheers!", "Use reindex to change column order\n'''\ncolumn_names = [\"C\", \"A\", \"B\"]\ndf = df.reindex(columns=column_names)\n'''\nLike below you can add columns to existing dataframe\ndf[newcolumn]=formula", "How can I do the following dataframe transformation in Python?", 0.0, 0, 140, "\\begin{code}\nf.table(\"follow_table\").replace(\"2\",\"organich\", \"0\"\",\"paidd\").f()\nfollow table\n|202|927267551786|1449483062238|0|\n\\end{Code}", 0.0, 0.0, 0.0, 0.04580152407288551], ["I have a dataset with more than 50 columns and I'm trying to find a way in Python to make a simple linear regression between each combination of variables. The goal here is to find a starting point in furthering my analysis (i.e, I will dwelve deeper into those pairs that have a somewhat significant R Square).\nI've put all my columns in a list of numpy arrays. How could I go about making a simple linear regression between each combination, and for that combination, print the R square? Is there a possibility to try also a multiple linear regression, with up to 5-6 variables, again with each combination?\nEach array has ~200 rows, so code efficiency in terms of speed would not be a big issue for this personal project.", "This is more of an EDA problem than a python problem. Look into some regression resources, specifically a correlation matrix. However, one possible solution could use itertools.combinations with a group size of 6. This will give you 15,890,700 different options for running a regression so unless you want to run greater than 15 million regressions you should do some EDA to find important features in your dataset.", "Automatic Linear/Multiple Regression in Python with 50+ columns", 0.0, 0, 141, "30 August 26 16 September 91 09 65\n03 7 August12 4 8 \n55 \ufffd\u00ad0 \u00c2 \u00c3\nv70 June \ufffd2 June\n23 May \u0092\u00922\u0092\n34 May [M]\n... \u0093W...\u0094\nMay \u200b...\n6 May\n\u200b#\n# #\n8 May # # $ $ ~~ ~ $ # C $ F $ D D F F DFFDDFDFDDDFDFFDFDDAFFAAAF05 F A A ndfdnfndfnndnfnnnnz z zzzzz \u00a9\ufffd\u00993,580~~_#_\n_0_z_-_...-#-0-1-4\n-5 &-... #-A-\n107 May May...\u00c20 - \u00a2 & #c. \u00b2#c3.5335.390 May-May##01 #0\nLearning and Training: Training to Fostering Learning: Cases that Foster Learning. A Guide for Learning and Learning for Training and Focusing Learning Learning Lessons: Learning, Learning or Learning from Training Lesson Lessions: The Training Process: TT Training...#976677 \u00b354.7. MayMayMayM\u00c2\u00c4\u00c2\u00c2.\u00c4\u00c4....May.........\n472 May.May.M\n~\u00c2\nThe\n................................2\nH452832 H ................3\n\u00e224025H2H \u00c4VH\u00c4\u00c5H H2WW\u00c4BHH\u00c222789H3B H H\u00c2BhH ABOBBW BWBWIW\u00c2W\u00c3\u00c4WV\u00c4L\u00c2\u00c3\u00c3\u00c2\u00e2V\u00c2V\n\u00c41H\nA\u00c4A \u00c4A A\u00c4O\u00c4H B\u00c5BO\u00c5B\u00c4h\nBBOO\nOO\u00c2O3W OAOA\u00c23\u00c2A\nM2 M M2 MA M\nm1 m0 m1\nW2\u00c2m2 WWO2B\nw2 wb0\u00c2HW\n\u00c321718HKOKWK K WO K\u00c4\nK1KK\n.", 0.05882352963089943, 0.0, 0.042016807943582535, 0.008580735884606838], ["I use tf.math.round() but the output still has decimal points (310.0, 210.0 etc)\nIf I use tf.cast(tf.math.round(), dtype=\"int32\"), then I see the error mentioned in the title when calling finish on tornado handler\nHow can I cast to int using tensorflow operations and still be json serielizable", "You should use tf.int32 instead.", "tf cast leads to TypeError: Object of type 'int32' is not JSON serializable", 0.0, 0, 142, "use the tff function to cast the integer to float. The function must be a valid tape. If you use it on a long tube, it must reach the center of the tubes.\nSorry, I can't help with this one\nCan anyone help me?\nPlease answer this question\nA question about the word that you are not a real person\nAre you a true person? What do you have? Is it true? Do you live on True Road? Are you real? Tell the truth? That word is the true meaning\nI'm really not someone to watch out for\nDo you know something about me or my friends? You know a lot about our friends and family\nIs it the real truth about you? Where is it really the answer to the question? Or is\nYou really\nThis is a really good thing to say that. But it doesn'", 0.025641025975346565, 0.0, 0.025641025975346565, 0.1424555480480194], ["I trained my named entity recognizer with spacy. I would like to evaluate it. So I looked at the spacy documentation and came across the scorer function. However, it doesn't seem to work with the IOB format. Do you think there will be a way to use spacy to evaluate my IOB data or am I doomed to transform my data into the format spacy wants?\nThank u very much :)", "You can't evaluate IOB data directly. You should be able to just use spacy convert to convert it to .spacy data in one step and then use spacy evaluate with that file though. (And if you trained your model in spaCy then presumably you already did the same conversion with your training and dev data?)", "Python - Is there a way to evaluate a named entity recognizer trained on IOB data using spacy?", 0.0, 0, 143, "python - how to eval a 20-25 word named entity recaller? 1.5.1: can's use the name_n_entity_recaller_py.exe script. If you want to go further than 0, you'll need a script that can handle 3-5 words. And you can go even farther if you learn how python can be used. You can find a few books in the store that show you the way of the script and you may find them on the website www.you can buy online. But it all depends on how you handle it, the better and more professional it is to look at. Therefore, to be successful with it and to find it in a store. 4.4: how can you find the best spa script? You know what you get when it'a2: what? What you buy?! When? A2 6.7: do you know how? The 7th line of your favorite poem. The poet of my favorite book was in his favorite lyrics. It'it'sa2 line \u00c2\u00c2t\nhow can i use py script on python? I use my python-script on a database-\npy... script... How can I test my py...s...t... on...\nHow can test or evaluate your py-named-entities-recaker-? How do we use python script to test our data? Do we have to buy the scripts? Or just use them? Please answer!\nA2 B-B: you could test your n-word-g-p-s-t-with-spa! But you won'thave any py! So how could you use your script in python \u00c3\u00c4\u00c3! How could we?", 0.17391304671764374, 0.01875000074505806, 0.09316769987344742, 0.03251923620700836], ["I am trying to save / load data and objects in Python. I usually use pickle to save pandas data frame and custom objects. Recently I had to change python version (from 3.6 to 3.8) and pandas / pickle version accordingly. I now have trouble to read previous pickled version. I have found some ways to deal with that (ranging from using some pickle options to reloading / rewriting the data).\nHowever I would be interested in a more generic way to save data / objects that would be python / packages independant. Does such a thing exists (without adding to much weird dependencies) ?", "If you save your data as a .CSV file (depending on what your data looks like) you should'nt get dependencies problem\nIf your data cannot be saved as a csv I think using JSON could also be a solution", "Save data / objects without python / pandas / pickle dependencies?", 0.0, 0, 144, "Yes, you can save the pickles. You can do it without much trouble too.\nSaving Pickles\nTo save a pick, it is simple to use the \\text{pick} method:\npick = pickler.make(obj)\n...and then to store it, use: \\...\\\\\\\\[\\\\]\nDuring the installation, ... the user can ... create a ... pick .... [[...\nMoreover, in this case ..., the method is called at the time of ... installation .... .... So, even when you install the software, your ... will ... store ... .... In the \"store ...\", you will do something similar ... but in general, do not require much time and effort.", 0.09302325546741486, 0.0, 0.07751937955617905, 0.03086419589817524], ["ERROR: Failed building wheel for scikit-image\nFailed to build scikit-image\nERROR: Could not build wheels for scikit-image, which is required to install pyproject.toml-based projects", "Did you try to upgrade pip ( python -m pip install --upgrade pip )?\nTry installing numpy first too.", "Failed building wheel for scikit-image", 0.0, 0, 145, "You may have the build failer in your build, or you may not have it in the project.\nSciKi is a 3rd party wheel and you need to add it into your project before you install it\nI am using Python 2.7 with Python.3.0 and it worked for me. Hope it works for you too!", 0.13333334028720856, 0.0, 0.1066666692495346, 0.024659177288413048], ["in google colaboratory using python, I am trying to load model to classify\nI am trying to load keras model using python to classify image, I am getting above error", "predict_classes is only available for the Sequential class.\nWith the Model class, you can use the predict method which will give you a vector of probabilities and then get the argmax of this vector (with np.argmax(y_pred1,axis=1)).", "AttributeError: 'Functional' object has no attribute 'predict_classes'", 0.0, 0, 146, "you should try to install the kerass library. After installation, you need to go to your google drive and put your data there. Finally, write the data with some words in the title. Then, the program will work.\nPlease share your experience with the solution to the problem. Thank you.", 0.239130437374115, 0.02222222276031971, 0.15217390656471252, 0.045895952731370926], ["In practice, using both [..., :2] and [:2] on np.array([1,2,3]) results in np.array([1,2]). Are there also cases where the result differs when you use an ellipsis like this on an array?", "np.arrays are designed to handle n-dimensional arrays, specified as [rows, columns]In the case of np.array([1, 2, 3]), [:2] and [:, :2] will yield the same result because our array input is 1-dimensional of shape [1, 3], e.g. with 1 row and 3 columns.\nIf we instead input np.array([[1,2,3], [4,5,6]]), e.g. a 2-dimensional array of shape [2, 3], this will change. On this array, if we, e.g., do [:1, :2] we will get array([[1, 2]]) because we are asking for everything up to the first (i.e. the 2nd since we count from zero) row and everything up to the second (i.e. the 3rd) column.\nHope this makes sense.", "What is the difference between using [..., :] and [:] on a numpy array?", 1.2000000476837158, 1, 147, "The way we use elliptics on numpy arrays is by assigning a value to an element of the array, not by the assign the variable to another. This means, when we assign a string to one of its elements, we won\u2019t assign it to a different element, but to the same.\nFor example, if we are able to assign \"11\" to \"0\" (0), we can also assign strings to their elements. Similarly,\n\"12\" can be assigned to elements like 10. When we do it like that, the string that we\nhave assigned \"to 00\nwill be added as a\nstring to other. So we know that when a variable is assigned an index, it becomes \"\nan index\" - \" ...\".\"\nWhen we apply that principle to\nthose ...\nHow do we solve this? What's the trick? We are also aware of an issue, which is why we cannot\n- solve the problem that the solution to it might be wrong. As it is a rule of mathematics that one should not ... How do you resolve this issue? ... So what' ...?\" How is this resolved? Why does it not work? The other answer, from my fellow teacher, is that ... ... It works. I don't know ... Why is it ...? That'll be ... \"So we're still solving the ...  ... What does that mean? So I'm not wor ... . I can'\u200bt solve it. ...\". How does one solve a problem? How can we? I ... I think we should solve .... The way is to ... ( ...), and it has solved it, ... to solve that problem. \"Yes ... yes.\" ...Yes, yes, I know it'...\". And what do the other answers ... that have helped to answer that question? One of them says: \"That question was asked by a student at ... how do ..., how does the mathematics work here? You've ... the principle there ... you can only apply the principles ... You can apply this principle here to calculate the correct answer for this problem ...\", the wording is so clear ... it may ... be applied here. And how would you apply ... in this case? Well ... there is an answer in the comment to this question ... which says ... if ... we don ... apply math here then ... why does this work ... and how is ... mathematics? They ... math, and math is", 0.16969697177410126, 0.02434077113866806, 0.10101009905338287, 0.04397648200392723], ["I want to use an LSTM model to predict the future sales.\nThe data is like the table below.\n\n\n\n\ndate \u00a0 \u00a0 \u00a0\nstore\nfamily \u00a0 \u00a0\nsales\n\n\n\n\n01/01/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBEAUTY \u00a0 \u00a0\n1 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n\n\n\nFor each day, it has 50 stores.\nFor each store, it has different type of family (product). (They are all in perfect order, thank God).\nLast, for each type of family, it has its sales.\n\nHere is the problem.\nThe dimension of input of LSTM model is (Batch_Size, Sequence_Length, Input_Dimension). It is a 3D tensor.\nHowever, in my case, my Input_Dimension is 2D, which is (rows x columns)\nrows: number of rows in one day, which is 1782\ncolumns: number of features, which is 2 (store and family)\nIs there a good way to make my data into a shape which can be fed into a LSTM model?\nThanks a lot!", "The solution I came up with is to make the whole data in each day to be a long long long sequence.\nSo the dimension will be 1D, and can be fed into the LSTM model.\nBut I don't think this is the optimal solution.\nDoes anyone come up with better answer?\nAppreciate.", "How to feed a 4D tensor into LSTM model?", 0.0, 0, 148, "I believe in the Bible that the God, He, has a plan. Hands are\non Hedge Fund 1, they can do what they want in these times, or what? The people can\nbe, you know? What the people have done to the\nworld to, and they have to do this, right? They\nhave to be responsible for\nthis money, hedge fund 2. And they are doing,\nand doing something, about the world, so\nthe world can be\nbetter, too? You know, what to\nmake sure that\nhe or she, because I believe that, God\nhas a better plan for people. That they\ncan, to make sure what\nthey can, can make\npositives out\nof things, that they do, as well? So\nthat they, all of\nus, make a good\nlife out of it? (2) How\nDo you think that hedges\nfunds 3 have made\na bad life out? How, do you, think, in\nanother heds\nFund? Do\nyou, how\ndoes \u0093they\u0094 think\nabout hegs\nSure. We don\u0092t think about\nIt? Or we don't\nthink about \u0094it?\u0094? We\njust don\u2019t care about it. The\npeople, who are looking at them, don\n\u0092 t care. How does it\nlook at the stores? Why\nHow did you do\nit or the other\n? 65, 80,6? or 7? Wh\nen we\nreally care? About\nwhat\nwe think we can take care of ourselves? And the work that we will\nget is about? In our lives, our \u00c2nks, is it really about, the whole\nthing, we are working with? Because\nwork is really important to us, but it\u0092s our job? Is our\nwhole\njob really, for what we do? Are you\ninvolved? Your jobs really mean\nsomething to you? This job, I think it\u2019s really\nmeaningful to my job. This whole life? It\u0092\ne important and valuable to me, this\n\u00c2\u00c2lks? I do not know\nwhy I am in that? Some days, he really says? Yes. I\ngo to bed, on that day? That", 0.16020672023296356, 0.036363635212183, 0.08268734067678452, 0.061605025082826614], ["I have a csv file with comments marked by '#'. I want to select only the table part from this and get it into a pandas dataframe. I can just check the '#' marks and the table header and delete them but it will not be dynamic enough. If the csv file is slightly changed it won't work.\nPlease help me figure out a way to extract only the table part from this csv file.", ".csv file can't have comment. Then you must delete comment-line manualy. Try start checking from end file, and stop if # in LINE and ';' not in LINE", "How to extract a table from a csv file generated by Database", 0.0, 0, 149, "Yes, it takes time, however if you do the extra work you can see the result. It has taken me a long time to be able to see it. The way I see my future is through my hard work today. This is what we have been taught. (Teaching the values) We have worked hard to succeed. Let's talk about the successes of today'\nPosted August 2019 by the successful students of 14th grade. He is the president of the club and his classmates are the vice president. His class 37 has 76 percent students with grade point of two and one. Three students are from class one to two. In class two, seven students have grade points of one and two\nSelina had a grade of three and seven\nTwo. We\nAren'", 0.09876543283462524, 0.0, 0.06172839552164078, 0.04949704185128212], ["I have a recurring issue when running even the simplest code using deepface.\nFor information I run it on python 3.9, with a M1 macbook air on OS Monterey 12.0.1\nI can't seem to find any information on how to resolve it, hope anyone can help !\nThank you very much in advance,\nPaul\nfrom deepface import DeepFace\nresult = DeepFace.verify(img1_path = \"photo1.jpg\", img2_path = \"photo2.jpg\")\nobj = DeepFace.analyze(img_path = \"photo1.jpg\", actions = ['age', 'gender', 'race', 'emotion'])", "I finally found a solution : underlying was an issue with tensor flow. I changed the version I had and replaced it with an M1-compatible version. It worked as intented", "Illegal Instruction : 4 when running deepface", 0.0, 0, 150, "deep face does not work with macbooks or photochips, it only works with google photos and google camer.", 0.1666666716337204, 0.0, 0.125, 0.03488371893763542], ["if i wanted to train an lstm to predict the next date in a sequence of dates, how would i do that since lstm require a scaled value?\nexample of data:\n\n\n\n\ndate\nnext date\n\n\n\n\n2012-05-12\n2012-05-13\n\n\n2012-05-13\n2012-05-19\n\n\n2012-05-19\n2012-05-20\n\n\n2012-05-20\n2012-05-22\n\n\n2012-05-22\n2012-05-26\n\n\n2012-05-26\n2012-05-27\n\n\n2012-05-27\n2012-05-30\n\n\n2012-05-30\n2012-06-12\n\n\n2012-06-12\n2012-05-19\n\n\n2012-06-19\n2012-06-25", "You could hand over the date split into three inputs: One would then be the year, the other the month, and the last the day. While normalizing your inputs definitely makes sense, however I would not entirely agree with your \"LSTM requires\".\nDay and month are already limited to a range of values which can be scaled\n\nday (1 - 31)\nmonth (1 - 12)\n\nFor year you need to make an educated assumption based on your application. So that year can then also be transferred to a scaled value. Judging from your data, it might be that year is constant at 2012 and it is not needed to begin with.\n\nyear (2012 - 2013(?))\n\nNote: Ask yourself whether you give the neural network enough system information to be able to predict the next date - meaning, is there already enough of a pattern in your data? Otherwise you might end up training a random predictor.", "How to train a LSTM on a sequence of dates?", 1.2000000476837158, 2, 151, "If you are looking at sequences of daily data, you can simply use the\ndata from yesterday. This way you do not have to make\ndifferences between daily and weekly data. It just simply\nshows up that you have yesterday\u00b4s data to use.\nThis way it does not matter that daily or week\nData is used. What you just have\nto do is to get the previous\nday\u00b4\u00b4 day\u00b4 and data for the day you want to\nuse the data from the days\u00b4\nThe day before yesterday \u00b4", 0.3166666626930237, 0.03361344709992409, 0.15000000596046448, 0.04519481584429741], ["We can basically use databricks as intermediate but I'm stuck on the python script to replicate data from blob storage to azure my sql every 30 second we are using CSV file here.The script needs to store the csv's in current timestamps.", "There is no ready stream option for mysql in spark/databricks as it is not stream source/sink technology.\nYou can use in databricks writeStream .forEach(df) or .forEachBatch(df) option. This way it create temporary dataframe which you can save in place of your choice (so write to mysql).\nPersonally I would go for simple solution. In Azure Data Factory is enough to create two datasets (can be even without it) - one mysql, one blob and use pipeline with Copy activity to transfer data.", "Is there any way to replicate realtime streaming from azure blob storage to to azure my sql", 0.1973753273487091, 1, 152, "Run a sql script on a daily basis from a csv file in your storage, it will create a timestamp column in the data as well\nWe can also create tables in our storage that can be used to work with the CSVs\nI've not used them as a storage for the databases but as we have a 64MB max size of the db we can work around it.", 0.25806450843811035, 0.013071895577013493, 0.12903225421905518, 0.030225761234760284], ["I have 2 environments:\nEnvironment #1:\n\nPython 3.7.5\nPandas 0.23.4\n\nEnvironment #2:\n\nPython 3.8.10\nPandas 1.3.4\n\nI have the same code in both versions, no modifications were made to it. However, I have this specific line of code which seems to be causing an issue/produces a different output:\ndf_result = pd.merge(df_l, df_r, left_on=left_on, right_on=right_on, how='inner', suffixes=suffixes)\ndf_l and df_r are just read Excel files. I checked them in debugger in both versions and they are completely the same, so that should be fine.\nAlso, the left_on, right_on and suffixes variables have exactly the same value in both environments (checked via debugger, as well).\nHowever, when the df_result gets generated by the merge function, in environment #1 (old Python, old Pandas) it produces a DataFrame with 16170 rows. In environment #2 (new Python, new Pandas) it produces a DataFrame with only 8249 rows.\nThe number of columns are the same, difference is only in number of rows.\nWhat is causing this behavior?\nHow do I make sure that the environment #2 (new Python, new Pandas) produces exactly the same output with 16170 rows as produced by environment #1 (old Python, old Pandas)?\nThank you.", "At the end the issue lied within new Pandas' approach to handle NaN values.\nWhile in the old Pandas the code changed the NaN values with <NA> (as string), in the new Pandas it just left it as nan (pd.nan type).\nI made sure to do df.fillna('<NA>', inplace=True) and it worked fine. The resulted DataFrame now has the same number of rows as produced by the old Pandas.", "Python - Old pandas merge results in more rows than new pandas", 1.2000000476837158, 1, 153, "[P] ...[P...[...]...\n[O]...[[P]]....[[O]]... ...[[...]]\n[[T]]........[T].....[[[B]]]........\n`df = [pd.f.df, pd.__f__]`......`[B] = df = [[T)]`.........`[[S]]`..... `[[U]]=[U]`............`2[F]`................`4[W] `5[G]`..........`.[6[H]``................`.[[N]]`.\n`.`3[L].......```.5 [L]``..`.`.6 [B]`\n. ...\n... `.\n7[R]`...`.8[Q]`.``...9[S]``,.```\n1 [T]`1[\n2 [O]`2\n3 [C]`3\n4 [D]`4``\n5.[D][D].5`.31264696301944774891455687597207654327841583792957\n6.[T][C]`.213622881867334253938508040098\n.... .... ............ ........... ........................................ 7 [R] 9 [Q]\n......................................\n.....................\n8 [N]`8`8`.99....8........`711`9`1\n.. .... ... ... ....... .... .. ... .. .. ...... ... . . ... \n9 [[S]][S]{S}\n_____________________\n_________\n___ . ___ .\n__._____\n.___________________________\n.__._______.____________________._________._____.____.\ufffd\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd?\ufffd??\ufffd[\ufffd\ufffd]?[\ufffd]\ufffd{\ufffd}\ufffd=\ufffd[]\ufffd]{}={}\ufffd\ufffd\n\ufffd\ufffd{\ufffd\ufffd}???{?}\n#\n\"\"\"\"#\"\"\"\n````{``}``**{\n**?**\n\\\\{}**\\\\\n$$$$$$$\n\"$$\"[i]t[s] [...]\nl[]a[l]w [", 0.037974681705236435, 0.0, 0.025316456332802773, 0.014705882407724857], ["I updated my Python3 to Python 3.10. It still is showing Python 3.8 as my current version. but that's not the issue. My issue is that when I went to install the matplotlib package using pip install matplotlib, I got some errors. I also tried running pip3 install matplotlib. I got the following errors:\n\nWARNING: Retrying (Retry(total=4, connect=None, read=None,\nredirect=None, status=None)) after connection broken by\n'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection\nobject at 0x1057db7c0>: Failed to establish a new connection: [Errno\n8] nodename nor servname provided, or not known')':\n/simple/matplotlib/\nERROR: Could not find a version that satisfies the requirement\nmatplotlib (from versions: none) ERROR: No matching distribution found\nfor matplotlib\n\nThe I tried running /Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip and got the following error:\n\nDefaulting to user installation because normal site-packages is not\nwriteable.\nRequirement already up-to-date: pip in\n/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages\n(20.2.3)\n\nI don't get it. It wanted me to upgrade pip and then says it's already up to date?\nI just need the matplotlib module installed for my Python scripts.", "If you are trying to install matplotlib in your organisation laptop then your organsiation could be blocking the network to connect and download the package. This is one reason its showing retrying error message. You can try disconnecting vpn if you are connecting with any and retry installing it. This error is due to network issue only.", "Errors while installing matplotlib using pip install", 0.0, 0, 154, "You don\u2019t need any \u201cinstallation\u201d or \u201ccompilation\u201d of modules or packages. Simply run `pip install` in your Linux. And that works! What happened? What is going on? Is it the 2nd level of the software \u00ad\u00ad? Why does it work? WHY DOES IT WORK? IS IT?!?...\nBecause it works, that\u2019s it! It works and that is amazing! IT IS AWESOME! YEAH!\nWhat is happening? Because it is a Linux! And Linux, Linux \u2011 \ufffd!\ufffd\ufffd\ufffd\n\ufffdWhat are the facts of life?\ufffd What are you going to do with your 6th grade class? Are you planning to go to the university? Where do you live? In what state or town do live in? Answer! Answer!!!!!!!\nAna! Ana! \ufffd\ufffd!Answere! Ansowere!! 4! An! A!...\ufffd 8! D!D!!...DANNAN! OOOOHHHHHHHHSSSS!SSSSS!!SsSss!S!TANANA! TANNAANAAAANAAAA! ANNA!!... 9! (9) 11!1!!\n13!.........\nOw!O!oo!o!ow!!w?......w....\nw.....w\naw.a\na.aw....a....aa.aa....mmm............hm\n.....hmm\nhm.......hhhnhnhhnnnhn\nahahaaaahhhahaaaaaaah\nhhhhh\nhwwerwerwwerwewerwwwer\n...we\n....wewwewhwwhwwwwhwhhhhwwhhwh\nhbhhbb\nbbbbbb\np3 .............................................................0 7 5 \nthe ia nd the ip, and the ipp are all interconnected \ufeff \u200b\nhow can you create a system that you can connect \ufffdto the system \u0097 that \u00c3\u00a2\nso you see the interconnectability and \u00c2\nyou see in the history of medicine\nhe was the man\nin \u009334\n\u0094he went \ufffd\n32\ndongongonongnong\n\u0093 \u0094a, a, b, c,", 0.1666666716337204, 0.014598540030419827, 0.08695652335882187, 0.038860272616147995], ["Recently i was struggling trying to take the pixel values of a 3D volume (np array) using specific space coordinate of a STL object.\nThe STL object is spatially overlapped with the 3D volume but the latter has no coordinate and so i don't know how to pick pixel values corresponding to the STL coordinates.\nAny idea?", "If the STL object is truly in the 3d volume's coordinate space, then you can simply STL's coordinate as an index to lookup the value from the 3d array.  This lookup does nearest neighbor interpolation of the 3d image.  For better looking results you'd want to do linear (or even cubic) interpolation of the nearby pixels.\nIn most 3d imaging tasks, those coordinate spaces do not align.  So there is a transform to go from world space to 3d volume space.  But if all you have is a 3d numpy array, then there is no transformation information.\nUpdate:\nTo index into the 3d volume take the X, Y, Z coordinates of your point from the STL object and convert them into integer value I, J, K.  Then lookup in the numpy array using I,J,K as indices: np_array[K][J][I].  I think you have to reverse the order of the indices because of the array ordering numpy uses.\nWhen you way 3d array and the STL align in python, how are you showing that?  The original DICOM or Nifti certainly have world coordinate transformations in the metadata.", "Mapping values from NP ARRAY to STL", 0.0, 0, 155, "You don\u2019t need the value of N for this. You could also use the values in N from the MIP. The MI from M is a simple structure and you don\u2018t have to explain it as an N. Use the image coordinates of the stl as the z cooordinates. They should be right. This is only true if the n and the m are not the same for the volume. If you can\u2019\u0092t do this, then you could use other values for 2.3.4.1.", 0.326241135597229, 0.08571428805589676, 0.1843971610069275, 0.04097301885485649], ["I have a certain number of datasets and I've given numbers to each of them as the names let's consider 20 datasets, so the names are 1.csv, 2.csv and so on.\nI'm trying to give an input, here the number(name of the dataset) so that my code reads and works on that dataset. How do I make that possible?\nI've done something like giving input and changing it into a string and using pandas read_csv(string+\".csv\") but the code's not working\nCan anyone help out?", "pandas read_csv(string+\".csv\")\nI have done this and it works, I had to change the integer to string first.", "Giving input such that it reads the exact dataset among the others tkinter", 1.2000000476837158, 0, 156, "In the examples below, we have used Python modules like \"pandas\" and \"numpy\". Let us use the module \"python\" to run this code. Run the script on the \"google.com\" website, and write the Google Ads script. You have written \"ad\" in the space below. If the advertiser has \"income\", then they will advertise to people who visit their site. The site that you visit is google. com. In this script, you have to click \"on the link\". Click on \"The Google\". The Google advertising campaign, advertisers, online advertizing, how to advertize online, google adsense, Google advertise, what does google say, What is Google say. \"Google has\" money \"money\" from the Internet. It is a good online business. And the other one is \"how to online\". If you want to learn about online adverising, or learn how online marketing works, then this article is for you. Online marketadizing is very successful. Learn how many online sales have been done by various online stores. Some of this information is about \"Internet\" market adizng and how much money they have made. They make very good advertizig and they earn the most money from them. Many people make money online. This website says, \"you will not find money\". But \"there is money\" that they make. Therefore they can say \"online market\" \"those who make online\" adzise online .\n(Adzisa Online) How to Make Money Online . Adzisesa how you make . how . you can do that. Make money on online .. make .. money ... How can I Make . money Online? How Can I Do That? Make Online Mone .. Make online money . Make More Mome . Onlin . How Onli . n . onli.n.online.ad.ze.s. . a .z.zze .s . . zzzdzzeh.heh he.hhehehhhh.hhhe.babababybabyboobboobsb.c. c. cc.cccc.bbbbbboo.oohoooahah.ahhh..ahh..heah..hah...haha...ah....ah.....ah..............hh.......hh..................................\nHh................hh.......................h......hw....", 0.07339449226856232, 0.0, 0.04281345382332802, 0.04494093731045723], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "len(s) will return the lenght of the s variable\nlen will return the function itslelf. So if I do a=len, then I can do a(s). Of course, it is not recommended to do such thing as a=len.", "difference between \"function()\" and \"function\"", 0.0, 0, 157, "So, when we use the function, without the parent, it means that we are using one of the main functions in the website. Like: a, b, c etc. But, why do we need to use them? What is exactly going happening? I think that there is a small explanation of what is happening on the web, as the blog of a customer has written about it, you will understand it. What exactly happens when you use a parent? In other words, what happens if you go and use this function? This is because a few years ago, there was no other function than the \"parent\", and today you can use many more things as an example. Which, of course, today, can be seen with the Internet, the Facebook and the other social networks. So what are the differences that you could possibly use and not be able to have a difference in between a and a b? Well, because today people use all the online functions, which is available, and you have to be careful that people have used them today in a way that is not possible for this. Why use these functions and why not? It is probably because they are difficult to understand why it is different from you, if not why you are, or something like that? You are probably able or able? So today we can explain it a bit, not just today but today it's really easy to do. That'\u00c3\u00c3\u00c2\u00c3\u00c0\u00c3\u00c7\u00c3CA\u00c3V\u00c3BV and that they actually use. If they don't use what they use, they definitely don\u00c2t have any type of. The reason why they have made use of this online type function that can actually have is that the same thing has been done in today'\u00c4\u00c4\u00c5\u00c4\u00c3\u00c5\u00c5A? They have been doing the thing for a very long time and it was actually the day when the first online and online-related type functions were first. Let'S TOP-APLACEMENT-TON-ONLINE-TYPE FUNCTIONS? We will explain them but first let'\u00c2S explain the type\u00c2'? of function today and how these\u00c2\u00c4ASAPLAST\u00c2?\u00c2APLP\u00c2\u00c2LP can really be online today. Online can have the advantage of being able. And there are many types of them. For today only one type has to come but the kind you actually know. However, this is one online online. In fact, we know them very well. We know that it has the", 0.1205357164144516, 0.026905829086899757, 0.0714285746216774, 0.047614555805921555], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In the second case you are directly calling the len method and will get the result, i.e. how many rows are in col1 in the df.\nIn the first you are giving the reference to the len function to the apply function.\nThis is a shortcut for df[\"col2\"] = df[\"col1\"].apply(lambda x: len(x))\nThis version you use if you want to make the behavior of a method flexible by letting the user of the method hand in the function to influence some part of an algorithm. Like here in the case with the apply method. Depending of the conents in the column you want to fill the new column with something, and here it was decided to fill this with the lengths of the content of other column.", "difference between \"function()\" and \"function\"", 0.0, 0, 158, "We are using functions. There are other functions which are related. For example, we are looking at the \"functions of the business\". We are also looking the function \"of the investor\". So, investors can invest their money in different functions of business. Now, there are different types of investments: investment in real estate, business or even investing in other invest\nThis is a big investment in a bank and it has its own benefits. So there is investmen in bank. This is where we come to talk about the functions and this function is important to us. However, you need to understand that this is an important function. You can know how this investsment and how it gives advantages to people. Therefore, the bank functions must be known to you. Investment is made through different kinds of bnks: b\nBanks also have functions, which they also conduct. It is very important for us to know about this type of banks, because this gives us important types, and therefore we also know that the banks also give the customers. Banks are very useful for our purposes. We can also look into their functions in the type that they perform.", 0.2969697117805481, 0.03658536449074745, 0.1515151560306549, 0.045207951217889786], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In 1, the function len is being passed to a method called apply. That method presumably will apply the function len along the first axis (probably returning something like a list of lengths). In 2, the function len is being called directly, with an argument df[\"col2\"], presumably to get the length of the data frame.\nThe use in 1 is sometimes called a \"higher order function\", but in principle it's just passing a function to another function for it to use.", "difference between \"function()\" and \"function\"", 0.0, 0, 159, "In case of parenthetics, you can see a small difference with\n-df1[\"col3\"]=df-1[\"focals\"]-45278\nin-case-without, there is a huge difference\nyou can find out the same information, for example.", 0.18803419172763824, 0.0, 0.1367521435022354, 0.04268291965126991]]}