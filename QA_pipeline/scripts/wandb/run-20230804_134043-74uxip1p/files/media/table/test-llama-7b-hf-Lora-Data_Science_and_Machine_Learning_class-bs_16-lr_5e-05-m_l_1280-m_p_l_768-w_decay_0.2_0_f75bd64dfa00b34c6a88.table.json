{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I would like to reuse the same Spark application across multiple runs of a python script, that uses it's Spark session object. That is, I would like to have a Spark application running in the background, and access it's Spark session object from within my script. Does anybody know how to do that?", "To the best of my knowledge, it is not possible. It is the security model of Spark to isolate each session to a distinct app.\nWhat I have done in the past:\n\nbuild a small REST server on top of Spark that listens to specific command. At boot time, the server creates the session and load the data, so that forthcoming transformations are fast.\n\ncache data in Delta lake, you still have the boot time and data ingestion, but it\u2019s much faster that accessing data from several sources and preparing the data.\n\n\nIf you describe a bit more your use-case, I may be able to help a little more.", "How to request the spark session object from a background-running Spark application from within a python script?", 0.0, 0, 0, "There are a few options that you can take:\n\\begin{itemize}\n \\item You can save the existing `SparkContext` from one `python script` to a file and read it on other `script`.\nFor this approach you need to create `pickle file` and use\n`pickles` for that.\n[Read](https://stackoverflow.com/questions/26837553/python-create-pickling-file)\n  for details. Here is an example of code: -\nI am creating the pickling file `save_sc.py` inside directory `/home/user`\nand then in python file I call that `Save_SC.save()` function and\nthen get the object of `sc` as below: \\begin {code} save_script =\nLoad_picking.load(path) sc = save.sc \\end{code}.\nYou can check the code below\nfor details: --\n-----------------------------------------------\nsc=sparkContext\nprint (sc)  # This is the saved SparkContext object\nsave.SavePickleFile('/home/' + path + '/' + pickle_file, sc) #This saves the sc object to pickles file\nload.LoadPCK(pickled_path + \".pkl\")  //load the file into the pck\nobj = load.pickletorest(pck, path)   //this creates the new sc and load the Spark Context to the variable obj\nres = sc.setLogLevel(\"WARN\").parallelize(1 to 100).collect()\n(res == obj) print (res)    #Now you should get a True value if the res and obj variables has the identical values. You will have to adjust the path variable according to your setup\nNow, if you want to use the `spark session` object in your new script as\nanother program then you have 2 options: Either you run that program\nagainst Spark standalone application or you create new `job id` of\nSparkl\nstandalone and run the program as a job. In both cases, you will\nhave to make sure you get your code working with the newly created\njob so that your program does not throw any `Exception`. But this is\njust my personal opinion, the other alternative would be to save your\nPython script in file system and the run it as", 0.2535211145877838, 0.03773584961891174, 0.11737088859081268, 0.04646465182304382], ["I am using pandas.read_csv() to read a csv file, but characters like the copyright symbol are getting converted to \ufffd\nFor example, in excel I will see this value - \n/ORGANIZATION/AFFLUENT-ATTACH\u00c3\u00a9-CLUB-2\nIn jupyter notebook in turns to this - \n/ORGANIZATION/AFFLUENT-ATTACH\ufffd-CLUB-2 in one dataframe \n/ORGANIZATION/AFFLUENT-ATTACH\u00c9-CLUB-2 in the other\nI need to do an inner join of 2 dataframes, both of which have a column with these unique IDs, but values like these are getting left out.\nI thought it might be something to do with the enconding, so I found that the encoding type is cp1252 for both csv files. I do not know if this information is useful.\nPlease help.", "Try to change the encoding of the file to UTF-8 or UTF-16 while reading.", "How to deal with special characters like \ufffd in python?", 0.0, 0, 1, "If your data in csv, you can use  replace special character with their code point and then read csv in pandas as bellow:\n`data = pd.io.json.loads(pd.compat.StringIO(data.replace(\"&#155;\", \"\u00a8\")))`.\nYou can check more details on encoding here https://pandas.pydata....\nAfter that you have to use replace again to get normal character. If you dont want use encoding,you can first remove special char from csv and load data with encoding cp850. Then replace back special to their encoding.", 0.05999999865889549, 0.0, 0.05999999865889549, 0.03389332816004753], ["I am using pandas to read in csv data to my python script.\nBoth csv files have the same encoding (Windows-1252).\nHowever with one of the files I get an error when reading the csv file with pandas, unless I specify the encoding parameters in pd.read_csv().\nDoes anyone know why I need to specify the encoding in one csv and not the other? Both csv's contain similar data (strings and numbers).\nThank you", "That just means that one of the files has a character outside the range 0x00 to 0x7F.  It's only the highest 128 values where the encoding makes a difference.  All it takes is one n-with-tilde or one smart quote mark.", "Encoding csv error with Pandas - have to encode one csv file but not the other -both have same encoding", 0.0, 0, 2, "\\begin{itemize}\n\\item If you don't provide a parameter `encoding='windows-cp1026'` Panda will use the `latin-7` which has a different alphabet. And in the error you can see it is latin 7. I can tell it with my `codecs` module. In the console. (or with any code)\n`codec.get_name(b'abcd')\n'lat'\ncode_type: 'lat`\nI know that with Windows 1-0-3 it'll be windows- cp1 (also with other platforms). So I think if you didn' give this paramter the program would have been using it. If it was using another encoding program could be not understanding the text. The encoding could even be different. For example if your computer is English (UK) then you have a difference of 2-4 and 5-2. So, in this case, your encoding is windows -1. Your program should be like this :\ndf = pd...\nEncoding 'utf-8' not supported. Encountered character utf-9 (9969), (codepoint 0xfffd) during string decode\nYou should try with encoding = 'windows_cp_1_0_3' (I am not sure of it) but it works. Then, you will see that it has not changed the data. It is the way of decoding your data\nAnd you should use this with the second csv. But the only thing you may have that is, for example, with this encoding you could write some letters in a way they are not encoded properly, but I don;t think it happens with your problem. Also, donwload the latest version and read this article to understand all the options:\nhttps://github.com/pydata/pandas/issues/5507\nand this one: [Python PANDAS- Encoded Data- Reading in CSV files](https://stackoverflow.org/questions/16804288/python-pandas-encoded-data-reading-in.csv-files) (this last one will help you to write the encoder) I hope I could help. Good luck\nPS: If my answer helped you, do not forget to vote for it and click the green check next to the answer. Thanks in advance. :)\nA.L.  C\u00f3", 0.12664908170700073, 0.015915120020508766, 0.07387863099575043, 0.08287292718887329], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Though I don't know the exact notations and equations, the answer has to do with the Big O Notation for the algorithms.\nBig O notation is a way of expressing the theoretical worse time for an algorithm to complete over extremely large data sets. For example, a simple loop that goes over every item in a one dimensional array of size n has a O(n) run time - which is to say that it will always run at the proportional time per size of the array no matter what.\nSay you have a 2 dimensional array of X,Y coords and you are going to loop across every potential combination of x/y locations, where x is size n and y is size m, your Big O would be O(mn)\nand so on. Big O is used to compare the relative speed of different algorithms in abstraction, so that you can try to determine which one is better to use.\nIf you grab O(n) over the different potential sizes of n, you end up with a straight 45 degree line on your graph.\nAs you get into more complex algorithms you can end up with O(n^2) or O(log n) or even more complex. -- generally though most algorithms fall into either O(n), O(n^(some exponent)), O(log n) or O(sqrt(n)) - there are obviously others but generally most fall into this with some form of co-efficient in front or after that modifies where they are on the graph. If you graph each one of those curves you'll see which ones are better for extremely large data sets very quickly\nIt would entirely depend on how well your algorithm is coded, but it might look something like this: (don't trust me on this math, i tried to start doing it and then just googled it.)\nFitting a decision tree of depth \u2018m\u2019:\n\nNa\u00efve analysis: 2m-1 trees -> O(2m-1 n d log(n)).\neach object appearing only once at a given depth: O(m n d log n)\n\nand a Log n graph ... well pretty much doesn't change at all even with sufficiently large numbers of n, does it?\nso it doesn't matter how big your data set is, these algorithms are very efficient in what they do, but also do not scale because of the nature of a log curve on a graph (the worst increase in performance for +1 n is at the very beginning, then it levels off with only extremely minor increases to time with more and more n)", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 3, "Tree-based algorithms will ignore the scaling of features at training time and apply it only at test time. Ensembles will also ignore feature scale at both training and test times. In fact, there is no such thing as scaling when using an ensemble of trees since you cannot scale your data at the level of individual trees. It is at best a scaling at feature level or a scale of your test data - not trees themselves. However, it is a common practice to scale and standardize the test dataset, which is then used as a benchmark for the performance of all the ensemblers. This is particularly helpful in ensuring that your results are comparable to other researchers. There are several good reasons for not scaling your features before you apply an algorithm to it. Firstly, you will not be able to find the best-performing algorithm for your feature set. Secondly (and more importantly), the feature selection will be affected as well as the hyperparameter tuning (e.g. cross-validation, or model selection), etc. The ensemble will do the right thing if you let it - it has evolved into what it should be, and it will take care of feature scales. So do not scale features prior to running an ensemler! However if your ensemble does not yield a good score on test, one possible reason is feature skewness or bias (usually imbalanced data set). So you should check your training data for such problems and adjust your model accordingly.", 0.3255814015865326, 0.04373177886009216, 0.12790697813034058, 0.04641349986195564], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Do not confuse trees and ensembles (which may be consist from models, that need to be scaled).\nTrees do not need to scale features, because at each node, the entire set of observations is divided by the value of one of the features: relatively speaking, to the left everything is less than a certain value, and to the right - more. What difference then, what scale is chosen?", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 4, "The reason is because the tree learner does not care about the \\strong{value} of your feature but only cares about its \\em{proportion} in the overall dataset. Since trees don\u2019t care, this is not something to scale with any standard of scaling feature. You could scale the features to zero mean and unit standard deviation and still the trees would work the same, so no need to perform feature scale on a tree. However, it is often useful to divide features into numeric categories and scaling them in this way would make sense. The data should not be scale free for a decision tree and that is why it has to be normalized or categorized before using a DecTree.", 0.3297872245311737, 0.05376344174146652, 0.1595744639635086, 0.07096774876117706], ["I am at a total loss as to why this is impossible to find but I really just want to be able to groupby and then export to excel. Don't need counts, or sums, or anything else and can only find examples including these functions. Tried removing those functions and the whole code just breaks.\nAnyways:\nHave a set of monthly metrics - metric name, volumes, date, productivity, and fte need. Simple calcs got the data looking nice, good to go. Currently it is grouped in 1 month sections so all metrics from Jan are one after the other etc. Just want to change the grouping so first section is individual metrics from Jan to Dec and so on for each one.\nInitial data I want to export to excel (returns not a dataframe error)\ndfcon = pd.concat([PmDf,ReDf])\ndfcon['Need'] = dfcon['Volumes'] / (dfcon['Productivity']*21*8*.80)\ndfcon[['Date','Current Team','Metric','Productivity','Volumes','Need']]\ndfg = dfcon.groupby(['Metric','Date'])\ndfg.to_excel(r'S:\\FilePATH\\GroupBy.xlsx', sheet_name='pandas_group', index = 0)\nThe error I get here is: 'DataFrameGroupBy' object has no attribute 'to_excel' (I have tried a variety of conversions to dataframes and closest I can get is a correct grouping displaying counts only for each one, which I do not need in the slightest)\nI have also tried:\ndfcon.sort('Metric').to_excel(r'S:\\FILEPATH\\Grouped_Output.xlsx', sheet_name='FTE Need', index = 0)\nthis returns the error: AttributeError: 'DataFrame' object has no attribute 'sort'\nAny help you can give to get this to be able to be exported grouped in excel would be great. I am at my wits end here after over an hour of googling. I am also self taught so feel like I may be missing something very, very basic/simple so here I am!\nThank you for any help you can provide!\nPs: I know I can just sort after in excel but would rather learn how to make this work in python!", "I am pretty sure sort() doesnt work anymore, try sort_values()", "python groupby to dataframe (just groupby to data no additional functions) to export to excel", 1.2000000476837158, 2, 5, "So I finally got it to work after a ridiculous amount of hours! (This project has me in a state of panic and all the stress is making me write code in circles it feels like, so it was great to actually get some coding done).\nHere is the code that worked for me to use a group by to generate a dataframe and export it into a excel sheet. The only difference in my code was that I was using a csv (csvreader) which required a slight change but worked just fine. Also, I had to rename my sheets into 'df1', 'd2', etc (my actual output data was df1,df2, etc). Anyways, here it goes. Note that it works with only one column groupbys, not multiple. Any other issues, please let me know! :)\n\\begin{code}\nimport csv\n#load the input data\nwith open('S:\\\\Filepath\\\\input_data.csv', mode='r') as fd: #input data for a CSV file\n    read = csv.reader(fd, delimiter='\\t', quotechar='\"')\nread2 = []\nfor line in read: read2.append(line) #to hold all data as a list\ncsvread = read(read.size)  #the size of the list to hold the entire data into the csvreader\nprint(csv.is_delim_whitespace(delimiter)) #determine if delimiter is tab or comma or spaces, then you know if you need to split it or not. Splitting a column with space delimiter will produce a 2d list so if the delimiter type is not space it needs to know so that the lists can be merged in one list and passed to df.from_csv(). Otherwise if it has a space, you want it in lists for the 3rd column to count. You can use the next row index as the row value to identify which team worked on it so you don' have to rely on the column header name. This way you have a more accurate breakdown of who is working on what. That's my take\nnew_list = list(list(next(pd.read_table(string.split(row, split_by), header=None))) for row in csvread)#to make a one liner for split function so I don\u2019t have 600+ iterations. Then I passed", 0.01066666655242443, 0.0, 0.01066666655242443, 0.03767058253288269], ["I have a numpy array with size (1000,6) and I fill part of it each time during my program. I need to find the first location of zero in this array. for this, I used np.where( array==0). but the output is a tuple of size 2 and each item of it is a numpy array and I do not how can I find the first index of occurring zero in this array. what should I do about this?", "The first element of the tuple that you got should be the index you are looking.", "how do I find the index of an specific value in numpy array?", 0.0996679961681366, 1, 6, "You can use np's `argwhere` as follows:\n`np.argWhere(array==1)`", 0.0714285746216774, 0.0, 0.0714285746216774, 0.0535714253783226], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 1.0, 22, 7, "Adam has been renamed to Adamax in kerasa.  See [this question](https://stackoverflow.com/questions/66090897/which-kerasa-adam-or-new-improved-model-has-the-same-params-as-previous-ones) for a discussion on this and the other options for how to use the new model.", 0.10000000149011612, 0.0, 0.06666667014360428, 0.045810941606760025], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "I think you are using Keras directly. Instead of giving as from keras.distribute import  \u2014> give as from tensorflow.keras.distribute import \nHope this would help you.. It is working for me.", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.0, 0, 8, "From https://kerastensor.io/optimizers.html:\n<a href=\"https://i.stack.imgur.com/4Y3cK.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nThe class `kerasiaopt.classifiers.sgd` is a wrapper for TF classifiers, so you cannot use the optimizers from that class.\nYou can use `tf.kerasi.KerasOptimizers` from the `tensorflow` module. From the [documentation](https:/github.org/tensorflow/models/tree/master/official/kerastex).\nIf you want to use Keras, you can directly use: `from tensorflow.python.ops.rnn_cell import KerasiLSTMCell`\n[Kerasi's docs](http://www.intel-ai.net/projects/cognitive-network-systems/docs/en/mfcc_models#training_and_deployment-optimization)", 0.1666666716337204, 0.015384615398943424, 0.10606060922145844, 0.03999999910593033], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "There are ways to solve your problem as you are using keras 2.6 and tensorflow too:\n\nuse (from keras.optimizer_v2.adam import Adam as Adam) but go through the function documentation once to specify your learning rate and beta values\nyou can also use (Adam = keras.optimizers.Adam).\n(import tensorflow as tf) then (Adam = tf.keras.optimizers.Adam)\n\nUse the form that is useful for the environment you set", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.03997867926955223, 1, 9, "It looks like `Adadam` is being used instead of `Keras.Optimizers`. It's worth reading [this](https://kerastorch.readthedocs.io/en/latest/kerasofficial.html) to understand how Keras handles optimizers, and use it this way instead.\nThe code looks something like this\n`optimized_model = model.fit(x_train, y_test, batch_size=batch_sizes, epochs=epochs, validation_data=(xval,yval), callbacks=[train_batch])`\nSo this is where you need to use the `optimize` function.", 0.19310344755649567, 0.0279720276594162, 0.11034482717514038, 0.02747252956032753], ["Is there a way to make partial dependence plots for the random forest with multiple classification in Python (using scikit-learn)?\nI'm raising a separate question about this because I'm not sure if such a function exists in scikit-learn. I've seen a few examples in R already. If the function doesn't exist, I will make the request in scikit-learn github, but just want to double-check with the community before making the request.\nIf you know of any other Python package other than scikit learn that could conduct the plot, please let me know. Thanks.", "You have to specify the class for which you want to plot the partial dependencies. This is done by the parameter \"target\" in the plot_partial_dependence function\nFor example, if you have three target classes \"low\", \"medium\", \"high\", you would say plot_partial_dependence(estimator, ..., target='high').\nHowever, I'm still trying to find some answers regarding the interpretations of partial dependency plots for multi-class-classifiers. If you have some information, let me know.", "Is there a way to make partial dependence plots for random forest with multiple classification in Python (using scikit-learn)?", 0.1973753273487091, 1, 10, "I created a [github issue for this request](https://github.com/scikit/sklearn/issues/2557)\nand it will be addressed in an upcoming release.", 0.07999999821186066, 0.0, 0.05999999865889549, 0.0731707364320755], ["how to get back 10 from df.iloc[10] where df = pd.DataFrame({'a':np.arange(1,12)})?\nI tried df.index but it returns a weird np.array which doesn't contain anything close to 10.", "The most simple solution if the index matches the row numbers is df.iloc[10].name which returns 10", "Get index of DataFrame row", 0.0, 0, 11, "You can use [`index.get_loc()`](https://pandas.pydata.org/docs/reference/api/pandas._series.Series.loc.html) for this\nlike\n`df.ix[df['A'] == 5]`", 0.08888889104127884, 0.0, 0.08888889104127884, 0.031578946858644485], ["I occasionally get the above error when making requests with the python requests library to Qualtrics APIs.\nIn a nutshell, I have a Google Cloud Function on Google Cloud that will trigger when a csv file is placed on a specific Cloud Storage Bucket. The function will create a Qualtrics distribution list on Qualtrics, upload the contacts and then download the distribution links.\nEvery day, three files are uploaded on Cloud Storage, each for a different survey, and so three Google Cloud instances will be started.\nMy gripes with the issues are:\n\nit doesn't happen regularly, in fact the workflow correctly worked for the past year\nit doesn't seem to be tied to the processed files: when the function crashes and I manually restart it by reuploading the same csv into the bucket, it will work smoothly\n\nThe problem started around when we added the third daily csv to the process, and tend to happen when two files are being processed at the same time. For these reasons my suspects are:\n\nI'm getting rate limited by Qualtrics (but I would expect a more clear message from Qualtrics)\nThe requests get in some way \"crossed up\" when two files are processed. I'm not sure if requests.request implicitly opens a session with the APIs. In that case the problem could be generated by multiple sessions being open at the same time from two csv being processed at the same time\n\nAs I said, the error seem to happen without a pattern, and it has happened on any part of the code where I'm doing a request to the APIs, so I'm not sure if sharing extensive code is helpful, but in general the requests are performed in a pretty standard way:\nrequests.request(\"POST\", requestUrl, data=requestPayload, headers=headers)\nrequests.request(\"GET\", requestUrl, headers=headers)\netc\ni.e.: I'm not using any particular custom option", "In the end I kind of resolved the issue with a workaround:\n\nI separated the processing of the three csv so that there is no overlap in processing time between two files\nimplemented a retry policy in the POST request\n\nSince then, separating processing time for the files reduced substantially the number of errors (from one or more each day to around 1 error a week), and even when they happen the retry policy circumvents the error at the first retry.\nI realize this may not be the ideal solution, so I'm open to alternatives if someone comes up with something better (or even more insights on the root problem).", "Qualtrics API, getting \"[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2570)\"", 0.1973753273487091, 1, 12, "Thank you for posting your issue.  It seems to match the symptoms we've seen in the last few weeks. At the time of this writing, we do not know what has caused this issue, although we are currently investigating it internally. We will share more information as soon as we can.", 0.15950919687747955, 0.02484472095966339, 0.11042945086956024, 0.02654840797185898], ["I just have a question on CNN which is should the model take all inputs used in training to predict new sample? what if i want to build a system for a hospital that predicts the disease from image and some features such as age and height but the user doesn\u2019t need to enter the features in case they are not available, so he can input the image only. Is that possible to do that in CNN? because as I know all input used for training should be entered for testing and predicting new data", "As I understood as per your description you want to predict the age and height from the image of patience. In that case for training, you need proper data and model. In model training, you have to specify X_train, Y_train at least. From there, the model will learn.\nX_train - provided image of a person\nY_train(label) -  the characteristic you want to provide (height & age)\nFor predicting purposes you have to modify the input image the same as you did before for X_train. then if you feed it into a trained model it will give you the prediction of height & age.", "Should the CNN take all the inputs used for training to predict new samples?", 0.0, 0, 13, "Yes, it is possible. You can build your model based on the data (in this case image) you have. And to get a better accuracy, you can use a larger set of data in your training set. However, if you are training your CNN model on a dataset with 100 images and you need predictions on images with more than 20,021, images, then your accuracy will be low. Therefore, to achieve a higher accuracy on your prediction, use as many images as possible for your data.\nFor a more detail answer on this topic, I would suggest you take a look at this [tutorial](https://docs.microsoft.com/en-us/azure/cognitive-services/contentmoderator/tutorials/introduction-to-cnn-image-classifier?tabs=visual-studio) and this one [here](http://cocoweb.org/2-2_CNN_ML).", 0.32499998807907104, 0.02521008439362049, 0.14166666567325592, 0.04999999701976776], ["I've got 17,000 CSV files, each ordered by timestamp (some with missing data). The total CSV files are around 85GB, which is much larger than my 32GB RAM.\nI'm trying to figure out the best way to get these into a time-aligned, out-of-memory data structure, such that I can compute things like PCA.\nWhat's the right approach?\n(I've tried to set up an xarray.DataSet, with dim=(filename, time), and then I'm trying to xr.merge() on each CSV file into the DataSet, but it gets slower with every insert, and I expect it will crash when RAM runs out.)", "Have you tried dd.read_csv(...).\nDask reads CSVs in a lazily and can perform certain operations in a streaming manner, so you can run an analysis on a larger than memory dataset.\nMake sure that Dask is able to properly set divisions when you read in your data.  Once the data is read, check dd.divisions and make sure they're values.\nYou can also use a Dask cluster to access more memory of course.\nThose files are really small and Dask typically works best with partitions that are around 100MB.  You might want to compact your data a bit.", "What's the best way to handle large timeseries in dask / xarray?", 0.0, 0, 14, "As you might have noticed, I have some experience in dealing with big datasets in Pandas and Dask. You could try to load these csv files with `pmap_from_dask_array`. For example, something like this:\n\\begin{itemize}\n  \\item `merge` and `join` the DF from the original dataset, keeping the index\n    `import pandas as pd\n        dta = pd.DataFrame({'x': range(10), 'y': ['a'] * 5 + ['b']*5})`\n      `df = ddf.to_records(index=True)`\n       `ddf = xds.from.darray(df, dim=('x', 't'))`  # you could have multiple timestamps per row\n          `dft = dd.drop_duplicates('t')`\n            `print(dta)` # or 'dtdf'`\n              `pd.concat(ddft, axis=1)`  (this may be too memory consuming, as a result of using pandas.)  You need to check with your own data how much memory this will take. To do so, try `memory_usage_gb()` on a `xr_dataframe` that fits into your RAM (and you should be able to do this), then check your memory usage with the `free` command. If you find it uses too much, you can try other approaches. One might be to `ffill` on the timeline of the df and do an `impute()` of missing values. This may help. A problem is that for some datasets you may have NaN'd timesteps, for example. So you need a good check of your data first. I believe the way you have constructed the file you provided is a little bit different from what I am describing here (which was not the main problem here, that was the memory issue).\n             `# this works only if you use 'merge'` # no longer supported in pandas 0.22\n                 `data['merge'] = df.index` # set the dataframe index as df'sf\n                `pandas.Series(data).reset_index()`\n               `result = pandas.read_csv('test.csv', usecols=['x','merge']) `\n `Pandas has some memory issues that are also described here` `https://towardsdatas", 0.27227723598480225, 0.04477611929178238, 0.12376237660646439, 0.027128174901008606], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "The more complicated your make your polynomial (e.g. 3rd order), the more constraints you need on your boundary conditions (e.g. C2). If you try to fit data to a cubic spline with only C0 conditions, then the problem is under-determined. You might as well fit with a line in that case. Use piecewise linear fit.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 15, "You need to set `cs_prior=True`.", 0.0923076942563057, 0.0317460335791111, 0.0923076942563057, 0.1071428507566452], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "Not out of the box, no.\nWith fixed breakpoints, it's just a linear least squares problem (with a continuity constraint), which you'll need to solve yourself.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 16, "You can control the continuation of a piecewise cubics splines by controlling the distance between knot points. The closer the knotted points, then the more continuos the curve, if you can ensure that the difference between consecutive knottings is less than the smallest interval between splinet points:\n(from the documentation)\n\\begin{itemize}\nC0_CONTINUITY - If True, splits the data into subsegments with no more than N-1 splined points between any two knote points or the beginning of the segment. Otherwise splins with up to N splints. Default is True. This is a faster algorithm that can fit to data with discontinuities and sharp changes. But, there are no continuouity guaranteas. Use a smaller value if the differences between data points can be small, otherwise, a larger value can speed the computation up. Values greater than 1 are not advised. Setting to None (the default) disables the method. If you want to control this property you should use the C_cont parameter in the construct method instead. See the discussion of this parameter below. Continuity is important for smoothing of piece-wise polynomials in time series analysis. Smooth curve do not fit in well with the discrete data. It is recommended to use a smal value for continuinity or it will be very slow to fit a curve. By default, it is slowed down by a factor of 2.0. More than a couple splint knonts may result in instability and lead to curve failures. To see a quick way of contolling this, use C1_COntinuous option. Then you control how many knoins to split the points at in your splining function\nYou can get C-continuous splinest if your knoopt points are less then 0,5*splinter. In other word your distance from knoot points must be at least than your interval. Try to write such splinner and you will have your function C continuoust. I did this some time ago.", 0.07428571581840515, 0.005747126415371895, 0.05714285746216774, 0.05303030088543892], ["I have 2 variables in a pandas dataframe which are being used in a calculation (Var1 / Var2) and the values consist of both floating point values and missing values (which I chose to coerce to 0). In my end calculation I am receiving 'inf' values and NA values. The NA values are expected but how do I derive a useable number instead of the 'inf' values?\nsome 'inf' values are appearing when VAR1 = float and Var2 = 0, others appear when both VAR1 and VAR2 are floats.\nMy initial approach was to round the floats to 2 significant figures before the calculation but I still received the inf values.", "You may be getting inf because you are dividing by zero. For example, if var1 = 5 and var2 = 0, then you are computing 5 / 0.\nIn pure Python this returns a ZeroDivisionError, but in lots of data libraries they avoid throwing this error because it would crash your code. Instead, they output inf, or \"infinity\".\nWhen var1 and var2 are both floats, it may be that var2 is extremely small. This would result in var1 / var2 being extremely large. At a certain point, Python doesn't let numbers get any larger and simply represents them as inf.\nRounding wouldn't help, because if var2 = 0, then it would round to 0, and if var2 is very small, it would also round to 0. As discussed earlier, dividing by zero causes the inf.", "Unwanted 'Inf' values in calculated measures", 0.1973753273487091, 1, 17, "You will never get an exact value in this case, as the answer will be an integer and you are trying to divide a float by a zero.  You can however do the following to at least get a reasonable approximation:\n\\begin{itemize}\n  \\item Round both variables to two decimal places\n    \\begin {code}", 0.227027028799057, 0.01092896144837141, 0.11891891807317734, 0.03813246265053749], ["i'm looking for a framework that is able to solve the following Data Science issue:\nI have several Teachers that can work for X amount of hours a week and have several subject that they can teach.\n\nTeacher 1: Math\nTeacher 2: Math + English\nTeacher 3: Sports + Art + English\nTeacher 4: Math + Art\nTeacher 5: Sports + Math + English\n\nIn a school, every subject needs a specific amount of hours per week. Some more than others\n\nMath: 12 hours\nEnglish: 8 hours\nArt: 4 hours\nSport: 2 hours\n\nLets say one teacher can do 2-3 hours just so you get my point^^\nThe Solution im looking for is a Framework or Algoritm that is filled (trained) with the data and then is able to distribute the teachers so all the subjects are capped or at least as close as possible. That means maybe Teacher 2 needs to teach only Math and Teacher 5 needs to teach 50% Sport and 50% English or 30% Math / 40% Sport / 30% English.\nSomeone mentioned Prolog but im not sure if it can handle this kind of problem? Maybe im wrong?\nIs there something that is fitting for my problem or am i destined to code that algorithm from scratch on my own?\nThanks in advance.", "The first step seems to be to translate a research problem (or a series of problem statements) into a precise form. Problem Characterization/Problem Conceptualization seems to be a technique for resolving that issue. Once the approach has been conceptualised, a technique must be identified for each of the sub-models and submodules.\nBreaking down the high problem statement into smaller problems is called problem conceptualizing. For every subproblem, a technique must be identified, and the methodology must be determined by the assumptions that have been stated previously.\nRealization of a Solution: Determines whether the assumptions are reasonable or whether the solutions meet his needs.\nThis can be compared to a flowchart that he has been creating with these subproblems, and also in general, it is attempting to reach a granularity level where he would determine the issue class. As a result, these issues can be classified as being either function optimization or categorization issues.", "LF Framework to solve Data Science Issue", 0.0996679961681366, 1, 18, "\\begin{itemize}\n\\item Create a matrix of numbers,\n(1) Teaches, (2) Hours per\nweek\nfor each teacher\nrow\ncolumn. (I'll assume the\nmatrix\nhas only 6\ncolumns and that a\nteacher\ncan\nwork\nas little as 0 and as\nmuch\n60 hours.  This is\nnot\nunreasonable\nbecause in your\nexample, Teache\nr  \\strong{4} \\em{can} work  only as little  as zero and no more\nthan 72. If your hours for each\nsubject were, for\ninstance, 9,  8,   4, and  2, then\nthe matrix might be: (\n0,     0   0  0\n1,       0  .    0.5\n2        0      1   2\n3         .  ...    ...\n5        ...   ...  ..)\nThis is the matrix that you\nwould use to represent your current\nsituation. It could be created\nmanually by hand, but I recommend\nwriting a program to make it\neasily. For instance, here is my\nprogram in Python, which would\ntake your data as a .csv file and\nmake the  matrix\nand a  table  with\nthese   elements: Teaching\nhours  (the total teaching hours) for the week, the name of\neach\n teacher and the teaching schedule\n for that week (or as many weeks as you might want to\nhave).\n    import csv\n   with open(\"mydata.csv\", \"rb\") as csvfile: #read your csv into memory\n      data = csv.reader(csvfile, delimiter=\",\")\n  for line in data:   #loop through all rows of the csv file\n     Tea  Teac,     H   = line\n         if   H     ==  \"0\":\n          Hour  = \"N/A\"\n        else: H  /=  6;\n                                           H = H / (H/2+1.0/6)   (this is to convert the time in hours to minutes, rounding up to a whole minute) #the teacher  could work only\n       #  if  I were only going to use the first week of data, I'd\n             #", 0.2511627972126007, 0.032710280269384384, 0.12093023210763931, 0.04571205750107765], ["I'm structuring a monitoring system for a photovoltaic plant with pvlib. As the modules are bifacial and are mounted on a solar tracker (2p), I am using pvfactors. I believe I have already resolved the dependencies: pvfactors 1.5.1, pvlib 0.7.0, and shapely reinstalled via conda.\nAs the modules do not have parameters for the Sandia model, I intend to use the de Soto model.\nI plan to run the code automatically once a day with the weather data collected during the period.\nI would like to know if anyone has any code developed with pvfactors and single diode models for the modules.\nSure of your attention, thank you in advance!\nBen Possatto", "You can model a single-axis tracked bifacial system using pvlib.tracking.SingleAxisTracker (inherits from a PVSystem instance) to calculate surface_tilt and surface_azimuth, then pass those results to pvfactors_timeseries to get the front and rear irradiance.", "How model tracked bifacial PV modules with python?", 0.0, 0, 19, "The model you have selected is:\npvmodels.pwvsol.deSoto\nYou should be able to call this model in python via pvs_dos, or via the pysol module if you are using PySOL. You might want to look at the python docs or sample code to see if that works.", 0.15909090638160706, 0.0, 0.11363636702299118, 0.05174380540847778], ["When we use a pretrained model, e.g. vgg16, as a backbone of the whole model which plays as a feature extractor, the model's data flow can be depicted as below:\nData --> vgg16 --> another network --> output\nAs for now, I've set False require_grads flags for all parameters in vgg16, and exclude those parameters from my optimizer's param list, so the vgg16 will not be modified during the training period.\nBut when I step further in my study, I'm now wondering which mode should vgg16 be used in? Should we call vgg16.eval() before running training epochs?", "However, in the general case, if you are freezing the model (with requires_grad = False) then you are not updating the running statistics anymore and should therefore use the running statistics, i.e. put the model in eval mode.\nVGG's backbone does not have any normalization layers nor dropouts. So in the end it does not matter whether you put the backbone into eval or training mode.", "When using pretrained model(vgg, resnet like) as backbone, should we use it in `eval mode` or in `train mode`?", 0.0, 0, 20, "I suppose that you are using this vg1 model as feature encoder. I am not sure, but i think that it can work only in eval mode (not in train). In this mode the weights will remain fixed and only the bias will be optimized. You can try to change the eval and train mode and see how it works.", 0.296875, 0.0476190485060215, 0.234375, 0.04952158406376839], ["I have a dataset where, after exploring data, I detect some patron:\n\nThe entire dataset have, imagine, 9 numerical variables, 1 dichotomous variable (take 'A' or 'B' value) and 1 numerical output\nThe output is a cost (in \u20ac)\nI find a sklearn regression model that, when 'A', using 4 of 9 variables I can predict output with good performance.\nI find another sklearn regression model that, when 'B', using the last 5 variables, I can predict output with good performance.\nIf I try to find a model which predict output with all the variables as input, encoding the dichotomous one with One-Hot-Encoder, the model has a bad performance.\n\nMy goal is to implement a unique model in Azure Machine Learning, using a .joblib/.pkl, but with this approach, I have two separated models with the same output (a cost) but different inputs, depending of dichotomous variable.\nIs there any way to merge the two models into a single one? So that with the 10 inputs, estimate a single output (internally discriminate options 'A' and 'B' to select the correct model and its inputs).\nNotice that using something like Voting Ensemble it's not valid because there are different inputs on each category (or I think it so)\nI accept another approach as a solution. Thank you.", "As you want to predict a value (regression), you can just train the two models separately (with the columns of your choice), you predict the output for each one and the prediction of the ensemble model is the mean of the two outputs.", "Merge distinct sklearn models into a single one", 0.0, 0, 21, "Have you considered using cross-validation? If you split your data into training, validation and test sets, you can train and validate the models on different subsets of the data. This way, your overall model is trained on all of your original data and can use the information from both the training and validation datasets. Your test set is then used to evaluate the performance of all your models and select which model you should use to make a prediction on the test data set. You can see [here](https://machinelearningmastery.com/cross-validated-deep-learning-model-performance-evaluation/) a detailed tutorial on how to use cross validation in deep learning. In the end you will get a predicted test dataset that uses all information provided in your training dataset and is independent from the different training/validation datasets", 0.2527472674846649, 0.04444444552063942, 0.18681319057941437, 0.06185566633939743], ["I want to plot graphs that share variables and datasets from other CoLab files, I would like to know how I could access those variables.", "You could create a new folder 'VARIABLES' where the variables are saved, read, and re-written (i.e. updated) as txt or csv files. Otherwise, defining a variable in one Colab Notebook will only be accessible within that Colab Notebook and not between Colab Notebooks.", "How to access/share datasets from different Colab notebooks", 0.1973753273487091, 1, 22, "There's no easy way to do this, but you can:\n\\begin{itemize}\n<li>\\em{Download datasets in the original notebok (see [the docs](https://colab.research.google.com/github/tensorflow/community/blob/master/notebook/data.ipynb#scrollTo=6aXyYP703P5b) for more details on download options)}\n</li>\n   <li><kbd>ctrl</kbd><em>+</em><strong>C</strong>, select \"Save\" from the list of options, and then \"save notepad as a file\" (which is also where you go if you want \"open saved notewok in new tab/window\", just type \"notepads\" for a list)</td> \\em{(see screenshot)}</t>h\n            <ol start=\"3\">\n                <p class=\"navlinks\">You can also \"share a dataset\" at the top of the page, you should then be able to click the \"download\" button and download the dataset, see screenshot below</p>  <td><a href=\"https%3A%2F%5C%E2%80%98%0Aco.laboratory%C3%A9.org%7D%602018.html#share_dataset\" target=\"_blank\">Screenshot</a></td></ol></p></li></ul>", 0.13526569306850433, 0.0, 0.07729468494653702, 0.045454543083906174], ["I'm trying to use optuna to tune hyperparameters of xgboost, but because of memory restriction, I can't set the attribute n_trials too high otherwise it would report MemoryError, so I'm wondering that if I set n_trials=5 and run the program for 4 times, would the result be similar to that I set n_trials=20 and run the program for one time?", "Yes, if you use the same database to store the study among different runs.", "A question about the \"n_trials\" in optuna", 0.0, 0, 23, "This question is outdated. It is very unlikely to be answered in the future, and the current answer is no.\nThe [documentation](https://optuna.readthedocs.io/en/stable/api/training_algorithms/nntrials.html) states:\n\\begin{itemize}\nFor single-run experiments, ntrial_max must be 1. In other words, this setting indicates how many times to repeat the search (e.g. 32).\nWhen tuning hyperparameter values for XGBRegressor with a tuner, the number of times you repeat each hyper-parameter trial is automatically determined by the tunner you choose. If you specify a value for this parameter, then you override the default hyper parameter search behavior provided by optune. For example, if you want to set a random seed for a hyperparmeter tuned with RandomSearch, you should pass this value in addition to the nTrials argument. Otherwise, optun will automatically determine the correct number to run n trial hyperparamters for each tuners. You should not pass nTrial_Max greater than the maximum number that the optimizer you are using for hyperoptimization can handle. That is, it is the same number you pass to ntune_search.maximum_n. See that parameter for more details. By default, hyperopt_parameter tunings are repeated a fixed number, unless you set this number explicitly. Optuna will use the given number when you call the opt.search() function (see the Optimization section below for the implementation details). However, when optuning hyperoptions with AutoTuner that supports parallelization, a different number is used, which depends on the parallel configuration. The following default values are used by Optune: nOptune search nTimes = max(1, num_threads) nMaxTuning 2 nMinTune 5 For details on how nnOptun searches each trial with random search, see below. Default Tuner. nSearch  = 6400\nDefault Tunner. Search nTimeS  (10)  For AutoRegression Tuning (XGBR) with MaxTripals set to 0.1 Optuun sets nNumSearch = nSample * nTrainingEpochs * (2 / 70). This is to ensure enough trials that are independent for optimization to occur. However for", 0.04747774451971054, 0.017910446971654892, 0.04154302552342415, 0.08196721225976944], ["For tf.keras.losses.SparseCategoricalCrossentropy(), the documentation of TensorFlow says\n\"Use this crossentropy loss function when there are two or more label classes.\"\nSince it covers two or more labels, including binary classification, then does it mean I can use this loss function for any classification problem? When do I have to use those binary loss such as tf.keras.losses.BinaryCrossentropy and similar ones?\nI am using TensorFlow 2.3.1", "BinaryCrossentropy ie like a special case of CategoricalCrossetropy with 2 classes, but BinaryCrossentropy is more efficient than CategoricalCrossentropy in calculation.\nWith CategoricalCrossentropy loss you should take the outputs as 2 dimension, while with BinaryCrossentropy 1 dimension is enough. It means you can reduce the weights by a half at the last layer with BinaryCrossentropy loss.", "Does \"tf.keras.losses.SparseCategoricalCrossentropy()\" work for all classification problems?", 1.2000000476837158, 1, 24, "Yes, I've found the answer for my question.\nThe function of tf_kerasp.crossentropyloss in TF 1.x is only for binary cross-entrophy loss. But in  Tf. keras in tensorflow2, the tf function supports multi-label classification and multi class.", 0.1855670064687729, 0.0, 0.10309278219938278, 0.019130559638142586], ["I got this err:\nimport pandas as pd\nModuleNotFoundError: No module named 'pandas'\nMy versions installed are:\nPython ver  3.9.7\npandas  1.3.3\npip 21.2.4\nPyCharm 11.0.12\nI can see pandas installed in pycharm, but when I am importing it I got that err.\nAny clue?\nThank you", "Try to reinstall pandas package.\ntype = pip install pandas\nwait for some time and then your panda package will get installed", "Python : ModuleNotFoundError: No module named 'pandas'", 0.0, 0, 25, "This is probably because you have installed pandas version 0 or  version higher than 4. But in your requirement.txt you are installing version of pandas 3 and 5 which is  conflicting to the installation version you currently have. You can install only version as your requirements. you can read more about this [here](https://blog.jetbrains.com/pycharm/2016/10/whats-new-in-pycha...).", 0.1627907007932663, 0.0, 0.09302325546741486, 0.07766620069742203], ["I'm trying to find a library (any language, but preferably C# or Python) which will let me open an XLSX file, iterate through the chart objects, and find data about the chart - ideally including the data backing the chart.\nThe Pandas Python package, or ExcelDataReader NuGet package have useful functionality for opening the file and reading a grid of numbers, as well as ways to add charts, but I don't find any way to read the charts.\nCurious to hear from anyone who has ideas/solutions.", "Hey I have a good solution for C#. In C# you can use OLEDB, this allows you to connect a C# code to a excel or access database (so long the database is in the C# code files). You don't need to get any addons for this is you have C# on Visual Studio.", "Reading chart data from an Excel file", 0.1973753273487091, 1, 26, "The [ExcelData Reader](https://github.com/exceldatareader/Excel-Data-Reader) works with Excel 2003+\n\\begin{itemize}\n1) 30 days trial\n2) Full featured open source. C#, .NET, VB.NET\n3) It's free. No cost. Not even the need to register. In fact, there is no cost of using it\n4) No restrictions on commercial, non-commercial, enterprise, business, home, personal use or even if you are running a single computer. It can be used to create the best charts in a very short time. You can create charts with a few lines of code and use them anywhere\n5) Excel Data Reaader uses .Net Interop to communicate with the Excel workbook. Excel has many API functions to provide us the power of Excel. The best thing is, the functions are very easy to use and powerful. These features have not changed over time\n6) The Excel interop can use the built-in functions and methods. There are only 4 Excel APIs available with it, however, we have the ability to make calls to all the methods of the object. So we are really independent of anything to get it. Even Excel is not there\n7) There is a chart property object of a worksheet which is called the work chart object, with which all of these operations are carried out. As per the requirement, you can get the name of chart, name, number of series, x-axis, y-axes, titles and titles of colors, chart type, series type and data points. Similarly, it provides all operations related to chart\n8) For example, if we want to know the type of color, font, border and all series of our chart and series or titles then the properties are shown as follows. To get these properties, please refer to the following image\n<a href=\"https: //i.stack.imgur. com/KfqYn.jpg\" rel=\"nofollow noreferrer\"><IMAGE></a>\nYou can also use Microsoft Graph API. https://docs.microsoft.c om/en-us/graph/api/datatables?view=graph-rest-1.0\nFor more details please see the below code snippet.", 0.16458852589130402, 0.02005012519657612, 0.08977556228637695, 0.07441859692335129], ["For some classification needs. I have multivariate time series data composed from 4 stelite images in form of (145521 pixels, 4 dates, 2 bands)\nI made a classification with tempCNN to classify the data into 5 classes. However there is a big gap between the class 1,2 with 500 samples and 4,5 with 1452485 samples.\nI' am wondering if there is a method that help me oversamling the two first classes to make my dataset more adequate for classification.", "actually there is a lib in python for that \"imbalanced-learn\" (although u can do it manually) .\nyou can check the docs it's very easy to use", "oversampling some classes from time series data", 0.3799489736557007, 2, 27, "There are a few options for what you're looking to do. If your data is in a time sequence format, I suggest you consider pre-processing your dataset to convert it to a feature matrix. Feature matrices can be manipulated easily in Python and have a nice time-series format that is useful for clustering and classification (as well as visualizations). The steps to preprocess your time sequences are:\n\\begin{itemize}\n1. Convert the time data to features\n2. Reshape your original dataset into a 3D tensor\n3. Perform normalization on your features (preferably mean-std) and reshape again to 7D tensors\n4. Optionally, perform dimensionality reduction\n5. Save into the format of your choice\n6. Import into your code\n7. Use it!\nYou can check out some of the features and how to perform preprocessing on it here: https://github.com/huawei-at/preprocessing\nIf you need more information or examples on how it's done, let me know!", 0.190476194024086, 0.04278074949979782, 0.13756613433361053, 0.06086956709623337], ["How do you convert a column of dates of the form \"2020-06-30 15:20:13.078196+00:00\" to datetime in pandas?\nThis is what I have done:\n\npd.concat([df, df.date_string.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f.%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f:%z')}))], axis=1)\n\nI get the error - time data '2020-06-30 15:20:13.078196+00:00' does not match format in all cases.\nAny help is appreciated.", "None of the formats mentioned by you above matches your sample.\nTry this\n\n\"%Y-%m-%d %H:%M:%S.%f%z\" (Notice the space before %H).", "Dealing with \"+00:00\" in datetime format", 0.0996679961681366, 1, 28, "You need to handle timezone: either remove it (with `dt.replace(...)` or use `tz_convert()`: [See doc](https://pandas.pydata.org/docs/reference/api/pandas_timedelta_to_datetime)", 0.03703703731298447, 0.0, 0.03703703731298447, 0.035087719559669495], ["Is there an efficient way to calculate the optimal swaps required to sort an array? The element of the array can be duplicated, and there is a given upper limit=3. (the elements can be in {1,2,3})\nFor example:\n1311212323 -> 1111222333 (#swaps: 2)\nAlready found similar questions on Stackoverflow, however, we have new information about the upper limit, that can be useful in the algorithm.", "Yes, the upper limit of 3 makes a big difference.\nLet w(i, j) be the number of positions that contain i that should contain j. To find the optimal number of swaps, let w'(i, j) = w(i, j) - min(w(i, j), w(j, i)). The answer is (sum over i<j of min(w(i, j), w(j, i))) + (2/3) (sum over i!=j of w'(i, j)).\nThat this answer is an upper bound follows from the following greedy algorithm: if there are i!=j such that w(i, j) > 0 and w(j, i) > 0, then we can swap an appropriate i and j, costing us one swap but also lowering the bound by one. Otherwise, swap any two out of place elements. The first term of the answer goes up by one, and the second goes down by two. (I am implicitly invoking induction here.)\nThat this answer is a lower bound follows from the fact that no swap can decrease it by more than one. This follows from more tedious case analysis.\nThe reason that this answer doesn't generalize past (much past?) 3 is that the cycle structure gets more complicated. Still, for array entries bounded by k, there should be an algorithm whose exponential dependence is limited to k, with a polynomial dependence on n, the length of the arrays.", "Algorithm to calculate the minimum swaps required to sort an array with duplicated elements in a given range?", 0.1973753273487091, 1, 29, "The general [selection algorithm](https://en.wikipedia.org/wiki/Selection_algorithm) is\n\\begin{itemize}\n  \\item Take the leftmost item. It must be greater than the rightmost\n    item, so the order of this pair will be reversed. Swap the\n     elements. The two elements are now adjacent. This is the new\n      right item and the old right most item is now the second\n       element. Repeat until there are no more items to be\n        exchanged. At that point, the result is ordered. If\n         duplicate elements exist, all elements that are\n          adjacent to a duplicate will also be adjacent after\n           the sorting operation. Any duplicates are not in order. A\n            selection of n items with m duplicate\n             elements will take nm swapping operations. See\n              https://stackoverflow.com/a/27202817/194567\n for a Python example. Here the Python code. I am\n   assuming that the duplicate element has been excluded in\n the question. That would have been necessary if the element was\n an integer. With a non-integer, it is not necessary because any\n duplicate in an even length array will never be exacerbated to\n its maximum duplicate value by exchanging two adjacent\n items. Only the initial selection will matter. In a 64\n byte array, if all but one byte is zero, you will have 4 swap\n operations to rearrange the bytes to get the desired result. Note\n that even in such an extreme case of a minimum of 8 swap operations\n to reach an intended order, a pair of comparisons is required\n as in any general algorithm and this is less than 3 if your language\n of choice supports comparison operations such as > or >=. And if\n it's 0x00  ..  ...  ,0xf0 the 5 bit comparison is only 9 bits\n so not as bad as you might have guessed (3+5).\n \\begin {code}", 0.30038022994995117, 0.02671755664050579, 0.12547528743743896, 0.034375183284282684], ["I'm absolutely new in python, so there is a question.\nI've splitted my original df to X_train, y_train, X_test, y_test.\nNow i want to drop from y_train (pd.series) outliers therefore i need to remove object with same index from X_train(pd.df).\nWhat is  the easiest and cleanest way to do it?", "try using y_train = y_train[X_train_new.index] where X_train_new is your new X_train after dropping some columns/row/outliers.", "Remove rows from X_train and y_train at once", 0.0, 0, 30, "\\begin{code}\ndata = [1,2,3,4,5,6,7,8,9,10, 223]\nresult = list(set(data))\n\\end{pre}", 0.0, 0.0, 0.0, 0.06756755709648132], ["I have 2 indexes, one named NIM, and one named Total Score. Both have 100X1 block matrices.\nWhen I run the code below the index gets removed.\n Final_Score = np.hstack((NIM, np.atleast_2d(total_score).T))\nIs there a way to combine several matrices into one and keep their indexes?", "In the end I used the same code but added:\npd.DataFrame(Final_score,columns=['NIM','Final Score'])\nNow, I can change np array into pd.", "How to keep indexes when combining matrices?", 0.0, 0, 31, "Try using `np.insert` and `insert`:\n\\begin{code}\nnp_insert = [[0, 0], [1,0],[2,1],[3,2],[4,3],\n          [0,-1],[-1,-2],[3,-3]]\nidx = [list(i) for i in np_matrices]\nprint('matlist',idx)\nmat_A,matB, matC = NumericDict(np)([i.tolist() for idx in idx for row in i])\nNumericList(matA.T, Nim, Total score).insert(0,[matC]) #this keeps NIMO\nfinal_Nim_totalscore = final_matrix = A+B+C\n[a,b,c] = zip(finalN_finalS.indices,finaln_finS,totalScore) # this keeps indices\ntotalN = sum(a),sum(b), sum (c)    # This keeps total scores\npd.DataFrame(dict(zip(idx,[list (a[idx],b[idx],c[i])]))) #This keeps the matrix\ndf = pd.concat([pd[mat],pd1[Num],NpD1.mat,pd2[Total score]],ignore_index=True)# This one keeps index\n#This is just an example, the above lines can be replaced by the below\nlist_ = []\nnlist=[]\nfor idx,a in enumerate(NumericalList([list1]):\n    list_.append(list([a]+ [b]+[c]))\nmatrix=pd(indexed=final Numercial list)  #replacing list in the list\nMatrix =pd[[n,list[np1].tollist()]]+\n           pd[[list,[b].t, list1]]   #list and list.t is a column matrix, to make\n                                      #it a list, just change the first column\n         +pd [ [c].T]   +[[matrix]] #to change matrix to row matrix you need to put\n            [a].transpose()+[[c]]  pd [ np1+np2.transpoease()\n       +[matrix] + [[Nm1_Mat,Matrix]]# this makes an np matrix + np", 0.13178294897079468, 0.015625, 0.06976744532585144, 0.01737387105822563], ["I am working with a medium-size dataset that consists of around 150 HDF files, 0.5GB each. There is a scheduled process that updates those files using store.append from pd.HDFStore.\nI am trying to achieve the following scenario:\nFor HDF file:\n\nKeep the process that updates the store running\nOpen a store in a read-only mode\nRun a while loop that will be continuously selecting the latest available row from the store.\nClose the store on script exit\n\nNow, this works fine, because we can have as many readers as we want, as long as all of them are in read-only mode. However, in step 3, because HDFStore caches the file, it is not returning the rows that were appended after the connection was open. Is there a way to select the newly added rows without re-opening the store?", "After doing more research, I concluded that this is not possible with HDF files. The only reliable way of achieving the functionality above is to use a database (SQLite is closest - the read/write speed is lower than HDF but still faster than a fully-fledged database like Postgres or MySQL).", "Pandas HDFStore caching", 1.2000000476837158, 0, 32, "A better way is to use pandas read_hdf or readonly.read_table instead of HdfStore to access the files.", 0.30985915660858154, 0.05797101557254791, 0.19718310236930847, 0.023255812004208565], ["What columns do I have to consider while implementing K Means? I have 91 columns after pre processing. And also to how many columns do I have to apply K Means clustering ? Is it all of them or only a few to be considered ?", "It's actually about trial and error. There is no straight way to say which columns are going to help you the most until you try and figure it by yourself.\nbut you can use dimensionality reduction algorithms like PCA to project data to a lower dimension without much data loss. It's a common approach and also helps with the speed of your clustering algorithm.", "K means algorithm implementation", 0.0, 0, 33, "\\strong{If your original dataset contains 3 classes}\n\\begin{itemize} \n \\item Calculate the distance of each data point to the centroid for each cluster.\n  \\begin {itemize  }\n   \\textit{Distance} is the Euclidean distance or the Mahalanobis distance which is square root of sum of the squares of distance. The distance for a data is calculated from the original data.  It will return a value ranging from 0 to 1. So you will have a new matrix of values  which tells how close a particular data points is to a respective cluster centers. This is similar to calculating the squared Eucilentian distance in python.   The below code snippet will help you to understand this concept in a step by step way. <a href=\"https://i.stack.imgur.com/3pq28.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n    <h4>Step-1.</h5> <IMAGE>  <\\h1>    Step-2.</p>      <th class=\"answer\">Cluster 2</th>                 <td class=answer value=\"0.012679247155625872\">3</td>          < td class =\"answer\" value =\"0\"><td><td></td>\\item    \\end{code}    [How to calculate the square of Euclidean distance from original values](https:). <https:%20%2F%3A%7E%5B.%40&.3F7w%&>73vk7l.php).     [Euclidean Distance from Origin](http://mathworld.wolfram. com/EucleadianDistance.html)\n     \\em{How you do your preprocessing?}\n      As a result of your clusteering, you would have the following. Please ignore the values and focus on the clusters.   21     0     2     51\n14     78     88\n36     34\n66\n70\n22\n57\n80    39\n98  77  89    45\n03  59  10  95 74 83\n43 60 54  31  08", 0.1370716542005539, 0.018808776512742043, 0.07476635277271271, 0.05387204885482788], ["here I used panda for export my data which is located in numpy array. but there is a problem that I cant export my data and also there is a erroe that you can see below.\nvalueError: Must pass 2-d input\nthis is my main variable AccZONE=c.T and The type of that is Array Of float64, and the size Of That is (710,1,1)", "From the error it looks like the array is 3 dimensions, you need to change it to 2 dimensions, it would be nice if you could provide some code.\nYou can try np.reshape(arr,(-1,1)) or np.ravel(arr).", "Export final data from numpy to excel", 1.2000000476837158, 0, 34, "\\begin{code}\nimport pandas as pd\npd.ExcelWriter('filename.xlsx')\nc = pd.DataFrame(AccZONES, columns=[\"name\"])\n# Create the data frame as a pandas dataframe\nfile = io.StringIO()\ndtypes = {int:str} # convert to string for excel sheet\ndf = c.to_excel(file, sheet_name='Sheet1', columns=[str(n) for n in c])\nout = file.getvalue() # get the string\n\\end{pre}", 0.10000000149011612, 0.0, 0.05999999865889549, 0.04093567654490471], ["I'm running python script present in .py file using ExecuteStreamCommand processor in Nifi. For reading a csv file pandas modules is required. I'm calling pandas in the program but I'm getting error mentioned as \"No modules Pandas found\"\nI have Python installed in my local and added to path to Command path.\nHow to install Pandas library?", "I\u2019ve had a similar issue with other modules. What you need to do is install the Python modules on the NiFi server that your script calls. What the error message is telling you is that it\u2019s trying to find the module called pandas but it isn\u2019t installed on the host.", "Python Modules in Apache Nifi", 0.0, 0, 35, "Please try to add `py-pip` library to `ExecuteStream command` processor. It will solve your problem. Also try this example from Apache nifi website\nhttps://nifi-documentation.readthedocs.io/en/latest/Content/Processors/ExecuteScript.html", 0.11363636702299118, 0.0, 0.06818182021379471, 0.037383176386356354], ["I've read many similar posts that say Excel's T.INV.2T(probability, degs_Freedom) can be reproduced in python using scipy.stats.t.ppf().\nIf I use the example of T.INV.2T(0.05, 58) excels yields 2.002.\nBased on other answers posted I should get the same answer using scipy.stats.t.ppf(0.05, 58), but I do not. I get -1.672.\nAny ideas on what is wrong with my syntax?", "In Excel, you have two functions for returning the inverse of the Student's t-distribution: T.INV and T.INV.2T.\nThe first returns a left-tailed inverse of the Student's t-distribution and the second returns a two-tailed one.\nscipy.stats.t.ppf also returns a left-tailed inverse of t-distribution. So, if you want to compare scipy.stats.t.ppf with Excel you need to use the T.INV formula and not T.INV.2T \u2013 or you should divide the probability by two and then use it with scipy.", "How to reproduce Excel's T.INV.2T in python?", 0.0, 0, 36, "`t` values are the cumulative distribution function, not the inverse.  The correct way to get 0,06 is `scipy/stats/t._ppfcdf(scipty.np.array([0]), [0])`\nHowever this is still an unreliable method for small values since `stats/_ppfit` [was implemented only recently](http://docs.scipython.com/doc/sci/reference/distributions.html).\nUsing `np/distn/cdf` with a low `N=50` you get:\n\\begin{code}\nscipty.cumsum(np_arr_5[1:], 1).cft[5:]\nOut[26]:\narray(['-156823.51045535', '-146099.16978774',\n        '-848837.39286493', '8073034.98125431', 952791708.85719411])\n=> (0,-1,-01,1)\n%pi\n>>> (1/(0-0))\n- \u03c0\n(1/1-(-1)) = - \u03a0\nand so on for all other negative values, except for the very low values.", 0.18269230425357819, 0.029126213863492012, 0.125, 0.058411214500665665], ["reading an excel file in python jupyter notebook i'm trying to change a column datatype of a pandas dataframe from object to float and every try I get the message of ValueError: could not convert string to float: 'Variable by base style'. What does 'Variable by base style' mean?", "the data you're trying to convert includes an item : \"Variable by base style\" which obviously cannot be changed to a float.", "Can't convert object data type to float in pandas data frame", 0.0, 0, 37, "I think the problem is with the `0.0` in your variable.\nYou can try to convert to object by the following steps:\n\\begin{itemize}\nIn Jupyter: `df = df.astype(str)`.\nThen use: `$` to evaluate the formula for each column. I hope it will fix your problem.", 0.2028985470533371, 0.02985074557363987, 0.17391304671764374, 0.03921378031373024], ["I'm writing a numerical solution to a partial integro-differential equation, and I need it to run quickly so I found that scipy.integrate.simps is best, but it's not always 100% accurate and produces the spikes in [1]. My solution was to remove them with scipy.signal.medfilt and then interpolate over the gaps with an interpolator (I've tried CubicSpline, PChipInterpolator, scipy.interp1d, akima,...) but all of them produce little \"hiccups\" in the solution that can be seen at y=0.1, (produced with 3rd order butterworth filter) and these errors grow as the solution is evolved. How do I remove the spikes and get a simple, smooth interpolation over the gaps? Thanks!", "I don't work with scipy, but from what I've gathered, some things stood out to me, and could possibly be what's causing problems.\n\nYour call to plt.show() which displays the data happens before you filter out the outliers with medfilt(), so the corrected data might not appear in your plot\nThe median filter from what I gather doesn't remove outliers from your data, instead it resets each data point with the median value amongst its k-neighbors.\nWith this in mind, I have two suggestions, (1) your median filter window might be too small, and that is causing the outliers to not be removed. Try setting it yourself using medfilt(self.n_, k_size=5), it defaults to 3 so try odd numbers larger than 3. (2) Given that you're not losing data points from using the medfilter, you might not need the lines that follow it which try to interpolate data that was presumably removed.", "How to remove spikes in solution and produce smooth interpolation with scipy?", 0.1973753273487091, 1, 38, "Try `scipy_interpolate` and a [spline fit](https://docs.scikits.net/scikit-splines/index.html#splinemethods-knots):\n\\begin{itemize}\n[scip-interpnate](http://scipitools.org/docs/0_13_0/reference/generated/skimage.transform.resample.InterpResample1D.fit_splined.py)\n`sksp.splinterpinext.SmoothInterpnInterrpolation`\nExample:\nimport numpy as np\nfrom skimage import transform, resize, img_as_float, interpn\nimg = resize(img_to_grayscale(np.asarray([[0,0],[0,.5],[-.5,.3]])), 20) # create an image for interpolation\nim_smooth = interp_resampling(im, spline=spliner, n_shifts=4)#create an interpoled image\nprint im_shape, im.shape\nplt.imshow(interpin.reconstruct(spl).astype(float)) # show the results\nplot of interpolated data vs original\n<IMAGE>\nScikit splines is a python wrapper around the scikit.gsl_lattice package.\nscipidot.gif\nhttps: //www.google.com/search?q=scipipy+image+interpoly&tbm=isch\nPython splinterinext library https://github.jw22/Piclisp/projects/Simply%2527SPIKES/tree/master/lib/image\nImage interpolation using Splines http://www1.lsbu.ac.uk/water/spl_web/examples.htm\nA 4th-order Hermite spliner\nHermite_Interppolator.pdf\nhttp: www.cs.huji. ac.il/~mss/herm.ps\nMSE: < 0 807/841 = 97.98%\n# MSE(est) = [2.36195316321718, 6.05726909291511]\nPCA", 0.10169491171836853, 0.0, 0.06779661029577255, 0.043263282626867294], ["I have a dataset with ages (int) and wages, I would like to have the average wage next to each row of a person respective of that age.\nI created a smaller dataset using\nmean = df.groupby('age', as_index=False)['lnWage'].mean()\nwhat is the best way to append (for 2000 rows)?\n\n\n\n\nAge\nWage\n\n\n\n\n30\n10\n\n\n30\n20\n\n\n\n\nthen\n\n\n\n\nAge\nWage\naveragewage\n\n\n\n\n30\n10\n15\n\n\n30\n20\n15\n\n\n\n\nthanks!", "The comments above are helpful, I have found this to be the easiest method, where average is the df with average wages. (ffr)\ndf_main['avgWage'] = df['age'].map(average_df['averageWage'])", "add average value to every row containing value in different column", 1.2000000476837158, 0, 39, "\\begin{code}\nfrom statistics import mean\ndf = pd.DataFrame({'Age': [31, 32,34], 'Wages':[1224,916,874]})\nwages_mean= df['Wgages'].agg(['mean'])\nnew_data=df.assign(averaged_wgage=wagers_means)\n\\end{pre}", 0.1538461595773697, 0.0, 0.1230769231915474, 0.0517241433262825], ["when I do this\n\n\n    my_list = df.loc[df['ID']  == \"9\", ['ID1','ID2','ID3','ID4']].values.flatten().tolist()\n\n\nI get the result\n\n\n    my_list = ['-1','32','63','-1']\n\n\nAnd then when I do my_list .remove('-1') I see\n\n\n    my_list = ['32','63']\n\n\nwhich is what I want to see .However when I try to do .remove in single step like\n\n\n    my_list = df.loc[df['ID']  == \"9\",['ID1','ID2','ID3','ID4']].values.flatten().tolist().remove('-1')\n\n\nthen my_list is empty.\nWhy is this happening?", "Because remove does the operation in place, modifying the list itself.  It doesn't return anything.", "using .remove seperately vs using it in tolist()", 0.1973753273487091, 1, 40, "For a list to be empty, all elements should be removed. In the second case, your list contains an element that's not a number, which is why removing `-1` would return an empty list. So you can fix the first case by not appending `remove` at the end of the command:\n`my_result = mylist.remove(-1)`\nAs for the `tollist` issue, that happens because the list doesn't contain numeric data, so the `.tolst()` method is unable to handle the data type. That'd be like doing `list([1, 'two']).tollst()`, where you would expect a type error. I'm guessing that you wanted to use the '.values' method instead.", 0.14173229038715363, 0.03200000151991844, 0.09448818862438202, 0.059451840817928314], ["Is there any magic way to create an huge absence presence matrix in python? pd.crosstab and pd.pivot_table fail because of the memory requirement.\nI have an extremely large dataset like this one:\n\n\n\n\nPerson\nThing\n\n\n\n\nPeter\nbike\n\n\nPeter\ncar\n\n\nJenny\nbike\n\n\nJenny\nplane\n\n\n\n\nand need this:\n\n\n\n\n\nBike\ncar\nplane\n\n\n\n\nPeter\n1\n1\n0\n\n\nJenny\n1\n0\n1\n\n\n\n\nNote, the matrix is rather sparse. It contains a lot of zeros.", "Computers used in data science sometimes have absurdly high amount of RAM (I think I've seen one with 1tb before...)\nIf you don't have that much RAM, then I think the only way to resolve this is to utilize the hard drive...\nI would say, process the data, write it as a structured data on to a hard drive, and loop through while reading, say 50mb at a time to check if the name has already been added to the file, and modify it.", "Create a very large absence/presence SPARSEE matrix in Python", 0.0, 0, 41, "`pandas.DataFrame.set_index()` will do it (but a better idea would be to use the `melt` function from the package `sparse-table`)", 0.11009174585342407, 0.0, 0.0917431190609932, 0.05405405908823013], ["I'm trying to run my code on anaconda prompt and it gives me this error, any suggestions?\nAttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", "Thats not an anaconda error, but an error with the Python code. You'll have to debug the code itself to see, where the error lies. Basically you are trying to access a function that doesn't exist.", "AttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", 0.0, 0, 42, "This is happening due to numpy not being installed on your system, as your error message is telling you. You can try installing numpy and using it as a module instead, but this is probably not a preferred way to do things. If you are using Anaconda I would recommend using scikit-learn, which comes bundled with the package (as the code snippet you posted is using a scipy method)\nInstalling sci-kit learn with:\n`conda install -c scikits.learn sciky-learn`\nThen using sklearn will not run into this problem.\nIf you absolutely have to use numpy, install it first: `conda list` to get a list of what packages are available, then try: `\"pip install numpy\"` (or whatever package manager your operating system uses).\nPlease read [here](https://scikit.io/docs/install.html#installing-via-pip) for more information.", 0.17977528274059296, 0.04545454680919647, 0.12359550595283508, 0.04968554526567459], ["I'm currently trying to start working with tensorflow.\nI work with anaconda and I tried to install the tensorflow packages in the root environment but it always displays the message: \"Several errors encountered\".\nWhen I looked it up it says the solution is to create another environment exclusively for tensorflow, I did and it worked. But I'd still like to know what the reason for this is.", "I have had the same question when I started out. It seemed like it is the \"correct\" thing to do, so I just did it, but never understood why. After working with TensorFlow for 2 years now, and on multiple machines, I realised just how specific the set of its requirements is. Only a few versions of python are compatible with it, the same thing with numpy, and if you want to use NVIDIA GPUs, good luck figuring out the specific versions of cuda and cudnn.\nYou don't want to have to tailor most of the python-related software on your machine to running tensorflow. In order to avoid breaking it whenever you install something that requires a higher version of numpy, for example, it is best to keep it in a separate environment. This way you have an isolated \"container\" that keeps everything just the way TensorFlow wants it, while still being able to use other software if needed.\nNot to mention that there are several versions of TensorFlow and they all have different requirements.", "Why do I need another conda environment from tensorflow?", 1.2000000476837158, 2, 43, "You have two solutions:\n\\begin{itemize}\nOne, you can use a condea environment for each package you want to use. It's probably best to do that. That way you avoid having problems like these in future. So I recommend you to always use condo environment when you install tensorflow or any other package in your system. In fact, if you don't want all the packages that an environment includes, there is an option to clean it before. For more information, see [this](https://conda.io/docs/user-guide/tasks/manage-environments.html#creating-conda-envs-from-scratch).\nTwo, create a virtual environment with only TensorFlow. You can do it in two steps: First, use the command `conda create -n tensorflow python=3.6` this will create the virtual env, and activate it: `activate tensorflow`\nNow, when activating the new environment, it will ask for the environment type. The default value will be python, but if it' not that you are looking for, select the python version. Then, type `python -m pip install -U tensorflow-gpu` to have all other requirements installed as well.", 0.4033149182796478, 0.08888889104127884, 0.1712707132101059, 0.05282112583518028], ["I am building a Streamlit dashboard that will have to read data from a DataFrame. The problem is that I have a local csv and a local Excel file form which I am reading data using pd.read_csv().\nHowever, when I share my dashboard url with others, the data will fail to load because they won't have the file locally.\nHow can I read the contents of a csv and Excel file and turn them into a \"hardcoded\" pandas DataFrame?\nI guess my question is: how should I store and read my data without having local csv and Excel files?\nEdit: sorry for no code or MRE, but I literallyu have no idea how to do this. If I had a piece of code, it would simply be a pandas dataframe with sample data in it.", "In R I would use dput() function to show me the code necessary to create a data frame.\nFor Python I know that print(df.to_dict()) would do something similar to be a \"hardcoded\" Pandas DF.\nSo I would do the following:\n1: print your df. df.to_dict()\n2: copy and paste the necessary code to create the data frame inside your streamlit app. Something similar to this: {'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}}\n3: \"load\" the data frames by creating them everytime the application is run. df = pd.DataFrame.from_dict({'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}})\nPS: note that this solution is not scalable neither would work if your data keeps changing from time to time. If that's the case, you would need to keep printing and pasting your new df to your code every time.", "How to save contents of local csv file into a \"hardcoded\" Pandas DataFrame?", 1.2000000476837158, 1, 44, "You could use the [readline](https://pandas.pydata.org/docs/reference/api/pandas-readlines.html) method. As an example, let's read a CSV that is in the root of the repository:\n\\begin{itemize}\n$$ ls\nmain.tsv\ntitle.csv\nname.txt\nfile1.dat\n$ cd main\n44859 2013-04-26 11:23:14 53249051\n3195662 00:06:55 716946\n2829833 99:40\n122545758 8421527 38\n64172\n5087935\n0.5e+02  1036180 47.65  5.1e-1 6.3\n-3.9e6   1    2.7e3    5    30e0    1    -1   -60  0 -9\n...\n776788  63973  374.8   5       1     3   6  2   0   4   8     0     8      6      1      8.2e8    43     1       8       6       7       5     2     4       0       3       4     5        2      4    6     7     6    8        9     9       9      3      0      2    0        1         2       2         7        5         6          6           3         1        6        0         3        4           0             0           1            1           7            7           5           4          0          1          5          2          9          8            9           8           9            6            8          7         4             5            3           2             7             6             8             1             3             4            0            4         8         5             9         9             2           6         0               5                  -                   9                      9               1               7                3               4                1                0                5               0               -              0______________________________________\nAbove is the text file. Let'd say I", 0.18644067645072937, 0.06818182021379471, 0.14124293625354767, 0.013472258113324642], ["I have a FastText trained model \"*.ftz\". My program runs in the multithread mode.\nIs there anyway to load a model once and use it without loading for each thread?", "After some efforts to find a good solution I used Fastapi and implemented model as a service.", "Load trained model only once", 0.0, 0, 45, "It is possible to create only one FT model for your whole application, but the downside is that you have to use the `Model` object to do the prediction, because the model isn't saved. If you want to have the possibility to save the whole model, the only way to go is to split your application to 1 thread that is using the FasterText, and another thread(s) that run the actual prediction. For this the you could use a `ThreadPool` or `ThreadExecutor` with a predefined number of threads (2 - 3 seems to be a good number) for the second part of the application.", 0.09836065769195557, 0.01666666753590107, 0.04918032884597778, 0.07791540026664734], ["I am working on a regression problem and my dataset is very imbalanced. My features are age, sex, weight, medication dose, some lab results and I am trying to predict one column of continuous values.\nIn my dataset some individuals are represented by more samples than others. For example 30 lines of data from one individual, 10 from a second individual and 1 from a third and so on. I do not know how to select the training set so that the model is not biased towards specific subjects.\nI divided the training and testing set in a way that there is no data from the same individuals in both sets but still training a model with a training set that is not balanced regarding the amount of data from each individual would bias the model.", "I would suggest to duplicate samples. So that, for example, every individual will have 30 rows of data.\nAs an alternative, you can also adjust the weights. So that an individual with 30 samples will have weight 1, an individual with 10 samples will have weight 3, and an individual with 30 samples will have weight 30 [it's an equivalent to duplicating, but doesn't increases the training set]", "How to create a training set for regression in Python if in a dataset some individuals are represented by more samples than others?", 1.2000000476837158, 2, 46, "You need to apply random oversampling to the larger data set, and then perform model training on the ovearxampled data. After training the models, you can test on held-out validation data, where you don't apply any data-cleaning, to see if it works well for the unbiased data samples. This is what I've done in many projects. There's a package called \"resampler\" that performs ovarxampled random sampling. You could also apply oeverxsampled bagging or random forest algorithms. If you are using the Python scikit-learn package, the [resample](http://scikit.org/docs/dev/api/sklearn.model_selection.grid_search.html#skllearns-model-selection-grid-search-resamp) function can perform overxsampling.", 0.19565217196941376, 0.010989011265337467, 0.08695652335882187, 0.05775076150894165], ["I have a Python program that is controlling some machines and stores some data. The data is produced at a rate of about 20 rows per second (and about 10 columns or so). The whole run of this program can be as long as one week, as a result there is a large dataframe.\nWhat are safe and correct ways to store this data? With safe I mean that if something fails in the day 6, I will still have all the data from days 1\u21926. With correct I mean not re-writing the whole dataframe to a file in each loop.\nMy current solution is a CSV file, I just print each row manually. This solution is both safe and correct, but the problem is that CSV does not preserve data types and also occupies more memory. So I would like to know if there is a binary solution. I like the feather format as it is really fast, but it does not allow to append rows.", "I can think of two easy options:\n\nstore chunks of data (e.g. every 30 seconds or whatever suits your use case) into separate files; you can then postprocess them back into a single dataframe.\nstore each row into an SQL database as it comes in. Sqlite will likely be a good start, but I'd maybe really go for PostgreSQL. That's what databases are meant for, after all.", "How to lively save pandas dataframe to file?", 0.1973753273487091, 1, 47, "You can store it in a csv-file and then append to the csv file as you go along.  A simple csv parser (like numpy-io) should be sufficient.", 0.2222222238779068, 0.020618556067347527, 0.14141413569450378, 0.062497470527887344], ["Below is the dataframe:\n\n\n\n\n\ndate\nopen\n\n\n\n\n26\n15-09-21\n406.5\n\n\n\n\nNow I need the value of open so I tried:- print(df.open)\nIt gave error:\n\nAttributeError: 'DataFrame' object has no attribute 'open'\n\ncolumn types are as follow: print(df.dtypes)\n\ndate ---->           object\nopen ---->         float64", "If you have a white space in your \" open\" column, just rename it by df = df.rename(columns={' open':'open'})\nI sometimes prefer to call a specific pandas column using this df[\"open\"] and press \"Tab\" for the auto-complete (in Jupyter Notebook or in Vscode). That way I am aware of any concealed typos such as that whitespace which you have", "Getting the value of particular column when its only one row in pandas dataframe", 0.0, 0, 48, "You could try this way: `df['open']`\nYou may get: AttributeError. I guess that you are trying to use `open` from the class `pandas.core.frame.DataFrame` instead of `pd.dataframe['column_name']`.\nThere is no need for creating a new column `print(new_df)`.", 0.336448609828949, 0.01904761977493763, 0.14953270554542542, 0.03652968257665634], ["I am using databricks-connect and VS Code to develop some python code for Databricks.\nI would like to code and run/test everything directly from VS Code using databricks-connect to avoid dealing with Databricks web IDE. For basic notebooks, it works just fine but I would like to do the same with multiple notebooks and use imports (e.g. use import config-notebook in another notebook).\nHowever, in VS Code import another-notebook works fine but it does not work in Databricks.\nFrom what I could find, the alternative in Databricks is %run \"another-notebook\" but it does not work if I want to run that from VS Code (databricks-connect does not include notebook workflow).\nIs there any way to make notebook imports that works both in Databricks and is supported by databricks-connect ?\nThanks a lot for your answers !", "Well, you can create packages .whl(wheel) install in the cluster and call via import in any notebook is a breeze", "Import notebooks in Databricks", 0.0, 0, 49, "If you want your code in databrick to be the one runnable from vsc then you should set your env variables such as:\n\\begin{itemize}\n  \\item `PWD` should point to the working directory of your data frames that should be accessible by your python script (or use databric-cd)\n`export PWD=your_dir`\nthen you can import whatever notebocks or code you need from there\nexample\nfrom notepad to another note book\nnotepad\nrun %notepaddoc_notebok\nor if you have more than one notes\n%notedoc\n{notedomain:name of the note} doc\nwhich means run the notedomain doc to notemap domain doc and the other note you mentionned. I dont know what the right syntax to use is since i dont use notes in a long time\nin addition you may want also to define a variable that hold the directory where your noteedoc is saved and set it with %export DOC_DIR=/your/directory\nto avoid the long path for the saved document\nand for that you could use an env variable\nexport LIB_PATH=/path/to/libraries\nthat would make your environment to recognize that it needs to go in the path you define\nhope this can help\nRegards\nRamzi\n<a href=\"https://i.stack.imgur.com/gLXJU.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nHi Ramzi,\nThank you for taking the time to answer my question. Unfortunately this is not quite what i was looking for, I need to keep everything under the Datbbricks platform. What I was trying to achieve was to write a python notbook with imports to other notbooks that I can call with databrics-run in my IDE and then run from databrica with my custom workflow. This is possible with the default workspace I have but not with datasets. In the future it would be nice to remove everything I wrote from my local system but for now I just need a simple work around.", 0.05847953259944916, 0.0117647061124444, 0.05263157933950424, 0.021493690088391304], ["I am working on a Dial a Ride Problem (DARP). I have a lage amount of nodes and edges (338 nodes and 826 edges). I've imported the node/edge data from OSMnx and am trying to solve the model with Gurobi Optimizer in Python.\nTo be able to use the OSMnx data with Gurobi, I created a matrix = len(nodes) x len(nodes) matrix and therein printed the length of the edge if two nodes were connected, and a large number otherwise. In the optimization, a x[i,j] = len(nodes) x len(nodes) binary decision variable is used to decide if an edge is traversed or not.\nThe problem I am encountering is a large computing time for just one request (+1 hour). I think this is because the model also has to consider all the other indices from this large matrix, even though they can be ignored completely since they represent that two nodes are unconnected.\nMy question therefore is if someone can help me find some preprocessing techniques or something else that might reduce my computational time. For example, tell the model that it can ignore indices from this matrix if the value is too high or maybe a more efficient node/edge storage file that Gurobi can use more efficiently.\nThanks in advance.", "If your graph is sparse, the optimization model should be sparse, too. Specifically, you should only create a variable x[i,j] if the edge (i,j) exists in the graph. For an example of how to do this, see the netflow.py sample in the examples/python subdirectory of Gurobi.", "Preprocess node/edge data or reformat so Gurobi can optimize more efficiently", 0.0, 0, 50, "You should be using an adjacency matrix rather than a dense graph.", 0.190476194024086, 0.06557376682758331, 0.1269841343164444, 0.05454545095562935], ["I'm making a FPS game in Ursina, and I'd like to be able to aim. I will do this, I think, by changing the camera's FOV : it is perfect !\nThe problem is that I'd like to be able to animate the transition of aiming. I cannot use a for loop, as the FOV only updates once it is finished, and I cannot use the animate method... I tried :\ncamera.animate(\"fov\", -30, duration = 2, delay=0, auto_destroy = True)\nWith the syntax :\nanimate(name, value, duration=.1, delay=0, curve=curve.in_expo, loop=False, resolution=None, interrupt='kill', time_step=None, auto_destroy=True)\nHere, my value (I'd like to decrease my FOV, so to zoom, by 30) doesn't mean anything : I can put whatever I want, and it will not stop until the fov is equal to 0.\nIs there a way to fix that ? Either by finding a method to update the camera in the for loop, or either by finding any way to animate the FOV transition", "Found the answer : the value parameter is actually not the value you want to increase or decrease your FOV (or anything) of, but it's actually the value it will go to ! So, if I put 1, my FOV will go to 1, that's why.\nTo animate -30 for my FOV, the correct syntax is :\ncamera.animate(\"fov\", camera.fov-30, duration = 2, delay=0, auto_destroy = True)", "Python ursina : aim by changing FOV's value (issue with animation)", 1.2000000476837158, 0, 51, "You can're using a `for` loop in this case. Instead you should use:\n\\begin{itemize}\n  \\item the [`Animation`](https://docs.python.org/3/library/pygame.animation.html#pygames-animation) object. You should define a constant number of frames that will be played. And then set the animation object to the current camera object, using [`camera_animation()`](https:/docs/python/api/pypy/gi/Animation/camera).\nYou should then define an object that defines your zoom animation. It could be a [`Scale`](http://www.pygtk.fr/docs-py3.1/class-Scale.Animation) or similar. This would be the first element of the frames of your animation: the initial position. Then you define the subsequent positions of each frame, like you want. When you use [`Pygame`]([pyopengl.Pose](http://www-opengameengine-org-docs2.github.io/OGE-Documentation/Pyside/apidocs/modules/opygamescene.pose.md.doc#Pymysg::Pixyg::GameObjects::SceneParts::Pos)), you need to have an instance of this kind of object in your [position](https: //docs:pystruct.gimp:types:structures:Position.h:doc_class:ScenePartPos) field. An example is a [Rotate](:Ope::scene_part_pos::struct:Rotation). See the example in its [docs]((http //www:docs :pysrc:OpenPNG:Structures :Scene_Part_Pos :Rotating :Position).h). And there is an [example]((:Ps :docs) for using [Pylab](() :Ode :Psg :scene :GameScene :PyOpenGL :pyogl_pyx :_camera :Camera :Animation :cameraAnimation):\nSee the [source]((\\_\\_source):(https:\\/\\/github:\\/_git\\:\\_oopen\\:lib:\\_\\_\\:\\P:\\)ogg:Animation:animation_basic_example.cpp) of that [module](\\_\\__module\\:) and the  [animation]((`Animation`) in [this](_\\_{git:}_git:\\:\\:og", 0.13084112107753754, 0.006269592326134443, 0.08099688589572906, 0.04590163752436638], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "If you are trying to get the parameters of your model:\nprint(model.get_xgb_params())", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 52, "As the [doc](https://xgbooster.readthedocs.io/en/latest/xgb.html#xgradict.train) says:\n\\begin{itemize}\n<h4>`n_trees : int | array (int) | None, optional (default = None)`</h3>\nSpecifies the maximum number\nof decision trees that should be\ntrained during one training phase. The\nnumber must be a positive integer or\na list of integers. If this argument is\ngiven, it will be used as a lower bound. That\nis, xgb-boosters will try to find the best\nconfiguration for the trees such that the\nresulting estimator reaches the specified\nminimum number, but may end up with more\ntrees. Otherwise, a bound of 40 is imposed.If\nthis argument cannot be interpreted as\npositive integers, an exception will\nbe raised. Setting `n` to zero implies\nthat no trees are allowed. Use the `max_depth`\nparameter to control the complexity of trees. `None` implies the same value of `min_split_improvement` as the value set to `best_init_\u6df1\u5ea6\u641c\u7d22\u7684\u5206\u6790\u6700\u5c0f\u53d8\u5316\u964d\u4f4e\u7387`.\n`None`, `0` or `x` means to not set this\nvalue. Default is `1`. If it is an integer,\nthe algorithm will always grow the tree at least\nas deep as that specified. It is possible to\nspecify a small value in order to force\nx-g-boosters to prune nodes aggressively. This\ncan lead to improved predictive accuracy but\nreduced computational efficiency. A good\ndefault value is 8. When it contains a list, `train_samples` is calculated using the sum of lengths of those lists. (Note that\neach list represents one tree to be trained\nand not one hyperparameter of that tree.)\nFor convenience, if you just need to set a single\nhyperparameter, you can set `hyperparameters` in\nplace of a numerical list. See the class\n[hyperparamters](hyperparams)\nin the API for a\nmore detailed description of the individual\nproperties. For example, say you want\nto specify max_features of\n100, then you could do this::", 0.04268292710185051, 0.0061349691823124886, 0.030487805604934692, 0.049382712692022324], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "model.get_params(deep=True) should show n_estimators\nThen use model.get_xgb_params() for xgboost specific parameters.", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 53, "I don't know if it is relevant for your case but if you need to get estimator numbers you can do this:\n\\begin{itemize}\nFirst, you have to make a grid search. Then, when you are done,\nit will return you the estimate number. You can then print it\nas you please. If it doesn's give you number it might be because of\nearly stopping. The estimating number will be in 'n_trees'\nparameter of the best tree model returned by the grid_search\ngrid_pars. Best is the index of your grid. It will look like: 56\ntrees, 49.8% training, early_stopping_rounds 20, max_depth\n7, min_samples_leaf 33, nbins_cats 16, etc\nAlso note that you will find all the information about the\ntree in `tree_.thetas`.\nI'm not sure how to do it for the other hyper parameter though...\nHope that can help you\nAxel\np.s.: I've noticed that the xgb.get_best_model() function\nhas been removed from xGBoost 0.90: [link](https://xgbooster.readthedocs.io/en/latest/pythonapi/xgb-api.html#xgz.xGBGetBestModel).\nMaybe it'll help... but I have no idea about other\nhyperparameter\nThanks for\nyou answer\nRegards\nJess\n[I know that my question is old but as I said in my first comment there're other ways of retrieving hyperparameter]\nhttps:www.pyxdg.com/tutorials/classification-with-x-g-boost-using-sklearn/", 0.07003890722990036, 0.0, 0.046692606061697006, 0.05566335469484329], ["I am getting below warning for python in console.I did not found any solution for these.We dont want to suppress warnings .\nAlso we have a big code base setup.how to know which code block is cause of this error as warning dont give code line number.\nI am using below version of python and numpy.Is it due to old verison's of python and numpy.\nPython version- 3.6.8\nNumpy Version-  1.19.5\nmatplotlib version is 3.3.4\npandas version is 1.1.5\nWarning:\n/python3.6/site-packages/matplotlib/cbook/init.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. obj[:, None]) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\npython3.6/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead", "It's the way you're accessing the array, using slicing. Matplotlib is going to remove that from how they handle arrays, but they haven't yet. It's just a recommendation to convert to a different type of array access, like Numpy, before that happens. Based of what you're showing, i'd guess it's as simple as 1. Create Numpy Array 2. Use identical slicing except using Numpy syntax. Should be good to go after that I'd imagine.", "Python warning :FutureWarning: Support for multi-dimensional indexing", 0.0, 0, 54, "There is a similar warning that is triggered when you use `np.argwhere(condition).astype(int)` and then index on the result with `[[i] for i in np.ravel(np_arr)]`\nSo basically this warning is caused by the fact that pandas is not happy when a matrix is indexed by more than two dimensions (at least with version 0.25).\nYou should be able to work around this by using a [pandas.factorize](https://pandas-docs.github.io/en/stable/generated/numpy.ndarray.html?highlight=factor%20in%3D#numpy-ndarr-factor), since the warnings seem to have gone away with the latest release.", 0.20430107414722443, 0.010869565419852734, 0.10752688348293304, 0.03867403417825699], ["I've been building an application using Apache Cordova - it's actually based on machine learning, but all my machine learning prototyping has been done in Python.\nIs there a way I could incorporate my Python libraries (like scikit-learn) into my Apache Cordova app, or is there something else I should include?\nThank you, any help would be appreciated!", "No, you can't embed a programming language as a plugin for Cordova. You can however do a remote call to a server running python.", "Can we use Python modules with Apache Cordova?", 1.2000000476837158, 1, 55, "You can load any python modules into apache cordova like you would any other program. You have to make sure you get the python module installed on the phone, I believe there're different platforms that have different python packages installed. I found a post about how to load python in [this stackoverflow question](https://stackoverflow.com/questions/21536141/how-to-run-python-code-on-android-with-cordova). I hope this helps!", 0.14432989060878754, 0.021052632480859756, 0.14432989060878754, 0.01904674433171749], ["I have two date columns having corresponding Dollars associated in two other column. I want to plot it in single chart, but for that data preparation in required in python.\nActual table\n\n\n\n\nStartDate\nstart$\nEndDate\nEnd$\n\n\n\n\n5 June\n500\n7 June\n300\n\n\n7 June\n600\n10 June\n550\n\n\n8 june\n900\n10 June\n600\n\n\n\n\nExpected Table\n\n\n\n\nPythonDate\nstart$\nEnd$\n\n\n\n\n5 June\n500\n0\n\n\n6 june\n0\n0\n\n\n7 June\n600\n300\n\n\n8 June\n900\n0\n\n\n9 June\n0\n0\n\n\n10June\n0\n1150\n\n\n\n\nAny solution in Python?", "I can suggest you a basic logic, you figure out how to do it. It's not difficult to do it and it'll be a good learning too:\n\nYou can read only the subset of columns you need from the input table\nas a single dataframe. Make such two dataframes with value as 0 for\nthe column that you be missing and then append them together.", "Data preparation to convert two date field in one", 0.1973753273487091, 1, 56, "You need to extract the information from your dataframe, as it looks like a JSON or something similar to a nested JSON format. Then the solution should be easier and straightforward. This would help you.", 0.3137255012989044, 0.019999999552965164, 0.1764705926179886, 0.06508875638246536], ["I am looking for a sentiment analysis code with atleast 80%+ accuracy. I tried Vader and it I found it easy and usable, however it was giving accuracy of 64% only.\nNow, I was looking at some BERT models and I noticed it needs to be re-trained? Is that correct? Isn't it pre-trained? is re-training necessary?", "you can use pickle.\nPickle lets you.. well pickle your model for later use and in fact, you can use a loop to keep training the model until it reaches a certain accuracy and then exit the loop and pickle the model for later use.\nYou can find many tutorials on youtube on how to pickel a model.", "Is it necessary to re-train BERT models, specifically RoBERTa model?", -0.1973753273487091, -2, 57, "Yes, retraining BERT is necessary, because the pretraining is trained on a different dataset (the Gutenberg Corpus) than the one you use for your task (you would use a proprietary dataset of your own). I've also seen that some people retrain it on the Github and Reddit corpus. [I have rebuilt a custom BAR-based BABE model for my tasks](https://github.com/PriyaDutta-AI/Sentiment-Analysis-with-R-BERTBABEBIRT)", 0.27906978130340576, 0.031496062874794006, 0.1705426424741745, 0.05349794402718544], ["For example, I've a table:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n5\n202\nTrue\nsend\n2021-10-20 13:15:18\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n8\n202\nFalse\nreply\n2021-10-20 13:15:25\n\n\n\n\nHere, True/False is based on whether column 'Action' has 'send' without colon or not. If 'send' then it's True, otherwise False.\nI want to delete rows based on condition of a row which is True. So, delete rows if:\ni) a column 'Number' has a same a number which corresponds to value True in column 'B', In this case: delete if Number== 202\nii)and if a column 'Datetime' is in range of 2 minutes of a column which corresponds to value True in column 'B'. Datetime corresponding to True value is '2021-10-20 13:15:18' and it's range of 2 is: [2021-10-20 13:13:18 ; 2021-10-20 13:17:18].\nOverall, deleted rows should have a number=202 and which are in range [2021-10-20 13:13:18;2021-10-20 13:17:18]\nNew table should look like this:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n\n\nSorry, if a question and task is not fully clear.", "Try this:\ndf = df.loc[df[\"Number\"]!=202 & df[\"B\"]!= 'True']\nIf type in column B is boolean, then change the string \"true\" to a boolean True.", "Pandas: How to delete rows based on some conditions?", 0.0, 0, 58, "The first filter condition seems to be very complex and I am not quite sure what you mean. Could you provide an example data set? I tried something like the following, but I think you will have to improve it further. The script assumes the data to work with is `df`.\n\\begin{itemize}\n  \\item Split the columns `Number` and `B` using `rpartition`\n     \\begin {code}", 0.1318681389093399, 0.0, 0.06593406945466995, 0.034167107194662094], ["I have a 45x45 matrix which Stack overflow isn't letting me include as it is too long. But if I throw this matrix into numpy.linalg.eig, it gives me an eigenvector of all zeros in the last column. What does that even mean?", "So it looks like the matrix is actually degenerate, which I suppose makes sense, actually.", "Numpy.linalg.eig function returning zero eigenvector?", -0.1973753273487091, -1, 59, "You are getting all zero entries in your eigenvectors because you have zero-eigenvalue matrix.  For example,\n\\begin{code}\n>>> np.eye(4)\narray([[1., 0., ., 1.],\n   [0.229824974679013, .0938063175715995,  0,     0],     # <--- eigen value\n  [ .105819607330368,   0     ,      .218504708844126,    0   ],\n    [  .377864325528769,       0      ,          .4894234002720,-    ]])\n\"\"\"\n> a = np_array(self.x) <-- A has 9 values, so that's the reason for\n2 eigen values for A.\nx = A\nA = -np.identity(2) * npdot(x, x.transpose())  <---- I can' see any reason why you'd need to\nsolve the eigenvalue equation for it.", 0.09900990128517151, 0.0, 0.059405941516160965, 0.03896103426814079], ["I am trying to write a function equilateral(x, y): that takes two np.ndarrays of shape (N,)   , where x and y are natural numbers and returns a point z an np.ndarray of shape (N,) such that (x, y, z) are are the vertices of an equilateral triangle.\nAny one please suggest.", "In order to get the third vertex, you could just rotate the point (x2, y2,...) by 60 degrees around point (x1, y1,...). The other admissible solution would be obtained with a rotation by -60 degrees, i.e., in the opposite direction.\nSo just rotate y around x by 60/-60 degrees and you have your 3rd co-ordinate.", "Python function to find a point of an equilateral triangle", 0.0996679961681366, 1, 60, "The [math package](https://docs.python.org/3/library/math.html) offers a [`triangle_area`](httpss://www.geeksforgeeks.com/python-program-to-find-an-equilatereal-triangle/) function that computes the area of a triangle given its sides. Use that to determine the vertex coordinates given two vertices:\n\\begin{itemize}\n  \\item Given any two points `(x1,y1), (xi,yi)`, we can determine `i` such\n   that `(xi+i,xj+yi)=(xi/2,xi-xi) + (i/x2 + x2 - x1)` and then\n    `(-x3,2x4)=x - (-x5,1.5) = (2.65256, 12/7.854)`, so `xi=22`, `y=8`.\n \\end{blockquote}", 0.1818181872367859, 0.01315789483487606, 0.10389610379934311, 0.04577464610338211], ["I have a model defined as\ngmodel=Model(a)+Model(b)\nresult=gmodel.fit(data,...)\nI use this model to fit the data, which gives me the parameters and their error estimates. Using the result.eval_components(), I could access the component a and component b of the model function. Also, using result.eval_uncertainty(), I could access the 1-sigma uncertainties in the model functions, which would be two lines. Now I want to know each component a and b in that 1-sigma uncertainty lines.Is there a easy way of doing this.", "There is not currently \"an easy way of doing this\" - the eval_uncertainty method belongs to the lmfit.ModelResult, not the lmfit.Model itself, at least partly because it needs the resulting covariance matrix.\nBut: I think eval_uncertainty method could probably calculate the uncertainty in any component models too.  I would suggest raising an Issue and/or using the lmfit mailing list to discuss making this change.", "Calculate the uncertainty in components of composite model in Lmfit", 0.0, 0, 61, "From the documentation for [Model.to_rdf](https://lmfit.github.io/lmfitter/api/generated/lfm.Model_1.html#lfminit-lfc-Model-1), you can get a dictionary `{\"a\": x}` for `x` being the value for a. For each variable you want a, b, etc, just make another line with the variable name. Then, you could use the above technique to get the covariance for that variable and its uncertainty. I suggest checking the lmfitexample package to make sure you are using the right syntax.", 0.22368420660495758, 0.013333333656191826, 0.15789473056793213, 0.05833333358168602], ["Im trying to convert a pil image to a numpy array but i want the image to keep it's transperancy, how can i do that?\nIv'e tried using numpy.array()\nBut it doesnt keep the transperancy", "I'm not quite sure what you mean by it doesn't keep its transparency. If you convert a PIL image with transparency using numpy.array() it should return a numpy array with shape width, height, 4 where the 4th channel will represent the alpha channel values. And after whatever processing you need to do, if you convert it back to a PIL image using Image.fromarray() and perhaps saving with Image.save() you should get back an image with the same transparency. Can't help you much more without seeing an actual snippet of the code and possibly the image.", "How can i convert a PIL image to cv2 numpy array", 1.2000000476837158, 0, 62, "PYTHON\ncv2.imread(filename)\n\\strong{output}\narray([[ 0, 199,  62, ...,   0],\n        [167,    0    , 528,\n         ...\n,...\n....,     0 ]])\nif you want to change RGB format into RGBA:\nimg[...,2:]=img[:,:,2]\nand the third channel is for alpha", 0.11764705926179886, 0.014925372786819935, 0.0882352963089943, 0.024946901947259903], ["I need a function to turn bytes back to a numpy array. Is there a simple way to do it?\nPickle doesn't work because my data is too long and everything else I tried fails as well... I'm trying to send a frame over a socket from my client to my server.", "Try this: x = np.frombuffer(n, dtype=i.dtype)", "What is the opposite function of tobytes()?", 0.0, 0, 63, "`bytearray.tobytes`\n\\begin{itemize}\n_{You could try to serialize your numpy data with [numpy.savez](https://docs.scipy.org/doc/numpy/reference/generated/ numpy.ndarray.__save__.html) and unserialize it afterwards.}\nFor example:\n`numpy_array_bytes = numpy_file.read()`\nthen:`f.write(numpy_.tostring( numpy_, 'wb')`\nand after:`.read()[numpy] = unpack(bytes(f)`\nThen your code can be rewritten to: `#code:`\n`.frombuffer(unpack_data, 'ubyte')` #data is a string array from numpy saved as string.#code` #this will only work if all bytes are stored in numpy#else: print(\"not all byte are saved in `numpy`\")#endif#then your final code is: `.fromarray(data)` #numpy data in bytes is already converted to numpy, you don' t have to use `.toarray()` #code: #now you can check if your data was correctly converted: if numpy._testargs(np_arr)==True:#if numpy fails #print(\"There was an error.\")#print('numpy failed')#if not numpy failed:#numpy array = np.array('your_numpy') #.shape #if there was a shape #check if this array has the right shape and size. If it is 4x608x1288#.reshape(4,6,1,2,8) #you have converted numpy into the proper format now #but there are still a lot of tests you need to run#check for negative, NaN and NaN values#for that you have `nan_test()` and you should also run `testall(nan)`.#finally #for this numpy version you could use numpy or pickle . #pickle only pickles numpy objects#it will not convert numpy to byte array#you can use your own method here#to check the data if it was converted correctly #test_yourself_picklereading_and_unpickling_example#testpickleresult#def test_self(pickled_obj_string):#pickeld_result = pickl.load(open(str(filename), 'rb'))#return pickledresulthelp(test)`}", 0.02684563770890236, 0.0, 0.020134227350354195, 0.0476190447807312], ["I am trying to use the python weka wrapper. I am using the Cross validation method. It then prints the classification results. Then i use build_Classifier and test on training data using test_model. It provides different no.of classification instances compared to the cross validation model.\nFrom what i understood, in the cross validation model, 10 different models are built, and then the accuracy is averaged while the models are discarded. Then it fits the entire data again and produces the classification results.\nthen when the data is the same, shouldnt i get the same results with the build_classifier model as well?\nor is it because i put randomstate in crossvalidation but did not randomize the data in build_model?", "When performing cross-validation, the entire dataset is essentially being used as test set. The predictions from the k folds get collected and used for calculating the statistics that get output. No averaging of statistics or models occurs.\nTraining and evaluating on the full dataset will yield different results, but you should see the same number of instances being used. It is possible that there is a bug in your code. But you need to post your code to determine the cause of that.", "WEKA training and cross validation", 0.0, 0, 64, "First, it looks like you are using \\strong{Weka version 3.8.5} with `Python 2.7.12` (see the `build_Model()` method and `classification_results.getClassStatistics()` and the latter call to `printStats()`).\nSecond, your question is not clear, you should give more details.", 0.203125, 0.0317460335791111, 0.09375, 0.03389830142259598], ["I have an array of n positive integers. I want to calculate a list of all contiguous subarray products of size k modulo p. For instance for the following array:\na = [3, 12, 5, 2, 3, 7, 4, 3]\nwith k = 3 and p = 12, the ordered list of all k-sized contiguous subarray products will be:\nk_products = [180, 120, 30, 42, 84, 84]\nand modulo p we have:\nk_products_p = [0, 0, 6, 6, 0, 0]\nwe can easily compute k_products using a sliding window. All we have to do is to compute the product for the first k-sized subarray and then compute the next elements of k_product using the following formula:\nk_product[i] = k_product[i - 1] * a[i + k] / a[i - 1]\nand after forming the whole list, we can compute k_product[i] % p for each i to get k_product_p. That's it. O(n) complexity is pretty good.\nBut if the elements of a[i] are big, the elements of k_product may overflow, and thus we cannot compute k_product_p. Plus, we cannot, for example do the following:\nk_product[i] = ((k_product[i - 1] % p) * (a[i + k] % p) / (a[i - 1] % p)) % p  // incorrect\nSo is there a fast algorithm to do this? Note that p is not necessarily prime and it is also not necessarily coprime to the elements of a.\nEdit: As mentioned in the comments, there will be no overflow in python, but working with very big numbers will be time-consuming.", "This is not a sliding window algorithm, but it is a simple and effective way to solve this problem in O(n) time without any division:\nLet A be your original array.  We will imagine that there is a \"mark\" on every kth element of A -- elements A[0], A[k], A[2k], etc.  This ensures that every k-length window in A will contain exactly one mark.\nNow, make two new arrays B and C, such that:\n\nIn array B, each element B[i] will contain the product (mod p) of A[i] and all following elements up to but not including the next mark.  If A[i] is marked, then B[i] = 1.  You can calculate this in a single pass backward from i=n-1 to i=0.\n\nIn array C, each element C[i] will contain the product (mod p) of A[i] and all preceding elements down to and including the previous mark.  If A[i] is marked, then C[i] = A[i].  You can calculate this in a single pass forward from i=0 to i=n-1.\n\n\nNow, you can easily calculate the complete product of any k-length window in constant time, because the product of any window from A[i]...A[i+k-1] is just B[i] * C[i+k-1].  Remember that there is exactly one mark inside the window.  B[i] is the product of the elements before the mark, and C[i+k-1] is the product of the marked element and the elements after it.", "Sliding window algorithm to calculate the list of all k-element contiguous subarray products of an array modulo p", 1.2000000476837158, 2, 65, "Since the question asked for C, here're some possible ideas: -\n\\begin{itemize}\n  \\item `gcd` the length `n` and the size of the sub-array `k`.\nIf the gcd is `1` then your solution will work, if not, proceed to next step\n  2. If `p` divides `a`[0], then proceed with: \\begin\n{code}", 0.1666666716337204, 0.019354838877916336, 0.0833333358168602, 0.05726872384548187], ["I have a pandas dataframe containing large volumes of text in each row and it takes up 1.6GB of space when converted to .pkl. Now I want to make a list of words from this dataframe, and I thought that something as simple as [word for text in df.text for word in i.split()] should suffice, however, this expression eats up all 16GB of ram in 10 seconds and that's it. It is really interesting to me how that works, why is it not just above 1.6GB? I know that lists allocate a little more memory to be able to expand, so I have tried tuples - the same result. I even tried writing everything into a file as tuples ('one', 'two', 'three') and then opening the file and doing eval - still the same result. Why does that happen? Does pandas compress data or is python that inefficient? What is a better way to do it?", "You can use a generator. For example map(func, iterable)", "Why do python lists take up so much memory?", 1.2000000476837158, 1, 66, "\\begin{blockquote}\nI know it allocates a bit more than needed, but is that really 20x more? Why is memory being allocated in this way? Is it python or pandas being inept? Or is pandas really ineficient and just being dishonest? [etc]\n\\end{itemize}", 0.03703703731298447, 0.0, 0.03703703731298447, 0.0], ["I have a model which requires solving a system of ODEs with tfp.math.ode.BDF, and I would like to find the individual least-squares fits of this model to n > 1000 datasets. That is to say, if my model has m parameters then at the end of the optimization process I will have an n by m tensor of best-fit parameter values.\nWhat would be the best way to perform this optimization in parallel? At this point I\u2019m planning to define an objective function that adds up the n individual sums of square residuals, and then uses tfp.optimizer.lbfgs_minimize to find the best-fit values of the combined n\u00d7m parameters.", "I believe the BDF optimizer and LBFGS both support batches (of problems), so you could have an outer \"problem\" axis to your data and leastsq return value. But since BDF is for stiff problems, it's likely to have much longer runtimes for some problems than for others, and you might be best served treating each problem separately (tf.map_fn) as opposed to running them all in parallel -- in a batch, you can't run ahead onto the next LBFGS iteration for problem X until you compute the BDF integration for problem Y. Or just use a python for loop over your problems, each time calling a @tf.function def lbfgs_over_bdf(data): ....", "performing many gradient-based optimizations in parallel with TensorFlow", 0.0996679961681366, 1, 67, "A great deal of flexibility and power can be achieved by writing your own optimizer which can take in a dataset and an algorithm, as well as a batch size to partition that dataset, an initial guess for the parameters, etc., and which calls `l1_ls_bfgm` internally. The optimizers from `tfp` are all built in this way, including the default `scipy_l2` optimiser. Note that the scipy optimisers can do [multiple parameter optimisation](https://scikit-learn.org/stable/modules/generated/sklearn/optimize.minima_scalar.html#sk-l01n-optimise-min) as an alternative to [this technique](http://daniel-nillson.github.io/blog/2021/06/15/parallel_optimization_of_nonlinear_models.pdf) but it is not yet available in TF2.1 or the TFP library.", 0.2343096286058426, 0.016877636313438416, 0.0920502096414566, 0.041516248136758804], ["I were using environments for months and they were working perfectly.. suddenly i can not execute any code in pycharm under any virtual environment and i get this error massage:\nfrom tensorflow.python.profiler import trace\nImportError: cannot import name 'trace' from 'tensorflow.python.profiler' (C:\\Users\\Nuha\\anaconda3\\envs\\tf_1.15\\lib\\site-packages\\tensorflow_core\\python\\profiler_init_.py)\nAny help please!!\nIt seams that it happens because i install more packages and maybe conflict occurs", "it was because environment conflict so i rebuild new environment and it works perfectly", "ImportError: the 'trace' from 'tensorflow.python.profiler'", 1.2000000476837158, 0, 68, "There is a bug in tensorflow 14.x.\nPlease check if `tensorflow-0.5.0` exists in the `site_packages` directory of anaconda. This directory contains your current `Profiling` package. If it does not, just delete it and run `pip uninstall tensorflow`.\nThis should resolve the issue.", 0.09836065769195557, 0.0, 0.06557376682758331, 0.06756129860877991], ["Is it possible to find the max and min of both the horizontal and vertical convolution axis without going through and performing the actual convolution?", "You simply cannot skip doing the convolution altogether. There's no way to just bypass it. This scenario would be similar to trying to find the height of the Eiffel Tower without out already knowing it, looking it up, or measuring it somehow. Although convolutions can be slow on many machines, you will unfortunately need to perform the operation to get the minimum and maximum values.", "Find max and min of convolution without doing convolution", 0.0, 0, 69, "No, this is not possible.\nThe convolution of two functions can be regarded as a special sort of integral, and the integral can only be evaluated numerically.  This is why the convolution is usually done with software.", 0.21359223127365112, 0.039603959769010544, 0.1553398072719574, 0.06666666269302368], ["I have a column with the following format:\nOriginal format:\n\n\n\n\nmm/dd/YYYY\n\n\n\n\n10/28/2021\n\n\n10/28/2021\n\n\n\n\nthe output after:\nprint(df['mm/dd/YYYY'])\n0        2021-10-28 00:00:00\n1        2021-10-28 00:00:00\nHowever when I am trying to convert to datetime I get the following error:\npd.to_datetime(df['mm/dd/YYYY'], format='%Y-%m-%d %H:%M:%S')\n\ntime data mm/dd/YYYY doesn't match format specified", "You are passing the wrong format. Try\npd.to_datetime(df['mm/dd/YYYY'], format='%m/%d/%Y')", "time data mm/dd/YYYY doesn't match format specified", 0.1973753273487091, 1, 70, "Use: `pd['Time'] = pd.DatetimeIndex(pd[['date']])`\nTo create a datetime index from a series of dates.", 0.17142857611179352, 0.0, 0.17142857611179352, 0.0], ["df1 =\n\n\n\n\n\nname\nage\nbranch\nsubject\ndate of joining\n\n\n\n\n1\nSteve\n27\nMechanical\nAutocad\n01-08-2021\n\n\n2\nAdam\n32\nElectrical\ncontrol sys\n14-08-2021\n\n\n3\nRaj\n24\nElectrical\ncircuit\n20-08-2021\n\n\n4\nTim\n25\nComputers\nclouding\n21-08-2021\n\n\n\n\ndf2= [['name','branch']]\nprint(df2)\n\n\n\n\n\nname\nbranch\n\n\n\n\n1\nSteve\nMechanical\n\n\n2\nAdam\nElectrical\n\n\n3\nRaj\nElectrical\n\n\n4\nTim\nComputers\n\n\n\n\nNow I have two data frames,\nI need only name and branch columns and remove the remaining columns, all these operations should apply to the original df1.  I don't want separately df2", "Simply, Overwrite the df1 only\ndf1= df1[['name','branch']]\nor\ndf2= df1[['name','branch']]\ndel df1\nTo delete df1 or df2.\ndel df1\nor\ndel df2\nBased on requirement", "how can I get only one data frame or how to overwrite the data frame?", 1.2000000476837158, 1, 71, "You can use `drop = False` for `assign()` like below\n    df.assign(new_col=df['col_name'])\nThis will work for your case.\nOr else you can remove columns in `df1` as below:\n\\begin{itemize}\n  \\item df = df[['subject','date_of_joining']];\n    2.print (df) # this will print only columns that you have mentioned in df\n   \\strong{3.df.drop(columns=['branch','name'], axis =1)};\n      \\em{This removes the rest of the columns.}\n        print( df )\n       \\begin {code}", 0.13333334028720856, 0.0, 0.07619047909975052, 0.059232182800769806], ["I need to compute a high dimension dataset, with clustering on Orange3 app. So, there's too many time spent to calculate the Distance Matrix between the objects. If I could use a graphic card for this tasks it will take much less time to complete the task. Anyone know, let's say, a workaround to do this?", "No. Orange uses numpy arrays and computes distances on CPU. Short of reimplementing the routine for calculation of distances (which in itself is rather short and simple), there's nothing you can do about it.\nOrange will start using Dask in some not too distant future, but until then try reducing your data set. You may not need all dimensions and/or objects for your clustering.", "Is there a simple way to use Oange3 with an Nvidia GPU?", 1.2000000476837158, 1, 72, "Orange has been ported to PyTorch. You may want to check this out: https://github.com/pytorch/orange\nPyTorch is a powerful machine learning framework written in Python. It can also work with NVIDIA GPUs.\nI think using PyGTA is the best way.", 0.1834862381219864, 0.018691588193178177, 0.07339449226856232, 0.05445544049143791], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "The reason is to allow the option to overwrite the dataframe object you are working on, or to leave it unchanged by creating a copy and assigning it to a different variable. The option is valuable as depending on the circumstances you may want to directly modify the original data or not.\nThe inplace parameter is one way in which you have the power to choose between the two options.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 73, "\\begin{blockquote}\nHowever, the pandas class Dataframe has to be redefined, because pandas wants to define its own methods, such as set_value\n\\end{cken} [Source](https://github.com/pandas-dev/python-pandas/blob/master/source/generated/class_methods.py)\nThis makes sense to me because, pandas does not want to have a method for renaming columns that you will never be able to overwrite with your own. It would be a bad idea for pandas to just call `renamed(dataframe, new_cols)` as this will then become your default method and there is no way for you to change it without copying that code in. So pandas has a list of all its custom methods and checks to make sure it is not being overwritten, and then it will re-instantiate your dataframe. This is also done for the other list methods that pandas defines, like `pop()` as explained in their [source](http://pandas.io/user_guide/object_oriented.html#list-methods)", 0.23175965249538422, 0.017316017299890518, 0.1287553608417511, 0.05637982115149498], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "Each class defines what changes can be done in-place and which can't, creating instead a new object.  The reasons are varied and can't be reduced to a few simple rules.\nThe underlying data structure of a list is designed for growth and shrinkage.  Even so some changes are cheaper than others.  append and pop at the end requires fewer changes of the data than addition or removal of items at the beginning or middle.  Even so, actions like blist = alist[1:] produce a new list.\ntuple is a variation on list that is immutable, and is widely used in the base Python for function arguments and packing/unpacking results.\nA numpy array has a fixed size. Like lists, individual values can be changed in-place, but growth requires making a new array (except for a limited use of resize).  numpy also has a view mechanism that makes a new array, but which shares underlying data.  This can be efficient, but has pitfalls for the unwary.\npandas is built on numpy, with indices and values stored in arrays.  As other answers show it often has a in-place option, but I suspect that doesn't actually reduce the work or run time.  We'd have to know a lot more about the change(s) and dataframe structure.\nUltimately we, SO posters, can't answer \"why\" questions authoritatively.  We can only give opinions based on knowledge and experience.  Most of us are not developers, and certainly not original developers.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 74, "You're confusing data structures with their views (the objects returned by their methods).\n\\begin{itemize}\n \\item You can use `pandas.Series.drop` to get a new view on the old Series, but that view itself is just a copy of the original. It does not affect the object itself. This is very useful in some situations; if you want to do a bunch of operations on a Series and then return the result, it's easier to drop a few columns from the view (and later restore them), and the fact that it is a view means it can change at any time without disturbing the rest of your code. (The converse is also true, of course: if there'd been no view of that Series then changing a column would have changed the Series itself.)\n  \\em{Example:}\n `s = pd['A','B','C','D'].copy()`;\n   `drop = s.copy(drop=['B', 'C'])`;     `df = drop.sort_values(by='A')`;  `print(df)`\n    {'A': [2.5, 4.3, np.nan, -28.6], 'D': np_nan_series, 'B': ['D'], 'E': 'F'}`; print(s)` `; s =  s; `\n - The `by` parameter makes it sort on 'A' in ascending order, with ties taking priority on\n 'descending' order. So for example if 'd' and 'e' have the same value of 'a', we'll\n put 'c' at the beginning, since they sort alphabetically. If we drop the columns\n we can't use to sort, then the final order will be the sort order that' not used. For\n example: we sort ['a','b','c','d'] as ['d', ..., ...], ['e',...',...] , but we have no\n way to tell if we should sort 'b' before 'g' or 'f' instead, so it will just be a random\n order for the values that aren' goint to be used as a way of sorting. But the 'sort'\n view does exactly that. Then the last step is to restore the dropped columns:\n`df2 = df.pop('B').sort(columns='D')`\nAnd we get:", 0.33856892585754395, 0.04203152284026146, 0.14659686386585236, 0.03651857376098633], ["Tensorflow is very heavy library , is there any way to save and load keras modeles(.h5) without using Tensorflow lib?", "keras framework is built upon tensorflow. If you want to use keras you will have to install tensorflow library.", "How to save keras models without tensorflow", -0.1973753273487091, -1, 75, "You can't. Keras is based on TF.\nEither use `keras.models.load_model()` or use TFB in your model.", 0.25641027092933655, 0.054054055362939835, 0.20512820780277252, 0.012195122428238392], ["i am currently using a config file to train a model in tensorflow. Inside the config file, i need to specify a python path. Since im on windows, my paths obviosly looks like these r\"C:\\path\\path\\path. But when tensorflow is using the configfile, i get this error:\nfine_tune_checkpoint: r'C:\\path\\path\\path\\ckpt-0': Expected string but found: 'r' \nAnyone has encountered a similar problem?", "Looks like it tripped because there was an r written outside of your quotation marks. I'd try to delete that and see if it works, or if the r is in your path, add it within the quotation marks.", "Python path problem: Expected string but found: 'r'", -0.3799489736557007, -2, 76, "It seems you're trying to use a windows path with quotes in a linux environment.\nTry using forward slashes instead of backslashes.", 0.190476194024086, 0.0, 0.095238097012043, 0.07339449971914291], ["I have a CSV file containing list of postcodes of different persons which involves travelling from one postcode to another for different jobs, a person could travel to 5 postcoodes a day. using numpy array, I got list of list of postcodes.  I then concatenate the list of postcode to get one big list of postcode using a = np.concatenate(b), after which I want to sort it in an alphabetical order, I used : print(np.sort(a)) is gave me error error AxisError: axis -1 is out of bounds for array of dimension 0\nI also tried using a.sort() but it is giving me TypeError: '<' not supported between instances of 'float' and 'str'\nPlease, can someone help", "Looks like you're passing in both floats and strings into your list.\nTry converting the values in b into a float before you concatenate them.", "Concatenating and sorting a list of list array", 0.0, 0, 77, "Use `sort` on `a` instead of `np`. Sort will only sort the string, the array is not required\nThe reason you are getting axis error is because of this line\n`a = list(z[i] for i in a)]`\nYou are setting a to the result of the `list` constructor on the variable `z` which is a `set`. `con` is just a number so I believe `i` was trying to go from -3, 4, -2, etc.", 0.11881188303232193, 0.0, 0.09900990128517151, 0.04272865504026413], ["I am currently on Linux x86-64 machine. I am trying to install opencv in my virtual environment using pip however the error I keep receiving is\nERROR: Could not find a version that satisfies the requirement numpy==1.19.3 (from versions: 1.19.2+computecanada, 1.21.0+computecanada, 1.21.2+computecanada)\nERROR: No matching distribution found for numpy==1.19.3\nI am running python 3.9.6 (64bit) and my current numpy version is 1.21.3. the command I've been using is pip install opencv-python. i've also tried uninstalling other instances of openCV and have tried the other options all with the same error. Does openCV not support numpy 1.21.3? Would anyone be able to point me in the right direction?", "Actually, this error happens if numpy version does not match OpenCV required version.\nfor my case:\nI used python 3.6. so I solved this error by following:\n\npip install numpy==1.19.0\npip install opencv-python==3.4.11.45\n\nafter installing numpy I search which OpenCV version support this numpy version, I found 3.4.11.45 so I install it by 2 number command and it is working.", "Installation issues using pip for OpenCv", 0.0, 0, 78, "You should install OpenCV from [CPAN](https://pypi.org/project/opencv/).", 0.07228915393352509, 0.02469135820865631, 0.07228915393352509, 0.014925369992852211], ["I built a box-embedding model on the latest wikipedia articles dump and i need to compare it with the word2vec model in gensim. I saw that if i generate the corpus data as a txt file using get_texts() method in class WikiCorpus there are a lot of stop words, so this make me think that WikiCorpus doesn't delete stop words isn't it?. Now once trained my box model on the wiki corpus txt i notice that calling the \"most similar\" function that i create appositely for box embedding prints very often stop words, instead the same word passed to the most similar function of word2vec model trained on the same corpus txt produce best results. Can someone suggest me why Word2vec model fit so well despite the corpus txt have a lot of stop words instead my box model on the same corpus not?", "How did you train a box-embedding, and why did you think it would offer good most_similar() results?\nFrom a (very) quick glance at the 'BoxE' paper by Abboud et al (2020), it appears to require training based on a knowledge base representation \u2013 not the raw text that'd come from WikiCorpus. (So I'd not necessarily expect a properly-trained BoxE embedding would have 'stop words' in it at all.)\nAnd, BoxE appears to be optimized for evaluating potential facts \u2013 not more general most_similar rankings. So I'd not expect a simple most_similar listing from it to necessarily be expressive.\nIn usual word2vec, removing stop-words isn't very important and plenty of published work doesn't bother doing so. The downsampling of highly-frequent words already tends to ignore many stop-word occurrences \u2013 and their highly diverse usage contexts mean they are likely to get weak word-vectors not especially close to other more-narrow-meaning word-vectors.\nSo in usual word2vec, stop-words  aren't very likely to be in the top-neighbors, by cosine-similarity, of other more-specific entity words.", "does WikiCorpus remove stop_words in gensim?", 0.0, 0, 79, "The `gensim` models do not remove `stopwords`, and the `WikiMatrix` class (which is what you are using to generate your word-vectors) does not perform any stopword removal either.\nIf your `box` model does find stopwords to be a problem then you might try using the model from [my open-sourced implementation of `Word2Vec`](https://github.com/mattmccutchen/python-word2vector).\n`gensims` and `wordvect` are both good software, and you will get many helpful insights from the many `Gensim`-focused Stackoverflow questions here. However, the open source code is free and I have had good success with it. You may also have good luck if you try [sklearn's implementation](http://scikit-learn.org/stable/modules/generated/sklean.word_vext.html) of the algorithm, which should also be more easily adaptable to your codebase, since it'll be already in your version of Python.", 0.24169184267520905, 0.012158054858446121, 0.11480362713336945, 0.04283801466226578]]}