{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have a dataframe where the indexes are not numbers but strings (specifically, name of countries) and they are all unique. Given the name of a country, how do I find its row number (the 'number' value of the index)?\nI tried df[df.index == 'country_name'].index but this doesn't work.", "Why you don make the index to be created with numbers instead of text? Because your df can be sorted in many ways beyond the alphabetical, and you can lose the rows count.\nWith numbered index this wouldn't be a problem.", "How do I find the row # of a string index?", 0.0, 0, 80, "You can convert your index to numeric and use `get_loc` and then take the integer. For example:\n\\begin{itemize}\n`df['country'].astype(int).get()`\nwill return an array of int values for the columns. You then simply take `df[0]` to get the first element. From there you can do whatever you want with the rest of that array. e.g. `list(df[[0]])` will return a list of all your country values. The list will contain the country names. To convert them into integers, you would need to do something like: `int(list_of_names[i])`\nHope this helps!", 0.2237762212753296, 0.028368793427944183, 0.139860138297081, 0.08602151274681091], ["I have a pandas Timestamp column that looks like this:\n2021.11.04_23.03.33\nHow do I convert this in a single liner to be able to look like this:\n2021-11-04 23:03:33", "use a regular expression by looking for the hour and minute second  pattern (\\d{4})-\\d{2}-(\\d{2})\\s+(\\d{2})_(\\d{2}).(\\d{2}).(\\d{2}) and use re.findall then get each group part then reassemble the datetime stringthen convert to a datetime", "Pandas Timestamp reformatting", 0.0, 0, 81, "You can do: `df['Timestamp'] = pd.to_datetime(df.Timestamp)`", 0.0714285746216774, 0.0, 0.0714285746216774, 0.0], ["I'm doing PID gain tuning for a DC motor\nI gathered real data from the motor which involve the position according to time.\nAnd i want to calculate the rise time, overshoot, and settling time from the data.\nIs there any function in matlab or python which can do this?\nThank you!!", "In the cases that you use the step command to extract the step-response characteristics of the system, the stepinfo command calculates the rise time, overshoot, and settling time, and so on. I don't know whether it is applicable in the data case or not but you can test it?", "Is there any function that calculates the rise time, overshoot, and settling time?", 0.0, 0, 82, "You can use the function `ode45` to numerically solve the differential equations of motion of your DC Motor. Once you have the positions and velocities, you can extract the settle time as\n$$\nT_{\\text{settle}}=\\frac{T_{c}\\left(v_{\\max}-v_{min}\\right)}{2a}\n\\quad\\text{(Rise Time)}\\quad\nv_i(t)=a\\sin(\\omega t)+\\left[v(0)+\\frac{\\omega a}{2}\\cos(\\gamma t)\\right]\\cos\\left(\\frac{(2\\pi i-1)\\omega}{N}\\frac tT\\right)\n\\\\\nt_c(T)=\\frac12 T\\ln\\Big(\n2(\\cosh(aT)+\\cos(2(i\\omega/N)\\pi)\\cos((2i-2)20\\gamma))\n-4\\cos(-2pi(\\alpha\\beta+\\omega\\kappa))\\\\ \\frac {\\cos (2((i+1/2)\\gamma -2/3)\\kappa)}{\\cos (\\omega \\kappa)}\n+2[(\\kappa+i/4)+((\\beta-\\alpha)/\\sqrt{3})(1-e^{-25\\alpha})\\frac {1-\\cos \\left((1+4i)\\frac \\omega N\\frac T{2T} \\right)} {\\sin \\pi (1+\\frac 4N \\beta)}]\n) $$\nwhere $v$, $a$, $\\omega$ are your constants. $\\gamma=0.29$ is the gyroscopic damping coefficient, $\\alpha$ and $\\beta$ from your equations, $N$ the number of steps and $T_C$ a constant. The parameter $\\kappa$ represents the ratio of dynonic energy of the first and the second order (in time derivatives). In the case of a perfect motor (no friction, no hysteresis), this parameter is usually assumed to be zero.", 0.18382352590560913, 0.04444444552063942, 0.11764705926179886, 0.06302520632743835], ["On google colab I installed conda and then cudf through conda. However now i need to reinstall all packages like sklearn etc which I am using in my code. Is there some way to install cudf without conda ? pip no more works with cudf. Also if there is some other similar gpu dataframe which can be installed using pip, it will be of great help.", "No, cudf is not available as a pip package.\nYou don't say why you need cudf, but I would try pandas, dask or vaex, surely one of those will do what you need.", "Install cudf without conda", 0.1973753273487091, 1, 83, "The package can't be install without `conda`, but you can install `cudd` and `CUDF`.\n<a href=\"https://i.stack.imgur.com/U65Ui5.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nFor more details please refer to the following links. https://anaconda.org/conda-forge/cuda-dataframe\nhttps:/https:\n.github.io/doc/CUDACUDA-docs/master/09_Recommended_Cuda_Dataframe_Libraries.html\nThanks!", 0.1489361673593521, 0.0, 0.10638298094272614, 0.035714291036129], ["While writing in CSV file, automatically folder is created and then csv file with cryptic name is created, how to create this CSV with any specific name but without creating folder in pyspark not in pandas.", "That's just the way Spark works with the parallelizing mechanism. Spark application meant to have one or more workers to read your data and to write into a location. When you write a CSV file, having a directory with multiple files is the way multiple workers can write at the same time.\nIf you're using HDFS, you can consider writing another bash script to move or reorganize files the way you want\nIf you're using Databricks, you can use dbutils.ls to interact with DBFS files in the same way.", "How to write in CSV file without creating folder in pyspark?", 0.0, 0, 84, "This is the code for writing to a csv. If you want to do it without a folder creation, you could skip the part of \"path\" of creating csv, it will not be creating a file if you don't mention a path to it. I don;t know how you are reading and writing the files, but in this way you can skip creating folders as well. Hope it helps.", 0.31707316637039185, 0.08641975373029709, 0.18292683362960815, 0.03701552003622055], ["For an assignment I have to write some data to a .csv file. I have an implementation that works using Python's csv module, but apparently I am not supposed to use any imported libraries...\nSo, my question is how I could go about doing so? I am no expert when it comes to these things, so I am finding it difficult to find a solution online; everywhere I look import csv is being used.", "I guess that the point of your assignment is not to have some else to do it for you online. So a few hints:\n\norganise your data per row.\niterates through the rows\nlook at concatenating strings\ndo all above while iterating to a text file per line", "How to write to a .csv file without \"import csv\"", 0.06656807661056519, 1, 85, "The .CSV is just a text file containing values separated by commas. You can write such a file using only built-in functions. In your case, you could open a binary file for writing, put some characters in it and close the file to finish. This is the simplest way to do it. If you want to add a header with the columns names, then you'll have add some text and end the row by adding \"\\n\".\nIf you prefer an easier way and you know the number of columns you have, consider using [csv](https://docs.python.org/2/library/csv.html) module to perform the operation for you. Then, read the csv file with csv.reader() method and do whatever you need.", 0.23121386766433716, 0.05847953259944916, 0.1387283205986023, 0.035175878554582596], ["I have a time series in which i am trying to detect anomalies. The thing is that with those anomalies i want to have a range for which the data points should lie to avoid being the anomaly point. I am using the ML .Net algorithm to detect anomalies and I have done that part but how to get range?\nIf by some way I can get the range for the points in time series I can plot them and show that the points outside this range are anomalies.\nI have tried to calculate the range using prediction interval calculation but that doesn't work for all the data points in the time series.\nLike, assume I have 100 points, I take 100/4, i.e 25 as the sliding window to calculate the prediction interval for the next point, i.e 26th point but the problem then arises is that how to calculate the prediction interval for the first 25 points?", "A method operating on a fixed-length sliding window generally needs that entire window to be filled, in order to make an output. In that case you must pad the input sequence in the beginning if you want to get predictions (and thus anomaly scores) for the first datapoints. It can be hard to make that padded data realistic, however, which can lead to poor predictions.\nA nifty technique is to compute anomaly scores with two different models, one  going in the forward direction, the other in the reverse direction, to get scores everywhere. However now you must decide how to handle the ares where you have two sets of predictions - to use min/max/average anomaly score.\nThere are some models that can operate well on variable-length inputs, like sequence to sequence models made with Recurrent Neural Networks.", "Interval Prediction for a Time Series | Anomaly in Time Series", 1.2000000476837158, 0, 86, "So I don' think you want an actual prediction, just a \"best guess\" of how far out you might go if you were going to move on the timeline. So, in a sense you are looking for some sort of time-weighted window. A simple method is just to add a constant time weight (e.g. 5) to the window width (default of 4).\nThis works because 95% of the values are within 8 of their nearest neighbor and adding 0.05 to a window makes the error go down, but only to 75%, which is still pretty good.", 0.2761506140232086, 0.016877636313438416, 0.1171548143029213, 0.05287155508995056], ["I have a list of different expressions. It looks like this:\nmy_list = [[1 ,2 ,'M' ,2], [1 ,2 ,'A' , 1], [1 ,2 ,'g' ,3], [1 ,2 ,'o' ,4]]\nI want to sort the list. The key should always be the first entry in the list, in this case the book positions A, M, g, o. However, upper and lower case should be ignored.\nIn python I used:\nmy_list.sort(key = itemgetter (3))\nOutput is:\n[[1, 2, 'A', 1], [1, 2, 'M', 2], [1, 2, 'g', 3], [1, 2, 'o', 4]]\nThe problem is that in my result the uppercase letters are sorted first and then the lowercase letters. How can I make lower and upper case letters sort together? The result should look like this:\n[[1 ,2 ,'A' ,1], [1 ,2 ,'g' ,3], [1 ,2 ,'M' ,2], [1 ,2 ,'o' ,4]]", "Use key=lambda lst: lst[2].lower().", "python sort multi-dimensional lists CASE INSENSITIVE", 1.2000000476837158, 1, 87, "You have 8 different characters in your key. You could create a dict of `ord(ch)` -> `i`\nto have the same key for the case-insensitive sort\nand then do the sort on the resulting dict. E.g. for your list: `mylist=[{k:i for i,k in sorted(mydict.items(), key=lambda t: t[0])} for k in list(map(len,myList))]`.\nIf you're interested I can provide an example on how to do this (including the dict).\nEdit: here's the code, let me know if it works :)\n`myDict = dict([(ord('A'), 0), (ord(\"a\"), 65),( ord('B'),1), ...])\nsorted_myArr = sorted([{i:j}for i in l for j in range(10)])`", 0.03252032399177551, 0.01652892492711544, 0.03252032399177551, 0.0], ["I'm building a model to identify a subset of features to classify an object belong which group. In detail, I have a dataset of 11 objects in which 5 belong to group A and 6 belong to group B, each object has been characterized with a mutation status of 19,000 genes and the values are binary, mutation or no-mutation. My aim is to identify a group of genes among those 19,000 genes so I can predict the object belongs to group A or B. For example, if the object has gene A, B, C mutation and D, E gene with no mutation, it belongs to group A, if not it belongs to group B.\nSince I have a large number of features (19,000), I will need to perform feature selection. I'm thinking maybe I can remove features with low variance first as a primary step and then apply the recursive feature elimination with cross-validation to select optimal features. And also don't know yet which model I should use to do the classification, SVM or random forest.\nCan you give me some advice? Thank you so much.", "Obviously in a first step you can delete all features with zero variance. Also, with 11 observations against the remaining features you will not be able to \"find the truth\" but maybe \"find some good candidates\". Whether you'll want to set a lower limit of the variance above zero depends on whether you have additional information or theory. If not, why not leave feature selection in the hands of the algorithm?", "Is it reasonable to use 2 feature selection steps?", 0.0, 0, 88, "A very common practice for feature subset selection is using `RFE` on the training data followed by a test data.  Then you can select the top features by how well they are selected in the test dataset and use the selected features on test and training sets.", 0.23529411852359772, 0.03418803587555885, 0.13445378839969635, 0.037734150886535645], ["I deployed Apache Spark 3.2.0 using this script run from a distribution folder for Python:\n./bin/docker-image-tool.sh -r <repo> -t my-tag -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build\nI can create a container under K8s using Spark-Submit just fine.  My goal is to run spark-submit configured for client mode vs. local mode and expect additional containers will be created for the executors.\nDoes the image I created allow for this, or do I need to create a second image (without the -p option) using the docker-image tool and configure within a different container ?", "It turns out that only one image is needed if you're running PySpark.  Using Client-mode, the code spawns the executors and workers for you and they run once you create a spark-submit command.  Big improvement from Spark version 2.4!", "Two separate images to run spark in client-mode using Kubernetes, Python with Apache-Spark 3.2.0?", 0.0, 0, 89, "You need a separate image for executers. Spark's Docker image already has executer containers in the `/spark-executor` directory. You could do\n\\begin{itemize}\n \\item Create another image with the Spark executor containers\n  and the Python image, use that to start a Spark cluster\n under client runtime\n <a href=\"https://i.stack.imgur.com/YjzBW.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n (https:\\begin(a)stackoverflow(.)com\\questions\\(15658125#117260429))\n or\n create another container and run Spark in that. Either way\n you'll need an extra container. So Spark Spark can have an image\n with multiple containers. (See https://spark.apache.org/docs/latest/job-management-etl.html#containers )\n If your code is complex, then you can make your Spark\n code a multi-stage Dockerfile: https:()stackoverflow.()com.//a\n24537398-spark3-multi-container-pipeline-in-a-dockerfile). This is\n also the way you create custom Docker images with additional\n containers (including executeres) for Spark applications. There\n is no point in creating a new Spark image each time if Spark has\n a builtin image available, and it'd be too much trouble to manually\n set up the environment etc. if you use a Docker multi container\n pipeline.", 0.16736401617527008, 0.025316456332802773, 0.0920502096414566, 0.060913700610399246], ["I have an array\narr = np.array([[1,1,2], [1,2,3]]).\nI want to get amount of unique element for each row and count mean\nI can do this np.array([len(np.unique(row)) for row in arr]).mean().\nBut seems, that it's a slow way. Is there another faster approach?", "set(arr.flatten()) will create your desired result. Not sure about how fast it is though.\nOutput:\n{1, 2, 3}\nEdit:\nYou wanted the number of unique elements, so you wrap the whole thing in len()", "Get amount of unique elements in numpy", 0.0, 0, 90, "\\begin{code}\nIn [4]: arr = numpy.random.randint(256, size=(300, 2))\n   ....: arr\nOut[4]=\narray(['47', '48', 170], dtype='int8')\n\\end{pre}", 0.10169491171836853, 0.0, 0.06779661029577255, 0.050420165061950684], ["For example, how many Mb will be required for EfficientNetB3? On drive model weights require 187 Mb of memory, does it mean that when the model will be loaded on GPU, it will use 187 Mb of GPU memory?", "It's difficult to calculate total, but you can estimate a minimum to just load a model, which would be roughly the model size. Tensorflow, for example, defaults to reserving 100% of the GPU memory. You can set limits, but the amount of memory to be used is based on many things, such as number of layers, input image size, batch size, etc.", "How to evaluate the required GPU memory for running neural network models?", 0.0, 0, 91, "The EffiNet model is composed of 407.9M parameters (750.1M if using BottleNet).\nA GPU is a lot more memory intensive than CPU, so I'd assume 200% of the parameter size for each model.\nSo you'll need a minimum of (455.7MB * 8GB RAM = 3.4GB of RAM).", 0.28070175647735596, 0.0357142873108387, 0.14035087823867798, 0.02462933026254177], ["How to get pandas dataframe when select only one column? In R there is drop = False for that.\nWe can use pd.DataFrame(df['breakfast']) or df[['breakfast']], but do we have smth like drop = False as it in R?\nPS: press F for breakfast)", "I think you are looking for something like index=df.index.\nThe question is a bit broad", "How to get pandas dataframe when select only one column? In R there is drop = False for that", 0.0, 0, 92, "You could try:\n\\begin{itemize}\n    \\item `df.loc[:, 'breakfatst']` to select a single column\nor\n   \\begin {code} <df.<column_name>\n 2.  df.<['<column name'>]\n        \\end{code}.", 0.13953489065170288, 0.0, 0.13953489065170288, 0.027775051072239876], ["I'm new to OpenCV and trying to use SIFT and SURF for a project.\nOn my laptop I have OpenCV version= 4.5.1.48 and also added OpenCV-contrib-python of version 4.5.1.48\ncurrently the problem I'm facing is the error I'm getting after following the documentation SIFT works perfectly after following documentation but SURF isn't working and giving me error for following codes\ncode 1\nsurf = cv.xfeatures2d.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'xfeatures2d'\ncode 2\nsurf = cv2.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'SURF_create'\nAfter reading many answers on Stack overflow I changed version of OpenCV and did many things but nothing is working for me\nI'm new to this please someone guide me through this\nI read about the patent expiring too but nothing is working in my case pls tell me if im wrong somewhere\nThanks in advance", "For patent reasons, opencv 4.5.1.48 does not include the whole algorithm\nYou can use Python3.6 (or Python3.7 maybe OK) and install opencv-pyhton==3.4.2.16 and opencv-contrib-python==3.4.2.16, then you can use the function that:\nsurf = cv2.xfeatures2d.SURF_create()\nor\nsift = cv2.xfeatures2d.SIFT_create()", "AttributeError: module 'cv2.cv2' has no attribute 'SURF_create' , 2. module 'cv2.cv2' has no attribute 'xfeatures2d'", -0.0996679961681366, -1, 93, "In \\strong{Python}, you have to first import `cv` and then `opencv`.\n\\strong{`import cv`}\nSo, you should first do like this. Because you are using `xfeat2`, `Suff` or `surfaces` modules which are part of `OpenCV`. So, in your code you need to import Opencv first and that will resolve your problem. Below, I am providing the steps for importing Open cv  in python. Follow this:\n<a href=\"https://i.stack.imgur.com/4RZYt.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nNow, go to your project folder and add the `Python` folder into it. So that it will look like a single project and will avoid any conflict. Then, open the file you created. That will open your Python file in a new window. Now go back to the Open CV. And then install it by running the command as shown below: (Note: You need Python 3.7 for installing OpenCv)\n`pip install opencv-Python --user`\nOnce the installation is done, restart your IDE(Which could be PyCharm). Then go for your python file and open it again. You will see a `import `block now. Add the following statement at the top of the code. I hope it helps. Good luck. :)\nP.s. If you want to learn more, visit https://www.opencvasound.org/ and search for any tutorial which suits you.", 0.12186379730701447, 0.014440433122217655, 0.0788530483841896, 0.03740577772259712], ["w2v = gensim.models.Word2Vec.load(\"w2v.pkl\")\nI am using this method to load pickle file through gensim but encountering an error.\nAttributeError: 'dict' object has no attribute '_load_specials'", "If you saved the model using pickle, you should be using some form of unpickle to re-load it. (Gensim has a utility method for unpickling a file containing a single pickled object at [gensim.utils.unpickle][1].)\nGensim's per-instance .save() and per-class .load() methods are a custom save/load protocol that internally makes use of pickle but does other things (& perhaps spreads the model over multiple files) as well. You should only Word2Vec.load(\u2026) a filename that was previously created by code like w2v_model.save(\u2026).", "Load pickle file in gensim", 0.1973753273487091, 1, 94, "It should be\n\\strong{wvv=gensim[\"wnetvw.w141\"]}", 0.08163265138864517, 0.02083333395421505, 0.06122449040412903, 0.022727271541953087], ["I am working in Google Cloud Platform and I am trying to use Pyspark to convert a csv file into an avro file. I have seen a lot of websites but I haven't been able to implment the solution. Thank you in advance. :)", "You can read the csv file into a dataset/dataframe using spark and use databricks library to write it as avro. Something like:\ndataset.write.format(\"com.databricks.spark.avro\").save(\"your output path\")", "How to convert a csv file to an avro file using PySpark?", 0.0, 0, 95, "You can use [csv-avro](https://pypi.org/project/csv2avrow/) library.\nThis will convert your csv to avrow and write into gs:\n[This](http://towardsdatascience.com/quickly-convert-csvfiles-into-google-cloud-platform-objects-using-spark-18e30c8700b3) is an example for the same\nI hope this helps.", 0.37037035822868347, 0.050632912665605545, 0.1975308656692505, 0.05945945531129837], ["I'm trying to extract project relevant information via web scraping using Python+ Spacy and then building a table of projects with few attributes , example phrases that are of interest for me are:\n\nThe last is the 300-MW Hardin Solar III Energy Center in Roundhead, Marion, and McDonald townships in Hardin County.\nIn July, OPSB approved the 577-MW Fox Squirrel Solar Farm in Madison County.\nSan Diego agency seeking developers for pumped storage energy project.\nThe $52.5m royalty revenue-based royalty investment includes the 151MW Old Settler wind farm\n\nHere I have highlighted different types of information that I'm interested in , I need to end up with a table with following columns :\n{project name} , {Location} ,{company}, {Capacity} , {start date} , {end Date} , {$investment} , {fuelType}\nI'm using Spacy , but looking at the dependency tree I couldn't find any common rule , so if I use matchers I will end up with 10's of them , and they will not capture every possible information in text, is there a systematic approach that can help me achieve even a part of this task (EX: Extract capacity and assign it to the proper project name)", "You should be able to handle this with spaCy. You'll want a different strategy depending on what label you're using.\n\nLocation, dates, dollars: You should be able to use the default NER pipeline to get these.\nCapacity, fuel type: You can write a simple Matcher (not DependencyMatcher) for these.\nCompany: You can use the default NER or train a custom one for this.\nProject Name: I don't understand this from your examples. \"pumped storage energy project\" could be found using a Matcher or DependencyMatcher, I guess, but is hard. What are other project name examples?\n\nA bigger problem you have is that it sounds like you want a nice neat table, but there's no guarantee your information is structured like that. What if an article mentions that a company is building two plants in the same sentence? How do you deal with multiple values? That's not a problem a library can solve for you - you have to look at your data and decide whether that doesn't happen, so you can ignore it, or what you'll do when it does happen.", "Information extraction with Spacy with context awareness", 0.0, 0, 96, "For a project with no predefined structure like this, it is quite difficult to write a parser. I would recommend that you have a look at some example parsers to understand the general approach. In Python, you can look up spaCy' article about \"How to create a model\" for a detailed example on how to parse a sentence like that. However, I believe the [Python documentation on Natural Language](https://www.tutorialspoint.com/python/natural_language_processing.htm) can also be a good source for the basic ideas.", 0.3272727131843567, 0.03663003817200661, 0.11636363714933395, 0.053240738809108734], ["Suppose you have a pandas.DataFrame like so:\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID1\n14.5\n0\n0.32\n...\n\n\nID2\n322.12\n1\n0.94\n...\n\n\nID3\n27.08\n0\n1.47\n...\n\n\n\n\nMy question is simple: how would one select rows from this dataframe based on the maximum combined values from two or more columns. For example:\n\nI want to select rows where the columns Feat1and Feat3 have their maximum value together, returning:\n\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID2\n322.12\n1\n0.94\n...\n\n\n\n\nI am certain a good old for loop can take care of the problem given a little time, but I believe there must be a Pandas function for that, hope someone point me in the right direction.", "You can play arround with:\ndf.sum(axis=1)\ndf['row_sum'] = df.sum(axis=1)\nor\ndf['sum'] = df['col1' ] + df['col3']\nAnd then:\ndf.sort(['sum' ],ascending=[False or True])\ndf.sort_index()", "Select Pandas dataframe row where two or more columns have their maximum value together", 0.0, 0, 97, "You can use `np.where` in `apply`\n: https://pandas.pydata.org/pandas-docs/stable/reference/api/numpy.select.html\nThe below example would pick any row that has the max Featurename1, and FeatureName2 in a single row: (Note this is case sensitive for string, so \"Name\" will not match)\nNote that I have converted your original Dataframe to a MultiIndex for readability. The original will be different for your dataframe if you are using strings. It is also possible that the code above will pick the wrong row depending on what you require. If that is the case, please comment and I will edit as needed.", 0.05673758685588837, 0.014388489536941051, 0.05673758685588837, 0.049382712692022324], ["Given a spatial related search phrase, such as \"Find cafes near the train station\", what would be the approach to handling this with NLP / semantic searching?\nIn this case, would all the 'cafes' need to have a qualitative token with regard to their distance to the train station (e.g. near / far)? Curious to know what the thought process would be for handling these kind of tasks.\nThanks!", "The way I would approach this is to look at the prepositions in the first place, in this case near means close by. You then identify the reference point (train station). Now you find cafes which are close to that, ie you should have a list of cafes with their coordinates, and you compare those against the coordinates of the train station, returning the ones that are closest.\nOther prepositions (opposite) or other descriptions (in the same street as) would need corresponding other metrics to evaluate whether they fit.\nThis is not a semantic search problem, as there is nothing inherent in language that describes whether something is close or far from another thing -- you need to map this onto the 'world', and make a decision from non-linguistic data.", "How would NLP handle semantic searches for spatial information?", 0.0, 0, 98, "Well, first of all you need a ontology. For this type of applications I'd use some geo ontologies (GeoSPARQL, for example). So the first step is the define an ontological classifier to relate geographic terms (like \"cafe\") with spatial terms. It's a good idea to make a general model and use a class name that encompass all cases and then make specific instances for specific cases (you might even do both).\nThen, you define a query using some spatial operator (spatial path) to say that you want to search for geospatial term near something else. The next step would use the ontologist to give the semantic information to these spatial expressions. So you can write a path like\n`SELECT * FROM Cafe WHERE CaFeNearTrainStation`\nThis would give back the list of caf\u00e9 found in the neighborhood of the specific geocoordinates of your train stations. If you use something like GeoKettle, the query would look like this\n\\begin{code}\nSELECT cafe FROM GeospasTile(18.91017899813143, -86.1949767592829, 32, \"http://www.openstreetmap.org/relation/125805\") WHERE caFe_name LIKE \"%%train%%station\"\ncaFe\n--------------------------------\nWaffle House\nCaffeine Ca\nHungry Bear Ca$\nStarbucks\nTurkish Delight\nGroupon\nBakery CaSe\nChase Bank\nCafe 54\nAirwick CaFe\nMocha Joe'z CaPhri\nLaFeria Caf\nGreek CaRn\nSun Coffee CaDh\nZombie Freeze\nDunkin Donuts\n...\n1\n8\n3\n216\n5\n0\n 9\n707\n6652\n    ...\nEND OF PATH\n================\n[2022-06-24 03:09:56,115] INFO {http} - Received request for /_/services/rest/doc/gk_geo.wadl\n423736279500ms", 0.27659574151039124, 0.03743315488100052, 0.10638298094272614, 0.039745576679706573], ["I know when you import everything you can do thinks like nltk.bigrams(nltk.corpus.brown.words() for bigrams and nltk.trigrams(nltk.corpus.brown.words() for triagrams, but how do you do four grams?\nI've seen other ways to do it, but they all do it with a \"string\" or a text they make up. How do you do it with the nltk.corpus.brown? Do you have to covert it to a string and if so, how?", "To get n number of items you can use nltk.ngrams() with the number to get as the second argument.\nIn your example, to get four-grams, you can use nltk.ngrams(nltk.corpus.brown.words(), 4)", "Finding Four Grams in an NLTK Corpus", 1.2000000476837158, 0, 99, "Use [ngrams](http://naturalsciences.github.io/nLTTK/doc/api/2.0/index.html#n-gram):\n`bigrps = ngram(\"n\", nlstkcorp.Brown)`", 0.14035087823867798, 0.0, 0.10526315867900848, 0.025641027837991714], ["I want to understand a few things about partioning a parquet on Dask.\nWhen I do it in a .csv file, the chunksize works as intended, doing 30 partitions based on 50 mb chunks.\nWhen I try to do it the same logic through the read_parquet, none partition is created, and when I force this with repartition(partition_size='50mb'), it create 109 partitions.\nCan someone explain to me why parquet doesn't seems to work at the same way like .csv when doing chunksizes?", "In CSV, the fundamental, non-splittable chunk of data is one row, usually the bytes between one \\n character and the subsequent one. This bytes chunk size is typically small. When you load data with dask, it reads from a given offset to the next \\n to be able to read an exact number of rows. You would find, if you made the chunk size too small, that some partitions would contain no data.\nParquet is not structured like this. Its fundamental non-splittable chunk is the \"row-group\", and there is often just one row group per data file. This is done for efficiency: encoding and compressing a whole row group's worth of data in one block will give maximum read throughput. Furthermore, because of the encoding and compression, it's much harder for dask to guess how big a piece of a dataset will be as an in-memory pandas dataframe, but it can be many times bigger.\nA row group could easily be >>100MB in size. In fact, this is generally recommended, as smaller pieces will have a higher fraction of their processing time in overhead and latency.\nTo summarize\n\ndask will not split a parquet dataset beyond the partitioning within the data files\nthat partition size might be many times larger in memory than on disk, so repartitioning after load may result in many partitions\nthese are tradeoffs required to make parquet as fast and space-efficient as it is", "Repartioning parquet file dask", 0.3799489736557007, 2, 100, "The issue is in how the chunk size is calculated when creating partitions using dlmread. Partition sizes are calculated using the file size - the number of files that can be created is then adjusted to account for potential compression and data skew. The same is true of dsk files. This makes it possible for the size of the partitions to be inaccurate in either direction. So when you have files of differing size, you can end up with inaccuracies when reading. Using dlpyread, that isn'tt an issue, as the correct partition size has already been calculated.", 0.30994153022766113, 0.05882352963089943, 0.1520467847585678, 0.058315325528383255], ["Given an integer n <= 10^18 which is the product of Fibonacci numbers, I need to factor it into said Fibonacci numbers.\nEach factorization has a score, which is one less than the count of factors plus the sum of the indices of the factors in the Fibonacci sequence that begins with f(1) = 1, f(2) = 2.\nIf multiple such factorizations are possible, I need the factorization that minimizes the score.\nExample:\n104 = 13 * 8 or 104 = 13 * 2 * 2 * 2\nf(6) = 13, f(5) = 8, f(2) = 2\nFor 104 = 13*8 = f(6)*f(5), we have a count of 2, indices of 6 & 5, giving us 2 + 6 + 5 - 1 = 12.\nFor 104 = 13 * 2 * 2 * 2 = f(6) * f(2) * f(2) * f(2), we have a count of 4 and indices of 6, 2, 2, 2, giving us 4 + 6 + 2 + 2 + 2 - 1 = 15.\nWe should pick 13 * 8 since it has the lower score.\nThe biggest problem I've come across is when we have a number like 1008, which is divisible by 144 and 21, but needs to be divided by 21 because 1008 % 7 == 0. Because my program is first dividing by the biggest numbers, number 144 is 'stealing' 3 from number 21 so my program doesn't find a solution.", "Carmichael's theorem proves that each Fibonacci number after 144 has at least one prime divisor that doesn't divide any earlier Fibonacci number.\nThere aren't many Fibonacci numbers under 10^18; fewer than 90.\nMake an array of all the Fibonacci numbers <= 10^18.\nGiven an input n which is the product of Fibonacci numbers, its factorization into Fibonacci numbers must include every Fibonacci number above 144 that divides it, repeated as many times as it divides it.\nGo through your Fibonacci numbers in descending order and keep dividing n by any such number that divides it, until you get to 144.\nNow we need to be careful because two Fibonacci numbers don't have any prime factors not seen in previous Fibonacci numbers. These are 8 and 144. Since 8 is 2^3 and 2 is a Fibonacci number, you can't render your number unfactorable into Fibonacci numbers by taking the 8. Under your optimization, you will always choose the 8.\nThen 144 is the only factor that you might need to reject for a smaller factor. This can only happen if 34 or 21 are factors, and the 144 eliminates a needed 2 or 3.\n34 = 2 * 17, 21 = 3 * 7\nThat was long-winded, but it gets us to a simple approach.\nGo through the Fibonacci numbers <= n in descending order until you get to 144, then skip to 34, then 21, then back to 144 and descending down to 2.\nThis will give you the optimal factorization under your weird scoring scheme.\n----- this order -----\n[679891637638612258, 420196140727489673, 259695496911122585, 160500643816367088, 99194853094755497, 61305790721611591, 37889062373143906, 23416728348467685, 14472334024676221, 8944394323791464, 5527939700884757, 3416454622906707, 2111485077978050, 1304969544928657, 806515533049393, 498454011879264, 308061521170129, 190392490709135, 117669030460994, 72723460248141, 44945570212853, 27777890035288, 17167680177565, 10610209857723, 6557470319842, 4052739537881, 2504730781961, 1548008755920, 956722026041, 591286729879, 365435296162, 225851433717, 139583862445, 86267571272, 53316291173, 32951280099, 20365011074, 12586269025, 7778742049, 4807526976, 2971215073, 1836311903, 1134903170, 701408733, 433494437, 267914296, 165580141, 102334155, 63245986, 39088169, 24157817, 14930352, 9227465, 5702887, 3524578, 2178309, 1346269, 832040, 514229, 317811, 196418, 121393, 75025, 46368, 28657, 17711, 10946, 6765, 4181, 2584, 1597, 987, 610, 377, 233, 34, 21, 144, 89, 55, 13, 8, 5, 3, 2]", "The smallest sum of divisors", 0.0, 0, 101, "Your approach is a good one: for `n` that is at least the number of positive integers up to `x`, it is sufficient to use `F(x+1)` instead of `f(...)` and to pick the solution where the largest factor (the biggest number `m` for which `2m * m = n`) is as small as possible. That means that the function that you have to minimize is `T(n) := T(f(-n)) + f(-1)`. It is now clear that we need a \\em{proper scoring function} for the pairs \\begin{code}\n(-m, m)\n\\end{Code} to get the optimum. You are given the numbers `1/2`, `3/5`,... and we want to select the \\strong{best} factor. We know that this is unique up `+n`, so that it equals to the unique \\emph{minimal} solution, `min(T,n)`.\nIn particular, we can assume that `0` is not a factor, so we know all pairs for integers. For `k := \u00b11`, we will have `P(k) <= n/k^2` if `|k| = O(sqrt(log n))` because the `i`th and `(-i)`th solutions are never equal. With `A(0) >= \u00bd`, the optimal `B(m) \u2265 \u03c0 - A(\u00bd) > \u221e` and then the minimal value is in `[A, A+B]`.\nWith `S := A[1] + B[0]` we get `V[n] = -A[S]` and with `M := -1 / (A + sqrt(S))` and a bit of simplification, the program\nlooks like\n<a href=\"https://i.stack.imgur.com/q0Gvx.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nwith the following values: \\sout{`A := (517255360*pi/86695637061922983811);`\n`B := sqr(A);` `C := S + C * sqn(F); `\n  `D := F * C - S;`\n `N := n / C;` `Z := N * D;``.", 0.2829268276691437, 0.022838499397039413, 0.1170731708407402, 0.031027767807245255], ["The dataset is large with over 15000 rows.\nOne row of x,y,z plots a point on a 3D plot.\nI need to scale the data and so far I'm using RobustScaler(), but I want to make sure that the dataset is either normally distributed or it isn't.", "Matplotlib histogram [plt.hist()] can be used for checking data distribution. If the highest peak middle of the graph, then datasets are normally distributed.", "I have a 3D dataset of coordinates x,y,z. How do I check if the dataset is normally distributed?", 0.0, 0, 102, "You can check the distribution of a series by looking at the p-value of the normal probability plot (NPP)\nThe normality hypothesis can then be tested by either doing a t-test or a Mann-Whitney U-Test. The t statistic will have the shape of normal distribution and the Mann Whitney has the size of Chi-square. Since your data are not ordinal or discrete, you can use the t distribution. So:\n\\begin{itemize}\n1. Compute the NPP of your original series. [See: http://www.mathworks.com/help/stats/npp.html].\n2. Reject the null hypothesis of equal variance and equal distribution if P-Value is less than 0.05. (0 if you are 95% confident in the difference) This would happen if your points are clearly outside the box plot region (the boxplot is the same as a NPp, but for a set of boxplots, i.e., an interval).\n3. Perform the following hypothesis tests: ttest for equality of two means, or mannwhit for unequal variances. If one of these tests are significant, reject the hypothesis. This is assuming you want a single t or MannWhirney distribution hypothesis test. Other tests can be done. I used to perform a Cramer V test on the variance to see if it is different than one in my application. In my opinion, this test is more conservative than the manwhireny test in that it detects a significant difference in variance but does not tell you if there is an obvious difference, for instance if one data set was a sample from a normal while the other was from Poisson (as I know of, the Cv test would reject a hypothesis that there are equal varainces, where a mannywhiney U test may not reject). In either case you need a decision rule. One I found was to reject if Mannwhirty two tailed is larger than or equal to ttable, (I am not sure what is currently recommended, maybe 4.65) I would be happy to add a new answer if I find a reference. Regardless, if both tests reject you may want further analysis to determine which is greater, and you would need separate p values for the two different hypotheses, one that compares means of each variable and another for each pair. You may also want", 0.0585365854203701, 0.014705882407724857, 0.043902438133955, 0.04408631473779678], ["I have used the .map_partitions with delayed functions and the result I got is a dataframe with delayed results in each row.\nIs there any way to unpack those delayed objects?\nAm I doing something wrong?\nThank you.", "A very short answer: you should use map_partitions with a normal function, not a delayed one. The dataframe interface already provides laziness and parallelism, so you don't need to add another nested level.", "How to unpack a dataframe of delayed dask objects?", 1.2000000476837158, 5, 103, "In this case you could use [get_delayed](https://docs.dask.org/en/latest/delay.html#d.delay/get) to return the delayed result:\n\\begin{code}\n>>> def delayed_add(x):\n...     return x + 1\n    # ...\n     # Do more stuff\n             .sum(delay=delay_scheduler)\n           .compute()\n            .get()    <-- here\n# result\n       0                        0\n       ...              ...        ...    ...  ...       ....        ..        ....\nName: a, Length: 3, dtype: int32\n[[1, 2, ..., ]\n [24,     ...      ...   , ...]\n [-28,    .....    .........    ]]>\n20 rows \u00d7 5 columns\n\"\"\"\n   a  b   c   d   e\n0  1   0   1  25  34\n1  NaN  0.1 NaN NaN\n..  .. ... ... ..\n97 NaN     NaN   48\n.  ....     ..    87\n498 NaN      NaN  -14412.27562177185\ndtype: object\n> delayed = d.a.map(lambda x: delayed(del_func)(x))\n[<dobj _._darray_1.0>, <dummy _.Delayed[function(int) -> int] ...>, ...>]<-- delayed\n<xarray.DataArray (a: 'datetime64[ns]')> ... >\narray([datetime.datetime(2...\nType:           <class 'dapi.array'>\nLengths:       3\nShape:         (1) <_dshape_639> (...)\nLabels\nDimensions\nAttributes\nCoordinates\nData Array\nIndex Labels (None) ... , ,\nValue Label ...    57826760010223686\nTime Label (datetime)   ...          2e+15 ...        296933135119\nXArray.ArrayBase\n_dataarray\n__array__\nshape\nndim\nnbytes\nsize\nitemsize <...>", 0.11578947305679321, 0.0, 0.05263157933950424, 0.04568614438176155], ["So, I want to implement a class that holds nested data. I would like to implement __getitem__ in a way so that obj[x][y] can be abbreviated by obj[x, y].\nHowever, I noticed a problem: The signature of __getitem__ is that it expects a single positional argument instead of *args. If multiple arguments are given, they are automatically put into a tuple.\nI.e. obj[a, b] and obj[(a, b)] both appear to be equivalent to obj.__getitem__((a,b))\nBut then how can I distinguish the two cases\n\nThe outer layer is indexed by tuples and obj[(a, b)] should return the value at that index\nThe outer layer is not indexed by tuples and obj[a, b] should return obj[a][b]\n\nThe only possible solutions I am aware of currently are\n\nAbandon the idea of coercing obj[x, y] into obj[x][y]\nIf we only want obj[x] always write obj[x,] instead.\n\nBoth are not really satisfactory.", "Short of trying to inspect the calling source code (which is extremely fragile, with all sorts of failure cases, nowhere near worth the instability and debugging headache), this is not possible.\nobj[a, b] and obj[(a, b)] mean exactly the same thing in Python. There is no semantic difference, no difference in how they are executed, and nothing to hook into to distinguish them. It'd be like trying to distinguish the whitespace in obj[a,b] and obj[a, b].", "Any way to distinguish `obj[x, y]` from `obj[(x, y)]`?", 1.2000000476837158, 2, 104, "Not possible: `__getattr__` cannot be used to make this work, because `__has_attribute__`, which is used during object construction, checks the full signature, not just the base type and the first argument. It would be possible to hack around that, but then it wouldn't be Python any more and it would take a lot of work to get it to work correctly in many different contexts. The only real option is to choose one of the alternatives in the question.", 0.29447853565216064, 0.02484472095966339, 0.17177914083003998, 0.050394997000694275], ["I am trying to remove the empty cells from my column A of my data which had text data in it.\nMy csv which I imported into data frame has 50k rows containing search data in column A.\nI tried the below options.\ndf= df.replace(r'^s*$', float('NaN'), regex = True)\ndf.replace(\"\", np.nan, inplace=True)\ndf.dropna(subset=['A'], inplace=True)\nStill there are empty cells", "Are you sure they are empty? Did you check to see they're not just empty strings (\"\")?\ndropna is the proper method, unless you want to also drop cells with empty strings.\nPlease elaborate, thank you", "Trying to remove empty cells in a column in csv from my data using pandas dataframe", 0.0, 0, 105, "You can use the method: [replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/index.html?highlight=replace#pandas101-replacement-function) with the function: nan in the column with a mask that takes in account only those rows that has not only the symbol \"s\", but that have also \"space\" as a character in them, for example:\n`df = df['columnA'].replace(' s ',np.NaN,regex=False).fillna('')`\nIf you want to replace all the cells with NaN that are not at least one symbol long (not only \" s\"), then you use: `df['coloumnA'] = np(df[\"column A\"]).replace(((\"s\" * 1)).ne(\"\").str.len(), np, regex= True,inplace =True)`", 0.16551724076271057, 0.0419580414891243, 0.12413793057203293, 0.02185792103409767], ["I'm trying \"from sklearn.linear_model import SGDOneClassSVM\"\nbut it doesn't work and raises an import error \"ImportError: cannot import name 'SGDOneClassSVM' from 'sklearn.linear_model\"", "Upgrade sklearn package using the command:\npip install --upgrade scikit-learn", "problem with importing SGDOneClassSVM from sklearn.linear_model", -0.1973753273487091, -1, 106, "SIGMOID_DECISION_FUNCTION is a module in the sk-learn package. You need to import it from there.\nIn the following example, I am importing it via the \"sk-linear-model.py\" module\nimport sk_linear as lm\nX_train, y_values = np.loadtxt('training.txt')\ny_pred = lmtrain =  lminst = None\nlmin = SIGNED_SQRT_TANH\nfitted_predictions, probabilities, classes =\nlm.fit_sgd(X, labels, sample_weight=None, solver='lrm',\nC=0.1, coef0=1.0, l2_ratio=True, class_prior=classes, tol=4e-05)\nsk_lin_mod.plot_probability_distributions(probabilities)", 0.05882352963089943, 0.0, 0.03921568766236305, 0.05879705771803856], ["I have a greyscale images dataset that I want to feed into a neural network.\nx_train_grey.shape is (32, 32, 73257)  so I understand it is (dimension_x, dimension_y, batch_size). Because the images are greyscale, there is only one \"depth\" dimension.\nHowever to feed this data to the neural network it needs to have this shape:(batch_size, dimension_x, dimension_y). With batch_szie at the beginning.\nHow do I reshape it to this format, so that batch_szie comes before the x, y images dimensions?\nOnce this is done, I expect to be able to pass this into a neural network (the first layer being Flatten()), like so:\nFlatten(input_shape=(32, 32, 1)),.\nCheers!", "Solved! By passing the correct shape into np.reshape().\nI really should get to know numpy better, before getting into deep learning.", "How to change the order of dimensions of images data's shape for a neural network?", 0.0, 0, 107, "You can reorder the dimensions via [`reshape()`](https://numpy.org/doc/stable/reference/generated/numpy/reserve.html#numpy-resample):\n`inputs = np.resize(x1, shape=(batchSize, height, width, channels))`\nYou can see a very simple example [here](https:stackoverflow.com/questions/45567719/creating-batches-of-image-data-from-a-numpyarray)\nAs far as I know, you can not change or rearrange dimensions in the `TensorFlow` API. Therefore, it seems that you would have to preprocess the data with python before feeding it into the tensorflow model.", 0.18691588938236237, 0.0, 0.11214952915906906, 0.0396825447678566], ["I have a python dataset that I have managed to take a sample from and put in a second dataset.\nAfter that I will need to produce another sample from the original dataset but I do not want any of the first sample to come up again.\nIdeally this would need any flag would only be there for a year so it can then be sampled again after that time has elapsed.", "Denote your original dataset with A. You generate a subset of A, denote it with B1. You can then create B2 from A_leftover = A \\ B1, where \\ denotes the set difference. You can then generate B3, B4, ... B12 from A_leftover, where Bi is generated from A_leftover = B(i-1).\nIf you want to put back B1 in the next year, A_leftover = A_leftover \\ B12 U B1, and from this, you can generate the subset for B13 (or you can denote it with B1 as 13%12 = 1). So after 12, you can say you can generate Bi from A_leftover = A_leftover \\ B(i-1) U B(i-11). Or you can use this formula from the very beginning, defining B(-i) = empty set for every i in [0,1,2,...,10].", "How I do I get a second sample from a dataset in Python without getting duplication from a first sample?", 0.0, 0, 108, "For my purposes, I've solved this with the following code.  If you do this, you need an index of your original dataframe.   `df1` and `dataframe_2` are the sample that you get from your dataframe, and you may have to replace the `pandas` module to `SciPy` or `numpy`\nThis code will allow you to create a new dataframe with a subsample of any dataset of a given length. You can make it sample multiple times by changing the number of `sample` you assign to your `sizes` list\n`df2 = pd.DataFrame(sdf)`", 0.2907488942146301, 0.0533333346247673, 0.14977973699569702, 0.0416666641831398], ["So, here I have a numpy array, array([[-1.228,  0.709,  0.   ], [ 0.   ,  2.836,  0.   ], [ 1.228,  0.709,  0.   ]]). What my plan is to perform addition to all the rows of this array with a vector (say [1,2,3]), and then append the result onto the end of it i.e the addition of another three rows? I want to perform the same process, like 5 times, so that the vector is added only to the last three rows, which were the result of the previous calculation(addition). Any suggestions?", "For the addition part, just write something like a[0]+[1,2,3] (where a is your array), numpy will perform addition element-wise as expected.\nFor appending a=np.append(a, [line], axis=1) is what you're looking for, where line is the new line you want to add, for example the result of the previous sum.\nThe iteration can be easily repeated selecting the last three rows thanks to negative indexing: if you use a[-1], a[-2] and a[-3] you'll be sure to pick the last three lines", "Iterate over rows, and perform addition", 0.06656807661056519, 1, 109, "\\begin{code}\n>>> a = np.array([-129214941798102723182011975847446252453, 61643855406907605677130348789151\n800886595,\n 42639968.66...\n\\end{pre}", 0.056603774428367615, 0.01923076994717121, 0.03773584961891174, 0.02255639061331749], ["I need to choose the best model to predict the traffic in a determinated hour.\nI think cluster is not for this problme, but i still don't know what would be the best option. If it's vector machine, decision tree, linear regression or Artificial Neural Networks.", "I think this depends mostly on your data. How much data do you have? If you only have few examples, I would go with VSM (Assuming you mean Support Vector Machines?). If you have a lot of examples I personally would go with a neural network.\nI guess you could even get a nice representation of the prolem if you use a recurrent network.", "Predict future with models / VSM /decision tree/ linear regression/ Artificial Neural Networks", 0.0, 0, 110, "Artifial neural network would not be my first choice in this case. I am guessing that there are a large number of possible reasons (weather, traffic conditions, road work, etc) for the changes you see in traffic at a given point in time. Since you need a good model for all possible scenarios you would need an ensemble of different ANN models and that may be too complex. A simple linear model could work. Use your data to find the set of predictors (variables) that has the highest correlation to your response. Pick the simplest set that explains your responses. For example if you have only 12 predictor variables and one response variable then it is most likely that you should just use linear models or multiple linear regressions. In general, use ANOVA to look at the importance of each predicting variable. You can also look into boosted trees.", 0.24413146078586578, 0.0379146933555603, 0.12206573039293289, 0.0363636314868927], ["I am newbie to data science and I am bit confused about working of map and applymap in pandas. As when I executed code df.applymap(lambda f: f*2) and df.apply(lambda f: f*2) provided same result. But when I change code for both which were df.applymap(lambda f: f*2 if f < 7 else f) and df.apply(lambda f: f*2 if f < 7 else f) then apply method caused an error. Upon my understanding, I came to conclusion that applymap works for each scalar value where apply does not work for each scalar value but instead it executes operation for whole column or series.\nKindly veterans help me out here if I am correct or not. Thanks in advance\nNote: df in code refers to whole DataFrame not series.", "Yes, apply works on a row or a column basis of a DataFrame, applymap works element-wise on a DataFrame.", "Working of map vs applymap in pandas, python", 1.2000000476837158, 0, 111, "You don't need to pass the `lambda` function. Instead, you can do `df['ColumnName']` directly:\n\\begin{itemize}\n  \\item `apply` will execute the function (if you give it a function) \\em{on each} row of the Dataframe\n    \\begincode{code} df[<some columns>].apply(...)\n       --> will do a loop over each row and execute that function, resulting in a DataSeries\n      for example: `a = df['col1'].apply(...)`  ==> will result in `Series('<output of function>', index = [Row(col = col1), Row(..., col=col2)]`\n   \\end{pre}", 0.18000000715255737, 0.0, 0.11999999731779099, 0.04494381323456764], ["I am given an array of elements and the sum K, and I am supposed to find a subarray (doesn\u2019t have to be contiguous) whose sum is equal to K.\nFor example:\nInput: [1, 9, 3, 2, 21], 30\nOutput: [9, 21]\nDo I need to use backtracking or is there another algorithm using dynamic programming for example?", "If it's not a big array you could use brute force: 2^n solutions!", "How do you find a subarray with the given sum?", 0.0996679961681366, 1, 112, "This problem is [NP-complete](http://en.wikipedia.org/wiki/NP_completeness) - this is because you can construct a 10-dimensional space in which the optimal solution is a (1-dimentional) line, which can be found by back-track with some probability (depending on how the elements of the array are distributed).\nIn other words, the probability of finding an optimal answer is not bounded.", 0.10000000149011612, 0.0, 0.05000000074505806, 0.0], ["I am using stableBaselines3 based on Open AI gym. The agent, in a toy problem version, tries to learn a given (fixed) target point (x and y coordinates within [0,31] and [0,25] respectively) on a screen.\nMy action space would thus be a box (Version A): self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([31,25])))). The reward obtained by the agent is minus the manhattan distance between the chosen point and target (the simulation terminates straight away). But when running the PPO algorithm, the agent seems to try only coordinates that are within the Box [0,0], [2,2] (ie coordinates are never bigger than 2). Nothing outside this box seems ever to be explored. The chosen policy is not even the best point within that box (typically (2,2)) but a random point within it.\nWhen I normalize to [0,1] both axes, with (Version B) self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([1,1])))), and the actual coordinates are rescaled (the x-action is multiplied by 31, the y- by 25) the agent does now explore the whole box (I tried PPO and A2C). However, the optimal policy produced corresponds often to a corner (the corner closest to the target), in spite of better rewards having been obtained during training at some point. Only occasionally one of the coordinates is not a boundary, never both together.\nIf I try to discretize my problem: self.action_space = gym.spaces.MultiDiscrete([2,32,26]), the agent correctly learns the best possible (x,y) action (nothing in the code from Version A changes except the action space). Obviously I'd like to not discretize.\nWhat are possible reasons for that whole behavior (not exploring, considering only/mostly corners, moving away from better rewards)? The rest of the code is too unwieldy to paste here, but does not change between these scenarios except for the action space, so the fact that the discretized versions works does not fit with a bug with rewards calculations.\nFinally, my action space would need to have one discrete component (whether the agent has found the target or will continue looking) on top of the two continuous components (x and y). The reward of a non-decisive fixation would be a small penalty, the reward of the final decision as above (the better the closer to the actual target). self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2),gym.spaces.Box(np.array([0,0]),np.array([31,25]),dtype=np.float32))) should be what I'm looking for, but Tuple is not supported. Is there any workaround? What do people do when they need both continuous and discrete components? I thought of making the binary component into a float, and transforming it to 0/1 below/above a certain cutoff, but that can't lend itself too well to learning.", "For posterity, stable_baselines seems to be sampling actions in mysterious ways. If the action space is defined as [0,1] or [-1,-1], stable_baselines will indeed sample that space. But if the action space is, in my case, [0,31], then the actions sampled are roughly within [0,3] or [0,4], with most values being within [0,1].\nSo the workaround seems to be to use Boxes using [0,1] or [-1,-1] for the action_space, and rescale the action returned by whatever SB3 algorithm you're using.", "stablebaselines algorithms exploring badly two-dimension box in easy RL problem", 0.3799489736557007, 2, 113, "\\begin{itemize}\n\\item For your first problem, I would recommend not using a Box and instead use a 1D space with max=30. This is because, from my experience with stable baselines, you'll have issues in general when you use more than one variable with continuous spaces in OpenAI Gym environments. My recommendation is to go with something like this:\nself.actions = [1]*80 + [5]*20\nwhere 8 is the number of actions your model has to choose from, each of which has 50% chance of being selected and 49% being rejected. You can then learn the probabilities for each action. To keep the Reward equal to what you have in your Box environment (that is  -1 when the distance is bad), I will set a min for both x and for y. I hope you know, OpenALG works only if you set the value for x to something between -5 and -0 in this case. And so for this action you can simply put something as  if(self < -2.5), (self > -3.3) then -4, else 6.66, something of this sort. By doing this you are ensuring that your x distance and your y distance have a minimum of - 7 and that you only end up explorating the center of your box. It's better not to make use of continuous space for exploration. Also, just so that I make myself clear. If you do set up a tuple as an action, it would not work for you at all. One reason that could be is, that as you said, stable Baselines is using Pairwise policy gradients with PPG or A3C which requires both variables to exist. In this manner, P(x) and P (y), P xy or Pxy. But even if we do not use stable policy gradient with the tuple, which I don' think will be working, we will also face other problems as one policy will update the other policy. For example, a policy that chooses x = 9,  y = -9 will lead to an update of x policy of  (9-5)/(9+5)=0 and this will make the x variable move to x=1. Similarly y will move in -infinity. That is a bad sign. Another thing to note is that, if the policy updates", 0.236286923289299, 0.012711863964796066, 0.10126582533121109, 0.03448009863495827], ["I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors' but I have this error ImportError: cannot import name 'KNeihgborsClassifier' from 'sklearn.neighbors' (C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neighbors_init_.py)", "You are importing KNeihgborsClassifier which is wrong, change it to:\nfrom sklearn.neighbors import KNeighborsClassifier", "I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors'", 0.0, 0, 114, "\\begin{code}\n# Import\nimport sklearn\nfrom skimble.clustering import DBSCAN\nclf = DBScan(min_samples = 20, metric = 'euclidean',\n            metric_params = dict(p = .05,\n                                metric='cosine'))\n\\end{pre}", 0.1463414579629898, 0.0, 0.09756097197532654, 0.04715018719434738], ["I have a task and the output should be a \"1-D np.array of dimension m\" and I don't understand how a 1-D array can have m Dimension, it has 1 per definition ?", "The word dimension can mean multiple things, in this case it means the size/length of the singular dimension, i.e. you can say an array has dimensions 2x2.\nTherefore, a 1D array of dimension m is equivalent to a list of length m.", "What is a 1-D np.array of dimension m?", 0.1973753273487091, 1, 115, "You can think of it like an array of length m with a single element.  If you had an mx1 array, you'd have an element for every element of the original array.\nFor a m by m array with the dimension 3, this would be the same as a list of 9 lists of size 4. Each element in this list represents one row and  one column of your original matrix.", 0.37931033968925476, 0.15789473056793213, 0.2068965584039688, 0.06906439363956451], ["I have speed data of many particles to which I want to fit the Maxwellian curve. I am trying to use the fit method from scipy.stats.maxwell to fit to my data and extract the temperature of the system from that.\nFrom the documentation, I am unable to put my finger on what the parameters that we are trying to fit exactly are and hence how they relate to temperature.", "It's related to scale.\nYou also likely want to set floc=0 in maxwell.fit(...) \nCf the argument of the exponential: with non-zero scale it's -x**2 / scale**2 / 2, which you compare to the expected mv**2 / 2kT.", "How to use scipy.stats.maxwell to find temperature?", 1.2000000476837158, 1, 116, "From the maxwell curve fit, the function `f` is basically a cubic polynomial, with coefficients that have to be determined by fitting the data. If you're looking for the first parameter, that is the `beta_0` parameter.", 0.22499999403953552, 0.0, 0.15000000596046448, 0.028901731595396996], ["i have LogisticRegressionCv model it's .pkl file and import data as images but i don't know how to get it on flutter please help me If you know how or if I must to convert my model to other file formats.\nplease help me.\nThank you for your help.", "as you've trained your model in python and stored it in pkl file. One method is in your flutter background, call python3 predict_yourmodel.py your_model_params and after the run, it will give your the model result.\nAnother way is implement a logisticRegressionCv in Flutter as it is a simple model, and easily be implemented. you can store all your params and l1 or l2 etc super-params in a txt instead of pkl file for readility.", "How to implement LogisticRegressionCv on flutter", 0.0, 0, 117, "Here is how you can do it\n\\begin{itemize}\nFirst download [Tensorflow](https://github.com/tensorflow/models/tree/master/research/object_detection)\nNow install and configure [Flutter TensorFlow](http://%20https%3A%5C%40%7B%60flutter%C2%B4org%E28%80Aorg.tensorflow%0D%A6%D822Tensors.flutterTfModule%F8.dill%H2) plugin. (make sure to include tensorflow_platform in build.gradle file as per above link) And\nfollow the steps given [here](#1069) for the final implementation.", 0.16993464529514313, 0.013245033100247383, 0.09150326997041702, 0.03553299233317375], ["Since I don't have pure knowledge of the pandas library, I just want to explore the range of functions that pandas library offers to users.", "use dir(pandas)\nBut you'd better go to the official documentation.", "Is there any command which can show what kinds of function are in-built in pandas or matplotlib?", 1.2000000476837158, 0, 118, "You can look at the [`pandas.DataFrame.info()`](https://pandas-docs.github.io/pandas/reference/api/html/info.html)\nfunction which shows all of that. In addition, the documentation for `pandas` will give you a summary of all the available functions as well.", 0.145454540848732, 0.0, 0.1090909093618393, 0.032786883413791656], ["Getting this error in python:\nValueError: cannot reshape array of size 14333830 into shape (14130,1,1286),\nHow do I solve this?\nThis is the code generating the error:\ndata_train1=data_train.reshape(14130,1,1286)", "For doing reshaping, your new shape should match the previous shape. If you multiply 14130 * 1286, you get 18171180 which is obviously not the same as 14333830. So you must write something correct.", "ValueError: cannot reshape array of size 14333830 into shape (14130,1,1286), how do I solve this?", 0.0, 0, 119, "Your `data` array has shape `(1090515, 6)`. Reshaping this to `(70717,)` will throw the `Value Error`. You need to reshap the data to something like this: `(X, Y, Z)` where `X` is `(6,)`. You can do that with numpy.\nLike so: `Y = np.delete(data, np-1).reshhape(-1) * data_scale`\nHere: the `-1` in `np-` removes the last dimension, the `(,)`, which is in your case the z-dimension. This will create a 2D array out of the 3D `y` data. `resshape` will then rescale `n` of these 598973 values to the desired `(2,504)`, since the number of rows should be equal to 74. With the multiplication by the factor `1 / data-scale`, you get the correct weight. Finally you add the y-value of your data and the multiplied z value.", 0.1807228922843933, 0.024390242993831635, 0.1204819306731224, 0.012194199487566948], ["I have a simple table which the datetime is formatted correctly on.\n\n\n\n\nDatetime\nDiff\n\n\n\n\n2021-01-01 12:00:00\n0\n\n\n2021-01-01 12:02:00\n2\n\n\n2021-01-01 12:04:00\n2\n\n\n2021-01-01 12:010:00\n6\n\n\n2021-01-01 12:020:00\n10\n\n\n2021-01-01 12:022:00\n2\n\n\n\n\nI would like to add a label/batch name which increases when a specific threshold/cutoff time is the difference. The output (with a threshold of diff > 7) I am hoping to achieve is:\n\n\n\n\nDatetime\nDiff\nBatch\n\n\n\n\n2021-01-01 12:00:00\n0\nA\n\n\n2021-01-01 12:02:00\n2\nA\n\n\n2021-01-01 12:04:00\n2\nA\n\n\n2021-01-01 12:010:00\n6\nA\n\n\n2021-01-01 12:020:00\n10\nB\n\n\n2021-01-01 12:022:00\n2\nB\n\n\n\n\nBatch doesn't need to be 'A','B','C' - probably easier to increase numerically.\nI cannot find a solution online but I'm assuming there is a method to split the table on all values below the threshold, apply the batch label and concatenate again. However I cannot seem to get it working.\nAny insight appreciated :)", "Since True and False values represent 1 and 0 when summed, you can use this to create a cumulative sum on a boolean column made by df.Diff > 7:\ndf['Batch'] = (df.Diff > 7).cumsum()", "Pandas create a column iteratively - increasing after specific threshold", 0.1973753273487091, 3, 120, "\\begin{code}\ndata$batch = paste(sort(unique(data)$diff[data] < 5), collapse = \" \")\n\\end{pre}", 0.08163265138864517, 0.0, 0.08163265138864517, 0.03658536821603775], ["Consider a vector [0 1 2] and a matrix of size 3 x n. How can I multiply each element of the vector with the corresoponding row of the matrix. Each element of row 0 should be multiplied with 0, each element of row 1 should be multiplied with 1 and so on?", "I assume you're using numpy. You can use matrix *= vector.reshape(-1, 1). This will convert the vector to a column, then multiply the rows.", "element wise multiplication vector with rows of matrix", 1.2000000476837158, 1, 121, "\\begin{code}\n#include <stdio.h>\nint main()\n{\n    int n;\n     scanf(\"%d\",&n);\n   for(int i=0;i<n;++i)\n        {\n            printf(\"%02d \",a[i]*v[1]);\n         }\nreturn  printf(\"\\n\");\n}   return  main();\n\\end{pre}", 0.1034482792019844, 0.0, 0.06896551698446274, 0.00841838400810957], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "If the data is small enough that you can use pandas to process it, then you likely don't need pyspark. Spark is useful when you have such large data sizes that it doesn't fit into memory in one machine since it can perform distributed computation. That being said, if the computation is complex enough that it could benefit from a lot of parallelization, then you could see an efficiency boost using pyspark. I'm more comfortable with pyspark's APIs than pandas, so I might end up using pyspark anyways, but whether you'll see an efficiency boost depends a lot on the problem.", "Databricks - Pyspark vs Pandas", 1.2000000476837158, 2, 122, "If you are using Python inside of Dataprk for data transformation, you can use pythons standard data types like `numpy`, `pandas`, etc..\nBecause Datbbrics uses Spark (which uses Scala) internally, all the transformations are happening on top of a distributed cluster. So pypy data is used to pass data to the Scala code to do transformations.\nWhenever you use Spark API for the first time, then you need to convert the dataframe that you want to manipulate. Once the API is converted, it can be passed to your code and manipulated like pandas. The performance gain would only come from using Scala API instead and not pandas.", 0.3004694879055023, 0.056872036308050156, 0.15962441265583038, 0.04393305629491806], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "Pandas run operations on a single machine whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark is the best fit which could process operations many times(100x) faster than Pandas.\nPySpark is very efficient for processing large datasets. But you can convert spark dataframe to Pandas dataframe after preprocessing and data exploration to train machine learning models using sklearn.", "Databricks - Pyspark vs Pandas", 0.0, 0, 123, "From [Databases vs. Python](https://databrinders.com/blog/databases-vs-python-spark-sql-databrick-benchmarks/), the answer is yes, you can replace pandas with Spark. Databybrick is a cloud provider, but for your purposes, it should work without any special configuration on your end.\n\\begin{itemize}\n    \\item Dataybricks has more memory available to each cluster instance than Amazon EC2 or Google Cloud does. So, Spark, being a memory-heavy compute engine will be able to run in the Datavybrics cloud much better than it would on Amazon. (Source)\n  \\strong{If you have enough memory on AWS or GCP, however, and your program can be scaled to take advantage of that, there\u2019s little reason to use Datavbric cloud (unless you want to try it out or have very stringent performance requirements.)} (Same Source) So if you're just using spark for manipulation and have no other requirements (like running pymssql or other external services), then it could be the ideal choice. Also, Datasets are usually stored on S3 and EMR clusters on GCE have little to no IOPS so if your data is big and you need to do joins or heavy processing on it, GEE and Datavia are not the optimal choices. But if its a data manipulating script, then Dataviribs is the perfect choice to get started. [This](http://bigdata.academy/what-is-the-difference-between-pysql-and-datavr-2/) is an excellent explanation of this scenario. It also contains some benchmarks of Datashock vs Datapy and they've tested Datapro in comparison. You can also go through Datatabriders blog post on [The importance of data processing for business intelligence](//databridgers.blogspot.in/2017/03/theimportance-of-data-processing-for-business-intelligence.html) which explains how Data Processing and data storage are different concepts and how you shouldn't try to mix them together.", 0.16886544227600098, 0.02122015878558159, 0.09498680382966995, 0.04400977864861488], ["I have a scenario in which I have a peptide frame having 9 AA. I want to generate all possible peptides by replacing a maximum of 3 AA on this frame ie by replacing only 1 or 2 or 3 AA.\nThe frame is CKASGFTFS and I want to see all the mutants by replacing a maximum of 3 AA from the pool of 20 AA.\nwe have a pool of 20 different AA (A,R,N,D,E,G,C,Q,H,I,L,K,M,F,P,S,T,W,Y,V).\nI am new to coding so Can someone help me out with how to code for this in Python or Biopython.\noutput is supposed to be a list of unique sequences like below:\nCKASGFTFT, CTTSGFTFS, CTASGKTFS, CTASAFTWS, CTRSGFTFS, CKASEFTFS ....so on so forth getting 1, 2, or 3 substitutions from the pool of AA without changing the existing frame.", "Let's compute the total number of mutations that you are looking for.\nSay you want to replace a single AA. Firstly, there are 9 AAs in your frame, each of which can be changed into one of 19 other AA. That's 9 * 19 = 171\nIf you want to change two AA, there are 9c2 = 36 combinations of AA in your frame, and 19^2 permutations of two of the pool. That gives us 36 * 19^2 = 12996\nFinally, if you want to change three, there are 9c3 = 84 combinations and 19^3 permutations of three of the pool. That gives us 84 * 19^3 = 576156\nPut it all together and you get 171 + 12996 + 576156 = 589323 possible mutations. Hopefully, this helps illustrate the scale of the task you are trying to accomplish!", "Generate the all possible unique peptides (permutants) in Python/Biopython", 0.0996679961681366, 1, 124, "You could first check the set of possible amino acid combinations,\nthen find out all permutations of the possible combinations (e.g. by using a built-in python function like [permapy](https://pypi.org/project/permpy/)). You can then do some calculations to ensure that each permutation is a unique sequence, and then finally return only the unique combinations.", 0.20725388824939728, 0.020942408591508865, 0.11398963630199432, 0.03225806728005409], ["When I try to install sklearn-crfsuite, I get the following error:\n\nfatal error C1083: Cannot open include file: 'basetsd.h': No such file\nor directory\n\ntrying this command pip install sklearn-crfsuite, also installed Microsoft visual C++ 2019 and the required libraries.\nPlease let me know if there is any solution to this, do I need to set any variable in the system path?", "If I understand your problem correctly, add the path to your header file in your project using Property->C/C++->General->Additional Include Directories.\nIf you want this to apply to all your projects use the Property manager to add this path.\nOf course make sure the header exists.", "fatal error C1083: Cannot open include file: 'basetsd.h': No such file or directory", 0.0, 0, 125, "This looks like it's a build issue. Is the latest `crfeval.py` file included in your sk-learn distribution? This file is a wrapper around the `basetds.c` header, and this header file must be available in order to compile. This header is available as part of [sklearn' ](https://www.lfd.uci.edu/%7Egohlke/pythonlibs/#scikit-image). You can verify that the file `skimage/__init__.pyc`, `numpy.core.multiarray.arrayobject.pxd`, and `scipy/constants/misc/basemath.o` are included and present in their respective locations in this distribution. You might also be able to use this to get an idea of what file paths are in use and might be needed by other modules.", 0.2073170691728592, 0.03703703731298447, 0.13414634764194489, 0.03375496715307236], ["ctx=con.cursor()\nctx.execute(select col1 from table1)\nresult=ctx.fetchall()\ndata=pd.DataFrame(result)\ndata.columns['field']\nfor index,row in data:\nupdate table2 set col2='some value' where col1=str(row['field'])", "Solution to this is:\nInsert the data into some transient table and then then use that table for update.\nFor insert :\ndata = panda.DataFrame(result)\njust use data.to_csv('file complete path',index=False,Header=True)\nusing put command place the file in internal stage and from there use Copy command to copy data into transient table.\nlater on you can use this table to update your target table.", "Updating snowflake table row by row using panda dataframe (iterrows()) taking lot of time .Can some one give better approach to speed up updates?", 1.2000000476837158, 0, 126, "The issue is that, the pandas DataFrame is not an iterable (and the iterator is also not thread safe, so there is no point in iterating).  You can still use the `.update()` method, but you need to use a `list` or a generator, and you can't pass a `.fetch()` as a list, you have to make the list upfront, like\n`list(ctx)` or `ctx.__iter__().__next__()`\n\\begin{code}\nupdates = list(data[0])\ncon = cxn.connect() #create connection object\nwith con: #with this block only the updates can be done\n    ctx = con.context()  #get cursor\n        ctx.set_string_types=False  ## set autocommit on\n   ## if this is on True the auto-commit would have kicked in and the update would not have happened\n       ctx_obj = ctx()   # get cursor as context\n         try:   ctx0=context(con, auto_commit=True)   ### get real cursor using real con\n             for index0, row0 in updates:  ### iterrows() is the key\n                 ctx1.update(where=\"id={}\".format(index0), table=\"table2\")\n            except pylongform.InConsistentError:    #this would mean no results from data[index], so try\n                #ctx2.executemany(\n              ctx2=update_many(updatae=\"VAL\", table='table1')\n     finally:#reset auto commit on real connection\n          ctx._reset_autocommit()## this does the work\n con=cxn\n cursor =con._cursor(cx=conn.cx) #to get the real underlying cursor from the connection context.\n with cursor:## no need for this\n try:#will work with normal update\n      cursor.next() ## move to the next entry in cursor, cursor is a real DB cursor so not the object that pandas returns\n except: pass\ntry: ## will work for multiupdate\n #cursor.multi_update()#this works only in postgres, I have not seen it working on mysql, hence using cursor.__multiupdate__\n cur.delete()      #deletes all entries\ncur.insert()       #inserts the updated row\npass\n#cursor.__setitem__", 0.18994413316249847, 0.01123595517128706, 0.10614524781703949, 0.04228634387254715], ["after model training, spacy has generated model\\model-best and model\\model-last folders. What's the difference between the two models and which one should be used for predictions?", "model-best is the model that got the highest score on the dev set. It is usually the model you would want to use.\nmodel-last is the model trained in the last iteration. You might want to use it if you resume training.", "difference between model-best and model-last in spacy", 1.2000000476837158, 1, 127, "The model in the `model\\x` folder has been retrained for the model named `x`. The `best` model is the one that gives the best results and should thus be retrapped in new languages.\nThe model used in `last` is just the latest trained model (not necessarily the same model as `better`)\nAs you'll see in this doc, there're no recommendations, I would say it depends on your needs but personally I prefer to keep the most recent model, since you may want to retrain it in order to be up-to-date with your data (new words, etc...)\nhttps://spacy.io/usage/modeling-and-evaluation#evaluating-models", 0.350649356842041, 0.09210526198148727, 0.2467532455921173, 0.07344633340835571], ["I am using compute_face_descriptor function in dlib which is the function of dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat').\nThere is an option to set \"num_jitters\". I set \"num_jitters\"=10, but the output embedding I am getting different on subsequent runs. I have tried setting seed using np.random.seed(43), but still, the output changes on subsequent runs\nIs there a way to set seed in this function using \"num_jitters\"=10 so that the output embedding doesn't change on subsequent runs?", "\"num_jitters\". means how many times dlib will re-sample your face image each time randomly moving it a little bit. That's why you are getting different embeddings.", "How to set seed to compute_face_descriptor function in dlib?", 0.0, 0, 128, "There is no way, within the current dlibs, to randomize the way the features are jittered.\nThe only way you can do this, is by randomizing the number of points, and then the points themselves.", 0.0625, 0.0, 0.03125, 0.07965521514415741], ["during i trained my own model, i have a simple question.\norigianl input image shape is (height : 434, width : 636), and i used resized image(416 x 416) for my train model(Unet++).\nI wonder if it is right to resize the test image when inference step, How can I resize the model output to the original image size when comparing test output with original test image.\n---------process\noriginal input size : (434, 636)\ntrain input size: (416, 416)\n\ninference\ntest img -> resize (416, 416) -> test model -> test output(416,416) -> comparing test output with test img", "It's absolutely right to resize the input image to the model input size else, it will generate an error if you feed an image of different size to the model. Coming towards your question, you can solve this either by rescaling the model output to the original size of your input images. A simple technique can be resizing the masks but there can be better ways. OR\nYou can resize your input images and their Ground Truths (masks) to the model size, and so you won't need to rescale the model's output. I hope that answers the question !!!", "Question about Inference for Image segmentation", 0.0, 0, 129, "I am going to assume that you have the right dimensions for your input/output image in your model. Let's say your original input shape of 500x556. In order to maintain the resolution of your test data you need to use [frozen_input](https://pytorch.org/docs/stable/generated/torchy.nn.Module.html#torcht.torcha.ffnn_utils.frozendata). Using frozen data allows you to pass in the desired shape and size of the data (in this case 258x282) into your data layer. See this [link](http://datascience.stanford.edu/papers/2018/11/Image_Super_Resolution.pdf) as an example. Another way to do it, if you don't have access to frozendatat is by converting to Numpy array. The [documentation](can be found [here](ftp://ftp.cs.washington.edufor the python script). You will need python2.7 to install NumPy (as it was not packaged with Anaconda)", 0.36947789788246155, 0.04858299717307091, 0.18473894894123077, 0.04255318641662598], ["I want to get the molecules from the SMILES using rdkit in python. The SMILES I used was downloaded from the drugbank.\nHowever, when I using the function Chem.MolFromSmiles, some SMILES would report but some wouldn't:\nExplicit valence for atom # 0 N, 4, is greater than permitted.\nI found some explanation about this problem: it is because the SMILES generated a invalid molecule that doesn't exist in real world.  But I am not a chemistry student.... So anyone know how to fix this?", "Your SMILES string would appear to have a neutral 4-co-ordinate nitrogen atom in it, which doesn't exist in real molecules.  4-co-ordinate nitrogen atoms have a positive charge, eg [N+] in a SMILES string.", "Problems encountered when using RDKIT to convert SMILES to mol", 1.2000000476837158, 2, 130, "The above issue has nothing to do with RDKit, its all the fault of SIMCE. SOM  is correct,\nThe input SmiIes are generated by SimeA which in the above case is creating a molecular cation. Cation valency is 1+ so any atom that has a valent of more than 5 (such as the nitrogen in N+1+) can be an issue. I would try this SMSIles in a different software package such as [Molecudy](http://molecu.dyndns.org/) and see if it works. If it does you'll know that its SIME that is the issue, if not the problem lies with your input file.", 0.1958041936159134, 0.014184396713972092, 0.0979020968079567, 0.03486147150397301], ["I am trying to write several Panda Dataframes into a SQL database. The dataframes are generated in different processes using the multiprocessing library.\nEach dataframe should get its own trial number when it is written into the database. Can I solve this using SQL autoincrement or do I have to create a counter variable in the Python code.\nIf I use the function pandas.DataFrame.to_sql and set an index as autoincrement, I get a consecutive index for each row.\nHere is an example how it should look like\n\n\n\n\ntrial number\ntimestamp\nvalue\n\n\n\n\n1\ntime1\nvalue1\n\n\n1\ntime2\nvalue2\n\n\n1\ntime_n\nvalue_n\n\n\n2\ntime1\nvalue1\n\n\n2\ntime2\nvalue2\n\n\n2\ntime3\nvalue3\n\n\n2\ntime_n\nvalue_n\n\n\n\n\nI use Python 3.9 and MariaDb as Database. I hope for help. Thanks", "You should have a separate trials table in your database where you cspture the details of each trial. The trials table will have an auto incremented id field.\nBefore writing your dataframes to your values table, each process inserts a record into the trials table and get the generated auto increment value.\nThen use this value to set the trial number column when you dump the frame to your table.", "Write Panda Dataframes to SQL. Each data frame must be identifiable by a trial number", 0.0, 0, 131, "You can add a `row_number()` field in a dataframe and then order it by it, then use that field as an autonumber. Something like this. For example (using SQLAlchemy, but it's the same idea):\n\\begin{itemize}\nhttps://docs.sqlalchemia.com/en/14/dialects/postgresql/operators.html\nrow.number()\nIt should have an integer value starting at 1, even across partitions. You'd need to set `include_values = False` for the dataframe to be written.", 0.2876712381839752, 0.0555555559694767, 0.1780821979045868, 0.06432749330997467], ["df['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : x.replace('.', ',',1).replace('.', '').replace(',', '.',1)).astype(float)\nSlowly learning lambda command, my understanding of this line of code is:\n\nChange dataframe type to str\nApply lambda with one perimeter x\nReplace all the string format . to , , (I don't understand what does 1 stands for, have done research prior asking, didn't find clue)\nReplace all the string format . to null value\nReplace all the string format , to . , (again still have no clue what does 1 stands for in this case)\nChange dataframe type to float\n\nPlease help me better understanding this line of code, thank you", "This replaces the first . in the string with a ,, removes the remaining periods, changes the first , back to a ., then converts the result to a float for the 'Current Ver' column in the dataframe.", "How to understand this lambda with 3 .replace() line of code", 1.2000000476837158, 2, 132, "The first `.replace()` is for `str` objects (strings), the second one is to convert the result to `float`, but if you have floats in your dataframe you won' get any errors, but the values won be converted.\nThe first call for `.astyp()` gives the dataframe `x` a `None` value for every row.", 0.3720930218696594, 0.1190476194024086, 0.25581395626068115, 0.04999999701976776], ["Initially, my dataframe had a Month column containing numbers representing the months.\n\n\n\n\nMonth\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\nI typed df[\"Month\"] = pd.to_datetime(df[\"Month\"]) and I get this...\n\n\n\n\nMonth\n\n\n\n\n970-01-01 00:00:00.0000000001\n\n\n1970-01-01 00:00:00.000000002\n\n\n1970-01-01 00:00:00.000000003\n\n\n1970-01-01 00:00:00.000000004\n\n\n\n\nI would like to just retain just the dates and not the time. Any solutions?", "get the date from the column using df['Month'].dt.date", "Date and Time Format Conversion in Pandas, Python", 0.0, 0, 133, "You need to parse the strings to datetime.  There are multiple ways to accomplish this. This may be one solution, using `toordinal`:\n`df['Month'] = df['month'].astype('str')  # change to int if you want year`\n  `df.['Date']=pd.Series(  df.Month.apply(lambda x: pd._simple_ordinals(pd._tosequence(x).strftime(\"%Y-%m-%d\")))  )`", 0.1538461595773697, 0.0317460335791111, 0.1538461595773697, 0.1271403282880783], ["I've installed the native version of python3 through miniforge and the intel version of Spyder through homebrew. Everything is working fine with one exception, my plots seem to work with the \"graphics backend\" \"inline\" option. When I try to select the \"automatic\" option, or any other option rather than inline, the IPython doesn't initiate. Has anyone had the same problem?\nKind regards,", "(Spyder maintainer here) This problem is fixed in our 5.2.0 version, released in November 2021.", "Spyder \"Graphics backend\" \"automatic\" option not working on M1 macbook", 0.0, 0, 134, "Ok, I got it working by installing the `pip` package for the plotly-python3 library for inline graphs through:\n`$ pip3 install plot.ly`\nIt seems that it's a bit tricky to install pip with home-brewed stuff, so I can' help you there. But, with this, Spygder seems to start correctly and plots are displayed correctly (see screenshot below).", 0.025974025949835777, 0.0, 0.025974025949835777, 0.012341790832579136], ["I have df = pd.concat(dict_of_df, axis=0) and sometimes [rarely] it might be the case that all of the df in the dictionary are empty in which case I would like Pandas to cheerfully return an empty dataframe. But instead I get a ValueError.\nI can write a loop to check for the length of each df before calling concat, but would prefer to not always do that, so at the moment I just embed the concat into a try/except... which doesnt make be really happy either because if there was a \"true\" ValueError I would like to have know it. So then I could do a try/except loop and if exception is thrown then do a count of all the dicts and ... ugh. This is getting crazy.\nIs there something more clean? Thanks.", "Sorry, I am going to withdraw the question. I now realize that\npd.concat([None,None]) produces the ValueError, whereas as noted above pd.concat(pd.DataFrame(),pd.DataFrame()) does exactly what you would hope. Also pd.concat([None,pd.DataFrame()]) is fine too. So it's not really fair of me to complain about concat. I need to stop feeding my routine non-existent datasets !\nThanks for feedback", "Can I avoid a ValueError concatenating empty dataframes?", 0.0, 0, 135, "Maybe I don't fully understand your problem, what you want to achieve is actually very simple. You can define a function like this:\n\\begin{itemize}\n \\item check if at least one of those `dict`s contains at `empty` dataframe\n    \\begin {code}", 0.14414414763450623, 0.01834862306714058, 0.09009008854627609, 0.018728096038103104], ["in pandas the inplace parameter make modification on the reference but I know in python data are sent by value not by reference i want to know how this is implemented or how this work", "Python\u2019s argument passing model is neither \u201cPass by Value\u201d nor \u201cPass by Reference\u201d but it is \u201cPass by Object Reference\u201d\n\nWhen you pass a dictionary to a function and modify that dictionary inside the function, the changes will reflect on the dictionary everywhere.\nHowever, here we are dealing with something even less ambiguous. When passing inplace=True to a method call on a pandas object (be it a Series or a DataFrame), we are simply saying: change the current object instead of getting me a new one. Method calls can modify variables of the instances on which they were called - this is independent of whether a language is \"call by value\" or \"call by reference\". The only case in which this would get tricky is if a language only had constants (think val) and no variables (think var) - think purely functional languages. Then, it's true - you can only return new objects and can't modify any old ones. In practice, though, even in purest of languages you can find ways to update records in-place.", "the inplace parameter in pandas how it works?", 1.2000000476837158, 1, 136, "I do not know the ins and outs of pandas. But it seems that pandas does some optimization.\nWhen you do `df.ix[2]`, `ix` is an integer, `2` can be found at `data_frame.index[index]`.\nThe `index` here is a hash table, and the value of the hash is the location of your row. The index is updated in this way. You can change the row of this hash to 1 to change your index to `0` to start the first row\nAs a result, when you set `inplace=True` `DataFrame.loc[new_index]` is called, which is equivalent to changing the entire DataFrame\nI think pandas can do this optimization\nHere is some documentation about the optimization:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/generated/ pandas._version.PandasVersion.html#pandas._compat.make_tuple\nNote that it's an optimization, not a necessity\nMaybe pandas' algorithm can not optimize this code and can't implement the above optimization", 0.3244837820529938, 0.05341246351599693, 0.14749263226985931, 0.059366755187511444], ["I have 2 columns of data with the first column being product codes (all filled out) and the second column with product description.\nThe first column has all the product codes filled out but there are some rows where the product description (second column) is missing.\nFor example row 200 has a product code of 145 but the description on that row is empty (NaN). However, there are other rows with product code 145 where the description exists, which is \"laptop\". I would like to have the description of row 200 to be filled with \"laptop\" because that's the description for that product code.\nI want to find a solution where I can fill out all NaN values in the second column (product description) based on the first column (product code).", "First, decide on a function that takes descriptions and picks out one of them. You could use min, max, mode, define you own get_desc, etc. Then you can separate the dataframe by product code with groupby and apply whatever function you decided on: df.groupby('product code').apply(get_desc) or df.groupby('product code')['product description'].apply(get_desc) depending on whether get_desc takes a dataframe or column as input. Then you can merge the resulting dataframe with your original dataframe. You can either replace the entire original product description column with the product description column of the groupby output, or have merge create a new column, then fillna the old product description with the new product description.", "Fill NaN in column 2 with median string based on value in column 1 in Python", 0.0, 0, 137, "Use [`pandas.fillna`](https://pandas-docs.github.io/pandas/reference/api/generated/pd.DataFrame.replace.html):\n\\begin{itemize}\n    \\item For numeric values use `value` argument. For string values, use `'value'`.\n      e.g. `df.column.astype(str).fill(value)`\n     \\strong{OR} use [`np.nan`](http://docs-ns.openpyxl.org/stable/python/openpynbextensions/nanfunctions.htm). e,g: `np_nan`\n       \\em{(I assume you are using the numpy package for the data)}\n         `s = df.iloc[:, 0].asti(npnan)\n        df = s.pdtm(1).pdlf`(s.pdxlf)``` (See example #1 from the linked doc)  # I hope that is correct, or if not, you can edit this to your desired format. I didn't use it for your example, but it will give you the idea of how to fill the NaN'd rows. You can also use a value (instead of np_null). This example is for a string column:\n          `d.s`   <-- replace with column of interest\n             `, np.where(d == npnan, \"value\", d)`     # Replace with what you want, `where` takes two values and `and`s them together, and returns the other, e.,g, in case of an `if` statement. The value \"nan\" is added to each NaN value.  If `nan = \"\", value = \"something\"` or something else. If not a single value, it returns a list of values. Example of multiple values:  `['value', 'value']` if that makes more sense. See this example of a pandas dataframe for string: [Pandas dfs NaNs](https%3A%2F% 3APython%  2Borg%C2%B A% % 7E%7A %3B%09%5C% C2 %5B0%E2 A075152C %2E4) `from%     % import%      pandas%       as%        pd%         df=pd%                 .read_", 0.21522310376167297, 0.01583113521337509, 0.09973753243684769, 0.05228055268526077], ["I want to horizontally cut the spectrogram of a wav file into 24 pieces,and after measuring the power of each piece, and finally rank the pieces by power orders what should I do please", "Could you show some code that you have written to try out the same? It would be easier to help if we have something to build upon and rectify issues, if any.\nAdditionally please try basic image manipulation to do the same. Instead of cutting you could divide the image into N (here 24) regions and analyze them in parallel using multiprocessing.", "cut the spectrogram of a wav file", 0.0, 0, 138, "This might be difficult to do with `fft`, since it might create a too large data structure.\nIf you would like to implement it using fft, this can be a simple implementation. The `ifft` is implemented with the `dgemm` method. There are also other methods to get the inverse FFT, like `idft`, `bfd`, but they might not be supported in every computer. So, we will try this implementation using the dgemv method:\nhttps://gist.github.com/Syamprasad33292/1535043f39871686d76a454821378c0f\nThe code is as follow: `cut_spectrograph_from_wav.cpp`.\nThis code can split the wave files into smaller parts, but cannot do the orderly comparison for the output. You need to compare the magnitude of the frequencies of smaller files. This comparison can also be done using `math.h`, and this code uses `std::abs` function.", 0.22335025668144226, 0.020512821152806282, 0.12182740867137909, 0.04577464610338211], ["I am trying to understand why one or two parameters in my Pytorch neural network occasionally become nan after calling optimizer.step().\nI have already checked the gradients after calling .backward() and just before calling the optimizer, and they neither contain nans nor are very large. I am doing gradient clipping, but I don't think that this can be responsible since the gradients still look fine after clipping. I am using single-precision floats everywhere.\nThis behavior happens randomly every hundred thousand epochs or so, and is proving very difficult to debug. Unfortunately the code is too long to reproduce here and I haven't been able to replicate the problem in a smaller example.\nIf anyone can suggest possible issues I haven't mentioned above, that would be super helpful.\nThanks!", "This ended up being ignorance on my part: there were Infs in the gradients that were evading my diagnostic code, as I didn't realize Pytorch's .isnan() method doesn't detect them.", "What are the main reasons why some network parameters might become nan after calling optimizer.step in Pytorch?", 1.2000000476837158, 0, 139, "\\begin{blockquote}\nGradients are scaled to have values in [-1, 1] range. This means they can have large values, which can lead to numerical instability. You can avoid this by using the clipped function. See here for a detailed description of the numerical stability problems of backprop: https://colah.github.io/posts/2015-08-Backprop/\n\\end{itemize}", 0.08888889104127884, 0.0, 0.04444444552063942, 0.03333333134651184], ["Can someone help me with transforming the following table using a PYTHON function?\nI need 2 new columns: A \"follower Type\" which will have entries as organic or paid and a \"Follower count\" which has the values corresponding to the type of follower.\nCurrent Table -\n\n\n\n\norg\norganic follower\npaid follower\nstart date\nstop date\n\n\n\n\nOne\n2\n0\n1634169600000\n1634256000000\n\n\nOne\n-1\n0\n1634256000000\n1634342400000\n\n\n\n\nDesired Table -\n\n\n\n\norg\nstart date\nstop date\nFollower Type\nFollower Count\n\n\n\n\nOne\n1634169600000\n1634256000000\nOrganic\n2\n\n\nOne\n1634169600000\n1634256000000\nPaid\n0\n\n\nOne\n1634256000000\n1634342400000\nOrganic\n-1\n\n\nOne\n1634256000000\n1634342400000\nPaid\n0\n\n\n\n\nIf anybody knows how to do this, please do let me know.\nThanks and Cheers!", "Use reindex to change column order\n'''\ncolumn_names = [\"C\", \"A\", \"B\"]\ndf = df.reindex(columns=column_names)\n'''\nLike below you can add columns to existing dataframe\ndf[newcolumn]=formula", "How can I do the following dataframe transformation in Python?", 0.0, 0, 140, "Use pandas [merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/generated/Series.merge.html) method. It will return new data frame based on given data frames.", 0.035087719559669495, 0.0, 0.035087719559669495, 0.0569620206952095], ["I have a dataset with more than 50 columns and I'm trying to find a way in Python to make a simple linear regression between each combination of variables. The goal here is to find a starting point in furthering my analysis (i.e, I will dwelve deeper into those pairs that have a somewhat significant R Square).\nI've put all my columns in a list of numpy arrays. How could I go about making a simple linear regression between each combination, and for that combination, print the R square? Is there a possibility to try also a multiple linear regression, with up to 5-6 variables, again with each combination?\nEach array has ~200 rows, so code efficiency in terms of speed would not be a big issue for this personal project.", "This is more of an EDA problem than a python problem. Look into some regression resources, specifically a correlation matrix. However, one possible solution could use itertools.combinations with a group size of 6. This will give you 15,890,700 different options for running a regression so unless you want to run greater than 15 million regressions you should do some EDA to find important features in your dataset.", "Automatic Linear/Multiple Regression in Python with 50+ columns", 0.0, 0, 141, "The fastest way to do this would be with the built-in `sklearn` library, using [partial_fit](https://scikit-learn.org/stable/modules/generated/sklearnsparse.lars_path.html) (the `sparse` param in the docs). However, that would also mean you're dependent on sklearn, which may not work if you later want to migrate to a different regression method. Here's an example for a single regression. This will iterate through each combo of your columns, build a linear model, use it to predict, then average the predictions. If `R^2` > 0.1, the comboliners will print out a row with `(X,Y,R2)`\n(I haven't tested this code at all, but it should give you the general idea)\n\\begin{itemize}\n \\item Convert to list/dictionary\n <a href=\"https.//i65.tinypic.com/f4f59s.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n2.Iterate over each column combi\n<a\nhref=\" https://i3054.asnt.co/wp-content/uploads/29-270937780388472251.jpg\"\nrel=\" nofollow noreferrer\" style=\" color: transparent;\">\n3.Build the model\n4 Use the linear_model to calculate the prediction\n5 Sum all predictions and divide by the total number of combos\n6.If `0 < R2 < 1` print a line\n7.Repeat\n8.For each col combination print R^squared to console\n9.Output to .csv\n  <table><tbody> \\begin {code} </tbody></table>    \\section{Combinations}    <p class=\"lead\">(X12,X45,XY32)    0\n     0   0  0      -0          0     -3415628264089853695\n    -1   4339746796191310660      0  -4901752114244863558\n                0", 0.19417475163936615, 0.019543973729014397, 0.08414239436388016, 0.06388888508081436], ["I use tf.math.round() but the output still has decimal points (310.0, 210.0 etc)\nIf I use tf.cast(tf.math.round(), dtype=\"int32\"), then I see the error mentioned in the title when calling finish on tornado handler\nHow can I cast to int using tensorflow operations and still be json serielizable", "You should use tf.int32 instead.", "tf cast leads to TypeError: Object of type 'int32' is not JSON serializable", 0.0, 0, 142, "For example, you can use `int_3` function to convert int64 to `tf int3`.\n\\begin{code}\ndef int_to_tf_int(i):\n    if i % 1 == 0:\n        return tf_constant(int((i / 63) * 3))\n      else: # < 79\n       return int((int(-i/127) / (66 - 4))* 5)  +  (int16(1) - i)%2\nint6_vec = tf1.convert_integer_types(\n                  tf2.constant([1,2,3,4,5,6,7,8,9,11])\n                , tf3.int8\n              , name='int')\nwith tf4.Session() as sess: sess.run(list(map(float, int1_arr))).print()\ntf3=sess.graph.as_graph_def()[tf2].values\nprint(snippet = '[15.2]') print(np.array(numpy.fromstring(str(round(eval(print('snapshot.eval({{{{{{0[4]}}}}}, {}, {{{0}})({{},}}}.format(json.dumps({'tf6': np.arange(6).astype(nnp).tobytes(),\n  'tf1': float3(0).npy(), 'b13': b173}))).data.decode('utf-8'), dtype='object')))))')", 0.04411764815449715, 0.0, 0.04411764815449715, 0.07138076424598694], ["I trained my named entity recognizer with spacy. I would like to evaluate it. So I looked at the spacy documentation and came across the scorer function. However, it doesn't seem to work with the IOB format. Do you think there will be a way to use spacy to evaluate my IOB data or am I doomed to transform my data into the format spacy wants?\nThank u very much :)", "You can't evaluate IOB data directly. You should be able to just use spacy convert to convert it to .spacy data in one step and then use spacy evaluate with that file though. (And if you trained your model in spaCy then presumably you already did the same conversion with your training and dev data?)", "Python - Is there a way to evaluate a named entity recognizer trained on IOB data using spacy?", 0.0, 0, 143, "Firstly, you should know that a \\strong{named entity} is a type of \\em{entity}. So you need to know what is an \\emph{entities} in spaCy before answering your question.\nIf you're interested in an entity, we have two kinds of methods to do that. The first one is with `spacy(df)`.\nFor example, `doc = spam(text)` and `pos = doc.ent_tags`.\nThe second one depends on what type entity you want to get the information about. For example `NER tagger = make_tagger(\"ner-english\")`. And then you can get an NER entities like `ner = tag.tag(doc)`. And you know it also have a `POS taggers` class if you have POS tags. But you cannot get a spans with this method. (Note: You need a PTB tagged with PFN format to make it work).\nI think you are confused with \\textit{pos tags}.\nIn the pos tag of IOBD format, the word entity and spanning are together. As far as I know, if spamy can work on it, spawy can'the tag it either. If you transform it to PT or PNF format (the two formats is provided by Penn Treaty) and the sentence will become like this. Then you will have another problem with tagging. That is because there are still more tags in the data such as \\textbf{is-a} (and probably other tags I didn'y know). You will lose these tags when you convert to either PTF or NF. You may know there is still another format called \\texttt{POS}. You can try to convert it there and see if that work. In PTS, as far I known, there won' be the \\cite{leacock2015combining} tag (maybe there could be), and you won be able to find its span.", 0.1766381710767746, 0.06303724646568298, 0.09686609357595444, 0.02834007889032364], ["I am trying to save / load data and objects in Python. I usually use pickle to save pandas data frame and custom objects. Recently I had to change python version (from 3.6 to 3.8) and pandas / pickle version accordingly. I now have trouble to read previous pickled version. I have found some ways to deal with that (ranging from using some pickle options to reloading / rewriting the data).\nHowever I would be interested in a more generic way to save data / objects that would be python / packages independant. Does such a thing exists (without adding to much weird dependencies) ?", "If you save your data as a .CSV file (depending on what your data looks like) you should'nt get dependencies problem\nIf your data cannot be saved as a csv I think using JSON could also be a solution", "Save data / objects without python / pandas / pickle dependencies?", 0.0, 0, 144, "\\begin{itemize}\n\\item Write your data in json or csv file.\nthen use Pandas to load and plot your file\n[here is a tutorial](https://www.tutorialspoint.com/python/file_io_in_python.htm)\nfor the example\n\"Save Data\"\nuse pickles.pickle.dump()\nLoad and Plot your json data\nYou can use the above link to learn about pickling and unpickling.I am a beginner in python and Pandas .\nso i am also learning new things and picklings.it might take some time and effort but after some research and work you can surely find the answer.and it can be very useful.i hope it helps you.Best of luck\nregards\nPankhuri\n1st year b.tech student\nJagannath University, India\nemail - pankhuiranjali27@gmail.co\nwebsite - www.pankku.xyz\n2. if you want to write data directly in sql then you could do it using sql_parse module\nhttps:stackoverflow.Com/questions/37706355/loading-csv-data-into-a-postgres-database-with-pyscopg2\nbest of\nLuck", 0.21463415026664734, 0.029556650668382645, 0.12682926654815674, 0.028901731595396996], ["ERROR: Failed building wheel for scikit-image\nFailed to build scikit-image\nERROR: Could not build wheels for scikit-image, which is required to install pyproject.toml-based projects", "Did you try to upgrade pip ( python -m pip install --upgrade pip )?\nTry installing numpy first too.", "Failed building wheel for scikit-image", 0.0, 0, 145, "As a quick fix, try:\n`pip install https://github.com/scikit/skimage/archive/5884b77a52549a9e291f6b68e3600c815f42b993.zip`\nAlternatively you could try building scikits-Image from source: https//github/ci/build/jobs/283945167", 0.21276596188545227, 0.04444444552063942, 0.1702127605676651, 0.05434782803058624], ["in google colaboratory using python, I am trying to load model to classify\nI am trying to load keras model using python to classify image, I am getting above error", "predict_classes is only available for the Sequential class.\nWith the Model class, you can use the predict method which will give you a vector of probabilities and then get the argmax of this vector (with np.argmax(y_pred1,axis=1)).", "AttributeError: 'Functional' object has no attribute 'predict_classes'", 0.0, 0, 146, "In `tensorflow-2.x`, the `predict()` function returns a `tf.Tensor` object, but you're trying a call to a non-existent method `classes()`. Try converting the result object to your desired class by doing something like this:\n`class_names = np.argmax(y_result, axis=1)`\nYou'll also have to use an appropriate loss function instead of `binary_crossentropy`.", 0.3529411852359772, 0.07999999821186066, 0.21568627655506134, 0.035353533923625946], ["In practice, using both [..., :2] and [:2] on np.array([1,2,3]) results in np.array([1,2]). Are there also cases where the result differs when you use an ellipsis like this on an array?", "np.arrays are designed to handle n-dimensional arrays, specified as [rows, columns]In the case of np.array([1, 2, 3]), [:2] and [:, :2] will yield the same result because our array input is 1-dimensional of shape [1, 3], e.g. with 1 row and 3 columns.\nIf we instead input np.array([[1,2,3], [4,5,6]]), e.g. a 2-dimensional array of shape [2, 3], this will change. On this array, if we, e.g., do [:1, :2] we will get array([[1, 2]]) because we are asking for everything up to the first (i.e. the 2nd since we count from zero) row and everything up to the second (i.e. the 3rd) column.\nHope this makes sense.", "What is the difference between using [..., :] and [:] on a numpy array?", 1.2000000476837158, 1, 147, "They are two ways to select a slice of an numpy ndarray.  The ellipisis means from the first element to the last element. : means start from one element and go to but not including the next element (or one past the end in case of a 1-element array).\nSo in your example, they are exactly equivalent.\nThere are also slice operators, `:` and `[...]`, which behave differently but produce the same results. See [the docs](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-slicing-and-index-selection)\nHere is some sample code and output:\n\\begin{itemize}\n  \\item [`[:2]` (array 2d)] (slice):\n    \\begin {code}[[1 4] [10 9]  [20  3]\n             [30   4]]  =>  [[1  4 3]]\n            [0    1] = [4, 6,  2], [5,7,8], 5\n        [6    7] =>   [ 7],  7\n         [8,9,12]= [9 8 0],   9\n          [7  ]=  []\n       [   0]=>  [],    0\n              [ ] = [] ,   3\n     [  13  =      ] ,  None\n           [        ]\n      [     0]=  False,   1\n               []      = True, None, [True, False]    # True is not None. False is\n                                          a tuple. In this case, you just get\n   the tuple value of False. This array contains a single boolean element,\n the Boolean value False\n[   22 ]=>[],      None   => []  (all None)    , None    => [None,None]     (0-d) array. None here means the value is  not found, which is a special case\n [...:2...]  -> [    3,        ],         =>    [ [False, True] ]  # Same as [...]\n => [[[True True False True True],\n                [ True  True   True      True]]]                    Same here as using :\n[[[1],3],None=>[[],     None]", 0.2857142984867096, 0.04597701132297516, 0.1371428519487381, 0.03689320385456085], ["I want to use an LSTM model to predict the future sales.\nThe data is like the table below.\n\n\n\n\ndate \u00a0 \u00a0 \u00a0\nstore\nfamily \u00a0 \u00a0\nsales\n\n\n\n\n01/01/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBEAUTY \u00a0 \u00a0\n1 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n\n\n\nFor each day, it has 50 stores.\nFor each store, it has different type of family (product). (They are all in perfect order, thank God).\nLast, for each type of family, it has its sales.\n\nHere is the problem.\nThe dimension of input of LSTM model is (Batch_Size, Sequence_Length, Input_Dimension). It is a 3D tensor.\nHowever, in my case, my Input_Dimension is 2D, which is (rows x columns)\nrows: number of rows in one day, which is 1782\ncolumns: number of features, which is 2 (store and family)\nIs there a good way to make my data into a shape which can be fed into a LSTM model?\nThanks a lot!", "The solution I came up with is to make the whole data in each day to be a long long long sequence.\nSo the dimension will be 1D, and can be fed into the LSTM model.\nBut I don't think this is the optimal solution.\nDoes anyone come up with better answer?\nAppreciate.", "How to feed a 4D tensor into LSTM model?", 0.0, 0, 148, "You can do that using Tensorflow, by reading the dataset as a tf.data.Dataset and then use the map function to do it. You have a lot of options with the Dataflow API to help you do this (like map and windowing). The dataset and the dataflows will generate a list of Tensors with shape (n_samples, feature_list, timestep). You then will use this information to build a L-ST-Lstm network. Please refer to this answer https://stackoverflow.com/a/66962172 to get started.", 0.24113474786281586, 0.0, 0.1418439745903015, 0.06103286147117615], ["I have a csv file with comments marked by '#'. I want to select only the table part from this and get it into a pandas dataframe. I can just check the '#' marks and the table header and delete them but it will not be dynamic enough. If the csv file is slightly changed it won't work.\nPlease help me figure out a way to extract only the table part from this csv file.", ".csv file can't have comment. Then you must delete comment-line manualy. Try start checking from end file, and stop if # in LINE and ';' not in LINE", "How to extract a table from a csv file generated by Database", 0.0, 0, 149, "\\begin{code}\nimport pandas as pd\nfrom io import StringIO\nwith open(\"test.csv\", \"r\") as infile:\n    # get the rows\ndata = pd.read_csv(StringIO(infile.next()), header=[1,2])\nprint(data)\n\\end{pre}", 0.09677419066429138, 0.0, 0.06451612710952759, 0.03225806728005409], ["I have a recurring issue when running even the simplest code using deepface.\nFor information I run it on python 3.9, with a M1 macbook air on OS Monterey 12.0.1\nI can't seem to find any information on how to resolve it, hope anyone can help !\nThank you very much in advance,\nPaul\nfrom deepface import DeepFace\nresult = DeepFace.verify(img1_path = \"photo1.jpg\", img2_path = \"photo2.jpg\")\nobj = DeepFace.analyze(img_path = \"photo1.jpg\", actions = ['age', 'gender', 'race', 'emotion'])", "I finally found a solution : underlying was an issue with tensor flow. I changed the version I had and replaced it with an M1-compatible version. It worked as intented", "Illegal Instruction : 4 when running deepface", 0.0, 0, 150, "It sounds like your version of Python is not compatible with the latest version, 0x2102, of the deep learning framework that is included with this tutorial. You can either download the version 2018b of that framework and use that instead of version deepFace (which you're already doing) or switch to Python 5.3 as described below. Python3 is an experimental version and is currently not production ready. I do not know if 6 or 7 will be any better. See the end of this message for links to get the ndarray  package which I believe will fix the problem you described. Also, please note that Python2 support will not be added to deepdream-python in the future. We are planning to replace it with an updated version soon. If you want to use deep dreaming, I suggest you download it for Python5 instead. Sorry for the inconvenience. https://github.com/chriskroepl/deepdreampy/issues/78\nhttps://stackoverflow.blog/25467481/thanks-for-the-feedback-heres-what-we-heard-and-are-doing-about-it/\nPlease read the installation instructions: https:/.github/c/install.md", 0.14150942862033844, 0.02857142873108387, 0.08490566164255142, 0.05755395442247391], ["if i wanted to train an lstm to predict the next date in a sequence of dates, how would i do that since lstm require a scaled value?\nexample of data:\n\n\n\n\ndate\nnext date\n\n\n\n\n2012-05-12\n2012-05-13\n\n\n2012-05-13\n2012-05-19\n\n\n2012-05-19\n2012-05-20\n\n\n2012-05-20\n2012-05-22\n\n\n2012-05-22\n2012-05-26\n\n\n2012-05-26\n2012-05-27\n\n\n2012-05-27\n2012-05-30\n\n\n2012-05-30\n2012-06-12\n\n\n2012-06-12\n2012-05-19\n\n\n2012-06-19\n2012-06-25", "You could hand over the date split into three inputs: One would then be the year, the other the month, and the last the day. While normalizing your inputs definitely makes sense, however I would not entirely agree with your \"LSTM requires\".\nDay and month are already limited to a range of values which can be scaled\n\nday (1 - 31)\nmonth (1 - 12)\n\nFor year you need to make an educated assumption based on your application. So that year can then also be transferred to a scaled value. Judging from your data, it might be that year is constant at 2012 and it is not needed to begin with.\n\nyear (2012 - 2013(?))\n\nNote: Ask yourself whether you give the neural network enough system information to be able to predict the next date - meaning, is there already enough of a pattern in your data? Otherwise you might end up training a random predictor.", "How to train a LSTM on a sequence of dates?", 1.2000000476837158, 2, 151, "You can use \\strong{minutes}, \\em{second} or \\textbf{secs} (depending on your resolution) as input for your \\begin{code}\nlstm\n\\end{} model.\nI believe that it would be better to consider one minute per instance as the default. That way you can make a \\textit{test} dataset (for e.g.)\nconsisting of: \\texttt{2 days 2 hours 5 minutes}. And you could do a fine tuning by testing 4800, 7240 and 960 seconds.", 0.24778760969638824, 0.01785714365541935, 0.1150442510843277, 0.0327380932867527], ["We can basically use databricks as intermediate but I'm stuck on the python script to replicate data from blob storage to azure my sql every 30 second we are using CSV file here.The script needs to store the csv's in current timestamps.", "There is no ready stream option for mysql in spark/databricks as it is not stream source/sink technology.\nYou can use in databricks writeStream .forEach(df) or .forEachBatch(df) option. This way it create temporary dataframe which you can save in place of your choice (so write to mysql).\nPersonally I would go for simple solution. In Azure Data Factory is enough to create two datasets (can be even without it) - one mysql, one blob and use pipeline with Copy activity to transfer data.", "Is there any way to replicate realtime streaming from azure blob storage to to azure my sql", 0.1973753273487091, 1, 152, "I think what you want to do here is to write a function which can replica data on a schedule, and then create a [trigger function](https://docs.microsoft.com/en-us/azure/stream-analytics/triggers-functions) to kick off the replication function on schedule.", 0.2153846174478531, 0.0, 0.10769230872392654, 0.035874441266059875], ["I have 2 environments:\nEnvironment #1:\n\nPython 3.7.5\nPandas 0.23.4\n\nEnvironment #2:\n\nPython 3.8.10\nPandas 1.3.4\n\nI have the same code in both versions, no modifications were made to it. However, I have this specific line of code which seems to be causing an issue/produces a different output:\ndf_result = pd.merge(df_l, df_r, left_on=left_on, right_on=right_on, how='inner', suffixes=suffixes)\ndf_l and df_r are just read Excel files. I checked them in debugger in both versions and they are completely the same, so that should be fine.\nAlso, the left_on, right_on and suffixes variables have exactly the same value in both environments (checked via debugger, as well).\nHowever, when the df_result gets generated by the merge function, in environment #1 (old Python, old Pandas) it produces a DataFrame with 16170 rows. In environment #2 (new Python, new Pandas) it produces a DataFrame with only 8249 rows.\nThe number of columns are the same, difference is only in number of rows.\nWhat is causing this behavior?\nHow do I make sure that the environment #2 (new Python, new Pandas) produces exactly the same output with 16170 rows as produced by environment #1 (old Python, old Pandas)?\nThank you.", "At the end the issue lied within new Pandas' approach to handle NaN values.\nWhile in the old Pandas the code changed the NaN values with <NA> (as string), in the new Pandas it just left it as nan (pd.nan type).\nI made sure to do df.fillna('<NA>', inplace=True) and it worked fine. The resulted DataFrame now has the same number of rows as produced by the old Pandas.", "Python - Old pandas merge results in more rows than new pandas", 1.2000000476837158, 1, 153, "It looks like there's an extra `Index` column from your data that'll throw off the inner join when you're creating your `df.index = left.columns + right.labels`. This is probably an artifact of the index column being defined in the dataframe (assuming the two datasets have different index columns, not the actual index). This may be a symptom of a larger bug with Panda 4.0 - it may not be fully released yet, and it doesn't support all of P1D'S functionality yet. You can try a merge in older Panders. P3D+ supports inner joins, but won'e get you the exact same result.", 0.20000000298023224, 0.02247191034257412, 0.13333334028720856, 0.03947368636727333], ["I updated my Python3 to Python 3.10. It still is showing Python 3.8 as my current version. but that's not the issue. My issue is that when I went to install the matplotlib package using pip install matplotlib, I got some errors. I also tried running pip3 install matplotlib. I got the following errors:\n\nWARNING: Retrying (Retry(total=4, connect=None, read=None,\nredirect=None, status=None)) after connection broken by\n'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection\nobject at 0x1057db7c0>: Failed to establish a new connection: [Errno\n8] nodename nor servname provided, or not known')':\n/simple/matplotlib/\nERROR: Could not find a version that satisfies the requirement\nmatplotlib (from versions: none) ERROR: No matching distribution found\nfor matplotlib\n\nThe I tried running /Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip and got the following error:\n\nDefaulting to user installation because normal site-packages is not\nwriteable.\nRequirement already up-to-date: pip in\n/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages\n(20.2.3)\n\nI don't get it. It wanted me to upgrade pip and then says it's already up to date?\nI just need the matplotlib module installed for my Python scripts.", "If you are trying to install matplotlib in your organisation laptop then your organsiation could be blocking the network to connect and download the package. This is one reason its showing retrying error message. You can try disconnecting vpn if you are connecting with any and retry installing it. This error is due to network issue only.", "Errors while installing matplotlib using pip install", 0.0, 0, 154, "It'll be really useful if you post the error text, in that way, we can easily identify what' the cause of the problem. If you want to have more details, you can refer to this link [Here](https://stackoverflow.com/questions/14287972/installing-matplotlibs-on-a-mac-with-pip).\nHowever, what you need to do is uninstalling the python interpreter and all the dependencies of matplotlib and reinstall it again.", 0.34375, 0.0476190485060215, 0.15625, 0.030821919441223145], ["Recently i was struggling trying to take the pixel values of a 3D volume (np array) using specific space coordinate of a STL object.\nThe STL object is spatially overlapped with the 3D volume but the latter has no coordinate and so i don't know how to pick pixel values corresponding to the STL coordinates.\nAny idea?", "If the STL object is truly in the 3d volume's coordinate space, then you can simply STL's coordinate as an index to lookup the value from the 3d array.  This lookup does nearest neighbor interpolation of the 3d image.  For better looking results you'd want to do linear (or even cubic) interpolation of the nearby pixels.\nIn most 3d imaging tasks, those coordinate spaces do not align.  So there is a transform to go from world space to 3d volume space.  But if all you have is a 3d numpy array, then there is no transformation information.\nUpdate:\nTo index into the 3d volume take the X, Y, Z coordinates of your point from the STL object and convert them into integer value I, J, K.  Then lookup in the numpy array using I,J,K as indices: np_array[K][J][I].  I think you have to reverse the order of the indices because of the array ordering numpy uses.\nWhen you way 3d array and the STL align in python, how are you showing that?  The original DICOM or Nifti certainly have world coordinate transformations in the metadata.", "Mapping values from NP ARRAY to STL", 0.0, 0, 155, "Since you don\u00b4t show how you have generated the Np Array, I will do it here:\nIf you use [Nifti](https://github.com/tatsu/niftitoolbox/wiki/Using-NiftiToolbox), you can use the function \\strong{nii2vtk}.\nThis will return the \\em{slice-by-slice} values for your 4D image. You can then use \\begin{code}\n<ImageRegionToIndex>\n\\end{Code} to get the slice indices you want. If you dont use Niftis, then you will have to do the slicing by hand. (and you wont get pixel-values, but only a list of indices)\nWith STl, you could for example use a contour of the shape to define a slice and get its indices from the vtk-code: \\sout{1st contours of 14, 2nd of...} (think, this was wrong, sorry)  This should work for a small volume: If your volumes are large, the list will be too long and will therefore crash the program. For large volumes, it would be necessary to create a binary image from your volumne first. This would create an 8-bit-image where each pixel represents a volume-point. Therefore the data would not be lost, and you would avoid the error. To get a single value from this image, use `v.getScalar()` instead of \\textbf{<SliceToVolumeIndex>.}", 0.3275434374809265, 0.04488778114318848, 0.13399504125118256, 0.046783626079559326], ["I have a certain number of datasets and I've given numbers to each of them as the names let's consider 20 datasets, so the names are 1.csv, 2.csv and so on.\nI'm trying to give an input, here the number(name of the dataset) so that my code reads and works on that dataset. How do I make that possible?\nI've done something like giving input and changing it into a string and using pandas read_csv(string+\".csv\") but the code's not working\nCan anyone help out?", "pandas read_csv(string+\".csv\")\nI have done this and it works, I had to change the integer to string first.", "Giving input such that it reads the exact dataset among the others tkinter", 1.2000000476837158, 0, 156, "The code below should work\n`path = input()`\nto check that you are getting the correct input.", 0.10810811072587967, 0.0, 0.10810811072587967, 0.06492950022220612], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "len(s) will return the lenght of the s variable\nlen will return the function itslelf. So if I do a=len, then I can do a(s). Of course, it is not recommended to do such thing as a=len.", "difference between \"function()\" and \"function\"", 0.0, 0, 157, "In Python, you \\strong{can} call a (nested) function by name.  It's equivalent to calling `f()` for the function `def f(): pass`.\nThere is no reason for using `()` when doing this. There \\em{is} a reason to use `()`.\nFor example, the following lines are the same:\n\\begin{itemize}\n  \\item `len([1])`\n    \\begin {code}", 0.2526315748691559, 0.02150537632405758, 0.12631578743457794, 0.03311258181929588], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In the second case you are directly calling the len method and will get the result, i.e. how many rows are in col1 in the df.\nIn the first you are giving the reference to the len function to the apply function.\nThis is a shortcut for df[\"col2\"] = df[\"col1\"].apply(lambda x: len(x))\nThis version you use if you want to make the behavior of a method flexible by letting the user of the method hand in the function to influence some part of an algorithm. Like here in the case with the apply method. Depending of the conents in the column you want to fill the new column with something, and here it was decided to fill this with the lengths of the content of other column.", "difference between \"function()\" and \"function\"", 0.0, 0, 158, "When calling a built-in Python function like `len`, you can use the function name without parentheses if there are no arguments:\n\\begin{itemize}\n  \\item `df['col'] = len(col)`\n    (but not `col = ...`!)\n   \\em{However, note that the builtin `func` is a special case. It needs parentheses, and only one argument. So: `x = func(y)`.} (I don't know why, this is just how it is.)\n        \\strong{But note this:}  `(func(x))` does NOT give the same result as `call(func, [x])`.  But you probably don\u2019t care about this specialty of `functools.func`.\n \\end{blockquote}", 0.2731277644634247, 0.02666666731238365, 0.13215859234333038, 0.032894738018512726], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In 1, the function len is being passed to a method called apply. That method presumably will apply the function len along the first axis (probably returning something like a list of lengths). In 2, the function len is being called directly, with an argument df[\"col2\"], presumably to get the length of the data frame.\nThe use in 1 is sometimes called a \"higher order function\", but in principle it's just passing a function to another function for it to use.", "difference between \"function()\" and \"function\"", 0.0, 0, 159, "Let's see:\n\\begin{itemize}\n[df[[\"name\"]].apply()](https://stackoverflow.com/questions/1339591/difference-between-apply-and-map) will apply function `df.apply()` to column names\nbut [df[[col_name]].__call__](http://pandas.pydata.org/pandas-docs/stable/indexing.html#calling-functions-by-columns-or-rows) call function on values. It returns Series. So [len](www.datatablez.io/operations/call_functions) is applied to each element of series. I think it'll be useful for you.", 0.2012578547000885, 0.012738853693008423, 0.138364776968956, 0.03999999910593033]]}