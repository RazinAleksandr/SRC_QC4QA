{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I was learning how to make graphs with python pandas. But I couldn't understand how this code works.\nfig , ax = plt.subplots( ) ax = tips[['total_bill','tip']].plot.hist(alpha=0.5, bins=20, ax=ax)\nI couldn't understand why the code words only when there is fig infront of ax.\nAlso I have no idea what 'ax=ax' means.\nI found everywhere but I couldn't find the answer...", "Pandas is using the library matplotlib to do the plotting. Try to read up a bit about how matploltib works, it will help you understand this code a bit.\nGenerally, plotting with matplotlib involves a figure and one or more axes. A figure can be thought of as a frame where multiple plots can be created inside. Each plot consists of an axes object which contains your x- and y-axis and so on.\nWith the command plt.subplots(), you create in a single function a figure object and one or more axes objects. If you pass no parameters to the function, just a single axes object will get created that is placed on the figure object. The figure and axes are returned as a tuple by the function in the form of (figure, axes). You are unpacking that tuple with the first line into the variable fig and ax.\nThen, when you call the plotting function on your pandas data, you tell the function on which axes object to do the plotting. This is what the parameter ax means in that function. So you are telling the function to use your axes object that your variable ax is assigned to by setting the parameter ax to ax (ax = ax).\nDoing ax = tips[['total_bill','tip']].plot... is redundant. The plotting function returns the axes object on which the plotting was performed by pandas. However, you are just overwriting your already existing axes with the returned axes, which in this case are the same object. This would only be needed if you don't pass the ax parameter to the plotting function, in which case pandas would create a brandnew figure and axes object for you and return the axes object in case you want to do any further tweaks to it.", "Why does python pandas need fix infront of ax to draw a graph?", 0.0, 0, 720, "`ax` is the name of the current axes. When you make a subplot, you can optionally assign a name to the axes you create. If you don' know what `fig` refers to, it's the parent plot that contains this sub plot. The line `plt.suplots()` is just a convenient way to create a plot object with a single line of code. You can create subplots any number of ways, and even use `pandas.DataFrame` objects to plot the data. For example:\n\\begin{itemize}\n  \\item `df = pd.read_csv('file.csv')`\n    \\begin {code}  fig,ax = df.plot()\n      \\end{code}.\n        `plot(df)` or `figure()` will create the plot, but won' t give you\n     the figure and axes objects. It will just return the result. In this\n example, `data` would be the `DataFrame`.\n \\strong{Note:} You should always use the variable name `x` for the x-axis and `y`\nfor the y-axix. They're not really \"axes\", they' re just objects that\ndefine a", 0.3956989347934723, 0.09503239393234253, 0.1806451678276062, 0.04957984760403633], ["Say I have two arrays X=[A,B,C] and Y=[D,E,F], where each element is a 3 by 3 matrix. I would like to make an array Z=[AD,BE,CF] without using for loop. What should I do?\nI have tried using np.tensordot(X,Y,axis=1) but it returns 9 products [[AD,AE,AF],[BD,BE,BF],[CD,CE,CF]]. the troublesome thing is that the matrix size for each element must be the same as the array length, say for 3 by 3 matrix, X and Y should have 3 elements each.", "It turns out the answer is incredibly simple. I just used np.matmul (X,Y) to achieve the result I wanted.", "How to do some tensor multiplication without using for loop in python?", 0.0, 0, 721, "You can use numpy.dot(np.matmul(x,y))\nHere is an example:\nimport numpy as np\nx = np [1,2,3]\ny =np [4,5,6]", 0.2083333283662796, 0.1304347813129425, 0.1666666716337204, 0.03448276221752167], ["I have a lot of categorical columns  and want  to convert values in those columns to numerical values so that I will be able to apply ML model.\nNow by data looks something like below.\nColumn 1- Good/bad/poor/not reported\ncolumn 2- Red/amber/green\ncolumn 3- 1/2/3\ncolumn 4- Yes/No\nNow I have already assigned numerical values of 1,2,3,4 to good, bad, poor, not reported in column 1 .\nSo, now can I give the same numerical values like 1,2,3 to red,green, amber etc in column 2 and in a similar fashion to other columns or will doing that confuse model when I implement it", "You can do this for some of the rated columns by using df[colname].map({})or LabelEncoder() .\nThey will change each categorical data to numbers, so there is a weight between them, which means if poor is one and good is 3, as you can see, there is a difference between them. You want the model to know it, but if it's just something like colors, you know there is no preference in colors, and green is no different from blue .so it is better not to use the same method and use get_dummies in pandas.", "Convert Categorical features to Numerical", 1.2000000476837158, 1, 722, "First, it's worth thinking about what you're trying to accomplish here.  I think it would be helpful to have the original data for us to answer this question. Second, I'm going to assume that you want to recode the categoric data into numeric data, but keep the data structure. If this is not the case, you can follow the steps I describe below, and then change the structure to a numeric column if that is your goal. (I also suggest reading the [Categoric vs. Numeric Data](https://scikit-learn.org/stable/modules/linear_model.html#categorically-vs-numerically) section of the Scikit Learn documentation for more background on this.)\n\\begin{itemize}\n  \\item Create a new column (or columns) that contains the numeric version of your original categorics. I am going assume here that there are four categories in each of these columns. You can do this by using the `cat2numer` function. This function takes a series of categories, one for each column, in the form of a list. Then, the function returns a column containing the re-coded categorizations. See [here](http://www.datas", 0.2947368323802948, 0.05653710290789604, 0.10526315867900848, 0.04368932545185089], ["I'm using a custom library to transform a 1D signal to a 2D representation. The output it's printed through plt.imshow(), used by a function inside of the library. I have the result but i don't want to save the picture locally. There is a way to get as a PIL image what is being used by plt.imshow?\nEDIT: The answer is yes, as @Davide_sd is pointing out ax.images[idx].get_array() can be used to retrieve the data", "You can use ax.images[idx].get_array() to retrieve the data, after which you can use it on PIL. ax is the axes where the image has been plotted. idx is the index of the image you are interested: if you have plotted a single image, then idx=0.", "Take the image printed with plt.imshow as a variable", 0.1973753273487091, 1, 723, "Try the following:\n\\begin{itemize}\n    \\item Open the imshow image with the plt package (in my case, I open the\nimage with an image called `im_image`)\n        \\strong{Code:}\n            `import matplotlib.pyplot as plt`\n             `plt.plot(x,y)`\n      \\em{Where, x and y are the 0th and 4th column of a matrix of size 300, 60} (these are my inputs)\n       \\begin {code} <IMAGE>\n           <a href=\"https://i.stack.imgur.com/GjD7F.png\" rel=\"nofollow noreferrer\"><IMAGE></a> `\n          \\end{code}`\n     \\textbf{Result:} < \\sout{https:}> https://imgur com/a/9NpXH6g  (the image is too big) < [https](https: //i .stack .imgur .com /a /9 Np X6 g) >\n         \\textit{Idea: Use plt to open a graph in the form of an imsave image, open it, use the values", 0.19540229439735413, 0.023255813866853714, 0.1149425283074379, 0.05511577054858208], ["I'm studying k-anonymization and the mondrian algorithm proposed by LeFevre. In it, LeFevre says that at one point in his algorithm, we have to choose a feature in the Dataframe depending on which feature has the largest range of normalized values.\nFor example, if I have the feature Age in my dataset with the values:\n[13, 15, 24, 30], I understand that the range is 13-30, but as soon as you make it normalized wouldn't it always be [0-1]?\nI know that the question seems strange, but I couldn't find anything on the internet nor on the paper itself that documented more what he meant.", "It depends on a normalization technique but yes. If we use min max it will always be between [0,1]. What you can do is split that variable into segments and the normalized your data. However you use minx-max normalization, the minimum value of that feature gets transformed into a 0, and the maximum value gets a 1. Maybe a\nmean normalization could give you a different result in that case.", "What is a normalized range of values?", 0.0, 0, 724, "It is the same as the [interquartile range](https://en.wikipedia.org/wiki/InterquARTile_range), which is just the difference between the third and first quartiles.", 0.1428571492433548, 0.0, 0.10204081982374191, 0.04137931391596794], ["I have two df's: one has a date in the first column: all dates of the last three years and the second column are names of participants, other columns are information.\nIn the second df, I have some dates on which we did tests in the first column, then second column the names again and more columns information.\nI would like to combine the two dateframes that in the first dataframe the information from the second will be added but for example if we did one test on 2-9-2020 and the same test for the same person on 16-9-2022 then from 2-9-202 until 16-9-2022 i want that variable and after that the other.\nI hope it's clear what i mean.\ni tried\ndata.merge(data_2, on='Date' & 'About')\nbut that is not possible to give two columns for on.", "With Python and Pandas, you can join on 2 variables by using something like:\ndf=pd.merge(df,df2,how=\"left\",on=['Date','About']) # can be how=\"left\" or \"inner\",\"right\",\"outer\"", "How do I combine two dataframes on two columns?", 0.0, 0, 725, "Try:\n`data[['About', 'Date']] = data_1[data['Date'].isin(df_test_dates)]`\nThe above assumes that all your data is in one data frame and that data and df_tests are both sorted by Date column. If your df is sorted and has the entire range, this should work.", 0.14814814925193787, 0.0, 0.07407407462596893, 0.025964953005313873], ["I'm currently working with a pandas data frame and need to save data via CSV for different categories.so I thought to maintain one CSV and add separate sheets to each category. As per my research via CSV, we can't save data for multiple sheets. is there any workaround for this? I need to keep the format as CSV(cant use excel)", "No.\nA CSV file is just a text file, it doesn't have a standard facility for \"multiple sheets\" like spreadsheet files do.\nYou could save each \"sheet\" as a separate file, but that's about it.", "Is there any workaround to save csv with multiple sheets in python", 1.2000000476837158, 3, 726, "You can use a dictionary as a default value for the key.\nFor example:\n\\begin{itemize}\nYou want to write a file with key `foo` with values `bar1` and `baz1`.\nThis is what your code will look like: `df.to_csv(outfile,index=False,header=True, index_label=foo,...)`\nAnd here is the result: <a href=\"https://i.stack.imgur.com/2g023.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nHere is a full example that writes 2 different files with the same keys and same values: https://www.geeksforgeeks.org/python-pandas-to-csv-with-multiple-outputs/\nIf you want one file for each key, you can change the `index_file` argument to `True`.", 0.2266666740179062, 0.013513513840734959, 0.11999999731779099, 0.05161182954907417], ["Thanks to everyone reading this.\nI'm a beginner to pytorch. I now have a .pt file and I wanna print the parameter's shape of this module. As I can see, it's a MLP model and the size of input layer is 168, hidden layer is 32 and output layer is 12.\nI tried torch.load() but it returned a dict and I don't know how to deal with it. Also, I wanna print the weight of input layer to hidden layer(that maybe a 168*32 matrix) but I don't know how to do that. Thanks for helping me!", "The state dictionary of does not contain any information about the structure of forward logic of its corresponding nn.Module. Without prior knowledge about it's content, you can't get which key of the dict contains the first layer of the module... it's possibly the first one but this method is rather limited if you want to beyond just the first layer. You can inspect the content of the nn.Module but you won't be able to extract much more from it, without having the actual nn.Module class at your disposal.", "How to print the model's parameters'shape and print the parameters while loading a .pt file?", 1.2000000476837158, 0, 727, "You can get the number of parameters using:\n\\begin{itemize}\n  \\item `torch._C._get_num_parameters`\n    [`torchtalk.cpp:1438`](https://github.com/pytorches/torchs/blob/master/libtorchantalk/src/tensor.inl.hpp#L150)\n     [`numpy.ufunc.reduce`](http://docs.scipy.org/doc/numpy/reference/generated/np.unique.html#numpy-unique) to get unique parameter names\n   \\strong{You can do this in a few different ways:}\n      \\begin {code}", 0.11249999701976776, 0.025316456332802773, 0.10000000149011612, 0.029177717864513397], ["I would like to create a CNN model in Python and I have organized my data in such a way that I have 100 csv files with different sizes (all of them have 141 colunms but some have 33 rows and others have 70 rows). All of those files can be categorized in 6 different categories. All the examples that I have seen so far for buiding a CNN model are using either just one dataset in pandas or using several images of the same size. So the question would be, Can I use my data for creating a CNN model in this fashion? If yes, Can anyone give me some tricks or/and tips of how to?\nThanks a lot in advance!\nI have seen some Tensorflow or PyTorch examples but I dont know how to use them with my data", "It depends on the reason that the data are separated in different files in the first place and what you want to achieve.\nIf each file contains observations for a different entity AND you want to predict observations about EACH specific known entity, you can build a model for each entity. In this case, the entities with more training data will of course have better results.\nStill, if the difference between those entities can be described with numerical values, depending on the exact problem, you can also try adding those to the training data and then concatenating everything. In this case, the added features will make the final classification to better classify the observations of each different entity without building 100 models. Note however that this could work only if the \"qualities\" of the entities actually affect the observations in some degree, otherwise (if the observations are randomly distributed amongst the entities) the results will probably be worse.\nIf however the observations of different entities are needed to train a model that works for any entity (including unknown ones), the data can be concatenated to a single table (pandas DataFrames were mentioned in the question) and then train your model with this combined dataset.", "Using several csv files of different sizes to build a CNN model in Python", 0.0, 0, 728, "You can use [pandarosa](https://github.com/Torsten-Fischer/panda_rosa), a library for Pandas to load images.  It is a wrapper around the PIL library, and it supports a variety of image formats, including `.jpg` and `.png`.\nYou can also use it for text files, which you can split on newline characters.\n\\begin{itemize}\nHere's a snippet of code that might help you:\nhttps: //gist.githubusercontent. com/nathanielmoore/51e57f7735f6081125a42c6e90c97965/raw/217672b6d4693e4d822687a064340954b3f4e130/CNN_Pandas.py\nThis is an example that uses the code to parse the 20,045 `.csv` files in a directory. It will create an array of images and then split it into `train_data` (7,501 images)", 0.24126984179019928, 0.03194888308644295, 0.13333334028720856, 0.03231292590498924], ["I have a set of images in which I need to detect which of them needs a perspective transform. The images might be plain documents or photos taken with phone cameras with perspective and I need to perform perspective transform on those. How can I detect which need perspective transform in opencv?\nI can do perspective transform, however, I'm not capable of detecting when an image needs to suffer a perspective transform.", "This could be a possible approach:\n\nTake a reference picture (which does not require a perspective transform).\nDefine four points of interest- (x1,y1) (x2,y2) (x3,y3) (x4,y4) in your reference image. Consider these points as your destination points.\nNow in every other image that you want to check if a perspective transform is necessary, you will detect the same points of interest in those images. Lets call them source points.\nNext you have to check if the source points match your destination points. Also you will have to check if the dimensions(width & height) match.\nIf neither of the two matches(the points or the dimension), there's a need for perspective transform.", "How to detect when an image needs perspective transform?", 0.0, 0, 729, "The best way to get the perspective transformation is to do it on all the images and store the results in a separate file. Then you can read the file to see which images need the transformation.", 0.2222222238779068, 0.0, 0.11764705926179886, 0.048746198415756226], ["I would like to change so True = False or more exact change so True = 0 and False = 1 is there a way to do this?\nI have a dataframe and would like to df.groupby('country',as_index=False).sum() and see how many False values there is in each country\nI have tried df['allowed'] = --df['allowed'] (allowed is the column with True and False values) to swap them but it didn't work", "Swapping booleans is easy with df[\"neg_allowed\"] = ~df['allowed']", "Is there a way to change True to False in python?", 1.2000000476837158, 0, 730, "Just use\n`df.iloc[0, 'allowed'].values[1] = -1`\nor\n`'allowed'.ilocal[index].value = [1, - 2]`", 0.23076923191547394, 0.0, 0.23076923191547394, 0.0517241433262825], ["What is the meaning of:\nshape=(1, 224, 224, 3)\nI mean what are all the values specifying given here for shape?", "When the shape is of length 4, it means that that you have a \"4D-tensor\". A 4D-tensor is a group of 3D-tensor. For instance if A is a 4D-tensor, A[0] is a 3D-tensor that is a the first element of this group. Here the first number 1 means that you group is only composed of one 3D-tensor. Then, you guess that a 3D-tensor is a group of 2D-tensor (also called matrices). Here your 3D-tensor is composed of 224 2D-tensors (the second number). Then each 2D-tensor is composed of 224 1D-tensors (vectors) of lenght 3.\nIn your particular case you can also (more simply) view your data as a group composed of one RGB image of size 224*224. Each pixel has 3 values (red, green, blue intensity).", "What are the 4 values passed in shape for ndarray in numPy?", 0.0, 0, 731, "`shape` is a tuple of ints (as opposed to a list of tuples).\nThe values are: 1 (rows), 5 (columns), and 6 (channels).  A 0-d array has 9 elements, and the first 8 are also 7-element tups.  The first item in a  shape tuple is just the number of rows.\nA shape of (5,) means  that there are  only  rows, no columns, etc.", 0.2926829159259796, 0.05911330133676529, 0.18536585569381714, 0.03920448198914528], ["I'm using colcon for creating ROS2 package. And I can't build any package because of error \"No module named 'numpy.core._multiarray_umath'\"\nwhen i do colcon build command, the terminal says next:\n`Original error was: No module named 'numpy.core._multiarray_umath'\nI've already tried update numpy\npip install numpy --upgrade\nIt didn't help(", "can you show your full terminal output?\nyou can also try with colcon ignore for the location you have got error and the colcon build", "Whet using colcon, I get an error \"No module named 'numpy.core._multiarray_umath' \"", 0.0, 0, 732, "You'll need to update the numpy package in order to get this to work.", 0.10000000149011612, 0.0, 0.10000000149011612, 0.0535714253783226], ["I have the data of banner clicks by minute.\nI have the following data: hour, minute, and was the banner clicked by someone in that minute. There are some other features (I omitted them in the example dataframe). I need to predict will be any clicks on banner for all following minutes of this hour.\nFor example I have data for the first 11 minutes of an hour.\n\n\n\n\nhour\nminute\nis_click\n\n\n\n\n1\n1\n0\n\n\n1\n2\n0\n\n\n1\n3\n1\n\n\n1\n4\n0\n\n\n1\n5\n1\n\n\n1\n6\n0\n\n\n1\n7\n0\n\n\n1\n8\n0\n\n\n1\n9\n1\n\n\n1\n10\n1\n\n\n1\n11\n0\n\n\n\n\nMy goal is to make prediction for 12, 13  ... 59, 60 minute.\nIt will be real-time model that makes predictions every minute using the latest data.\nFor example, I made the prediction at 18:00 for the next 59 minutes (until 18:59). Now it is 18:01 and I get the real data about  clicks at 18:00, so I want to make more precise prediction for following 58 minutes (from 18:02 to 18:59). And so on.\nMy idea was to mask-out the passed minutes with -1\nI created the example of 11 minutes.There are targets:\n\n\n\n\nminute\ntarget vector\n\n\n\n\n1\n-1  0  1  0  1  0  0  0  1  1  0\n\n\n2\n-1 -1 1 0 1 0 0 0 1 1 0\n\n\n3\n-1 -1 -1 0 1 0 0 0 1 1 0\n\n\n4\n-1 -1 -1 -1 1 0 0 0 1 1 0\n\n\n5\n-1 -1 -1 -1 -1 0 0 0 1 1 0\n\n\n6\n-1 -1 -1 -1 -1 -1 0 0 1 1 0\n\n\n7\n-1 -1 -1 -1 -1 -1 -1 0 1 1 0\n\n\n8\n-1 -1 -1 -1 -1 -1 -1 -1 1 1 0\n\n\n9\n-1 -1 -1 -1 -1 -1 -1 -1 -1 1 0\n\n\n10\n-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0\n\n\n11\n-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n\n\n\n\nHowever it seems strange to me to train a model to predict this mask values of -1. I think for neural network it will be not obvious that these -1 are just a padding.\nThe another idea was to use a current minute as a feature and ,therefore, to predict always the sequence of 60 - minute length and then cut the extra prediction. However, the input will have  different lengths anyway, so it does not solve the problem.\nSo how I should preprocess the data to use LSTM? Should I use described above padding so all vectors will be have the same length of 60? Is there any better solution?", "An RNN (or LSTM) will return an output for every input, as well as the final hidden state (and cell state for LSTM). So one possible solution: Pad your input of future minutes with with a different token and use an embedding layer with 3 embeddings (0, 1, 2 where 2 represents unseen value). For example, at timestep 3 the input = [0, 0, 1, 2, 2, 2,...2].\nAfter this goes through an embedding layer each token will mapped to some embedding dimension (e.g. 16) and this would be pass to the LSTM. So the input size for your LSTM would be 16 and the hidden size would be one (so that you get a scalar output for every timestep of the input). Then you pass this output through a sigmoid so each prediction is between 0 and 1 and use binary cross entropy between the predictions and targets as your loss function. Additionally, since you probably don't care how accurate the predictions are for the minutes you've already seen, you could ignore their contribution to the loss.", "How to train real-time LSTM with input and output of varying length?", 0.1973753273487091, 1, 733, "There is no easy way to do this. You need a very long sequence (in your example it would be 365*24*6, where 2 is the number of hours, etc). So if you want a 72-hour prediction, you would need 4096*7*15=193,520 sequences. This is not something you can handle with a single machine, at least not efficiently. For example on my machine the training of a LMNN takes ~1 hour for a sequence length of ~256. It would take ~32 days to fit the model for your case. And even if the problem is solved, then you need huge batch sizes to get good performance. So this is a problem that you have to think of as part of the solution. If you are using Keras, just use the Sequence model and define the output shape. Then you don't need any padding or masking, the models will take care of that.", 0.3421829044818878, 0.05341246351599693, 0.16519173979759216, 0.05863678827881813], ["TorchVision Detection models have a weights and a weights_backbone parameter. Does using pretrained weights imply that the model uses pretrained weights_backbone under the hood? I am training a RetinaNet model and um unsure which of the two options I should use and what the differences are.", "The difference is pretty simple: you can either choose to do transfer learning on the backbone only or on the whole network.\nRetinaNet from Torchvision has a Resnet50 backbone. You should be able to do both of:\n\nretinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1)\nretinanet_resnet50_fpn(backbone_weights=ResNet50_Weights.IMAGENET1K_V1)\n\nAs implied by their names, the backbone weights are different. The former were trained on COCO (object detection) while the later were trained on ImageNet (classification).\nTo answer your question, pretrained weights implies that the whole network, including backbone weights, are initialized. However, I don't think that it calls backbone_weights under the hood.", "TorchVision using pretrained weights for entire model vs backbone", 1.2000000476837158, 0, 734, "As you can see [here](https://github.com/pytorch/vision/blob/v0.4.1/src/main/torchescript_detections.py#L315), weights are the entire detector network. They're not just the weights of a back-bone, they'd be a much smaller network than the detecor itself. I'm not sure about the difference between weights vs weights back bone. If they don't have the same size, I would guess that weights is the whole detnet and weights bake is just a pre-trainged version of it.", 0.29145729541778564, 0.05076142027974129, 0.15075376629829407, 0.05444447696208954], ["Say I have m objects and I want to pick which n will be chosen (where m and n are both known). I could run multi-label classification and get the probability that each of the m is chosen and take the n most likely, but that ignores the correlation between items. I'm wondering if there is a modeling approach (ideally in Keras?) that considers the correlations.\nFor example, suppose a soccer team has 18 players and I'm trying to predict which 11 will start the next game. The 11 players who are individually most likely to start do not necessarily comprise the most likely group of 11 players to start. For instance, maybe the team has two goalkeepers, each of whom has a 50% chance of starting, but no configuration will start both of them.\nOne option is to predict the set of 11 directly, but that would be multiclass categorization problem with (18 choose 11) cases... Any thoughts on better routes?", "Seems kind of similar to a language model where you want to predict the most likely sentence. If you have the output probabilities for all words, you wouldn't just pick the n likeliest since the sentence would probably make no sense. Instead you condition it on the words you've already chosen.\nSo in your case, the input would include the already selected players. Each pass through the model you add the player with the highest output to the team. To increase the quality you may also want to use beam search, where you keep the best k teams each pass through.", "Multi-label classification predicting exactly n out of m options", 0.0, 0, 735, "Your question is about correlation, so you need to learn the dependencies between the labels. So you probably have to use some sort of graphical model, and there are several options. But it is not clear to me whether you are interested in the \\em{probability} of a specific label, or in a ranking, since there will always be uncertainty. In the latter case, you can simply use a [ranking loss](https://en.wikipedia.org/wiki/Ranking_loss). In both cases, the exact answer is also not possible to achieve, if you have too few samples.", 0.28140702843666077, 0.030456852167844772, 0.16080401837825775, 0.06741572171449661], ["I am using Pandas to Convert CSV to Parquet  and below is the code, it is straight Forward.\nimport pandas as pd\ndf = pd.read_csv('path/xxxx.csv')\nprint(df)\ndf.to_parquet('path/xxxx.parquet')\nProblem\nIn a String for Example :- David,Johnson. If there is a , getting error saying there is a problem in the data.\nIf i remove the , the CSV File is converting to Parquet.\nAny suggesions, need help\nThanks\nMadhu\nIf i remove the , the CSV File is converting to Parquet", "Do you need to keep comma in the name of the file? Otherwise you can do input='David,Johnson', output=input.replace(',','_'). I don't think it is generally a good practice to have comma in your file names.", "Pandas Converting CSV to Parquet - String having , not able to convert", 0.0, 0, 736, "You can use `readcsv` with `sep=','` option\nYou should change the default delimiter to comma.", 0.2222222238779068, 0.03846153989434242, 0.14814814925193787, 0.012658230029046535], ["i am getting this error upon running my code.\ntext = str(text.encode(\"utf-8\"))\nAttributeError: 'float' object has no attribute 'encode'\nI tried to convert my data into string using df['Translated_message']=df['Translated_message'].values.astype('string')\nbut that doesnt worked.", "Text is a float. Check to cast as str before encoding.", "Python, Twitter Sentiment analysis", 0.0, 0, 737, "You need to cast the data type of the column. If you have a float column, cast it to float. You can do it either like this\n`df[\"Translted_Message\"] = df[\"message\"]` or `df = pd.cast(df, df.message.dtype)`\nIf you don't do this, you'll get a value error.", 0.1269841343164444, 0.06557376682758331, 0.1269841343164444, 0.09080921858549118], ["I am attempting to subset a pandas DatFrame df with a list L that contains only the column names in the DataFrame that I am interested in.  The shape of df is (207, 8440) and the length of L is 6894.  When I subset my dataframe as df[L] (or df.loc[:, L]), I get a bizarre result.  The expected shape of the resultant DataFrame should be (207, 6894), but instead I get (207, 7092).\nIt seems that this should not even be possible. Can anyone explain this behavior?", "[moving from comment to answer]\nA pandas dataframe can have multiple columns with the exact same name. If this happens, passing a list of column names can return more columns than the size of the list.\nYou can check if the dataframe has duplicates in the column names using {col for col in df.columns if list(df.columns).count(col) > 1} This will return a set of every column that that comes up more than once.", "Subsetting pandas dataframe with list returns an apparently incorrectly sized resultant dataframe", 1.2000000476837158, 0, 738, "The reason for this is that `df.ix[:,[a for a in df_col_names]]` will return a `MultiIndex` with one level for each unique value in `L`. You can find more information in [this blog post](http://pandas.pydata.org/pandas-docs/dev/indexing.html#index-selection-vs-indexer)\nTo subset using `ix` you need to provide a tuple or list of column indices as the `index` argument, not a single index.\nThe following should work:\n`df[df_column_indexes]`", 0.3670886158943176, 0.07692307978868484, 0.15189872682094574, 0.039772726595401764], ["I am training a yolox model and using wandb (weight & biases library) to follow training evolution. My problem is that when I am loading wandb library (version 0.13.5) I get an error message, which is:\nwandb: ERROR Failed to sample metric: Not Supported\nThe surprising thing is that when I run the exact same code on google collab (that has the library version), it works perfectly (problem: can't have unlimited GPU access on collab). So I have to find out how to avoid this error.", "Engineer from W&B here! Would it be possible if you could share the console log so that we can find the line where the error originates.", "weights & biases : ERROR Failed to sample metric: Not Supported", 0.1973753273487091, 1, 739, "Error was because of different versions of wandbox and wandlib. When I updated the wandlab (from 1.30 to 2.00) it fixed the problem.", 0.19230769574642181, 0.0, 0.11538461595773697, 0.045454543083906174], ["I am trying to run a Streamlit app importing pickle files and a DataFrame. The pathfile for my script is :\n\n/Users/myname/Documents/Master2/Python/Final_Project/streamlit_app.py\n\nAnd the one for my DataFrame is:\n\n/Users/myname/Documents/Master2/Python/Final_Project/data/metabolic_syndrome.csv\n\nOne could reasonably argue  that I only need to specify df = pd.read_csv('data/df.csv') yet it does not work as the Streamlit app is unexpectedly not searching in its directory:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/myname/data/metabolic_syndrome.csv'\n\nHow can I manage to make the app look for the files in the good directory (the one where it is saved) without having to use absolute pathfiles ?", "In which directory are you standing when you are running your code?\nFrom your error message I would assume that you are standing in /Users/myname/ which makes python look for data as a subdirectory of /Users/myname/.\nBut if you first change directory to /Users/myname/Documents/Master2/Python/Final_Project and then run your code from there I think it would work.", "Streamlip app not searching files in the good directory", 0.0, 0, 740, "Just add the directory to the PYTHONPATH\nFor example\n`export P_PATH=\"/Users//Documents//Master//Python//Final Project//data\"`\nor\nyou can do the same on terminal with the command\n\\begin{code}\nsudo export PPATH=/Users/$USER/Library/PrivilegedHelperTools/\nexport PATH=$PPATH:$PATH:/Users\necho $PATH: `cat ~/library/privilege/helpertools/path`\n#/usr/local/sbin:/usr//bin/:/bin/:/bin\nI/lib/python2.7:/Library//Frameworks/PyQt5.framework/Versions/5:/opt/anaconda3/Scripts/:$PWD/../streamlib_script.sh:.\n$P_PYTH\npython3\n:/opt//anacondan/pkgs/ipykernel_launcher-5_5/1.9.1/venv/site-packages/pip-20.0-py3.8-macosx-10_15-x86_64.egg/pkg_resources/__init__.py:", 0.16470588743686676, 0.0357142873108387, 0.10588235408067703, 0.0363636314868927], ["I'm trying to find a way to search inside the uploaded files.\nIf a user uploads a pdf, CSV, word, etc... to the system, the user should be able to search inside the uploaded file with the keywords.\nIs there a way for that or a library?\nor\nmaybe should I save the file as a text inside a model and search from that?\nI will appreciate all kind of reccommendation.", "Well If you save the file text in the db and then search it seems to be a practical idea.\nBut I feel there mi8 be decrease in performance.\nOr maybe you If you upload the file in S3 bucket and use the presigned url to generate the file from the db once uploaded and then perform search operation.", "How to search inside an uploaded document?", 0.1973753273487091, 1, 741, "you can use this library to convert pdf to html\nhttps://github.com/alex-w/node-pdf-to-html", 0.10256410390138626, 0.0, 0.07692307978868484, 0.03749999776482582], ["I have two data frames that I want to merge on a same column name but the values can have different variations of a values.\nExamples. Variations of a value :\n\n\n\n\nVariations\n\n\n\n\nUSA\n\n\nUS\n\n\nUnited States\n\n\nUnited States of America\n\n\nThe United States of America\n\n\n\n\nAnd let's suppose the data frames as below:\ndf1 =\n\n\n\n\ncountry\ncolumn B\n\n\n\n\nIndia\nCell 2\n\n\nChina\nCell 4\n\n\nUnited States\nCell 2\n\n\nUK\nCell 4\n\n\n\n\ndf2 =\n\n\n\n\nCountry\nclm\n\n\n\n\nUSA\nval1\n\n\nCH\nval2\n\n\nIN\nval3\n\n\n\n\nNow how do I merge such that the United States is merged with USA?\nI have tried DataFrame merge but it merges only on the matched values of the column name.\nIs there a way to match the variations and merge the dataframes?", "Use .count to count how many times United States is stated in the list and then make an if command to see if united stated is listed more than once in the list. Do it to all of the other options and make a final if command to check if either any of them are in the list to output the value that you want.", "How to merge two dataframes with different variations of a column values?", 0.0, 0, 742, "You can use `fillna` and `str.contains` with a regex: `df.merge(df, on='Country', how='inner', suffixes=('_1', '_2'))`\n\\begin{itemize}\n  \\item `merge` the df with `on='country'` and set suffix values (you can change `'_1'` to any string)\n    \\em{Note: The `suffixes` argument was deprecated in Pandas 0.22 and is now an argument for the `pd.concat` method. [Source](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd..merge.html)}\n   \\strong{You can set as many suffix strings as you want, but I think you can only have one suffix for each merge.}\n \\begin {code}", 0.19512194395065308, 0.012345679104328156, 0.12195122241973877, 0.04651162400841713], ["To be more specific the error variance of the x value is half of the variance of error in y.\nI looked over sklearn and couldn't find a function which takes the error variance of x into account.", "for anyone who might find it useful,\nthe lecturer told us the answer, it required using DEMING REGRESSION", "How to use Linear regression when my **X** values are normally distributed?", 0.0, 0, 743, "\\strong{EDIT:}\n\\begin{itemize}\nCopy the below code and run it\n`import matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\nx = np.linspace(-1, 1) # 0-1\ny = 2*x - 3*np_randn(20) + 5 # -3 to 8\nplt.plot(x, y)  # plot the data\n# get the covariance matrix of y\ncov = (y-np._asarray(y).T).dot(np(cov_matrix=np.(np.-y.T)).T) `\nand then use it for regression\nb = lr(data=y,x=x)# use this line of code for linear regression using y and x\nprint(b) and print(f'y: {b}')\nif you want to see the plot use the code\nfrom skimage.io import show\nshow(pl)                  ` and for the graph.  It will be like the attached image\n<a href=\"https://i.stack.imgur.com/", 0.09090909361839294, 0.0, 0.06493506580591202, 0.05494505912065506], ["On a RFC model, I am trying to figure out how the feature importances change my classification when i am perturbing my data, like\nfeatures(no perturbation)= features(perturbed data)-features(perturbation)\nThen using the features(no perturbation) on my already fit model.\nDo you if it is possible to manually set or change the feature importances of an RFC model ? I tried looking for but no results.\nThank you.", "The general convention in scikit-learn code is that attributes that are inferred from your data / training end with _. feature_importances_ attributes respects that convention as well. They represent impurity-based importances and they are computed / inferred from your training set statistics.\nYou have the option to act on the weight you give to the different samples through sample_weight argument as well as weighting your classes through class_weight parameter.", "Random Forest Classifier: Set feature importances?", 1.2000000476837158, 0, 744, "You cannot change feature importance by yourself. Feature importance is the importance of a feature in the prediction process. You can't change it. It is not possible. If you have a model that is already trained, you can use the model and its feature weights and try to predict the new samples. But if you want to change something, there is no way.", 0.2222222238779068, 0.015037594363093376, 0.14814814925193787, 0.07820126414299011], ["I have a data file that I'm cleaning, and the source uses '--' to indicate missing data.\nI ultimately need to have this data field be either an integer or float.  But I am not sure how to remove the string.\nI specified the types in a type_dict statement before importing the csv file.\n6 of my 8 variables correctly came in as an integer or float.  Of course, the two that are still objects are the ones I need to fix.\nI've tried using the df = df.var.str.replace('--', '')\nI've tried using the df.var.fillna(df.var.mode().values[0], inplace=True)\n(and I wonder if I need to just change the values '0' to '--')\nMy presumption is that if I can empty those cells in some fashion, I can define the variable as an int/float.\nI'm sure I'm missing something really simple, have walked away and come back, but am just not figuring it out.", "OK, we figured out two options to make this work:\nsolution 1:\ndf = df.replace(r'^--$', np.nan, regex=True)\nsolution 2 (a simplified version of #1):\ndf = df.replace(r'--', np.nan)\nBoth gave the expected output of empty cells when I exported the csv into a spreadsheet.  And then when I reimported that intermediate file, I had floats instead of strings as expected.", "Replacing a string with NaN or 0", 0.0, 0, 745, "Try:\n`df['var'] = np.where(np.isnan(data), 10, data)`\nwhere `data` is the original data and `1` and  `np` are some functions, so you can replace it with whatever you want.", 0.14432989060878754, 0.0, 0.10309278219938278, 0.013888547196984291], ["I need to create a row if Current End date compared to Start date from next row are discontinuous by each Employee Number. The dataframe looks like this:\n\n\n\n\nEmployee Number\nStart Date\nEnd Date\n\n\n\n\n001\n1999-11-29\n2000-03-12\n\n\n001\n2000-03-13\n2001-06-30\n\n\n001\n2001-07-01\n2002-01-01\n\n\n002\n2000-09-18\n2000-10-05\n\n\n002\n2000-10-06\n2001-06-30\n\n\n002\n2004-05-01\n2005-12-31\n\n\n002\n2008-01-01\n2008-11-25\n\n\n\n\nA Continuous flag column needs to identify these discontinuous values:\n\n\n\n\nEmployee Number\nStart Date\nEnd Date\nContinuous Flag\nExplanation\n\n\n\n\n001\n1999-11-29\n2000-03-12\nY\n2000-03-13 is 1d after 2000-03-12\n\n\n001\n2000-03-13\n2001-06-30\nY\n2001-07-01 is 1d after 2001-06-30\n\n\n001\n2001-07-01\n2002-01-01\nNaN\nmissing 2023-01-01 End Date row\n\n\n002\n2000-09-18\n2000-10-05\nY\n2000-10-06 is 1d after 2000-10-05\n\n\n002\n2000-10-06\n2001-06-30\nN\n2004-05-01 is not 1d after 2001-06-30\n\n\n002\n2004-05-01\n2005-12-31\nN\n2008-01-01 is not 1d after 2005-12-31\n\n\n002\n2008-01-01\n2008-11-25\nNaN\nmissing 2023-01-01 End Date row\n\n\n\n\nThen, for those rows that are 'N', a row needs to be inserted with the discontinuous dates to make them continuous in between rows. If there is no next row, use '2023-01-01' by default. Here is the expected output:\n\n\n\n\nEmployee Number\nStart Date\nEnd Date\nContinuous Flag\n\n\n\n\n001\n1999-11-29\n2000-03-12\nY\n\n\n001\n2000-03-13\n2001-06-30\nY\n\n\n001\n2001-07-01\n2002-01-01\nY\n\n\n001\n2002-01-02\n2023-01-01\nNaN\n\n\n002\n2000-09-18\n2000-10-05\nY\n\n\n002\n2000-10-06\n2001-06-30\nY\n\n\n002\n2001-07-01\n2004-04-30\nY\n\n\n002\n2004-05-01\n2005-12-31\nY\n\n\n002\n2006-01-01\n2007-12-31\nY\n\n\n002\n2008-01-01\n2008-11-25\nY\n\n\n002\n2008-11-26\n2023-01-01\nNaN\n\n\n\n\nI tried idx for loop without success", "Plan A:  (Filling in gaps)\n\nCreate a table of all possible dates (in the desired range).  (This is easy to do on the fly in MariaDB by using a seq_..., but messier in MySQL.)\nSELECT ... FROM that-table-of-dates  LEFT JOIN your-table ON ...\n\nAs for filling in the gaps with values before (or after) the given hole. I don't understand the goals.\nPlan B:  (Simply discovering gaps)\nDo a \"self-join\" of the table with itself.  For this you must have consecutive ids.  Since you don't have such, I am not sure what to do.\nThen check whether the (end_date + INTERVAL 1 DAY) of one row matches the start_date of the 'next' row.\nPlan C:  (requires MySQL 8.0 or MariaDB 10.2)\nUse LAG() (or `LEAD() windowing functions to compare a value in one row to the previous (or next) row.\nThis may be the simplest way to set the \"continuous flag\".\nBe sure to check for discontinuity in EmployeeId as well as INTERVAL 1 DAY as mentioned above.", "Create row from previous and next rows if date are discontinuous", 0.1973753273487091, 1, 746, "You can use `shift` with `fillna` to get the previous row for each employee.  Then use the boolean comparison to decide whether or not to insert a new row.\n\\begin{itemize}\n    \\item `s = df['Start'].shift(1).fill('0')\n      s = s.eq(df['End']).replace('1', 'Y')`\n        This adds the missing rows from the beginning of the dataframe. For example: 0,0->1,Y,1->2,N,2->3,NaN,3->4, Y,4->5, N,5->6, NaN,6->7, etc. Note: It also replaces the '1s' with 'Yes'.\n       \\end{code} `\n     `    `   ` `  `        `      `     \\begin {code}\\begin\n          `         \\ `       ` \\\n                                   \\   ```\n         `                       ``` \\  \\`     ```    ```   ``\\\n           `            ```      \\ \\``  ```\\ \\```  ``````\\``\n             `          ```     `\\ \\    `````\\ ``", 0.23616236448287964, 0.014869888313114643, 0.11070110648870468, 0.022594911977648735], ["I am currently using pandas to read an \"output.csv\" file by specifying the filepath as :\ndf = pd.read_csv(r'C:\\users\\user1\\desktop\\project\\output.csv')\nWhile this works perfectly fine on my local machine, is there a way I can code this so anyone who runs the script can use it? I want to be able to hand this script to coworkers who have no knowledge of Python and have them be able to run it without manually changing the username in the path.\nI have tried using os.path to no avail:\ndf = pd.read_csv(os.path.dirname('output.csv'))\nSOLUTION: df = pd.read_csv('output.csv'). Simple, embarrassing, and a wonderful building block to learn from. Thank you all.", "If you're shipping out the output.csv in the same directory as the python script, you should be able to reference it directly pd.read_csv('output.csv').\nIf your need to get the full path + filename for the file, you should use os.path.abspath(__file__).\nFinally, if your output.csv is in a static location in all your coworkers computers and you need to get the username, you can use os.getlogin() and add it to the path.\nSo there's a bunch of solutions here depending on your exact problem.", "How can I specify a file without listing the entire file path?", 1.2000000476837158, 0, 747, "\\begin{code}\n# you can get your current working directory\ncwd = os._getcwdu()\n\\end{pre}", 0.1111111119389534, 0.01886792480945587, 0.07407407462596893, 0.0], ["I noticed that when I implement/serve some opencv drawing functions within flask, they are slower compared to when running just the opencv stuff and run imshow. I am thinking this might be due to the fact that when the application (flask) is started it serves as the parent thread which then creates child threads for each request/context and thus creates more cpu overhead for executing cv2 calls.\nIs it possible to serve flask app separately from the actual services the API is serving like cv2.putText() etc? If so, what is the better design for optimized cv2 calls?", "The solution we were able to come up is to make the opencv process pinned to one CPU. It improved the operation drastically. It might be the memory pipeline is now being utilized by single core and does avoid forking it to the other core.\npsutil.cpu_affinity()", "Opencv Drawing functions are slow when used within Flask", 1.2000000476837158, 0, 748, "The bottleneck is not the flask application, it is your network I/O. You have to understand that your image processing is very CPU intensive and will never be fast enough to do it on the client side. The solution is to download the image and send it back to your client. This can be done by using a REST API, which is what you are already doing. Once the request is received, you can parse the URL, get the file size, download it and then show it to user. Your client-side code will be very simple and you won't have any bottle neck on client's machine.", 0.2405063360929489, 0.03846153989434242, 0.17721518874168396, 0.09300675243139267], ["I have 2 pandas dataframes and trying to compare if 2 of their columns are equal then update the rest of the dataframe if not append the new data so concat or something like that .\ni tried this amongst other stuff\n\nif demand_history[['Pyramid Key','FCST_YR_PRD']] == azlog_3[['Pyramid Key','FCST_YR_PRD']]:\ndemand_history['DMD_ACTL_QTY'] ==azlog_3['DMD_ACTL_QTY']", "demand_hist_sku_date = demand_history['Pyramid Key'] + demand_history['FCST_YR_PRD']\nazlog_3_sku_date = azlog_3['Pyramid Key']+ azlog_3['FCST_YR_PRD']\ndemand_history.loc[demand_hist_sku_date.isin(azlog_3_sku_date), 'DMD_ACTL_QTY' ] = azlog_3['DMD_ACTL_QTY']", "how to compare if 2 columns in pandas dataframe are equal then update the rest of the dataframe", 0.0, 0, 749, "Try this:\n\\begin{itemize}\n    \\item Merge the two data frames on the `Pyamid_Key` column\n  \\begin {code}", 0.0317460335791111, 0.0, 0.0317460335791111, 0.04127989709377289], ["thinking about a problem\u2026 should you standardize two predictors that are already on the same scale (say kilograms) but may have different ranges? The model is a KNN\nI think you should because the model will give the predictor eith the higher range more importance in calculating distance", "It is better to standardize the data even though being on same scale. Standardizing would reduce the distance (specifically euclidean) that would help weights to not vary much from the point intial to them. Having huge seperated distance would rather have more calculation involved. Also distance calculation done in KNN requires feature values to scaling is always prefered.", "Standardize same-scale variables?", 0.0, 0, 750, "If the two variables are on a same unit scale then there is no need for standardization. If one of the variables has a different range then you can use the range of that variable as the standard deviation.", 0.1649484485387802, 0.0, 0.14432989060878754, 0.0755481943488121], ["Pandas not working in AWS GLUE 4.0 version:-\nI tried importing pandas in AWS Glue 4.0 but getting following error, pandas is working in AWS Glue 3.0 version but not in 4.0.\nModuleNotFoundError: No module named '_bz2'", "I have also encountered this issue and have contacted AWS support about it. It appears that it is an AWS issue and is happening to anyone who uses it. They are currently working on a fix.", "Pandas not working in AWS GLUE 4.0 version", 1.2000000476837158, 0, 751, "\\begin{itemize}\n\\item [The pandas version available in Glance is 0_22](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-transform.html)\nso it is incompatible with AWS-GLUE-3_0\nand as such will not work. You will need to install it manually. [A list of the compatible Panda versions is available here.](https:/aws/pandas/versions/)\n[To upgrade to the latest version of pandas, you will first need install pip.](http://pandas.pydata.org/docs/install.txt) Then you can upgrade the version in your glue workflow.  [You will also need this version to run a gluegen python version 2.x workflow.](//docs-awsawsgluedocsaws_gluegentools.pdf)  You can [download this zip file](http: //pythonarchive.python.net/dist/pypanda-0-24/) and extract it into a folder. Then [use it as follows](//www.geeksforgeeks. org/python-install-packages-from-zip", 0.15555556118488312, 0.01123595517128706, 0.10000000149011612, 0.03448276221752167], ["I try to run a script which starts like this:\nimport os, sys, subprocess, argparse, random, transposer, numpy, csv, scipy, gzip\nBUT, I got this error:\nImportError: cannot import name 'transpose'\nI work on slurm cluster. Should I install transposer? I work with conda as we don't have permission to install on cluster. But, there is no conda env for that.\nWould you please help on this? Thanks", "pip install transposer ? worked for me.", "ImportError: cannot import name 'transpose'", 0.0, 0, 752, "Install [numpy](https://anaconda.org/anacondanerd/numpy).\n`conda install -c anaconda numpy`\nThis command will install numpy in Anaconda environment.", 0.07692307978868484, 0.0, 0.07692307978868484, 0.07894737273454666], ["As an example,\nWe have two algorithms that utilize the same dataset and the same train and test data:\n1 - uses k-NN and returns the accuracy;\n2 -applies preprocessing before k-NN and adds a few more things, before returning the accuracy.\nAlthough the preprocessing \"is a part of\" algorithm number 2, I've been told that we cannot compare these two methods because the experiment's conditions have changed as a result of the preprocessing.\nGiven that the preprocessing is only exclusive to algorithm no. 2, I believe that the circumstances have not been altered.\nWhich statement is the correct one?", "It depends what you are comparing.\n\nif you compare the two methods \"with preprocessing allowed\", then you don't include the preprocessing in the experiment; and in principle you should test several (identical) queries;\n\nif you compare \"with no preprocessing allowed\", then include everything in the measurement.", "Does the preprocessing of one algorithm change the conditions of the experiment?", 0.1973753273487091, 1, 753, "The pre-processing isn't part, it'd be part-of. The two algorithm' are separate. So if you perform pre processing on algorithm 1, then use the results to determine the performance of algorithm2 (as in your example) then you're comparing apples to oranges.", 0.32258063554763794, 0.06593406945466995, 0.19354838132858276, 0.07441859692335129], ["For some odd reason when I do \u201cimport sklearn\u201d it says ModuleNotFound or something like that. Can anyone please help?\nI tried going online and using bash to fix it but still didn\u2019t work.", "open a  shell in the workspace with ctrl-shift-s\non mac command-shift-s command prompt and run this command, it will install scikit\n\npip install scikit-learn", "Import sklearn doesn\u2019t exist on my replit", 1.2000000476837158, 0, 754, "I am not sure how you're trying to import it, but you can install it using `pip install skimage`.\nThis will install the latest version of `sklearn` on your system.", 0.2666666805744171, 0.06896551698446274, 0.13333334028720856, 0.04543280601501465], ["I have two series of stock prices (containing date, ticker, open, high, low, close) and I'd like to know how to combine them to create a dataframe just like the way Yahoo!Finance does. Is it possible?\n\"Join and merge\" don't seem to work", "Use pd.concat([sr1, sr2], axis=1) if neither one of join and merge work.", "Combining series to create a dataframe", 0.5370495915412903, 3, 755, "You can use `pd.merge()` or `merge` from the `pandas.DataFrame` class.\nThe `left_on`, `right_index` and `how` parameters determine how the join will be done.", 0.2380952388048172, 0.05000000074505806, 0.1428571492433548, 0.0], ["I have trained and saved an xgboost regressor model in Jupyter Notebook (Google Colab) and tried to load it in my local machine without success. I have tried to save and load the model in multiple formats: .pkl using pickle library, .sav using joblib library or .json.\nWhen I load the model in VS Code, I get the following error:\n\nraise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: [10:56:21] ../src/c_api/c_api.cc:846: Check\nfailed: str[0] == '{' (\n\nWhat is the problem here?", "The issue was a mismatch between the two versions of xgboost when saving the model in Google Colab (xgboost version 0.9) and loading the model in my local Python environment (xgboost version 1.5.1).\nI managed to solve the problem by upgrading my xgboost package to the latest version (xgboost version 1.7.1) both on Google Colab and on my local Python environment. I resaved the model and re-loaded it using the newly saved file.\nNow the loading works well without any errors.\nI will leave my post here on Stackoverflow just in case it may be useful for someone else.", "XGBoost Error when saving and loading xgboost model using Pickle, JSON and JobLib", 1.2000000476837158, 1, 756, "The problem is that you are trying to run the XGboost with a Python version lower than 3.6.0. It seems that the library is not compatible with Python 2.7.x.", 0.18840579688549042, 0.014705882407724857, 0.11594203114509583, 0.0546875037252903], ["Help me, I am try to convert CSLA method from R to Python from this paper \"DOI 10.1186/s12953-016-0107-\" and R code available at \"https://github.com/tystan/clsa\".", "Thank you, I have solved my problem.", "How to use the continuous line segment algorithm (CSLA) method to substract baseline in Python?", 0.0, 0, 757, "You should have a look at the [numpy.polyfit](https:https%3a%2f%40%7enpydata.org%60numpy%5e%A0poly%0A) and [poly1d](http://docs.scipy.ndarray.io%E2%80) functions.\nIn this case, you need to provide the `fitted` array and the polynomial coefficients `coeffs`. The number of coefficients is 5, as stated in the paper. Then, the first coefficients are the intercept and it's a linear function. Finally, to obtain the baselines you have to subtract the fitted line from the original data.", 0.043478261679410934, 0.0, 0.043478261679410934, 0.033313971012830734], ["I have two lists of coordinates:\n\n[37.773972, -122.431297]\n\n\n[37.773972, -122.45]\n\nI want to create a list of tuples like so:\n\n[(37.773972, -122.431297), (37.773972, -122.45)]\n\nI've tried using zip but that merges the two.\nthanks", "list1 = [37.773972, -122.431297]\nlist2 = [37.773972, -122.45]\ntup=[tuple(list1),tuple(list2)]\nprint(tup)", "Create list of tuples from two lists", 0.0, 0, 758, "\\begin{code}\n>>> list1 = [3,4] * 10\nTraceback (most recent call last):\n  File \"<stdin>\", line 20, in <module>\nTypeError: can't multiply sequence by non-int of type 'list'\n\\end{pre}", 0.042553190141916275, 0.0, 0.042553190141916275, 0.052294693887233734], ["Ive a question about python plotting. How to make a frequency plot for example how to see some behaviours\u2019 frequency ( coded as True or False) based on time of the dat (coded as AM or PM)\nMy ideal plot will be like this but indicates frequency of some behaviour that varies by time of the dat", "Here is the code:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\nplt.rcParams[\"figure.autolayout\"] = True\nfig, ax = plt.subplots()\ndf = pd.DataFrame({'numbers': [2, 4, 1, 4, 3, 2, 1, 3, 2, 4]})\ndf['numbers'].value_counts().plot(ax=ax, kind='bar', xlabel='numbers', ylabel='frequency')\nplt.show()", "How to plot a frequency plot use python", 0.0, 0, 759, "You can use the `pandas.Series.value_counts()` method to get a dictionary with the frequency counts of each value in the Series.\nThen, you can plot it using the matplotlib `bar` plot, passing the dictionary as the first argument.", 0.18000000715255737, 0.020408162847161293, 0.11999999731779099, 0.03534187749028206], ["In a general case, is there a way in VSC that each time we launch the debugger we pre-load the session with objects coming from a file? For example, dataframes? Because each time each launches it has no objects at all and needs the running code to start creating the objects.\nSo the challenge is when we already have some files on hard-drive with heavy objects, but when we launch the debugg I can't tell it to just pre-load those objects stored in those files, say dataframes or lists or connection objects or whatever.\nTo be more specific. If there's a python code that has two sections of code:\n\nSection 1: Code we know works perfectly and makes heavy calculations to create some\nobjects\nSection 2: Code that takes those objects and performs operations. We want to debug this section. We also know no code line in this section interacts in any way with the code or stacks of Section1. It simply takes the objects created from section 1\n\nExample: Section 1 queries an enormous table and puts it as a dataframe, sorts it, filters, etc... Then Section 2 just needs that dataframe and makes some statistics.\nIs there a way that we just launch from section 2 and we load that dataframe we have stored in a csv? Or do we need to launch always from section 1, run the connection, get the dataframe from a query (which takes a lot of time) and then finally arrive to section 2 to start debugging?\nNote. I could just make a .py file having section 2 code, and hard-coding on it at the begging to just read the csv I have. But is there a fancier way to do this without having to make another .py file for debugging and manually writing code to it, and then debugging that .py file?\nThe question is: Launch VSC python debugger telling it to load python objects from files in folders, rather than launching the session with no objects. Waiting for the code to create them", "There is no way to convert csv files to python objects before debugging since all Python objects are in-memory.\nIf you don't want to set them in your code, I would suggest using an environment variable for it, and set it by adding \"env\" in your launch.json.", "VSC pre-load session with heavy objects from file instead of creating them on running code", 0.0, 0, 760, "You can load the data from your files into a data frame, just open the file in the console (for example: `df = pd.read_csv('data.csv')`) and use it in your code. Note that this won' t work for every case but for many. For a more complete answer, you should clarify what you want more clearly.", 0.24074074625968933, 0.03773584961891174, 0.12962962687015533, 0.018955392763018608], ["I've got multiple excels and I need a specific value but in each excel, the cell with the value changes position slightly. However, this value is always preceded by a generic description of it which remains constant in all excels.\nI was wondering if there was a way to ask Python to grab the value to the right of the element containing the string \"xxx\".", "try iterating over the excel files (I guess you loaded each as a separate pandas object?)\nsomehting like for df in [dataframe1, dataframe2...dataframeN].\nThen you could pick the column you need (if the column stays constant), e.g. - df['columnX'] and find which index it has:\ndf.index[df['columnX']==\"xxx\"].  Maybe will make sense to add .tolist() at the end, so that if \"xxx\" is a value that repeats more than once, you get all occurances in alist.\nThe last step would be too take the index+1 to get the value you want.\nHope it was helpful.\nIn general I would highly suggest to be more specific in your questions and provide code / examples.", "Is there a Python pandas function for retrieving a specific value of a dataframe based on its content?", 0.1973753273487091, 1, 761, "You can do this by using `pandas.Series.str.extract`:\n\\begin{itemize}\n^{Taken from [the pandas documentation](https://pandas-docs.github.io/pandas/api/generated/series.html?highlight=str%20extract#pandas._libs.tslibs.__series_parser.StringMethods.parse)}\n`pandas_string.read_excel(path_to_your_file, sheet_name=None)`\nthen\n `my_df = pandas_read.pandas__read__.pandas.__Series_parse__(myfile)` then\n do `print(mydf[0].str[1])`\nor\nuse: `pd.eval('my df[2]')`\nto get the 2nd element from your dataframe\n(if you know the column name)\nif not, you can use `df.columns` and then `list(df).index[x]` to get any column value. This is useful if you don't know which column has the x value\nthis will return a series\nand if the df is of type DataFrame and not a Series, then you need to use:\n   `dfs.loc[df_", 0.30651339888572693, 0.054054055362939835, 0.14559386670589447, 0.02925989404320717], ["I have pyarrow table which have column order ['A', 'B', 'C', 'D'] I want to change the order of this pyarrow table to ['B', 'D', 'C', 'A'] can we reorder pyarrows table like pandas dataframe ?", "cols = ['B', 'A']\ndf = df[cols]\n\n\n\n\nB\nA\n\n\n\n\n4\n1\n\n\n5\n2", "how to reorder columns in pyarrow table", 0.0, 0, 762, "You can use `table.reorder_columns()`\nhttps://arrow.apache.org/docs/python/generated/pyarrow/table/Table.html#pyarw.table-table", 0.0, 0.0, 0.0, 0.061224497854709625], ["If there's a 8G-RAM GPU, and has loaded a model that takes all the 8G RAM, is it possible to run multiple model prediction/inference in parallel?\nor you can only run a prediction at a same time period", "If your single model uses all 8gb of RAM, then it would not be possible to run another model in parallel using the same resources. You would have to allocate more memory or schedule the second model to run afterwards.", "Is there a way for a single GPU and model to run deep learning model prediction/inference in parallel", 0.0, 0, 763, "You can do this using TensorFlow'slayer API. The layers will be executed in different processes.\nYou can see an example here: https://www.tensorflow.org/tutorials/image\nAnd there is also a tutorial that explains how to do it in Keras: http://keras.io/examples/sequential_predict_generator/\nThe model can be loaded using different methods, but I prefer to use the `tf.saved_model.load` function.", 0.20370370149612427, 0.0, 0.12962962687015533, 0.04301075637340546], ["I am new to python and am struggling with this code. I have a csv file and am trying to create a function. The file, personal_info.csv , has a few columns with one labeled house_area and one named full_name. I am trying to create a code that will find the house with the largest area and return the name of person who owns it.\nI am also not allowed to import anything besides the csv file, so I cannot use pandas.\nHere's what some of the data looks like:\n\n\n\n\nhouse_area\nfull_name\n\n\n\n\n40.132\nJohn Smith\n\n\n85.832\nAnna Lee\n\n\n38.427\nEmma Jones\n\n\n\n\nSo in this small sample I'm trying to find the house with the largest area (85.832) and print the person's name, Anna Lee. Except the actual file has a lot more rows", "One simple way you can do this is by creating a variable called to track the largest house area. Let's call this largest_area and set it to the value 0 (assuming all house areas in your CSV are greater than 0).\nThen using, the csv library, go through each row in the CSV file, grab the house_area, and compare it to the largest_area variable you created. If it is greater, update largest_area, otherwise ignore and continue through the CSV file.\nAfter you have finished going through the CSV file, the greatest area should be in your largest_area variable.", "Finding a house with the largest area and returning who lives there (no pandas)", 0.0, 0, 764, "\\begin{code}\nimport csv\nfrom collections import Counter\nwith open('personal_data.txt', 'r') as datafile: # replace with full path\n    data = csv.reader(datafile, delimiter = ',')\nnext(csv.DictReader(next((data)), delimiter=' ')) # skip header row\narea_dict = dict()\nfor row in data:  # for each row of data\n   area = row['house area']\nif area not in area_dic: area += ''\nelse:area += '.'\ncount = Ccounter()  ###  counter for total count of areas\n# print(count)  --> output is [4, 3, ..]\nlist_of_areas = list(area for area in count) # extract the areas from the CCounter\nsorted_list = sorted(list(are_l), key=int)   # sort in descending order\nprint(sorted(house, key = area) )\n\\end{pre}", 0.21800947189331055, 0.028708133846521378, 0.12322274595499039, 0.05882338806986809], ["I am working on some data and i am required to carry out a person-fit statistical analysis in R.\nWhat is the python equivalent module (that can be imported into Jupyter notebook) of person-fit statistical analysis in R?\nI looked up google and saw goodness of fit but this is not the same as person-fit analysis", "Chuks, unfortunately as far as I know there isn't any direct equivalent in Python to person-fit stat analysis used in R.", "Python equivalent of Person-fit Statistics in R", 1.2000000476837158, 0, 765, "It seems like you want to use the `statsmodels` module to perform statistical tests. This is a very flexible module that will be able to do what you are looking for. To perform the tests, you can use `np.stats.kstest` to compute the K-S test:\n\\begin{itemize}\n    \\item https://www.geeksforgeeks.org/k-s-test-python-tutorial/\n      \\section{k_s_test() method} https:wwwgeeksforg.comkstatstemplate.txt\n     \\em{The k_stat() function computes the k-sample test for two independent\n samples of sizes N1 and N2. It is equivalent to the test of two\n population variances. A paired two-tailed test is performed. The\n results of the t-statistic and the p-value are returned.}\n       \\begin {code} [ <type 'str'> ]\n   [<type'float'>]\n           t_value: <float>\n         p_val:  < float >\n            \\end{code}[1](https://i.stack.imgur.c\nom", 0.07500000298023224, 0.0, 0.02500000037252903, 0.029999999329447746], ["Is it possible to use .resample() to take the last observation in a month of a weekly time series to create a monthly time series from the weekly time series?  I don't want to sum or average anything, just take the last observation of each month\nThank you.", "Is the 'week' field as week of year, a date  or other?\nIf it's a datetime, and you have datetime library imported , use  .dt.to_period('M')   on your current date column to create a new 'month' column, then get  the max date for each month to get the date to sample  ( if you only want the LAST date  in each month ? )\nLike max(df['MyDateField'])\nSomeone else is posting as I type this, so may have a better answer :)", "Using .resemple() in python to go from weekly to monthly time series", 0.0, 0, 766, "\\begin{code}\ndf.resampling(\"M\").resemble(df)\n\\end{pre}", 0.04444444552063942, 0.0, 0.04444444552063942, 0.03846153989434242], ["input:\nimport pandas\noutput:\n\nModuleNotFoundError: No module named 'pandas'\n\nI installed the package with the command line - pip3 install pandas,\nmy python version is 3.10.7\nThe source of the installed panda package:  c:\\users\\kfirs\\appdata\\local\\programs\\python\\python310\\lib\\site-packages", "not sure if this will help but in the command prompt type pip --version check to make sure you're on a version that is fluent with pandas, don't know much about pandas but I assume you should try and do the same with pandas. My knowledge is limited but try installing on the same drive as python as this could possibly be why things are not working. 3rd sometimes I have issues with windows after installing pip packs so try restarting your pc sometimes my imports right after installing don't work and this usually fixes but only if it's truly installed where it needs to be and the version needed. Hope I could help.", "I installed the pandas package and I'm still getting the same error: No module named 'pandas'", 0.0, 0, 767, "The problem is that you are using python3, which is not compatible with Python 2.\nYou should uninstall pandas from python2 and reinstall pandas with pip in python 6", 0.2312925159931183, 0.013793103396892548, 0.13605442643165588, 0.036764707416296005], ["I have a 2x2 matrix of distances from a depth sensor.\nThe matrix is cropped so only the points we are interested in is in the frame(All the points in the cropped image contains the object).\nMy question is how can we determine if this object is flat or not?\nThe depth image is acquired from Realsense d435. I read the depth image and then multiply it by depth_scale.\nThe object is recognized using AI for the rgb image that is aligned with the depth image.\nAnd I have 4 points on the object. So, all the distances in that rectangle contains the distance of the object from the sensor.\nMy first idea was standard deviation of all the points. But then this falls apart if the image is taken from an angle. (since the standard deviation won't be 0)\nFrom an angle the distance of a flat object is changing uniformly on the y axis. Maybe somehow, we can use this information?\nThe 2x2 matrix is a numpy array in python. Maybe there are some libraries which do this already.", "You can define a surface by choosing three of the four 3D points.\nEvaluate the distance from the remaining point to the surface.\nHow to choose the three points is... it may be good to choose the pattern that maximizes the area of the triangle.", "How to determine if an object is flat or not from depth image?", 0.0, 0, 768, "Well, it's not easy.  The best I can think of is to compute the average depth.  2D images are easy to analyze. They are either flat, or have an edge. The average distance from camera to flat objects, is the mean of depths, if all objects are flat. If the camera is not flat (has a tilt), the best average is from flat camera, and from above the objects. This can be computed by averaging the heights of objects and depth of camera. For flat surfaces, the sum of height and distance is zero, so the resulting average should be zero.", 0.2857142984867096, 0.013793103396892548, 0.20408163964748383, 0.08539924025535583], ["I am trying to better understand how two groups of documents relate to one another through topic modeling. I have performed similarity scoring on them and would like to try and peer deeper into how these documents relate through topic modeling. Rather than just observing the most relevant topics for each document using LDA, is there a method where I could have a model trained on both documents combined as a single corpus and visualize what topics have the most relevance to both documents combined?\nI tried just running LDA on a combined corpus but it returned topics that were clearly divided in relevance between the two different underlying documents of origin. Instead, I want to see what smaller topics the two documents overlap with the most.", "There's no one method for doing this in Gensim. But once you've trained a topic-model (such as LDA) on the combined corpus of all documents, you could do things like:\n\ncompare any two documents, by comparing their topics\ntally top-N topics for all documents in one of the original corpuses, and then top-N topics for all documents the 2nd original corpus, then contrast those counts\ntreat the original two corpuses as two giant composite documents, calculate the topics of those two synthetic documents, and compare their topics", "Is there a method in Gensim to find the most relevant topics between two corpuses?", 0.0, 0, 769, "Yes, there is a way to do this, it is called [topic overlap](https://radimrehurek.com/gensim/models/ldamodel.html#topic-overlap). The example on that page has `gaussian_noise=0.05`.\nYou can also look at `model.topics_overlaps` which is the overlap for the words of the topics, that is, the intersection of words in the 2 topics. You can get this as follows:\n\\begin{code}\nfrom gensim.models.lda import LdaModel\nmodel = Ldm(vocabulary=list(model_1.voccab), corpu2=corpus2, corputype=model1, gaussian=1)\nprint(list([topic.stop_words for topic in model.all_top_docs(max_nb_tokens=5)]))\n# [['topic 1', 'topic2', ...], ['topic1123', topic1345, ...]]\nfor topic, doc in enumerate(range(len(corpu1)), start=2):\n    print(f\"doc {", 0.27230048179626465, 0.01895734667778015, 0.13145539164543152, 0.03655913844704628], ["I am working on a project topic modeling tweets using the tweetopic Python library. I want to understand what the parameter \"n_components\" for the tweetopic.dmm.DMM class is. I see in the documentation it's described as the \"Number of mixture components in the model.\" I'm new to topic modeling, so am not quite sure what that means.\nThank you!\nHere is my code:\ntweetopic.dmm.DMM(n_components=10, n_iterations=100, alpha: float = 0.1, beta: float = 0.1)", "Tweetopic is like any other sklearn-compatible topic model. In all of sklearn's topic models you specify the number of topics with n_components.\nI might change the documentation so that this gets clearer. It says mixture components, because DMM is a mixture-model, meaning that it assumes that all texts come from a mixture of distributions, and each distribution (component) can be thought of as a topic.\nI hope I could be of help :)", "What is the 'n_components' parameter for tweetopic.dmm.DMM class?", 0.0, 0, 770, "As the name suggests, the `n` number of components means that there are `1+n+2+...+N` (N being the size of the vocabulary) number topics to be modeled. `alpha` and `beta` are the two hyperparameters of DMM. They control the probability of a word belonging to a topic and the variance of topic distribution.", 0.3206106722354889, 0.03100775182247162, 0.1679389327764511, 0.0310056209564209], ["I want to optimize four input parameters on a numerical model. I have an input file where I have these parameters. I run an application in Python using subprocess and obtain the results on csv files. I run these simulations around 300 times to have some Monte Carlo simulations, obtaining a range of possible values to compare with real data (20 points that follow a Weibull distribution) I have.\nWhich optimization algorithm can I use with the goodness of fit from the quartiles between numerical results and real data (this is the OF) to get optimal initial parameters?", "Regression models are fit on training data using linear regression and local search optimization algorithms.\nModels like linear regression and logistic regression are trained by least squares optimization, and this is the most efficient approach to finding coefficients that minimize error for these models.\nNevertheless, it is possible to use alternate optimization algorithms to fit a regression model to a training dataset. This can be a useful exercise to learn more about how regression functions and the central nature of optimization in applied machine learning. It may also be required for regression with data that does not meet the requirements of a least squares optimization procedure.", "Optimizing simulation input parameters to fit statistical data in Python", 0.0, 0, 771, "You could try using an optimization routine like [L-BFGS](https://www.johnzon.com/lbfgs/optimization.html) or [COBYLA](http://docs.scipy.org/doc/scikits/reference/cobyla.optimize.cobb.zip) (note the zip file).\nHowever, as you are running multiple simulations per optimization, it might be a bit of overkill to use an optimizer.  Instead, you could use [RANSAC](ftp://ftp.cs.ucd.ie/pub/Ransac/) which will do the same job but is much faster.", 0.16483516991138458, 0.02222222276031971, 0.10989011079072952, 0.041025642305612564], ["I am looking for guidance to generate multiple classifier training data from document. e.g. if particular document has three sections with each 10 pages in each sections. (total 30 pages)\nI am looking for open source library, where I can pass on document (explicitly specifying section 1, section 2 and section 3 pages) then it can give me list of important words to be used as training data to identify \"section 1\" vs \"section 2\" vs \"section 3\". (multiple classification)", "I had this quite a long time ago and I am not sure if it will help you at all but a book called \"Deep Learning with Python\" by Fran\u00e7ois Chollet 2018 could give you some clues in terms of how to generate such data samples from your document. However, the drawback might be that you would have to prepare such a document in a certain way before you can generate data samples. My comment is based on the fact that I have read something about it a long time ago so I could misremember it. Good luck!", "generating multi classifier training data from document", 1.2000000476837158, 0, 772, "[Apache OpenNLP](http://opennlp.apache.org) has a [chunking](https://github.com/apache/incubator-opennlp/tree/master/opentools/src/main/java/org/openxmlformats/schemas/spreadsheetml/x2006/cellx) function to determine the section of a document and has several useful [tools](ftp://ftp.open-source-encodings.info/pub/oowf-2.1.0/trunk/toolbox/) to do some useful tasks, including [extraction of named entities](HTTP://OPENNPTools.JOOMLA.COM/EN/extract-named-entities).  I'm not sure that it has all the functionality you need, but it seems to have a lot of useful functionality.\nYou could also take a look at [Pipeline](htpp://www.cs.jhu.edu/~lapata/code/pipeline/) for doing some of the preprocessing you'll want to use.", 0.23744292557239532, 0.018433179706335068, 0.13698630034923553, 0.052173912525177], ["One of my friend said I should study about GUI for turning on the webcam in that system, Is it correct? or any other solution.\nI made image detection system using opencv in python but its now switching on the camera, can anyone please tell what can be the issue", "Based on your description, an image input source is essential for an image detection system, and there are many methods to open an image as a source, like cv2.imread(), and camera image source is also acceptable in OpenCV, so the detection system turned on the camera is quite reasonable.\nThe suggestion about WebCam is good if you want to run code in the local but get photos in the remote, else it is unnecessary to use WebCam because OpenCV can use your local camera. A GUI is a choice but not a must.\nIf you don't want it to open your camera, but just read a picture in your local disk, then you can remove those codes in the system controlling the camera as an image input source and add some codes to make a change for the source from camera to your Disk.", "I made image detection system using opencv in python but its now switching on the camera, can anyone please tell what can be the issue?", 0.0, 0, 773, "It's quite difficult to troubleshoot without seeing your code, but if it' s using OpenCV, the most likely cause of your issue is that you're not importing the OpenCv library correctly. It should be:\n`import cv2 as cv`\nand not\n\\begin{code}\nimport Opencv as Open_C\ncv = cv.cv2\nprint(cv.__version__)\n#will give you Open CV version you are using\nfrom open_cv import cv\n    print(open_c.version)# will give Open Cv version\n        #you are importing\nopencv.load_video_reader()\nor\nvideo = opencV.VideoCapture(0) #will work only for 0 because it is the default\n         #Open C v camera\n      else:  # if you have opened any video camera in the previous session\n       openCV.video.open(camera_name)   # will open camera using the name of camera. Open c v video.camera. open\n          # Open cv video\n  else\n     openC v . videoCaptur e (0, 1)  // open cv camera 2 with width", 0.2760942876338959, 0.020338982343673706, 0.12794612348079681, 0.05662671849131584], ["I've got a Keras subclassed model in Tensorflow which stays at a constant GPU memory usage throughout an epoch, then when starting a new epoch it appears to be allocating a whole new set of memory for that epoch.\nIs this normal, expected behaviour?\nCurrently, I'm getting OOM on only my third epoch, and I'm not sure what sort of data needs to be retained after the previous epoch other than the loss. If it is expected behaviour, what large quantity data exactly does need to be retained (e.g. does Tensorflow need to store historic weights for some reason?)\nI've not included any code as I'm asking this as more of a general question about Tensorflow and CNN model behaviour.", "My instinct is that you might see increases in the first two epochs but you should generally have steady state after that.\nOff-handedly, you might want to compare weights between epochs and so get 2N memory that way.\nMaybe there's an out of control snapshot mechanism?", "Should GPU memory be increasing substantially after every epoch in Tensorflow?", 1.2000000476837158, 1, 774, "No, that is not the expected behavior. It is normal for the memory to increase in certain cases, but in your specific case, it seems that your program is allocing a lot of random access memory, which is probably not what you are expecting. The most common reason for this is that you have a bug in one of your functions that are calling `tf.Variable()` functions. In this case you should have some output that looks like this:\n<a href=\"https://i.stack.imgur.com/N1L6E.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nIn your case the \"current\" and \"peak\" memory used by the program are huge, while the allocated memory (which is what the peak memory is) is much smaller. So your function is likely to have an issue with memory management.", 0.19889502227306366, 0.03351955488324165, 0.12154696136713028, 0.05828635394573212], ["When I run this code on google colab.\nfrom google.cloud import aiplatform\nThe following error occurred\nImportError: cannot import name 'WKBWriter' from 'shapely.geos' (/usr/local/lib/python3.8/dist-packages/shapely/geos.py)\nDoes anyone know how to solve this problem?\nI was working fine on 2022/12/16, but today it is not working.", "Running into a similiar issue. So far, I am able to tell that google.cloud actions will not run if I have shapley files installed. When I delete the shapley files on my computer I am able to run google.cloud methods", "cannot import name 'WKBWriter' from 'shapely.geos' when import google cloud ai platform", 0.0, 0, 775, "Looks like you're using an older version of Shapley (0.5.1). If you upgrade to a newer version (the current version is 0:5:2) it should solve the problem.", 0.13513512909412384, 0.0, 0.054054055362939835, 0.0390625], ["I have a numpy array:\na = np.array([-1,2,3,-1,5,-2,2,9])\nI want to only keep values in the array which occurs more than 2 times, so the result should be:\na = np.array([-1,2,-1,2])\nIs there a way to do this only using numpy?\nI have a solution using a dictionary and dictionary filtering, but this is kind of slow, and I was wondering if there was a faster solution only using numpy.\nThanks !", "thanks a lot!\nAll answers solved the problem, but the solution from Matvey_coder3 was the fastest.\nKR", "Remove first occurence of elements in a numpy array", 0.0, 0, 776, "You can use [np.where](https://docs.scipy.org/doc/numpy-dev/reference/generated/ numpy .where.html)\n\\begin{code}\n>>> np = [10, 3, -3]\n# Create a mask\nmask = (np > 0) | (~np)  # 1 or 5\nnp =  np[mask] \nprint(np )\n[-1] # -1 is only element which meets the condition\n[2]    # only 4 elements\n>\n%timeit np[:1][mask.nonzero()]   # first element that is  >0 and nonzero\n0.253 s \u00b1 74.6 ms per loop (mean \u00b5s per iter) - 60 loops\nTime taken: 86.9 ms\nLargest loop index: int64\n1st run:1.097s\n2nd run : 996ms\n3rd run  :  980ms (27% improvement) -> \u03bb=0\n4th run   :   878ms(30% reduction)", 0.032786883413791656, 0.0, 0.032786883413791656, 0.021052632480859756], ["I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. Can someone please help me to resolve this issue?\nAttributeError: module 'numpy' has no attribute 'typeDict'", "Spacy still hasnt upgraded to latest numpy versions. I degraded numpy to 1.21 and that worked.", "AttributeError: module 'numpy' has no attribute 'typeDict'", 0.0996679961681366, 1, 777, "That's because you are using numpy, which is a separate library.\nI would suggest you to use theano.", 0.2222222238779068, 0.0, 0.1666666716337204, 0.025308066979050636], ["I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. Can someone please help me to resolve this issue?\nAttributeError: module 'numpy' has no attribute 'typeDict'", "I was able to solve this by upgrading the scipy package to 1.10.", "AttributeError: module 'numpy' has no attribute 'typeDict'", 0.0, 0, 778, "try installing numpy using pip.\npip install numpy\nIt worked for me\nI am using windows 10 and python 3.8\nHope this helps!\nAnd I have the numpy 2.1.0\nThanks\nKenny", 0.2083333283662796, 0.0, 0.1666666716337204, 0.01886792480945587], ["I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. Can someone please help me to resolve this issue?\nAttributeError: module 'numpy' has no attribute 'typeDict'", "You have to degrade your Numpy and pandas version, everything depends on the version that tensorflow supports.\nNo other solution for now", "AttributeError: module 'numpy' has no attribute 'typeDict'", 0.0, 0, 779, "You need to run the installation command in a python interpreter.\nFor example:\n\\begin{itemize}\n  \\item Installation for Python 3: `pip install tensorflow`\n    \\strong{or} `python3 -m pip install --upgrade tensorflow-gpu` to upgrade\n   pip\n \\end{blockquote}", 0.16949152946472168, 0.0, 0.1355932205915451, 0.060867246240377426], ["I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. Can someone please help me to resolve this issue?\nAttributeError: module 'numpy' has no attribute 'typeDict'", "I had the same issue. I restarted the kernel and the issue was gone. Try restarting your kernel if you have the correct version of tensorflow and numpy.", "AttributeError: module 'numpy' has no attribute 'typeDict'", 0.0, 0, 780, "Try installing it with pip\npip install tensorflow\nIf that doesnt work try using anaconda,\nconda install -c conda-forge tensorflow", 0.12244898080825806, 0.0, 0.12244898080825806, 0.08181817829608917], ["I have an excel file that I have imported into Python using pandas and it has two columns, purchase price and sales price. They are both number values. I want to use python to automatically do the math for me to find the difference between the two, in this case I want it to be Sales Price minus Purchase Price. Is it possible to write a script for this? Thanks in advance for any help.", "import pandas as pd\nRead the Excel file into a pandas DataFrame\ndf = pd.read_excel('file.xlsx')\nFind the difference between the two columns\ndf['difference'] = df['column1'] - df['column2']\nPrint the resulting DataFrame\nprint(df)", "Using Python to find the difference between two columns of numbers from an excel file", 0.0, 0, 781, "Yes, you can use pandas to calculate the differences between columns.  I've added some comments in the code below. Here is a sample of what your code could look like:\n\\begin{code}\nimport pandas as pd\n#Load the excel spreadsheet into a dataframe\ndf = pd.read_excel(\"file.xlsx\")\nprint(df)\ndiff = df.purchase_price - df['sales_prices']\nout = diff.values\nfor i in out: print(i) #This is to print the data\nresult = out.sum()\noutput = \"The difference is \"+str(result) + \"\\n\"\ndisplay(output)   #To output the result\nexit() #Or use sys.exit(0) if you want the script to exit when it's done\n    #Notice that the \"display\" function is included. It is called at the end of the program and will display the output you entered into the spread sheet. If you have any questions let me know!\n     #You can also use the print() function to see the results of your calculations\n         #And you could even put it into an if statement to only print out the", 0.25961539149284363, 0.13592232763767242, 0.20192307233810425, 0.02028203010559082], ["For a university project I\u2019m trying to see the relation oil production/consumption and crude oil price have on certain oil stocks, and I\u2019m a bit confused about how to sort this data.\nI basically have 4 datasets-\n-Oil production\n-Oil consumption\n-Crude oil price\n-Historical price of certain oil company stock\nIf I am trying to find a way these 4 tables relate, what is the recommended way of organizing the data? Should I manually combine all this data to a single Excel sheet (seems like the most straight-forward way) or is there a more efficient way to go about this.\nI am brand new to PyTorch and data, so I apologise if this is a very basic question. Also, the data can basically get infinitely larger, by adding data from additional countries, other stock indexes, etc. So is there a way I can organize the data so it\u2019s easy to add additional related data?\nFinally, I have the month-to-month values for certain data (eg: oil production), and day-to-day values for other data (eg: oil price). What is the best way I can adjust the data to make up for this discrepancy?\nThanks in advance!", "You can use pandas.DataFrame to create 4 dataframes for each dataset, then proceed with combining them in one dataframe by using merge", "How to forecast data based on variables from different datasets?", 0.0, 0, 782, "You can use a `pandas.DataFrame` to combine your data and make it easy for you to do the analysis. A `numpy.array` is also a good choice, but `df` will be better in your case.", 0.2711864411830902, 0.14035087823867798, 0.2711864411830902, 0.13265614211559296], ["My dataset looks as follows:\n\n\n\n\nCountry code\nValue\n\n\n\n\nIRL\n10\n\n\nIRL\n12\n\n\nIRL\n11\n\n\nFRA\n15\n\n\nFRA\n16\n\n\nIND\n9\n\n\nIND\n11\n\n\nUSA\n19\n\n\nUSA\n4\n\n\nHUN\n30\n\n\nHUN\n1\n\n\nHUN\n31\n\n\nHUN\n11\n\n\n\n\nI am attempting to extract rows with specific country codes using the .loc function, however this doesn't seem to work when multiple strings are added into the function.\nMy code looks as follows:\nsubset = df.loc[df[\"Country Code\"] == (\"IRL\", \"FRA\", \"IND\")]\nWhen I do this, my code doesn't return an error, but rather gives me an empty subset, so I am curious, what is wrong with my syntax, and what is my current code actually doing?", "df[\"Country Code\"] == (\"IRL\", \"FRA\", \"IND\") checks for equality between the tuple (\"IRL\", \"FRA\", \"IND\") and each item in the column Country Code - which is why it doesn't error out and would give you nothing (as none of the values in your column is a tuple).\nyou want to use pd.Series.isin i.e. df[\"Country Code\"].isin((\"IRL\", \"FRA\", \"IND\")) instead", "Syntax issue in creating subsets based on column contents pandas", 0.0, 0, 783, "In order to do a comparison using multiple conditions you need to use `in` in the `loc` method: `df.index.in([\"Ireland\", ...])`. In addition, the following syntax works: \\begin{code}\ndf[<condition>].index\n\\end{itemize}", 0.2222222238779068, 0.04123711213469505, 0.14141413569450378, 0.04736842215061188], ["As the title suggests, how can I obtain the feature importances from a OneVsRestClassifier model?\nI tried using model.feature_importances_ but the error message was\n\n\"OneVsRestClassifier' object has no attribute 'feature_importances_\"\n\nTried searching from the internet but was not able to find any clue.", "OneVsRestClassifier() basically builds as much binary classifiers as there are classes. Each has its own set of importances (assuming the base classifier supports them), showing the importance of features to distinguish a certain class from all others when generalizing on the train set. Those can be accessed with .estimators_[i].feature_importances_.\nAlternatively, you may study other sorts of feature importances, like sklearn.inspection.permutation_importance, which are universally applicable.", "Feature Importance from OneVsRestClassifier", 1.2000000476837158, 0, 784, "I am not sure what you are trying to do, but if you want to see the features that contributed most to the decision, you can do the following:\n\\begin{itemize}\n  \\item Load the model\n  2. Use `model.predict_proba`\n   3. `print(model)`\n     4. Find the `feature importences` attribute\n       5. Print the attribute value\n         6. Note that it is an array, so print the first element\n           7. Do the same for the second, third etc. element of the array\n            8. You will get the top features from your model that were\n               used to predict the probabilities. This should give you\n                the most important features for your classifier. (You\n                 can also do this by doing a `for` loop, with\n`i` as the index, and doing `if` statements)\n              You can find the class probabilties from `np.argmax(\npredictions, axis=1)` to get an idea of how important these features\nare. It is important to note that the order of these values are\nimportant. They should be in the descending order. If you", 0.17499999701976776, 0.0, 0.10833333432674408, 0.031608935445547104], ["I am trying to merge multiple dataframes to a master dataframe based on the columns in the master dataframes. For Example:\nMASTER DF:\n\n\n\n\nPO ID\nSales year\nName\nAcc year\n\n\n\n\n10\n1934\nxyz\n1834\n\n\n11\n1942\nabc\n1842\n\n\n\n\nSLAVE DF:\n\n\n\n\nPO ID\nYr\nAmount\nYear\n\n\n\n\n12\n1935\n365.2\n1839\n\n\n13\n1966\n253.9\n1855\n\n\n\n\nRESULTANT DF:\n\n\n\n\nPO ID\nSales Year\nAcc Year\n\n\n\n\n10\n1934\n1834\n\n\n11\n1942\n1842\n\n\n12\n1935\n1839\n\n\n13\n1966\n1855\n\n\n\n\nNotice how I have manually mapped columns (Sales Year-->Yr and Acc Year-->Year) since I know they are the same quantity, only the column names are different.\nI am trying to write some logic which can map them automatically based on some criteria (be it column names or the data type of that column) so that user does not need to map them manually.\nIf I map them by column name, both the columns have different names (Sales Year, Yr) and (Acc Year, Year). So to which column should the fourth column (Year) in the SLAVE DF be mapped in the MASTER DF?\nAnother way would be to map them based on their column values but again they are the same so cannot do that.\nThe logic should be able to map Yr to Sales Year and map Year to Acc Year automatically.\nAny idea/logic would be helpful.\nThanks in advance!", "Generally this is impossible as there is no solid/consistent factor by which we can map the columns.\nThat being said what one can do is use cosine similarity to calculate how similar one string (in this case the column name) is to other strings in another dataframe.\nSo in your case, we'll get 4 vectors for the first dataframe and 4 for the other one. Now calculate the cosine similarity between the first vector(PO ID) from the first dataframe and first vector from second dataframe (PO ID). This will return 100% as both the strings are same.\nFor each and every column, you'll get 4 confidence scores. Just pick the highest and map them.\nThat way you can get a makeshift logic through which you can map the column although there are loopholes in this logic too. But it is better than nothing as that way the number of columns to be mapped by the user will be less as compared to mapping them all manually.\nCheers!", "Automatically Map columns from one dataframe to another using pandas", 0.0, 0, 785, "You can use [`merge`](https://pandas.pydata.org/docs/reference/api/pandas.__merge__.html) with `on` parameter to use as column mapper: `df1.merge(df2, on='col_name', how='left')`.", 0.07960198819637299, 0.01005025114864111, 0.049751244485378265, 0.0363636314868927], ["I use Pyrosm for parsing *.osm.pbf files.\nOn their websites it says \"When should I use Pyrosm? However, pyrosm is better suited for situations where you want to fetch data for whole city or larger regions (even whole country).\"\nHowever when I try to parse to big .osm.pbf files, I get memory problems.\nIs there a solution for that, e.g. like chunking in pandas?\nOr do I need to split up the file, if yes, how?", "So my solution is to split the files up via osmium-tools.", "How to handle very big OSMdata with Pyrosm", 0.0, 0, 786, "You can use [pandas.io.parsers.read_table](https://pandas-docs.github.com/pandas/reference/api/generated/series.str.split.html) on the .pab file to create a list of OSMStrings, then you can parse each of those strings into a GeoJSON object using `geojson.from_json` and `osmapproxy.Client.create_geom`.  You should also be able to use `pandas` to group and sum the results as you go, to speed up your processing.", 0.09302325546741486, 0.0, 0.06976744532585144, 0.018518516793847084], ["df.hist() gives you histograms for all variables.\ndf.hist(column='age') gives you a histogram for just age.\nWhat if I want histograms for all variables except one? Do I have to do them separately? And what's the difference between using df.hist() and the Matplotlib version anyway?", "Save the column that you want to exclude in a variable:\nexclude = [\"age\"]\nAnd the plot the histogram accordingly:\ndf.loc[:, df.columns.difference(exclude)].hist(figsize=(15, 10));\nThis should solve your problem.", "How do I create histograms for all variables except one in Python?", 1.2000000476837158, 0, 787, "Just make sure the variable name is not in the list of columns:\n`df[['col1', 'col2', ..., 'exclude_col']].histo()`\nThis is the same as doing `df['excluded_column'].hits()` by itself.", 0.3384615480899811, 0.0, 0.1846153885126114, 0.06917683035135269], ["I have created an environment and have python and all other packages installed in it. Openssl is also available when I check using conda list. But unfortunately, I realized pytorch is missing when I check the list of installed packages. When I try to download the pytorch I get the following error.\nCondaSSLError: Encountered an SSL error. Most likely a certificate verification issue.\nException: HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/main/win-64/current_repodata.json (Caused by SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\"))", "I think this problem is related to ssl updates\nrun the below code in terminal and try again;\n\nconda update --all --no-deps certifi", "Error with openssl when trying to install pytorch", 0.0, 0, 788, "There are two things to do here:\n\\begin{itemize}\n \\item Add python 3.8 and openshift 2.0.2 to the requirements.txt file and install them. It is advisable to use the cona-forge channel for installing these packages, you can use conan-build-info to create the required requirements file. I am not sure how you are using the python or openssh, but if you use Anaconda, add the below line to your requirements: `open-ssl=3.*,3*`\n  \\begin {code}", 0.15094339847564697, 0.01923076994717121, 0.09433962404727936, 0.009174310602247715], ["I have a dataframe that has an animals column with different animals (say [\"cat\", \"dog\", \"lion\"]) as rows and a value corresponding to each animal. There are 10 unique animals and 50 entries of each. The animals are not in any particular order.\nI want to split the dataframe into two with one containing 40 of each animal and another containing 10 of each animal. That is one dataframe should contain 40 cats, 40 dogs etc and the other dataframe with 10 cats, 10 dogs etc.\nAny help would be greatly appreciated.\nI have tried to sort by unique values but it did not work. I am not very familiar with Pandas yet and this is the first time I am using it.\nEdit:\nAdding a better example of what I need\n\n\n\n\nAnimal\nvalue\n\n\n\n\ndog\n12\n\n\ncat\n14\n\n\ndog\n10\n\n\ncat\n40\n\n\ndog\n90\n\n\ndog\n80\n\n\ncat\n30\n\n\ndog\n20\n\n\ncat\n20\n\n\ncat\n23\n\n\n\n\nI want to separate this into 2 data frames. In this example the first dataframe would have 3 of each animal and the other one would have 2 of each animal.\n\n\n\n\nAnimal\nvalue\n\n\n\n\ndog\n12\n\n\ndog\n10\n\n\ndog\n90\n\n\ncat\n14\n\n\ncat\n40\n\n\ncat\n30\n\n\n\n\n\n\n\nAnimal\nvalue\n\n\n\n\ndog\n80\n\n\ndog\n20\n\n\ncat\n20\n\n\ncat\n23", "Does this work? df.groupby('animal', group_keys=False).apply(lambda x: x.sample(frac=0.2))  You could then remove these rows from your original dataframe to create the one with 40 of each animal.", "Split dataframe based on number of rows with a column value", 0.0996679961681366, 2, 789, "\\begin{code}\n# Create dataframe\nimport pandas as pd\ndf = pd.DataFrame({'Anmial':['cat', 'dog',  'liono']})\n\\end{pre}", 0.11320754885673523, 0.0, 0.07547169923782349, 0.057681355625391006], ["I read somewhere suggesting that in case there are multiple features(multi linear model) no feature scaling is needed because co-efficient takes care of that.\nBut for single feature(simple linear model); feature scaling is needed.\nIs this how python scikilt learn works or I read something wrong?\nNeed answer from someone who has tested both with and without feature scaling in simple linear regression", "Scaling is used when we want to scale the features in a particular range. In particular algorithms, the model will be sensitive to outliers so it is recommended to scale the features in a particular range. Algorithms like distance-based need feature scale. It also depends on data not in particular for any dataset such as multiple linear regression or linear regression. Sometimes features scaling is not recommended as the data points will shift from a particular range to a normal distribution range as it will lead to an impact on modelling.", "Simple linear regressions vs multiple linear regression model scaling", 1.2000000476837158, 0, 790, "There are some subtle points to be aware of here. The key point to keep in mind is that linear models are linear functions of the feature variables. In particular, it doesn't matter what units you use to measure the features, or how the scales are defined. That is, you can have a linear relationship between, say, inches and feet, as long as the units are the same. So in this sense, the scaling of a feature variable doesn\u2019t really matter. However, this doesn\u2018t mean you don\u2019 t need to normalize your feature data before fitting the model. Let\u2019s consider an example of what could happen if you do not normalized your data. Suppose we have two features: a height variable and a weight variable. We want to predict weight using height as a predictor. Now let\u2019 s say we want the scale of our height variables to have units of inches, and the weight scale to use units in pounds. When we normalise our data, we are saying that we will divide each of these values by the appropriate scale factor. For instance, if our scale for height is in inches but our weight is scaled in units pounds, then we would normalised our feature by dividing the height scale by 1", 0.31715211272239685, 0.04560260474681854, 0.12944984436035156, 0.06812109053134918], ["I'm having a problem with a pytorch-ignite classification model. The code is quite long, so I'd like to first ask if anyone can explain this behavior in theory.\nI am doing many classifications in a row. In each iteration, I select a subset of my data randomly and perform classification. My results were quite poor (accuracy ~ 0.6). I realized that in each iteration my training dataset is not balanced. I have a lot more class 0 data than class 1; so in a random selection, there tends to be more data from class 0.\nSo, I modified the selection procedure: I randomly select a N data points from class 1, then select N data points from class 0, then concatenate these two together  (so the label order is like [1111111100000000] ). Finally, I shuffle this list to mix the labels before feeding it to the network.\nThe problem is, with this new data selection, my gpu runs out of memory within seconds. This was odd since with the first data selectin policy the code ran for tens of hours.\nI retraced my steps: Turns out, if I do not shuffle my data in the end, meaning, if I keep the [1111111100000000] order, all is well. If I do shuffle the data, I need to reduce my batch_size by a factor of 5 or more so the code doesn't crash due to running out of gpu memory.\nAny idea what is happening here? Is this to be expected?", "I found the solution to my problem. But I don't really understand the details of why it works:\nWhen trying to choose a batch_size at first, I chose 90. 64 was slow, I was worried 128 was going to be too large, and a quick googling let me to believe keeping to powers of 2 shouldn't matter much.\nTurns out, it does matter! At least, when your classification training data is balanced. As soon as I changed my batch_size to a power of 2, there was no memory overflow. In fact, I ran the whole thing on a batch_size of 128 and there was no problem :)", "Data shuffling changes gpu memory use drastically", 1.2000000476837158, 0, 791, "A possible reason why you are having memory issues is due the memory layout of the GPU memory, in particular the order of loading data. You have to make sure that the output of one shader is loaded after the input of another one, otherwise the two shaders will access the same data and that would lead to memory corruption. As for the cause of your data imbalance, it's likely that you have the problem that is known as \"class imabalance\" (which is different than what you think it is). Check out this article for more information about class imblance: https://medium.com/pytorch/how-to-prevent-class-imbalanced-data-in-pytorch\nHope this helps!", 0.21145375072956085, 0.0, 0.11453744769096375, 0.03125], ["What is the difference between flip() and flipud() in NumPy?\nBoth functions do the same things so which one should I use?", "flipud can only flip an array along the vertical axis and flip will flip along a given axis. Very similiar.", "What is the difference between flip() and flipud() in NumPy?", -0.0996679961681366, -1, 792, "I assume that you are referring to the `flip` and `numpy.flopy` methods.\n`numpy` also has the method `ndarray.T` which returns the array in transposed form. You can use this to transpose an array. For example:\n\\begin{code}\nimport numpy as np\na = np.array([[1, 2], [3,4]])\nprint(a)\n# [[1 3]\n #  [2 4]]\nb = a.transpose()\nc = b.flatten()  # flatten array of shape (2,1) into a single array\nd = c.reshape(2)  ## reshape (1,-1), to (n,-) where n = 1\ne = d.argsort()     ## sort along the last dimension (last axis) and return it as a tuple\nf = e.astype(int)    ## convert to int type and use as the index to a list\ng = [x[f] for x in d]    # now this is an indexed array, not a numpy array anymore\nh = g.shape   # shape of the list", 0.11180124431848526, 0.03773584961891174, 0.09937888383865356, 0.06666666269302368], ["I have the following dataframe:\n\n\n\n\ncountry\ncoin\n\n\n\n\nUSA\ncoin1\n\n\nUSA\ncoin2\n\n\nMexico\ncoin3\n\n\n\n\nEach coin is unique, and it can change the country. For example:\n\n\n\n\ncountry\ncoin\n\n\n\n\nUSA\ncoin1\n\n\nMexico\ncoin2\n\n\nMexico\ncoin3\n\n\n\n\nWhat I'm trying to find is a way to see which lines have changed. My desired output:\n\n\n\n\ncountry\ncoin\n\n\n\n\nMexico\nCoin2", "You could use concat to combine them, and then use drop_duplicates to get the difference. For example:\nconcat([df1,df2]).drop_duplicates(keep=False)\nEDIT:\nTo get just the one row, you can get the negation of everything common between the two dataframes by turning applying list to them and using .isin to find commonalities.\ndf1[~df1.apply(list,1).isin(df2.apply(list,1))]", "Get the differences from two dataframes", 1.2000000476837158, 2, 793, "You can use pandas groupby.\nFor example\ndf_a = df.groupby('country')\ndfr_b = (df.copy())\nfor col_name in dfr.columns: df_c = dfra_ b[col_nam]\nIf you want to keep only one group, you can do this: dfa_ a = pd.concat(dfa_ for col in dfa.keys())", 0.17241379618644714, 0.035087719559669495, 0.13793103396892548, 0.06794048845767975], ["So I created a machine learning model to make predictions on future output at work. So far its 97% accurate.\nI wanted to predict the output using the date along with 2 other inputs and since you can't use datetime directly in regression models.\nI converted the date column using ordinal encoding, will I then be able to use the date as an input then?\nOr is there a better method?", "Ordinal encoding is't the best approach for handling date/time data, especially if in your data occurs seasonality or trends. Depending on your problem, you could extract a lot of different features from dates, e.q:\n\nyear, month, day ....\nhour, minute, second ....\nday of week\nseason\nholiday\netc ...\n\nWhat should you use exactly highly depends on your problem, you should first investigate your data, maybe plot your predicted variable against dates and search for patterns which can help you then achieve best prediction results.", "Machine Learning predictions using dates", 0.0, 0, 794, "I'm not sure I understand what you're asking, but if you want to have date and time as a feature, then you should have it as numeric, not as string. For example, if your data is in a csv file, you may want it to be like this:\n`2021-010-12 14:50:00`\nNot like that: `22-Jan-23 04.52.05 PM`", 0.22516556084156036, 0.02684563770890236, 0.10596026480197906, 0.03747820854187012], ["Assuming that I have monthly datasets showing like these:\ndf1\n\n\n\n\ncompany\ndate\nact_call\nact_visit\npo\n\n\n\n\nA\n2022-10-01\nYes\nNo\nNo\n\n\nB\n2022-10-01\nYes\nNo\nYes\n\n\nC\n2022-10-01\nNo\nNo\nNo\n\n\nB\n2022-10-02\nNo\nYes\nNo\n\n\nA\n2022-10-02\nNo\nYes\nYes\n\n\n\n\ndf2\n\n\n\n\ncompany\ndate\nact_call\nact_visit\npo\n\n\n\n\nD\n2022-11-01\nYes\nNo\nNo\n\n\nB\n2022-11-01\nYes\nNo\nYes\n\n\nC\n2022-11-01\nYes\nYes\nNo\n\n\nD\n2022-11-02\nNo\nYes\nNo\n\n\nA\n2022-11-02\nNo\nYes\nYes\n\n\n\n\nI want to compare the two dataframes and count several conditions:\n\nthe number of company that exists in both dataframes.\n\nthe number of company that exists in both dataframes that has at least one act_call as 'Yes' and act_visit as 'Yes' in df2, but has po as 'No' in df1.\n\n\nFor the 1st condition, I've tried using pandas.Dataframe.sum() and pandas.Dataframe.count_values() but they didn't give the results that I want.\nFor the 2nd condition, I tried using this code:\n(((df1[['act_calling', 'act_visit']].eq('yes'))&(df2['po'].eq('no'))).groupby(df2['company_name']).any().all(axis = 1).sum())\nbut, I'm not sure that the code above will only count the company that exists in both dataframes.\nThe expected output is this:\n\n3, (A, B, C)\n\n1, (C)\n\n\nI'm open to any suggestions. Thank u in advance!", "To See The Companies That Are In Both Data Frames\n1st part\ncombined_dataframe1=df1[df2['company'].isin(df1['company'])]\ncombined_dataframe1['company']\n2nd part\nTo see the company that satisfies your conditions\ncombined_dataframe2=df2[df2['company'].isin(df1['company'])]\njoined_dataframe=pd.merge(combined_dataframe1,combined_dataframe2, on='company',how='outer')\nAs per your condition\nfinal_dataframe=joined_dataframe[joined_dataframe.columns][joined_dataframe['po_x']=='n0'}[joined_dataframe['act_call_yes']=='yes'][joined_dataframe['act_visit_y']=='yes']\nprint(final_dataframe)", "Comparing and Count Values from 2 (or More) Different Pandas Dataframes Based on Certain Conditions", 0.0, 0, 795, "Try this\n\\begin{itemize}\n \\item `pandas.merge`\n to `concat` the data\n `df = pd.concat([df_1 ,df ])` or `pd.DataFrame.append` and `reset_index` to reindex the `columns` for `merge`.\n For the above code,\n  the dataframe `a` is `appended` after the merge. So, `company` column should be sorted first. If the column `actCall` contains `Yes` as well as `No`, it will return `NaN` in the end. \\strong{Hence, sort the columns alphabetically} and use `fillna` method to replace the value of `nan` with `0` before merging. For `2`, Use `mask` function and `.any` it to check the condition. `groupby` followed by `any`. The above solution is a bit long, so you may use this in `for loop` \\em{as shown below}.\n    `list = []`  , `i` ,`j` (indexes of the df_2) , \\begin {code}", 0.13953489065170288, 0.0, 0.09302325546741486, 0.0398244634270668], ["I need to connect data frame and dict like this . the number of frames for each cell is different\n,so the number of \"0\",\"1\"and so on is different .Total number of cells 16.How can", "To combine a pandas data frame with a dictionary, you can use the pandas.DataFrame.from_dict() function. This function takes a dictionary as input and returns a pandas data frame.\nFor example, you can create a dictionary with keys as column names and values as data for each column, and then pass this dictionary to the from_dict function to create a data frame:\nimport pandas as pd\ndata = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\ndf = pd.DataFrame.from_dict(data)\nprint(df)", "How to connect pandas data frame and dict?", 0.0, 0, 796, "Use [`to_dict`](http://pandas.pydata.org/pandas-docs/stable/generated/ pandas.DataFrame.to.html):\n\\begin{itemize}\n  \\item [df.info()](http%3A%2F%www.python.com%pandas%docs%reference%api%dataframe.index.information)\n    `df = df.set_index(df['index'].tolist())`\n  2. [to-dict](https://github. com/py-datastore/panda-store-2-0/blob/master/docs/%0Apandas/io/json.md#to%0D%D0%98dict) `out=df[df!='0'].setindex('index').todict()`\nor\n`out = pd.io.json_normalize(pd.Series(list(map(lambda x: x.astype(str), df[['0', '1', ...]))).toarray())[0]`\nthen `print(out)`\nyou can use `tofile()` `with open('filename', mode='w')`\nbut if your file is big, you need", 0.2448979616165161, 0.030927835032343864, 0.11224489659070969, 0.0245535708963871], ["I want to store a numpy array to a file. This array contains thousands of float probabilities which all sum up to 1. But when I store the array to a CSV file and load it back, I realise that the numbers have been approximated, and their sum is now some 0.9999 value. How can I fix it?\n(Numpy's random choice method requires probabilities to sum up to 1)", "Due to floating point arithmetic errors, you can get tiny errors in what seem like ordinary calculations. However, in order to use the choice function, the probabilities don't need to be perfect.\nOn reviewing the code in the current version of Numpy as obtained from Github, I see that the tolerance for the sum of probabilities is that sum(p) is within sqrt(eps) of 1, where eps is the double precision floating point epsilon, which is approximately 1e-16. So the tolerance is about 1e-8. (See lines 955 and 973 in numpy/random/mtrand.pyx.)\nFarther down in mtrand.pyx, choice normalizes the probabilities (which are already almost normalized) to sum to 1; see line 1017.\nMy advice is to ensure that all 16 digits are stored in the csv, then when you read them back, the error in the sum will be much smaller than 1e-8 and choice will be happy. I think other people commenting here have posted some advice about how to print all digits.", "How can I store float probabilities to a file so exactly that they sum up to 1?", 0.0, 0, 797, "In general, this cannot be done, because probabilties are not always positive (or even real) numbers. If you have positive probabiities, you can simply write them as a number of 32-bit integers (each integer takes 4 bytes), and read them back.\nIf you need to deal with probablilities that are negative or complex, then you will need a different way of storing them. For example, if you're happy to use 64-bits integers, instead of having -127 as 2^15-1,  you could use the complex plane to represent negative numbers, with i=0, -i=1... -j=2... etc. and then the probability is just a complex number. You'd need some other way to encode the probability' (i.e. whether it is + or -) though.", 0.260869562625885, 0.04040404036641121, 0.10033445060253143, 0.04006814956665039], ["I am currently reading from dropbox offline using pyspark on my local machine using this code\npre_test_quiz_df = spark \\ .read \\ .option('header', 'true') \\ .csv('/Users/jamie/Dropbox/Moodle/Course uptake/data use/UserDetails.csv')\nWhile working on from a server I am not able to read dropbox on my local machine. Is there a way to read the same file but from the dropbox on my browser.\nHave tried reading with pandas and converting to pyspark dataframe although it did not work.", "I found a work around. I didn't find any direct way of doing this, so the next alternative was using the dropbox API, which works pretty well. You can check their documentation or youtube on how to set up the API.", "How to read dropbox online using pyspark", 0.0, 0, 798, "Try this:\n\\begin{itemize}\n \\item install Dropbox desktop application\n  \\begin {code}", 0.07547169923782349, 0.0, 0.07547169923782349, 0.069444440305233], ["I have a dataframe (more than 1 million rows) that has an open text columns for customer can write whatever they want.\nMisspelled words appear frequently and I'm trying to group comments that are grammatically the same.\nFor example:\n\n\n\n\nID\nComment\n\n\n\n\n1\nI want to change my credit card\n\n\n2\nI wannt change my creditt card\n\n\n3\nI want change credit caurd\n\n\n\n\nI have tried using Levenshtein Distance but computationally it is very expensive.\nCan you tell me another way to do this task?\nThanks!", "Levenshtein Distance has time complexity O(N^2).\nIf you define a maximum distance you're interested in, say m, you can reduce the time complexity to O(Nxm). The maximum distance, in your context, is the maximum number of typos you accept while still considering two comments as identical.\nIf you cannot do that, you may try to parallelize the task.", "How can I resolve write errors that I have in my data?", 0.0, 0, 799, "If you know that the text will be in English, then you can use a combination of `ngrams` and `replace`.\n\\begin{itemize}\n  \\item Split your data into tokens using `str.split`. We can then use `df.apply(ngrams, n=3)` to find the most common ngram sequences of length 3. You can adjust `3` to your liking. We're looking for the top 50 sequences. Here's a sample: `d=df[['ID','Comment']].apply([lambda x: np.random.choice(x.Comment.str[:3], 20).str[0:3]`).\n  2. If we do `print(d[d.n == 0])`, we'll see that we have 7,900 rows in our dataframe. `np.unique(np_n)` will return a list of those 4,532 most frequent n-gram occurrences. Let' see how the result looks like: `[['change my', 'change your', ..., 'credit card',  'i wanna', ....,  ...], ['I change',...,", 0.21568627655506134, 0.029702970758080482, 0.12745098769664764, 0.0652858167886734]]}