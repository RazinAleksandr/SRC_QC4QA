{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I got and LSTM that gives me output (4,32,32) i pass it to the Linear Layer(hidden size of LSTM, num_classes=1) and it gives me an output shape (4,32,1). I am trying to solve a wake word model for my AI assistant.\nI have 2 classes i want to predict from. 0 is not wake up and 1 is the wake up AI.\nMy batch size is 32. But the output is (4,32,1). Isnt it should be 32,1 or something like that so i will know that there is one prediction for 1 audio mfcc?", "Not quite. You need to reshape your data to (32, 1) or (1, 32) in order for your linear layer to work. You can achieve this by adding a dimension with torch.unsqueeze() or even directly with torch.view(). If you use the unsqueeze function, the new shape should be (32, 1). If you use the view function, the new shape should be (1, 32).", "How to correctly combine LSTM with Linear layer", 0.1973753273487091, 1, 320, "In your LstM layer, you will have the shape of the hidden state as 4 x 64. The last 500 time steps will be used for classification. So, it is actually (36, 768).\nIf you are using the embedding layer before LStM and the input shape is `(40,)` then the LStrM will add 80 timesteps to it and give the final shape as `(84 , 96)`.\nFor classification, this will give a final output of `(38,72)`.", 0.24113474786281586, 0.014388489536941051, 0.1702127605676651, 0.0402861051261425], ["I had trained a weight file to detect an object and another weight file to detect another specific object using yolov5. If i want to detect both objects in a single images, can i ,like, use both weight files together? Or is there a way to combine the both trained files into a single one, without training the datasets again together?", "Actually, no. There is no way to aggregate models, trained to detect different objects into one. You can sequentially detect objects by first and second model. Proper approach is to train model again with two classes.", "Is there any way to detect two kinds of objects from an image or video using two seperate trained weight files in **YoloV5**?", 0.0, 0, 321, "You can use the same file for training and detection.\nThe training file is used to train the classifier and the detection file contains the objects that you want the trained classifiers to recognize. You will have to use a `yolo.xml` file in order to have the model detect the object you trained it for. This file can be downloaded from [here](https://github.com/pytorch/faster_rcnn/blob/master/examples/yoloxm_object_detection/config.yxm).\nYou will also need to generate an `annotations_map.txt` files using [this](http://objectdets.blogspot.in/2017/05/labeling-images-for-yoli-object.html) tutorial. The annotations map file gives you the bounding boxes of the detected objects. After you have done this, you can train your model on the training files. To use your trained model for object detection, all you need is a new image and a yolo object detector file.", 0.16042780876159668, 0.021621622145175934, 0.11764705926179886, 0.06043117120862007], ["I have two numpy arrays with 0s and 1s in them. How can I find the indexes with 1 in the first array and 0 in the second?\nI tried np.logical_and\nBut got error message (builtin_function_or_method' object is not subscriptable)", "Use np.where(arr1==1) and np.where(arr2==0)", "2 different specified elements from 2 numpy arrays", 0.11942730098962784, 3, 322, "\\begin{code}\na = np_array_1\nb =  np._array(np_2)[:,np.where(a==1)]\n\\end{pre}", 0.29629629850387573, 0.07999999821186066, 0.29629629850387573, 0.049713291227817535], ["I have two numpy arrays with 0s and 1s in them. How can I find the indexes with 1 in the first array and 0 in the second?\nI tried np.logical_and\nBut got error message (builtin_function_or_method' object is not subscriptable)", "tow numpy array given in problem.\narray1 and array2\njust use\none_index=np.where(array1==1)\nand\nzero_index=np.where(array2==0)", "2 different specified elements from 2 numpy arrays", 0.0, 0, 323, "Use `np.ix_`\nhttps://numpy.org/doc/stable/reference/generated/numpy-1.15.0.html#numpy_1_s16_ix\nYou can use it to find out what elements of an array are present in another array.\nTo find all the indices of the array where the values are 3 and the value of second array is 4:\n`np_a = np_b[np_.ix_(np_, np_, [3, 5, np_.all(np_)])]`\nHere is a little code snippet that you can try it out.", 0.18867924809455872, 0.0, 0.11320754885673523, 0.06956521421670914], ["I installed Tensorflow using pyenv. But whenever i import it gives me this error.I am using Debian in raspberry pi4.My python version is 3.7.12 and tensorflow version is 2.5.0.\n'''  pi@raspberrypi:~/project $ python\nPython 3.7.12 (default, Mar 22 2022, 14:27:41)\n[GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport tensorflow\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/home/pi/.pyenv/versions/3.7.12/lib/python3.7/site-packages/tensorflow/init.py\", line 41, in \nfrom tensorflow.python.tools import module_util as _module_util\nFile \"/home/pi/.pyenv/versions/3.7.12/lib/python3.7/site-packages/tensorflow/python/init.py\", line 40, in \nfrom tensorflow.python.eager import context\nFile \"/home/pi/.pyenv/versions/3.7.12/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 37, in \nfrom tensorflow.python.client import pywrap_tf_session\nFile \"/home/pi/.pyenv/versions/3.7.12/lib/python3.7/site-packages/tensorflow/python/client/pywrap_tf_session.py\", line 23, in \nfrom tensorflow.python._pywrap_tf_session import *\nImportError: SystemError: <built-in method contains of dict object at 0x7f741852d0> returned a result with an error set\n'''", "The error message tries to say that Tensorflow needs a recent version of numpy. So,try to upgrade numpy pip3 install --upgrade numpy", "Tensorflow shows errors after importing using python", 0.0, 0, 324, "You can install tensorflow with `pip` instead of `pyenvs`.\n`pip install --user tensorflow`\nYou can also use conda to install it.", 0.1818181872367859, 0.0, 0.1818181872367859, 0.0277777761220932], ["(apologies about formatting, this is my first question, and when finalizing question, there was no back button?)\nmy categorical columns that i KNOW are categorical and even object_cols CONFIRMS this are:\n'x34', 'x35', 'x41', 'x45', 'x68', 'x93'. All 6 of them are present in xtrain and xvalid. But why is x41 being kicked out by the issubset operation? Even though we can clearly see x41 is present in object_cols_val\nwhy is the issubset messing up and throwing out x41?\nwhat is this doing:\n[col for col in object_cols if set(xvalid[col]).issubset(set(xtrain[col]))]\nI thought it's checking each column from object_cols in xvalid, then checking to see if it's a subset of xtrain, WHICH IT IS. Ugh, why is it treating x41 differently? (probably not related but x41 has a $ and numbers, but why would that matter? as long as the column is present in both sets?)\nall categorical columns\nobject_cols_train=[col for col in xtrain.columns if xtrain[col].dtype =='object']\nprint(\"object_cols are:\",object_cols)\nobject_cols_val=[col for col in xvalid.columns if xvalid[col].dtype =='object']\nprint(\"object_cols_val are:\",object_cols_in_val)\n\"good\" columns that can safely be ordinal encoded\ngood_label_cols=[col for col in object_cols if set(xvalid[col]).issubset(set(xtrain[col]))]\nprint(\"good_label_cols are:\",good_label_cols)\n\"bad\" problematic columns that should be dropped (for now, but i believe we should NEVER drop)\nbad_label_cols=list(set(object_cols)-set(good_label_cols))\nprint(\"bad_label_cols are:\",bad_label_cols)\n\noutputs:\n\n\nobject_cols are: ['x34', 'x35', 'x41', 'x45', 'x68', 'x93']\nobject_cols_val are: ['x34', 'x35', 'x41', 'x45', 'x68', 'x93']\ngood_label_cols are: ['x34', 'x35', 'x45', 'x68', 'x93']\nbad_label_cols are: ['x41']\n\nI'm still beginner/intermediate, i tried separating out the sets to see what they look like, but cant because 'col'.\nI tried:\nxtrain[col]\nset(xtrain[col])\nset(xvalid[col]).issubset(set(xtrain[col]))\nI KNOW what xtrain['x41'] and xvalid['x41'] look like.\nMaybe i should include here:\nxtrain['x41'].head(),xvalid['x41'].head()\n\noutput:\n\n(22449    $-996.73\n39178    $-361.51\n33715      $851.5\n36010    $-765.51\n13370    $-1391.9\nName: x41, dtype: object,\n34320     $412.48\n27355    $-473.03\n18144    $-208.31\n20740    $-434.41\n10805     $203.53\nName: x41, dtype: object)", "embarrassed to admit that it took wayyy tooo looonnnggg to bring my mind at peace.\nissubset is taking EACH VALUE within x41 column and comparing in xtrain and xvalid. the thing is, x41 has 17000 unique values, so obviously when doing the split, the values will not be in BOTH sets. Thus, it is NOT a subset, because not all xvalid['x41'] values are in xtrain['x41'].\nphew. life makes sense again.", "why is issubset knowingly reducing one of my columns", 0.0, 0, 325, "I figured it out: the issue was with the object-columns!\nThe object columns were not in the same order in each set. I reordered the data and problem solved.", 0.1764705926179886, 0.0, 0.13725490868091583, 0.023255107924342155], ["I am working with the YOLOv3 model for an object detection task. I am using pre-trained weights that were generated for the COCO dataset, however, I have my own data for the problem I am working on. According to my knowledge, using those trained weights as a starting point for my own model should not have any effect on the performance of the model once it is trained on an entirely different dataset (right?).\nMy question is: will the model give \"honest\" results if I train it multiple times and test it on the same test set each time, or would it have better performance since it has already been exposed to those test images during an earlier experiment? I've heard people say things like \"the model has already seen that data\", does that apply in my case?", "For hyper-parameter selection, evaluation of different models, or evaluating during training, you should always use a validation set.\nYou are not allowed to use the test set until the end!\nThe whole purpose of test data is to get an estimation of the performance after the deployment. When you use it during training to train your model or evaluate your model, you expose that data. For example, based on the accuracy of the test set, you decide to increase the number of layers.\nNow, your performance will be increased on the test set. However, it will come with a price!\nYour estimation on the test set becomes biased, and you no longer be able to use that estimation to talk about data that your model sees after deployment.\nFor example, You want to train an object detector for self-driving cars, and you exposed the test set during training. Therefore, you can not use the accuracy on the test set to talk about the performance of the object detector when you put it on a car and sell it to a customer.\nThere is an old sentence related to this matter:\n\nIf you torture the data enough, it will confess.", "Is the performance of a deep learning model affected if it has \"seen\" the same test images before?", 0.0, 0, 326, "\\strong{It will not affect the accuracy of your model.}\nThis is because the only thing that really matters is the \"loss\" (or the mean-squared error) of training and not the training data.\nTo put it simply, the test data is \"fresh\" for you every time you run the experiment. The model just looks at the loss during training (and not at training images) and updates its parameters based on that.", 0.31501832604408264, 0.07380073517560959, 0.1684981733560562, 0.07033339142799377], ["I'm trying to load an already trained NER model, which was loading normally until today, but I'm getting the following error, either importing the trained model or importing pt_core_news_lg:\nnlp4 = spacy.load('/content/gdrive/My Drive/spacy_NER4')\nValueError: Cannot create vectors table with dimension 0. If you're using pre-trained vectors, are the vectors loaded?\nI'm on Google Colab, following the installations:\n!pip install spacy==2.3.4\n!python -m spacy download pt_core_news_lg\nWhen I import my model, it generates this error. Does anyone have a tip or solution to this problem?\nIf I install spacy-nightly it throws another error:\nOSError: [E053] Could not read config.cfg from /content/gdrive/My Drive/space_NER4/config.cfg\nHowever, when loading pt_core_news_lg, it loads normally", "I solved this error by changing google account. I simply imported all my templates into the other account and it worked. However, the reason for the error of not loading in the account, I did not find", "Error loading already trained ner spacy model", 0.0, 0, 327, "It is not very clear what you mean by \"pre-built vectors\" as far as I can see.\n\\begin{itemize}\n    \\item If they are pretraind vectors which are not in your model: you can load them by\n        `import spaCy`\n         `from spaciy.vocab import get_voca_from_file` and then\n            `get_vectors_dict(getVocabsPath() + '/vocs/news/pt_nl/vct/en/')`.\n       \\strong{Note}: you have to make sure that your vocabulary and your vector paths are consistent. I.e. the vocabs path should be the same as in the config file. E.g. if the model path is `path/to/model/`, then the path to your vectors should also be `model` + `/vcs` (i. e. `/path`)\n      \\begin {code}", 0.17105263471603394, 0.013333333656191826, 0.10526315867900848, 0.024330148473381996], ["I am using ARIMA to forecast the time series of some medical data. I was wondering if I can take the ARIMA model I fit to my data and get some numbers that describe just the trend and seasonality separately. This would be useful for me because it would allow me to see what my model's trend rate is without seasonality affecting the results. Please let me know if you have any questions. Thanks.\nI was unable to find anything from a google search and have idea where to start. I looked into seasonal decompose but that seems to get trend and seasonality of my actual data, not the model fit to the data.", "I think that if you want to know the trend and the seasonality of your model you should first make prediction on a large range of date using .forecast(bignumber). Then on this prediction you could do decomposition using statsmodels.tsa.seasonal.seasonal_decompose. Like that you will have a clear idea of the trend and the seasonality learned by your ARIMA model. After, if you want to estimate the expression of your trend you can train a linear or polynomial model on the trend decomposed.", "Can I break out my ARIMA model into trend and seasonality specific components?", 1.2000000476837158, 0, 328, "If you are using the R package \"forecast\" you can use the function \"decompose\" to break the seasonally adjusted series into its season and trends parts. If the series is not seasoned, use \"addseasonal\" instead of \"season\" in the call to decoppose. The function returns a list of 3 elements: the original series, the \"trend\" and the adjustment parts of the decomposition. In your case, you could fit a trended ARMA model with the latter two elements of this list.", 0.3734939694404602, 0.08536585420370102, 0.22891566157341003, 0.05699308216571808], ["I am using faiss indexflatIP to store vectors related to some words. I also use another list to store words (the vector of the nth element in the list is nth vector in faiss index). I have two questions:\n\nIs there a better way to relate words to their vectors?\nCan I update the nth element in the faiss?", "You can do both.\n\n\nIs there a better way to relate words to their vectors?\n\n\nCall index.add_with_ids(vectors, ids)\nSome index types support the method add_with_ids, but flat indexes don't.\nIf you call the method on a flat index, you will receive the error add_with_ids not implemented for this type of index\nIf you want to use IDs with a flat index, you must use index2 = faiss.IndexIDMap(index)\n\n\nCan I update the nth element in the faiss?\n\n\nIf you want to update some encodings, first remove them, then add them again with add_with_ids\nIf you don't remove the original IDs first, you will have duplicates and search results will be messed up.\nTo remove an array of IDs, call index.remove_ids(ids_to_replace)\nNota bene: IDs must be of np.int64 type.", "Update an element in faiss index", 1.2000000476837158, 1, 329, "Yes, there are better ways to use FAISS. For example, you can use a FAST vector to index your words, or FASS with sparse indices.\nI am not sure I understand your second question, but you should be able to do a query like `faiss::indexflat::search_element(index, query, ...)` to get the index of an arbitrary vector.", 0.30392158031463623, 0.019801979884505272, 0.1666666716337204, 0.06513410061597824], ["pd.set_option(\"precision\", 2)\npd.options.display.float_format = '{:.2f}'.format\nIm not able to figure out what these code line do", "These code lines fix float numbers precision to two decimal places for pandas output. I belive it's done because your data is banking data, which contains a lot of different money amounts, which should be displayed with 2 decimal places (because there are 100 cents in a dollar)", "can anyone explain me these pandas code, Im looking at a EDA project on a Banking Data", 0.1973753273487091, 1, 330, "I am not sure if I understand the question correctly, but I think you are looking for this:\n\\begin{itemize}\n  \\item pd set_options is used to set some options for pandas (for example, the default decimal format).\n    [https://pandas.pydata.org/pandas-docs/stable/options_selection.html](https%3A//pandas%2Epy%E2%80%93data%5Eorg%F0ur-doc%A7%B1%C2d%7Coptions%D0n%60ptions_sele%0F%13t%CD%BC%4D%AB%CE%AF%AC%BE%EF%AE%BD%EB%AD%EC%BA%ED%BF%BB%CF%DB%EE%FC%DF%FF%FE%DD%DE%DC%DA%CB%CA%CC%CH%CI%CG%CO%CP%CS%CT%CV%CM%CN%CL%CR%CSS%CAT%TK%P%L%X%Q%R%S%TT%VV%W%XY%Y", 0.1034482792019844, 0.011627906933426857, 0.04597701132297516, 0.04237287864089012], ["I'm working on a graph theory problem with a given disconnected unweighted, undirected graph (given an edge list). The following operation can be made on the graph:\nA set of edges represented by two vertex pairs (i, j) and (x, y) can be switched into (i, y) and (x, j)\nThe following are the tasks that need to be accomplished:\n\nDetermine if it is possible to connect the graph.\nIf yes, find the minimum number of operations to connect the graph.\nOutput the edges used per operation in the following order: i j x y\n\nAn additional constraint is that a vertex cannot be connected to itself. Vertices can be up to n = 10^5\nI have already implemented the switch function defined above, however, my current solution is very inefficient and may not be applicable to all possible inputs. It basically checks for any four vertices with multiple connections and applies the switch operation, then runs a DFS (depth-first search) algorithm to check if the graph is connected or not, so it runs a DFS every time it makes an operation. Additionally, this doesn't deal with the minimum operations, it just does operations until the graph becomes connected. Are there any implementation tips or algorithms that can help in solving the problem, preferably in Python?", "Say your graph G has n vertices, e edges, and k > 1 components.\nA solution is possible iff there are no isolated vertices and e >= n-1. It will take k-1 switches.\nIf e < n-1 then there aren't enough edges for a connected graph to exist. If there's an isolated vertex, switch operations can't affect it so it will remain isolated.\nOtherwise, repeatedly perform the following switch: One edge should belong to a component where its removal won't disconnect the component. This is guaranteed to exist if there are multiple components and e >= n-1. The other edge can belong to any other component (and it doesn't matter if its removal would disconnect the component). Performing the switch operation will merge these components, reducing the total number of components by one.\nWe perform k-1 switches, reducing the number of components from k to 1, at which point the graph is connected.\n--- O(V+E) approach ---\n\nConfirm there are no isolated vertices and that E >= V-1.\nUse BFS to split the input graph G into its components. While doing this, keep track of every edge that completes a cycle (connects to an already visited vertex). Call these 'swappable edges'\nRepeatedly perform swap operations where one edge is swappable, and the other is an arbitrary edge of some other component.\nNote that if the other edge is also swappable, then both new edges after the swap are on a cycle. Choose one of them (arbitrarily) to add to the list of swappable edges.\nEach swap reduces the number of swappable edges by one and the number of components by one. Keep going until you're down to one component.", "Given a disconnected graph, find the minimum operations to rearrange the vertices such that the graph becomes connected", 0.1973753273487091, 1, 331, "The question is hard to answer since it's not clear how many vertices are connected, but here is a solution that works for all inputs: it may be in-efficient, though. I have chosen to work with disjoint sets. The set containing the connected vertices will be called $C$, the set with unconnected vertices $U$, and the original graph $G$.\n\\begin{itemize}\n    \\item Create a dictionary $D$ with $N$ (the number connected nodes) keys, $n$ being the number nodes in $V$, where $i \\in N$ is the key of the node $v_i$. $u_j \\to i$ means that $j$ has an incoming edge from $g_u$ and $d_k \\le i $ means the the degree of $k$ in graph G is less than the incoming edges of node in set $I$. These relationships are represented in a data structure called disjunctive graph by a function called DFG_create(graph G, vector V).\n      \\begin {code}                   int D[N];                                              //dictionary\n        for (int i=0; i<N; ++i)", 0.29580575227737427, 0.026607539504766464, 0.12803532183170319, 0.04684995114803314], ["I'm currently trying to use a number of medical codes to find out if a person has a certain disease and would require help as I tried searching for a couple of days but couldn't find any. Hoping someone can help me with this. Considering I've imported excel file 1 into df1 and excel file 2 into df2, how do I use excel file 2 to identify what disease does the patients in excel file 1 have and indicate them with a header? Below is an example of what the data looks like. I'm currently using pandas Jupyter notebook for this.\nExcel file 1:\n\n\n\n\nPatient\nPrimary Diagnosis\nSecondary Diagnosis\nSecondary Diagnosis 2\nSecondary Diagnosis 3\n\n\n\n\n\nAlex\n50322\n50111\n\n\n\n\n\nJohn\n50331\n60874\n50226\n74444\n\n\n\nPeter\n50226\n74444\n\n\n\n\n\nPeter\n50233\n88888\n\n\n\n\n\n\n\nExcel File 2:\n\n\n\n\nPrimary Diagnosis\nMedical Code\n\n\n\n\nDiabetes Type 2\n50322\n\n\nDiabetes Type 2\n50331\n\n\nDiabetes Type 2\n50233\n\n\nCardiovescular Disease\n50226\n\n\nHypertension\n50111\n\n\nAIDS\n60874\n\n\nHIV\n74444\n\n\nHIV\n88888\n\n\n\n\nIntended output:\n\n\n\n\nPatient\nPositive for Diabetes Type 2\nPositive for Cardiovascular Disease\nPositive for Hypertension\nPositive for AIDS\nPositive for HIV\n\n\n\n\nAlex\n1\n1\n0\n0\n0\n\n\nJohn\n1\n1\n0\n1\n1\n\n\nPeter\n1\n1\n0\n0\n1", "Maybe you could convert your excel file 2 to some form of key value pair and then replace the primary diagnostics column in file 1 with the corresponding disease name, later apply some form of encoding like one-hot or something similar to file 1. Not sure if this approach would definitely help, but just sharing my thoughts.", "How do i use medical codes to determine what disease a person have using jupyter?", 0.0, 0, 332, "Use the [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/index.html) package to read in the excel files and then do a merge on the dfs.  Once you have a dataframe with all of the columns you can use the `pandas` [merge](http://docs.python.com/3/library/pd.merge.api.htm) function to get a new dataframe.", 0.18487395346164703, 0.017094017937779427, 0.11764705926179886, 0.03900709003210068], ["I have two NumPy arrays saved in .npy file extension. One contains x_train data and other contains y_train data.\nThe x_train.npy file is 5.7GB of size. I can't feed it to the training by loading the whole array to the memory.\nEvery time I try to load it to RAM and train the model, Colab crashes before starting the training.\nIs there a way to feed large Numpy files to tf.fit()\nfiles I have:\n\n\"x_train.npy\" 5.7GB\n\"y_train.npy\"", "Depending on how much RAM your device has, it may not be possible from a hardware point of view.", "How to feed large NumPy arrays to tf.fit()", 0.0, 0, 333, "You can use the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Datasets) API to stream data from the file. See [this tutorial](http://%20https%3A//colab.research.google.com/github/tensorflow/docs-assets/tutorials/11_basic_data_sources.ipynb) for an example on how to use it. It is a little long to copy and paste here, but the general idea is: you can load the data using numpy.load() and then feed that data to a Dataset. The Datasource uses a function that you supply to read data line by line and it will return a tuple of the numpy array and the index of that line. This is where you get the \"chunk\" size from. Then you use tfds.features.FileFeature to create a feature that has the line number and a label of 0 or 1. Finally you define a dataset with that feature. For more information, you should take a look at the tutorial.", 0.07692307978868484, 0.011111111380159855, 0.06593406945466995, 0.032967034727334976], ["I have an image with which consists of 8 bits for each pixel, how I can create new picture that consists of 5 bits for each pixel using python and OpenCV?\nI know that in an RGB image, each pixel is represented by three 8 bit numbers associated to the values for Red, Green, Blue respectively, but I can't figure it out how I can create an image from 8 bits for each pixel to a new image with 5 bits of each pixel.", "You can rescale the value of the pixels i.e. multiply every pixel value with 32/256 (or just divide by 8) if you want to generate a mapping value on 5 bit scale for your image.", "Create an image with 5 bits for each pixel", 1.2000000476837158, 0, 334, "You can use a 256x240 image as the source for the new 128x108 image:\n\\begin{itemize}\n \\item Convert the R,G,B values of the original image to grayscale.  This is easy in Open CV. For example: gray = (r + g + b) / 3\n    The values 0-2047 represent white, 65535-131,179 represent black, and 429496727-4,095,911 represent shades of gray in between. To convert from the gray scale value to an  RBG value, multiply the value by 768. (74 is the average of all R values in the image)\n  \\strong{NOTE:} If you want to convert the entire image in one step, you can do this with a single for loop. However, if you are only interested in a few pixels, it is faster to loop over the pixels one at a time. This makes it easier to keep track of what you've done. A single loop would look like: `for y in range", 0.20618556439876556, 0.0729166641831398, 0.14432989060878754, 0.09090908616781235], ["for the equation Ax = b, let A = USV.t, i need to calculate inverse of (S.T@S). I noticecd that using np.linalg.inv() and np.linalg.pinv() gives extremely different results. np.allclose() infact returns false.\nI want to know why this is happening, any mathematical insight? maybe due to some property of A? here A is a non-linear function of a dynamic time series.\nBasically when can you expect pinv() and inv() to give very different results?", "kind of figured it out. np.linalg.pinv, works by SVD decomposition and if A = USVt\nthen pinv(A) = V S^-1 Ut, and the shape of U and V are changed such that S^-1 is either mxm or nxn matrix. also, there is a cutoff for singular values, where less than cutoff are treated as zero. so if there are many small singular values, many rows/columns of V/U will be ignored and as such inv() and pinv() will give significantly different results.", "When will numpy.linalg.inv() and numpy.linalg.pinv() give very different values?", 0.0, 0, 335, "I am not sure I understand your question. You can use `piv` to compute the inverse. If you are looking for an analytical solution of `Ax=b` then `inv(A)` should be a good choice. Otherwise `solve` will be your friend.", 0.17322835326194763, 0.01600000075995922, 0.09448818862438202, 0.016759781166911125], ["I am working on building a hashtag recommendation system. I am looking into what are the best ways I can evaluate the system.\nThe problem statement is: For a given hashtag I need to recommend most relevant (3 or 5) hashtags to the user.\nThe dataset contains post id in each row. And each row contains the hashtag contained in the post.\n\n\n\n\n\npost_id\nhashtags\n\n\n\n\n1\n100001\n#art #artgif #fanart #digitalArt\n\n\n\n\nThese are the steps I have followed.\n\nPreprocessed the hashtag data.\nTrained a fastText model on the entire hashtag corpus.\nGenerate word embeddings of all the hastags.\nUse K Nearest Neighbor to recommend hashtags.\n\nI am trying to evaluate the model using MAP@K.\nSo for each unique hashtag I check what are the top 3 or top 5 recommendations from the model and then compare with what are the actual hashtags that occurred with those hashtags.\nI am using MAP@K to evaluate the recommendations and treating the recommendation like a ranking task. Since a user has a finite amount of time and attention, so we want to know not just three tags they might like, but also which are most liked or which we are most confident of. For this kind of task we want a metric that rewards us for getting lots of \u201ccorrect\u201d or relevant recommendations, and rewards us for having them earlier on in the list (higher ranked). Hence MAP@K (K=3 or 5) [Not finalised the value of K].\nBelow table shows how I am evaluating my recommendation for each hashtag.\n\n\n\n\n\npost_id\nquery_hashtag\nhashtags\nrecommended_hashtags\n\n\n\n\n1\n100001\n#art\n#art #artgif #fanart #digitalArt\n#amazingArt #artistic #artgif\n\n\n1\n100001\n#artgif\n#art #artgif #fanart #digitalArt\n#fanArt #artistic #artgif\n\n\n1\n100001\n#fanart\n#art #artgif #fanart #digitalArt\n#art #wallart  #fans\n\n\n1\n100001\n#digitalArt\n#art #artgif #fanart #digitalArt\n#crypto #nft  #artgif\n\n\n\n\n\nI am basically looking for answers to 4 questions.\n\nAm I moving in the right direction to evaluate the hashtag recommendations?\nShould Calculate the MAP@K on the entire dataset (which I cam currently doing) or split the dataset into training and testing set and calculate the metric. In case I decide to split the dataset. Should I also restrict the hashtags to be seen by the model from the\ntesting data? I am unable to figure this out.\nWhat value of MAP@K is good enough for 5 recommendations, I am getting approximately 0.12 for MAP@5\nAny other evaluation metric that can help me to understand the quality of recommendations", "Answers:\n\nperhaps, read-on\n\"cross-validation\" tests like MAP@k require that the data is split into \"test\" and \"training\" data. save 20% of the data for the \"test\" part then train the model on the rest. For the \"test\" set get a hashtag and make the query of the model. For every time the query returns a tag associated with the \"test\" datum you have a positive result. This allows you to calculate MAP@k. You can perform subsequent splits to use all data and combine the results but this is usually not necessary.\nthere is no fixed \"good\" for MAP@k. Find MAP@k for a random dataset as well as using your dataset to create \"popular\" hashtags. Using random and popular tags will give you 2 more MAP@k results. These should be significantly lower that the recommender MAP@k. Also the MAP@k for recs can be used as a baseline for future improvements, like changes to word embeddings. Better than the baseline means you have have a better recommender.\nresults with humans are the best metric since a recommender is trying to guess what humans are interested in. This requires an A/B test for 2 variants, like random and recs -- or no recs and recs. Set your test up with where the app has no recs or random recs. This will be the \"A\" part and the \"B\" will be using your recs. If you get significantly more clicks using \"B\" you have clearly improved results for you app -- this assumes your app considers more clicks to the the thing to optimize. If you want to optimize time-on-site, then replace your metric for the A/B test.", "How to evaluate hashtag recommendation system using MAP@K and MAR@K?", 0.0, 0, 336, "You can use MSE (Mean Squared Error) to measure the performance of your recommendation. It is the most commonly used metric in recommendation systems. The lower the error, the better your model is performing. To calculate MSEE, you need the sum of the squares of differences between the predicted and actual values. Here is a brief explanation of how to calculate it. First, we have a dataset with the number of times a certain hasthag was seen in a post and the corresponding number you are predicting for it:\n\\begin{itemize}\n<h5>`x`</h4>\n`[0,0] [0 ,0 ] [1,1] .. [n ,n] `\nwhere `n` is number rows in dataset, `x[i,j]` is for i-th row and j-the column. You have to put `1` if the i -th post has the j -the hastag. `0` otherwise. Next, calculate mean of `y` for all values of j, then square each value. Finally, sum up all squared differences. That is your MSIE.", 0.281879186630249, 0.03146067261695862, 0.14765100181102753, 0.05500655621290207], ["I have a series of data points which form a curve I do not have an equation for, and for which i have not been able to satisfyingly calculate an equation with either libreoffice or the online curve fitting tools in the first 2 pages of google results.\nI would like the equation for the curve and ideally a python implementation of calculating y values for a given x value along that curve in case there are unexpected hoops to jump through. Failing that I would like any more elegant python solution than a list of elif statements incrementing y if x is high enough for it to increase by a whole number, which is the ugly solution of last resort - my immediate plans do not require decimal precision.\nThe curve crosses the zero line at 10, and every whole number incrementation of y requires x to be incremented by one more whole number than the previous, so y1 is reached at x11, y2 at x13, y3 at x16 etc, with the curve bending in the other direction in the negatives such that y-1 is at x9, y-2 is at x7 etc. I suspect i am missing something obvious as far as finding the curve equation when i already have this knowledge.\nIn addition to trying to use libreoffice calc and several online curve-fitting websites to no avail, i have tried slicing the s-curve (I have given up on searching the term sigmoid function as all my results are either related to neural nets or expect my y values to never exceed +-1) into two logarythmic curves, which almost works - 5  *(np.log(x) - 11) gets something frustratingly close to the top half of the curve, but which i ultimately haven't been able to use - in addition to crossing the number line at 9 it produced some odd behaviour when I returned round() rounded y values directly, displaying results in the negative 40s when returned directly, but seeming to work fine when those numbers are fed into other calculations.\nIf somebody can give me two working logarythms that round to the right numbers for x values between 0 and 50 that is good enough for this project.\nThank you for your time and patience.\n-EDIT-\nthese are triangular numbers apparently, x-10 is equal to the number of dots in a triangle with y dots on each side, what I need is the inverse of the triangular number formula. Thank you to everyone who commented.", "What you're looking for are a class of functions called \"Sigmoid functions\". They have a characteristic S-shape.  Go to Wolfram and play around with some common Sigmoid funcs, remembering that the \"a\" in a function, f(x-a), shifts the entire curve left or right, and appending a value \"b\" to the function, f(x-a) + b will shift the curve up and down.  Using a coefficient of \"c\", f(c*x - a) + b here acts as a scalar.   That should get you where you want to be in short time.\nExample: (1/(1 + C*exp(-(x + A)))) + B", "Creating an s-curve based on data points", 0.0, 0, 337, "You have already figured out that you want to take the logarithm of each value, I'm not sure what you mean by \"logarithmic curves\", but I think you're looking for something like this. The sine function is a nice way to approximate the sigmoide. A better fit is actually the tangent function, though, because it does a better job of approximating the sharp rise in slope at zero. But I didn'y have the time to make a new plot, here is one from a few years ago. In this example I show the rise from zero to 3 in 60 steps. Notice that in each step the function rises a little more than in previous steps, that's because the sin function has a frequency of \u03c0/6, the same as the period of a s wave. It'll keep rising a bit more each time, until it reaches the next s period. By the way, you can use numpy' to plot it with just one line of code. Just do:\n`import numpy as np`\nthen: `np(sin(0.1*x))`\nHere' what it looks like: <a href=\"https://i.stack.imgur.com", 0.29054054617881775, 0.0476190485060215, 0.18243242800235748, 0.038554105907678604], ["I've tried to perform static quantization with a trained EfficientNetB0. Once I apply the torch.quantization.convert to transform the model from float32 to int8, an error occurs, specifically: NotImplementedError: Could not run 'aten::silu.out' with arguments from the 'QuantizedCPU' backend. I wonder if anyone has run into the same error and been able to resolve it. I have also tried with mobilenet and I get an error of the style (not the same).\nThank you very much in advance.", "Silu, Leaky ReLu is not easy to be implemented in 8bit quantization and a lot of frameworks are not able to implement it without model degradation (256 numbers represent for the whole output range).\nHowever, there is a trick to quantize this layer in 8-bit\n1/ You can try to inference the training dataset and measure the output activation range of this layer to replicate the boundary from this function. This is usually called PostQuantize\n2/ Reduce the complexity of the activation function. SiLu x*sigmoid(x) => HardSwish x*relu6(x + 3)*0.166666667. This is the idea of ReLu => ReLu6 where the output is bounded from 0-6.", "Static quantization on efficientNet", 0.0, 0, 338, "This is not a problem with static q-net but a general problem in GPU computing, where the CPU backend does not support some of GPU backend's functions. The error message is:\n\\begin{code}\nNotImlemented:Could not convert 'ten::conv.in' to 'Tensor'\nUsing the GPU version of this function would require you to compile the code using the CUDA compiler. It is also possible to implement this using dynamic_quantize. See https://pytorch-cpu.ai/docs/master/quantized_nets.html#torchtorch.dynamic_qnet\nfor more details. If you are using GPUs, you need to make sure the backend of your model matches the device you're running on. You can check this by doing: torchtensorboard.set_backend('cpu').\nThe backend you have is 'CPUD' while the current device is a GPU. Please try: 'torchte.backend.cpu_only'.\nI'm not sure whether this error can be fixed.", 0.3320463299751282, 0.06225680932402611, 0.1621621549129486, 0.05065665766596794], ["I have run the same code(with packages I needed) before and it worked, not sure what's happening now. This show the error,\nAttributeError: module 'PIL.Image' has no attribute 'Resampling'. Probably it's small issue, but I can't figure it out, I am working in databricks.", "Same happened when I upgraded some module. I just restarted runtime and it helped.", "Module PIL has not attribute \"Resampling\"", 0.0, 0, 339, "It is likely that you are using a version of Pillow that doesn' think that the 'resampler' parameter exists. You can check the version with the command 'pip freeze' and make sure that it is the latest. If not, you should upgrade.", 0.0714285746216774, 0.0, 0.0714285746216774, 0.04343468323349953], ["While working with the matplotlib, when I got to the colormap, I realized that the default colormap is jet colormap, but I heard that this choice is not very good. Can someone tell me the reason or is there a reason that this was chosen by default?", "the most commonly used \"jet\" colormap \u00a0(also known as \"rainbow\") is a poor choice for a variety of reasons.\n\nWhen printed in black and white, it doesn't work.\nFor colorblind persons, it doesn't work properly.\nNot linear in color space, so it\u2019s hard to estimate numerical values from the resulting image.\n\nAnd 'jet' is no more the default colormap in matplotlib. 'Viridis' is the new default colormap from Matplotlib 2.0 onwards.", "Default colormap in Matplotlib", 1.2000000476837158, 1, 340, "Well, [jet](http://en.wikipedia.org/wiki/Jet_%28color%3F%5D) is indeed the best choice if you have to choose between several palettes. The most famous one is probably the [viridis](https://stackoverflow.com/questions/5761126/default-colormaps-for-matplotlib-in-python/8179453#8251529) palette.\nA lot of people would disagree with you. For instance, see the answers on [this question](//stackoverflowcom//questions//20554397//).\nThere are a lot more arguments on why [it is a good choice](./http//matplotblog.wordpress.c/21040078741/), but the most important is that it has a [continuous color scale](../http://www.sciencedirect.science/scienc/article/pii/0923670302/7732483) (instead of discrete colors), and this makes it more intuitive.", 0.2857142984867096, 0.02139037474989891, 0.11640211939811707, 0.025773193687200546], ["i m searching for something (library,function ecc..) in Python to generate a random discrete trajectory in the 2-D space.\nFor example: by providing the dimensions lenght and width of the plane and a starting point (x,y) i need to generate a sequence of points that represent the movement of an object (E.G. a human walking) over a random path.\nAre you aware of any such library or tool that helps accomplishing this?\nI have tried searching for something like that, but without success, I was searching for a shortcut/an easy to implement method", "I don\u2019t know any tools that can make that, however you can easily make a function that generate randoms points on a plane (representing a path). If you don\u2019t want points to be too far away from the previous one, you generate a random point in a specific area around the point.", "Movement/Trajectory generation in Python", 0.0996679961681366, 1, 341, "This is what [random.walk](http://docs.python.org/library/random-examples.html#randomwalk) is for.", 0.02857142873108387, 0.0, 0.02857142873108387, 0.03225806728005409], ["I need to find the point that will minimise the sum of Manhattan distances from a list of points.\nso given a list lets say [[x0, y0], [x1, y1] ...] (not sorted) I have to find the point to minimise the manhattan distance from that list of points. I understand the question but am having trouble with how i can complete this in O(n) time.", "You can find the median of a list of numbers in linear time.\nTaking the x-coordinates and y-coordinates separately, find the median of each. If you need integer coordinates, round half-values to the nearest integer.\nThe distance minimizing point (DMP) is (median of x-values, median of y-values). There may be multiple DMPs, but this will be one of them.\nWhy? Well, along either axis, if there are more points in one direction than the other, say p to the left and q to the right, p < q, then moving 1 to the right will increase the distance to p points by 1 and reduce the distance to q points by 1, so reduce the sum of Manhattan distances to points by q-p.", "How to find coordinate to minimise Manhattan distance in linear time?", 1.2000000476837158, 1, 342, "\\strong{Hint:}\nA point $P$ is the closest to the set of given points if\n$$\\sum_{x_i,y_j\\in A} \\min\\left\\{|P.x-x|,|y-y| \\right\\} = \\sum_x \\left(\\min_{y\\neq y_0} |P_y - x| + \\max_{i \\neq i_1}  |x - P_ix|  \\ \\text{and}   \\ 0\\leq y \\leq M-1 \\  0 \\le i \\to M\\right)$$\nor\n$P = (i_M,i_{M-0.5}) + (j_m,j_{m-M})$, where $i = i_{mod}(i+1)$ and $j = j_{0}(j+0)$.\nThe distance between a point and a line is\n$\\min(|x_{line}-x|^2 + |y_{point} - y|^6)$\nand the distance is 1 iff $y = x$ and 2 otherwise. The Manhontan distance can be obtained by summing the distances in $O(N", 0.3178294599056244, 0.03125, 0.19379845261573792, 0.026143791154026985], ["I have multiple CT datasets in Dicom format all with varying number of slices or 2D CT images.\nExample:\nDataset 1 Shape: (512 x 512) x 100\nDataset 2 Shape: (512 x 512) x 130\nDataset 3 Shape: (512 x 512) x 122\nHow can I resize the data such that the depth (number of slices) is the same for al datasets?\nThe idea being this data will be passed into a 2D CNN with input shape: [slices, 512, 512, channels 1]\nThanks for the help", "IMHO the short answer is you can't, but also you shouldn't even try.\n\nClinical data is like that.  Even for a scan of the same anatomical region (say pelvis), each scan (depending on clinical protocol, organization's protocols, slice thickness, technician decisions, patient symptoms, ..., ...) will have a varying number of slices.\n\nIf you try to train an algorithm based on a fixed number of slices you are guaranteed to develop an algorithm that may work for your training/test data, but will absolutely fail in real clinical use.\n\nI would suggest you google why AI algorithms fail in clinical use so often to get an understanding of how AI algorithms developed without a) broad clinical understanding, b) technical understanding of the data, c) extensive and broad training data and d) understanding of clinical workflows will almost always fail\n\nYou could, in theory, try to normalize the data's dimensions based on anatomy your looking at, but then you need to be able to correctly identify the anatomy you're looking at, which itself is a big problem.  ...and even then, every patient has different dimensions and anatomical shape.\n\nYou need to train with real data, the way it is, and with huge training sets that will cover all technical, clinical and acquisition variability to ensure you don't end up with something that only works 'in the lab', but will fail completely when it hits the real world.", "Resize the Depth of CT data in Python", 0.0, 0, 343, "You can do this with Numpy. I don't know if there is a function to do that, but this works:  `new_shape = (1,)*len(d1.shape),d2.reshape(newshape)`\nWhere d1 is one of your datasets.", 0.12274368107318878, 0.02181818149983883, 0.072202168405056, 0.03999999910593033], ["I have a point cloud and meshes (vertices=points of the point cloud).\nI want to project the point cloud with a certain virtual camera.\nHere, since the point cloud is sparse, the rendered result includes the points which should be occluded by foreground objects.\nTo resolve this issue, I want to use mesh information to identify which points should be occluded.\nIs there any smart way to do this in python?\nKind advice will be greatly appreciated.", "After hours of searching, I conclude that I have to re-implement a novel rendering pipeline to achieve my goal.\nSo, instead of this, I use a mesh-based renderer to render a depth map.\nAnd then I simply project the points of the point cloud with a projection matrix.\nHere, I use the depth map to check whether the point fits with the depth or not.\nIf the projected point is the one that should be occluded, then the depth of the point would be larger than the depth map value at the corresponding pixel.\nSo, such points should be ignored while rendering.\nI know that this is a less elegant and inefficient trick but anyway it works very well :)", "Projection of point cloud on 2D image based on mesh information", 0.1973753273487091, 1, 344, "You can use the mesh to make a projection of your points. For this, you can calculate the intersection between the 3d points and the triangle mesh. The mesh triangle is your projection point, and you just need to calculate their coordinates on the image plane. This way, your projected points will only be visible if they are behind the front surface of their mesh projection.", 0.27807486057281494, 0.021621622145175934, 0.14973261952400208, 0.03858521208167076], ["I am trying to create a script in Python to be used with Notepad++'s PythonScript plugin. So far I've gotten Pandas working correctly in the environment and I've read in a CSV and sorted it the way I desired, the end goal is to remove all of the text in Notepad (which I know how to work with at this point) and write in the text from my Pandas sorted CSV.\nThe issue is that when I write the text from that CSV to the console to check it, Pandas has reformated my CSV to make it easier to look at, it removes all of the quotes from the fields and adjusts the tab sizes (my files are tab delimited, with some tabs having different length). I need my CSV to be the exactly the same just sorted differently, If anyone can help it would be greatly appreciated.\nSome statements I'm using:\n(csv is a String containing all of the text in my CSV file)\npanda_csv = pd.read_csv(csv, sep=\"\\t\")\nsorted = panda_csv.sort_values(by=[\"Name\"], ascending=True, inplace=False)\nconsole.write(sorted.to_string())", "Since your original file seems to be tab-delimited, you can use the following to write output with a tab separator.\nsorted.to_csv('output.csv', sep ='\\t')", "How to read a CSV with Pandas but raw?", 0.0, 0, 345, "Panda' original delimiter is `\\t`, so you must specify this delimiter if you want to have the file read exactly as it is. This is the case of a file with a tab as delimiter, so, you could use `sep=\"\\\\t\"`, and `inplace` should be `False`.\nIf your file has a different delimiter (e.g. comma or tab and comma), you should tell Pandia to use a `delimiter=....` parameter. You should also set the `usecols` option to `None` (this means that Pander will read all columns) to avoid that it reads only one column (the first one, usually).\nMoreover, if the CSV is in this format `1,2,3,4,5,6`, Pandra will not read the last column, because the first column has only 1 value (1), and the second column also only has 3 values (2 and 5). This can be solved using the option `skipinitialspace= True`\nHere the full code: `pd.io.csv` has an example of how you can do this\n`pd = pandas.DataFrame(data=my_data, index=['a', '", 0.1890547275543213, 0.04020100459456444, 0.1094527393579483, 0.06045152246952057], ["I am looking to develop a machine learning model that would predict staff performance (e.g. staff ID 12345 will sell 15 insurance products next month.) I don't want to input staff ID into the training dataset because it will skew results. However I do need to be able to associate each staff with their predicted performance once the model is functional.\nIs the only way to go about this to develop the model excluding staff detail, then for prediction passing in a dataframe w/o staff ID, then associate the model output with staff detail by index / instance order?\nIt just seems like a round-about way for doing this.", "I think so. That is the only way I can think of too. Because you need to know you should not include the staff ID as the training data in your training model.\nSince you have used the Pandas module, you can easily search for which staff you want by using the DataFrame. Don't worry. I think it is a quite straightforward and fast way to map your predictions back to the staff IDs.\nSorry for not providing a new and better way. But I don't think you need to worry too much about the existing solutions, because I can't think of any bad effects like runtime. Hope it is helpful for you.", "Advice re: retaining client ID when training machine learning model", 0.1973753273487091, 1, 346, "If you are using `sklearn` you can use the `one_hot_encoding` function. It will create a new column in your data frame that will be a binary column with 0 and 2. The 3rd number is the index of your original column. So if you have a column of 4 you will get 9 columns of (0,1,2,4).", 0.2711864411830902, 0.05714285746216774, 0.1355932205915451, 0.057521555572748184], ["Is it possible to train a neural network using a subset of class labels?\nFor eg, I have a set of cifar10 images and I intend to train on [0-3,4-6,7-9] class labels, will it affect testing accuracy?", "obviously --if you measure accuracy over the full set of labels-- as your network will never be able to predict unseen classes reliably", "Pytorch - training on a subset of class labels, does it affect testing accuracy?", 0.0, 0, 347, "Yes. If you train the model on 70% of your data and the remaining 30 on the other labels you can expect to see a drop in performance. This is because the labels that are not in the training set will not be correctly trained and thus won't be able to generalize to unseen data.\nThis will happen to all neural networks if they are trained with only a part of the data available.", 0.2448979616165161, 0.0625, 0.20408163964748383, 0.03534461557865143], ["I don't know if this question has been covered earlier, but here it goes - I have a notebook that I can run manually using the 'Run' button in the notebook or as a job.\nThe runtime for running the notebook directly is roughly 2 hours. But when I execute it as a job, the runtime is huge (around 8 hours). The piece of code which takes the longest time is calling an applyInPandas function, which in turn calls a pandas_udf. The pandas_udf trains an auto_arima model.\nCan anyone help me figure out what might be happening? I am clueless.\nThanks!", "When running a notebook as a Job, you have to define a \"job cluster\" (in the contrast with an \"interactive cluster\" where you can attach to the notebook and hit run). There is a possible delay when the \"job cluster\" has to be spun up, but this usually only takes less than 10 minutes. Other than that, makes sure your job cluster's spec is the same as your interactive cluster (i.e. same worker's type, worker's size, autoscaling, etc).", "Databricks notebook runs faster when triggered manually compared to when run as a job", 0.0, 0, 348, "There are a few things you can do to reduce your runtime. You could try to run your job in parallel. That will split up the execution time. Another way would be to try a more efficient implementation of the pandas UDF. For example, if you have an example of your Udf, I could take a look at it.", 0.326241135597229, 0.04316546767950058, 0.1560283750295639, 0.025102589279413223], ["as the title says really. I've managed to implement a very simple LSTM model that takes an input sequence and outputs a given float value.\nFirstly, I'd like to know if it's possible to get answers taken from a set of possible answers. E.g. if I know the answer should be in [1,2,3] to output the answer as being 1.\nSecondly, if possible I'd like the output to be a probability distribution on the possible answers, e.g. [0.5,0.3,0.2].\nI've implemented my simple LSTM model in Python using the various keras packages. Any pointers to the right direction to learn about how to implement this would be great!", "LSTM is basically one type of recurrent neural network which provide many to many functionality. For that you need to add final dense layer with same number of input class in softmax layer, so you will get exact probability for each input class.", "Is it possible for LSTM to output a list of probabilities from a given list of possible outputs?", 0.3799489736557007, 2, 349, "Your first question is yes, it is possible. There are plenty of questions about this online. In the example below I use an LR network for this. The data I used was generated from an [IRIS](https://www.kaggle.com/c/iris-flowers) dataset. First I split the data into training and testing sets:\n\\begin{itemize}\n  \\item split_test_set(X_train, y_val, test_size = 0, shuffle = True, random_state = None):\n    X_traindata = X[:int(len(y_training) * test)]\n     ytrain = y[:test]\n      Xtestdata =X[int:(len (ytraining)-test)] # get the other 2/3 of the test data\n       ytest =y[test:]\n   \\strong{Xtrain.shape} = (15007, 4)\n \\begin {code}][Xtest.train(0).shape] =(561054,4)\\end{code}>\n        Xtraina = np.array( Xtrain) # training set\n        xt", 0.08433734625577927, 0.0, 0.0602409653365612, 0.04368932545185089], ["Assume we have a list of numbers (samples)\n\ndata = [0,0,1,2,3]\n\nI would like to fit a probability mass function for this dataset, in such a way that if I do something like\n\npmf.fit(data)\n\nand by executing something like\n\npmf.eval(0)\n\nI get\n\n0.2\n\nas return\nand\nby executing\n\npmf.eval(-1)\n\nI get\n\n0\n\nas return.\nNote that I am working with a discrete random variable here, so I am not fitting a pdf...", "I finally figured out myself\n\nrandom_array = [0,0,1,2,3]\n\n\nunique, counts = np.unique(random_array, return_counts=True)\n\n\nrandom_variable = sp.stats.rv_discrete(a = 0, b = np.inf, values = (unique, counts/np.sum(counts)))", "Is there any easy way to fit probability mass function to a given dataset?", 0.0, 0, 350, "You can use [`statsmodels.distributions.discrete.Multinomial`](http://www.stats.ox.ac.uk/pub/MASS/doc/generated/stats/discontinuous/multinom.html) for that, like this:\n\\begin{itemize}\n  \\item `Multicnomial(n=4, p=0., size=None)` returns the multinormal distribution with four categories and `p` probability to be in each category\n    (if `n` is 4 and not given, it is inferred from the number of\n     observations) with `size` of observations as the sample size. For\n   the example, we need to set `N = len(samples)`.\n      `0` and `-1` are the values of the index of `samples`.\n        We could also set it as `1:len(pm)`. For example: `pm = samples`\n       and then ` Multinorm(N=len, size =pm, n=3,p=1/3)`\n \\end{blockquote}", 0.08805031329393387, 0.0, 0.050314463675022125, 0.043478261679410934], ["I am trying to write a model and have two input tensors of shape = (None, 8, 384) and I need to select them based on index in their second position and combine them to get eight tensors of size (None, 2, 384).\nFor example, suppose T1 has a size of (None, 8, 384), which corresponds to the first variable with 8 cities and 384 days. T2 has a size of (None, 8, 384), which corresponds to the second variable with 8 cities and 384 days.\nI want to select the first city (None, 1, 348) from both T1 and T2 and combine them to make a new tensor of size (None, 2, 384).", "column_indices = tf.concat([tf.gather(T1, [0], axis=1),tf.gather(T2, [0], axis=1)], axis=1)", "How to get specific index (Column) in Tensors and merge them using TensorFlow", 1.2000000476837158, 0, 351, "You can select a particular slice of your tensor using the slicing function, like this:\n`your_tensor[2][0][2]`\nIf you want a single value of that slice you can use: `yourtensor.shape[1] * your_slice_index`", 0.0714285746216774, 0.0, 0.0714285746216774, 0.03552977740764618], ["I used 3 different algorithms (Linear Regression, Logistics Regression, Decision Tree) to solve the same prediction problem and I have to compare their error measures. The problem at first was that the MAE, MSE, and RMSE values kept changing with each run, it was really problematic for me. The suggested solution was to use random_state.\nThe \"random_state\" argument works for Logistic Regression and Decision Tree but Linear Regression doesn't take this argument. In that case, how do I keep the error measure values from changing? Is there any alternative to \"random_state\" for Linear Regression?", "The answer is simple : you don't need it since there is no local optima to stuck in with different random seeds\nbecause generally in logistic regression problems; there is a global optimum.", "Alternative to \"random_state\" for Linear Regression?", 0.0, 0, 352, "You can use [sklearn.linear_model.LinearRegression](http://scikit-learn-tutorials.readthedocs.org/en/latest/machine-learning-models/linear-regression/introduction/basic-example/index.html) with `randomize_data` parameter set to `True`.\nAlso [this answer](https://stats.stackexchange.com/questions/102946/how-to-use-random-state-with-logistic-and-linearregressions) describes this solution in more detail.", 0.17977528274059296, 0.0, 0.08988764137029648, 0.0486486442387104], ["As part of a university research project, I scraped job posts for 4 professions in Germany. Because I could not get enough job posts in only 1 language in the time frame I have, I decided to scrape for both English and German posts.\nI already went through the whole NLP workflow with both the English and the German text (tokenize, lemmatize, POS, stopwords,...) using different tools due to the language being different.\nNow I would need to extract the most common skills required for each profession and differences between them.\nI realize that this is a problem I should have predicted, but now I have two corpuses in two different languages which have to be analyzed together.\nWhat do you suggest is the best way to reach a scientifically sound end result with input data in two languages?\nSo far, no good solution came to my mind:\n\ntranslate the German input to English and treat with the rest\ntranslate the German input after processing word by word\nmanually map English and German words", "I work at a company that analyses news agency data in various languages. All our analytics process English texts only. Foreign language input is machine translated \u2014 this gives good results.\nI would suggest that for job adverts this should also work, as it is a very restricted domain. You're not looking at literature or peotry where it would cause a real problem.", "Best practice for dealing with NLP input in multiple languages for combined text analysis?", 0.0, 0, 353, "I'd take your time and split the data.  For example, you could run the NER in both languages and then split your results based on the type of job.", 0.17204301059246063, 0.0, 0.12903225421905518, 0.05732612684369087], ["Where is the linear predictors (eta) located in the statsmodels.GLM class?\nIf a fitted model mdl = sm.GLM(Y, X, family = family()).fit() is equal to R's mdl <- glm.fit(X, Y, family = family()), then R's eta can be found mdl$linear.predictors. But i can't seem to find eta in statsmodels.\nRight now i calculate them by X @ mdl.params, which seems a bit tedious", "eta is not a very descriptive name. The internal name in statsmodels is linpred.\nThe linear predictor including offset and exposure can be obtained using the results predict method\nresults_glm.predict(..., linear=True)\nor md1.predict in your case.\nOffset can be set to zero using the keyword to obtain the linear predictor without offset, similar for exposure.", "Where is eta in statsmodels GLM?", 1.2000000476837158, 2, 354, "The linear model predictor is not stored in `statsmodels`. The formula `Y ~ X` can only be used to predict values of `X`. To predict `y` given `x` you have to call `mlefit` from `sm.glm` or `fit()` from the `glrm` class. These methods are not available in R, but the Python equivalent is\n\\begin{itemize}\n  \\item `lrm = glrm.smooth(formula, params, method='mlogit')`\n   \\begincode{python} lrm=glrmsmooth(model,params)\n        lsm=lrcsm(lrf, model, etarho=eta_rho) #linear smoothing model\n    lrf=rfit.lfm(eta, lsf, rho, k=k)#linear regression formula\n      rfit=rf.Rfit('formulas/lm', 'formulations/cloglog', model)  #R fit\n     rrf = rf.rrf(y, x, alpha, p, b, w)   #r regression\n          rr = stats.rlm(x,", 0.27586206793785095, 0.03488372266292572, 0.14942528307437897, 0.05880766361951828], ["I have a df:\n\n\n\n\nmonth\nA\nB\nC\nD\n\n\n\n\n1994-07\n1\n2\nNAN\nNAN\n\n\n1994-08\n5\n2\n3\n4\n\n\n1994-09\n1\n2\n1\n1\n\n\n1994-10\n1\n2\n3\n1\n\n\n1994-11\n1\nNAN\n3\n1\n\n\n1995-07\n1\n2\n2\n4\n\n\n1995-08\n1\n2\n3\n4\n\n\n\n\nI want, for each column, to get the product of a rolling window of size 5, ignoring NAN values. Which means, in this case:\n\n\n\n\nmonth\nA\nB\nC\nD\n\n\n\n\n1994-11\n5\n16\n27\n4\n\n\n1995-07\n5\n16\n54\n16\n\n\n1995-08\n1\n16\n54\n16\n\n\n\n\nFor D(1994-11), for example, I would get 4 (4111), and C (1995-07) results in 54 (2331*3). I have tried:\ndf = df.rolling(window=5,axis=0).apply(prod(min_count=1))\nIt is an attempt of applying the function product from pandas.\nBut I get the error \"NameError: name 'prod' is not defined\"", "In case somebody comes to this: I have solved in the following way:\ndf = df.rolling(window=5, min_periods=1).apply(lambda x: np.prod(1 + x) - 1)", "Compute product with rolling window", 0.0, 0, 355, "You can do it by using [rolling function](https://pandas.pydata.org/pandas-docs/stable/reference/api/generated/numpy.prod.html) and [mask](http://%20https%3A%https2.%e2%80%98%E2)\n\\begin{itemize}\n  \\item `df['A'].rolling('5').apply('mask',axis = 1)`\n    => `mask` is a function which will return 0 if the value is NaN and 2 if it is 3. So, the result of masking will be a masked array. The `axis` argument is the axis in which we want to perform the mask operation. In this example `2` denotes the second axis. If we do `0` for the same axis, it will mask the values  from all the rows. Hence, we are masks the NaN values from the first axis i.e. column `A`\n   \\begin {code}", 0.15286624431610107, 0.0, 0.06369426846504211, 0.029629627242684364], ["I have a 3D MR image as a NIfTI file (.nii.gz). I also have a 'mask' image as a NIfTI file, which is just a bunch of 0s and 1s. The 1s in this mask image represent the region of the 3D MR image I am interested in.\nI want to retrieve the intensities of the pixels in the 3D MRI image which exist in the mask (i.e. are 1s in the mask image file). The only intensity feature I have found is sitk.MinimumMaximumImageFilter which isn't too useful since it uses the entire image (instead of a particular region), and also only gives the minimum and maximum of said image.\nI don't think that the GetPixel() function helps me in this case either, since the 'pixel value' that it outputs is different to the intensity which I observe in the ITK-SNAP viewer. Is this correct?\nWhat tool or feature could I use to help in this scenario?", "use itk::BinaryImageToStatisticsLabelMapFilter", "Getting the intensities of a certain region of an MR image", 0.2913126051425934, 3, 356, "It sounds like you have two images with different pixel depths, but you want a combined image with a single pixel size.  This is a common problem, and it's solved by using [Reslice](http://itk-www.itksystems.../Reslicer_8.2.0/doc/html/itkk/classitkreconreslic.html) from the [ITK Recon](https://www2it.org/software/recon) library. It'll take your 2D mask and give you a resliced image of that mask, with pixel values scaled to match the pixel sizes of your input 4D image, so the two image series will be aligned and you can just get the mean or median or whatever of each slice to get your final result.", 0.017241379246115685, 0.0, 0.017241379246115685, 0.06566531956195831], ["Editing this to reflect addition work:\nSituation\nI have 2 pandas dataframes of Twitter search tweets API data in which I have a common data key, author_id.\nI'm using the join method.\nCode is:\ndfTW08 = dfTW07.join(dfTW04uf, on='author_id', how='left', lsuffix='', rsuffix='4')\nResults\nWhen I run that, everything comes out as expected, except that all the other dataframe (dfTW04uf) values come in as NaN. Including the values for the other dataframe's author_id column.\nAssessment\nI'm not getting any error messages, but have to think it's something about the datatypes. The other dataframe is a mix of int64, object, bool, and datetime datatypes. So it seems odd they'd all be unrecognized.\nAny suggestions on how to troubleshoot this greatly appreciated.", "Couldn't figure out the NaN issue using join, but was able to merge the databases with this:\ncallingdf.merge(otherdf, on='author_id', how='left', indicator=True)\nThen did sort_values and drop_duplicates to get the final list I wanted.", "`join` method importing `other` dataframe values as `NaN`", 0.0, 0, 357, "If you are using `pandas 1.0.3` or later you can use `merge` instead of `pd.merge`. It is much faster and less confusing. You can read more about it here [here](https://pandas.pydata.org/docs/reference/api/pandas-merge.html).\nIf you want to keep the dataframe with `object` datatype then you could use this: (I am using your `df1` as an example)\n`df2.assign(authorid=df.author.astype('int'))` and then `np.where(pd1['author']==pd2['Author'],pd3['other_author'],np_na)`\nIn the above code `author` is the column in `DF1`, `Author` the one in DF2, `na` can be any value you wish to replace the `nan` with. `and then` part is because `assign` does not return the modified dataframe, so I am creating a new one.", 0.17241379618644714, 0.011627906933426857, 0.1149425283074379, 0.02970297448337078], ["Editing this to reflect addition work:\nSituation\nI have 2 pandas dataframes of Twitter search tweets API data in which I have a common data key, author_id.\nI'm using the join method.\nCode is:\ndfTW08 = dfTW07.join(dfTW04uf, on='author_id', how='left', lsuffix='', rsuffix='4')\nResults\nWhen I run that, everything comes out as expected, except that all the other dataframe (dfTW04uf) values come in as NaN. Including the values for the other dataframe's author_id column.\nAssessment\nI'm not getting any error messages, but have to think it's something about the datatypes. The other dataframe is a mix of int64, object, bool, and datetime datatypes. So it seems odd they'd all be unrecognized.\nAny suggestions on how to troubleshoot this greatly appreciated.", "You can use merge instead of join since merge had everything join does but with more \"power\". (anything you can do with join you can do with merge)\nI am assuming the NaN is coming up since the results aren't being discarded when you asked the first join to use on author ID and then include suffixes fo x an y. When you left join with merge you are discarding the non matches without any x and y suffixes.", "`join` method importing `other` dataframe values as `NaN`", 0.0, 0, 358, "I think that `left_index` and `right_on` are not necessary:  they are used if you want to left join on the same column in both data frames.  If you use `how='inner'` then they won't be needed.", 0.21848739683628082, 0.017094017937779427, 0.10084033757448196, 0.07692307978868484], ["Traceback (most recent call last):\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\flask\\app.py\", line 2073, in wsgi_app\nresponse = self.full_dispatch_request()\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\flask\\app.py\", line 1518, in full_dispatch_request\nrv = self.handle_user_exception(e)\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\flask\\app.py\", line 1516, in full_dispatch_request\nrv = self.dispatch_request()\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\flask\\app.py\", line 1502, in dispatch_request\nreturn self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\nFile \"C:\\Users\\admin\\Desktop\\VScode\\WorkProjects\\2022\\Product_Classification\\retention_ml.py\", line 169, in output_result\nresult_28 = xgboost_reg_281.predict(data[col_reg_28])\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\xgboost\\sklearn.py\", line 1047, in predict\nif self._can_use_inplace_predict():\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\xgboost\\sklearn.py\", line 983, in _can_use_inplace_predict\npredictor = self.get_params().get(\"predictor\", None)\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\xgboost\\sklearn.py\", line 636, in get_params\nparams.update(cp.class.get_params(cp, deep))\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\xgboost\\sklearn.py\", line 633, in get_params\nparams = super().get_params(deep)\nFile \"D:\\Miniconda3\\envs\\ppy39\\lib\\site-packages\\sklearn\\base.py\", line 205, in get_params\nvalue = getattr(self, key)\nAttributeError: 'XGBModel' object has no attribute 'callbacks'", "Check your xgboost library version. I loaded a model saved from xgboost==1.5.0 env to a xgboost==1.6.0 env and got the same error when operating on the model. I downgraded xgboost to 1.5.0 and everything worked fine. I suspect the model saving format is changing since 1.6.0 as it gives warning about me loading a binary model file using pickle dump.", "AttributeError: 'XGBModel' object has no attribute 'callbacks'", 0.1973753273487091, 2, 359, "You may want to check out this link:\nhttps://github.com/dmlc/xgb/issues/1701\nIssues with XGBoost and Python 3.7.8", 0.08602150529623032, 0.0, 0.06451612710952759, 0.04040403664112091], ["Say I have multiple images/plots produced by seaborn or matplotlib in a Python Jupyter Notebook. How do I save all of them into one PDF file in one go?", "in notebook : file => Download as => PDF ...\nor\nyou can import your file in google drive, and open it with google colab then : file => print => save it as a pdf.", "Jupyter Notebook Save Multiple Plots As One PDF", 0.0, 0, 360, "You can use the `pdfkit` library for this. The library can be installed with the command `pip install pdfkit`.\n\\begin{itemize}\n  \\item Import the library as follows:\n    `from pdfKit import PdfKit`\n  2. Create the PDF document.\n     `p = PDfKit('output.pdf')`  # create pdf document\n       \\begin {code}  p.setTitle('Output')  end{code)\n      \\strong{p.save()}  <-- save the document into the specified location\n        \\em{output file location} or `save('c:/path/to/file/output')`\n            \\end{blockquote} `\n          \\sout{  You will now have a pdf file with your plots in it!}\n         \\section{You will have to convert the pdf into an image. I don't know how to do that.}\n             \\textbf{I will try to find out how later.} \\textit{Edit: I couldn' t find a way to save it as an Image so I will just copy and paste the plot into word and save that file as a PDF. That is the only way I know", 0.20879121124744415, 0.0555555559694767, 0.1318681389093399, 0.014925369992852211], ["So, I have a metadata dataset with labels and their descriptions.\nA sample from the dataset looks like the following:\n\n\n\n\nLabel\nDescriptions\n\n\n\n\nRelease Date\nDate of formal issuance\n\n\nLanguage\nThe language of the dataset\n\n\n\n\nI want to train a ML model which learns the relationship between Label (Input X) and Descriptions (Target Y-categories) and then, can predict the category or the description of an unseen Label from the given list of Categories / Descriptions. Here we assume, the new label would be similar, in spelling or in meaning, to one of the labels used in the training model.\nUnfortunately, most of the algorithms try to map the description (which is usually a text document, review etc) to one of the categories (positive or negative etc)\nWould be great to get some help here, as to which algorithm would help me solve this problem.\nThanks in advance!!", "I don't think it is possible: it can't be framed as a classification task nor a translation/transformation one, in fact at a high level the description is a better explaination of the label, tailored with external (domain?) knowledge that cannot be expressed in any model I know of.\nBesides that, I don't think you have the necessary data amount and variability to express a sufficent generalization over the output.", "Is there a ML Algorithm to map labels (Single or max a couple of words) to description (a text telling more about a label)", 0.1973753273487091, 1, 361, "This is a typical multi-label classification problem, so you can apply any multi label classification algorithm. In this case, you have 2 options: 1) You can choose an algorithm that has a good performance for your data and 99% of your labels are positive, or  you may choose a multi class SVM, for which you need to know the proportion of positive labels. I recommend you to try both and compare the performance of both algorithms.", 0.2800000011920929, 0.027027027681469917, 0.1599999964237213, 0.04309449344873428], ["I have clustered the pixels of an image into clusters of different sizes and shapes. I want to max pool each cluster as fast as possible because the max pooling happens in one layer of my CNN.\nTo clarify:\nInput is a batch of images with the following shape [batch_size, height of image, width of image, number of channels]. I have clustered each image before I start training my CNN. So for each image I have a ndarray of labels with shape [height of image, width of image].\nHow can I max pool over all pixels of an image that have the same label for all labels? I understand how to do it with a of for loop but that is painstakingly slow. I am searching for a fast solution that ideally can max pool over every cluster of each image in less than a second.\nFor implementation, I use Python3.7 and PyTorch.", "I figured it out. torch_scatter. scatter_max(img, cluster_labels) outputs the max element from each cluster and removes the for loop from my code.", "How can I speed up max pooling clusters of different sizes and shapes of an image?", 1.2000000476837158, 0, 362, "First of all, if you want the image to be a tensor of size `[batch, image_height,image_width,channels]`, then you can just stack your label tensor into a new tensor: `labels = labels.reshape([batch*imageHeight,1,label_shape])`. Now you have to use a `torch.nn.functional.max_pooling2d()` over your `label` tensor, and then add it to your image tensor. The whole thing will look something like this: (pseudocode, i'm not using pyTorch)\n\\begin{itemize}\n1. create a matrix of indices that correspond to the pixel locations in your cluster, for example, let's say that the cluster is 10x11 pixels, then the matrix will be of shape: [12,4,2] (one column for x, one for y, two for channels).\n2. the output of the `maxPooling` will have 2x2 output with channels, you need to flatten this into an output matrix: `[2*2]`\n3.`add_` to both of your tensors. You'll need the label,", 0.11827956885099411, 0.0, 0.07526881992816925, 0.023996863514184952], ["Normally I would use the code torch.save(model.state_dict(), 'test.pth') to save the best model based on the performance of the validation set.\nIn the training phase, I print the loss and Accuracy in the last epoch and I got Loss:0.38703016219139097 and Accutacy:86.9.\nHowever, When I load the model which I just got from the training phase to print the loss and Accuracy, I would get the same Accuracy and different loss:  0.38702996191978456.\nWhy would that happen? I try different datasets and neural networks, but get the same result.", "If I've understood correctly, at the end of each epoch, you print the training accuracy/loss, and also save the model if it beats the current best model on the validation set. Is that it?\nBecause if my understanding of the situation is correct, then it is perfectly normal. Your \"best\" model in regards of the TRAINING accuracy/loss is under no obligation to also be the best in regards of the VALIDATION accuracy/loss. (One of the best examples of this is the overfitting phenomenon)", "Why the accuracy and loss number is different compared with the train phase if I load a trained model?", 0.0, 0, 363, "This is because of how the \\em{train} and \\strong{eval} phase works. When we run the \"train\" phase the network is trained on a specific dataset, and then in \"eval\" we use that model to evaluate on an other dataset. The loss, accuracy, etc. is calculated on different dataset so it can be different.", 0.2957746386528015, 0.014285714365541935, 0.1690140813589096, 0.04878048971295357], ["I am observing that if a is a list (or a numpy array) with elements [1,2,3] and I ask for a[1:-1:-1], then I get the empty list. I would expect to get [2,1] assuming that the slicing spans the indexes obtainable decrementing from 1 to -1 excluding the last value (that is excluding -1), that is indexes 1 and 0.\nThe actual behavior may have some justification but makes things more complex than expected when one needs to take a subarray of an array a starting from some generic index i to index i+m (excluded) in reverse order. One would tend to write a[i+m-1:i-1:-1] but this suddenly breaks if i is set to 0. The fact that it works for all i but zero looks like a nasty inconsistency. Obviously, there are workarounds:\n\none could write a[i+m-1-n:i-1-n:-1] offsetting everything by -n where n is the array length; or\none could write a[i:i+m][::-1].\n\nHowever, in case 1 the need to know the array length appears rather unnatural and in case 2 the double indexing appears as a not very justified overhead if the slicing is done in a tight loop.\n\nIs there any important reason that I am missing for which it is important that the behavior is as it is?\n\nHas this issue been considered by the NumPy community?\n\nIs there some better workaround than those I came up with?", "List[1:-1:-1] means List[start index : end index : jump]\nIndexing in List:\n\n\n\n\nNumber\n1\n2\n3\n\n\n\n\nIndex\n0\n1\n2\n\n\nIndex\n-3\n-2\n-1\n\n\n\n\nSo, if we take list a[1,2,3] and find list of a[1:-1:-1] means starting index = 1, ending index = -1, jump = -1\nSo, list traversing through the\n\nindex 1 (i.e. number=2) to index -1 (i.e. number=3) but jump = -1 (means backward position)\n\n\nSo, return an empty list i.e. []", "Why does a[1:-1:-1] with a=[1,2,3] return []?", 0.049958374351263046, 1, 364, "If we change the `a` list to `range(3)`, we see the following: `[1,-2,-3]`\nThis is because Python slices are from the end of the list, so `[3:-3:1]` is `[-3, -4, ..., 4]`.\nPython's [documentation](https://docs.python.org/3/tutorial/introduction.html#lists) explains it: \"Slices of lists have the same meaning as sifting: the leftmost value is omitted.\"\nIf we were to use the numpy version of `array`, we would see `[2.0,...,-4.5]`, which is exactly what we want. This is due to the fact the the python `list` version sifts from left to right, but the C/C++ numpy arrays sift from right to left. If you are used to c/c++, numpy might take some getting used too, because python is not exactly the most logical language. Here is an example of how to slice a `numpy` array: https://numpy.readthedocs.io/en/latest/reference/arrays.indexing.module-introspection.slicings.ht", 0.2049180269241333, 0.024793388321995735, 0.1147540956735611, 0.03812316432595253], ["I am observing that if a is a list (or a numpy array) with elements [1,2,3] and I ask for a[1:-1:-1], then I get the empty list. I would expect to get [2,1] assuming that the slicing spans the indexes obtainable decrementing from 1 to -1 excluding the last value (that is excluding -1), that is indexes 1 and 0.\nThe actual behavior may have some justification but makes things more complex than expected when one needs to take a subarray of an array a starting from some generic index i to index i+m (excluded) in reverse order. One would tend to write a[i+m-1:i-1:-1] but this suddenly breaks if i is set to 0. The fact that it works for all i but zero looks like a nasty inconsistency. Obviously, there are workarounds:\n\none could write a[i+m-1-n:i-1-n:-1] offsetting everything by -n where n is the array length; or\none could write a[i:i+m][::-1].\n\nHowever, in case 1 the need to know the array length appears rather unnatural and in case 2 the double indexing appears as a not very justified overhead if the slicing is done in a tight loop.\n\nIs there any important reason that I am missing for which it is important that the behavior is as it is?\n\nHas this issue been considered by the NumPy community?\n\nIs there some better workaround than those I came up with?", "-1 as an index has a special meaning [1], it's replaced with the highest possible = last index of a list.\nSo a[1:-1:-1] becomes a[1:2:-1] which is empty.\n[1] Actually, all negative indices in Python work like that. -1 means the last element of a list, -2 the second-to-last, -3 the one before that and so on.", "Why does a[1:-1:-1] with a=[1,2,3] return []?", 0.14888502657413483, 3, 365, "The reason why this is not considered an issue is that Python does not define any special syntax for slices of arrays. This means that sifting through the data in an efficient way is always the responsibility of the programmer. In your case, the `a[2:-2:1]` would return the desired values, but would also return `[2]` for `m=1` and `[3]` if `n=0`. You could use a `np.empty` array to create the list of indices to use for the slice: `ind = np.concatenate([-np-ind, ])`.\nNote that in Python, array indexing starts from `0`, so you'll have to add `1`, too. Then the resulting indices will be `[1,-1]`. Finally, you can pass the indices as the second argument to the [`numpy.s_`](https://docs.scipy.org/doc/numpy/reference/generated/ numpy.ufunc.html) function to make the subtraction: `-np._s_(ind[::2], 3)`.\nFor example: in your code, if you replace `i=2` with `2`, then the code will look like this: (", 0.30042919516563416, 0.034632034599781036, 0.16309012472629547, 0.05098039656877518], ["I have a data set that has the following columns. funciton: pd.melt()\nyears name date m1 m2 m3 m4 m5 m6 \u2026. to m12\nI set me variable name to month and try to include m1-m12, but I just cant get it to work. it will instead put everything in the new week column which looks like\nweek\nyear\nname\ndate\nm1\nm2\nI don't want the week year name date, is there a way to just put m1-m12 in like indexing? i have tried it it didn't work", "sample output\nmonth\nm1\nm2\nm3\n...\nm12\nhere is the answer i come up with using iloc!\nsorry for asking a easy question that I can figure out myself\npd.melt(.......value_vars=billboard.iloc[-12:])", "How do I include many columns in pd.melt function instead of just typing each and everyone out?", 0.0, 0, 366, "Use the `value_counts` method from the [pandas.Series](https://pandas-docs.github.io/pandas/reference/api/generated/numpy.ndarray.html) class.\nThis is the simplest way I can think of to get the unique values. I am not sure if that is what you want. It will not make a new column for each unique value, it makes a series with the counts in a single column.", 0.19230769574642181, 0.03921568766236305, 0.09615384787321091, 0.040462426841259], ["I am new to RASA. I gone through updated documentation Rasa 3 but I don't know how to pre-process the message of the user before nlu-model.\ne.g., if user enter hi, so i want to read that text before any action taken by rasa like tokenization etc.\nIf anyone can please guide me for this.\nEDIT: I want to capture user text in rasa itself, before any other pipeline action, so that I can do my own processing. (for learning purpose)", "In such scenario, you can handle the user message from front end (chatbot widget), specifically from JS script.", "RASA preprocessing, user entered text", 0.0, 0, 367, "If you use the Python client, you can use [`RasaClient.ask`](https://rasa.com/docs/rasamessaging/api/#rasaclient-ask) method. It returns a [`Message`](http://docs.rasainc.io/en/latest/core/message-api.html) object which contains the actual text (sent to the bot) and the text that was entered by the person. The `Text` attribute contains all the words that were input by user, and can be used to do whatever you want with it. You can also use it as a preprocessor before you send the input to your NLU model.", 0.09009008854627609, 0.01834862306714058, 0.07207207381725311, 0.05882352963089943], ["I have 8Khz data for voice recognition model training, but the model does not support 8Khz, so I want to upsample it to 16Khz. How can I upsample through the scipy library?", "This is going to get dirty, fast.\nYou need an interpolation filter.  It upsamples the input, then low-pass filters it back to the original passband.  This preserves the original signal and does not add \"interesting\" artifacts to it.\nStart by Googling \"interpolation filter\".\nGood luck.  You're going to need it.  (Yes, I've been down this route, a little bit, but that was some years ago and the code is most emphatically not releasable.)", "Upsampling Audio data", 1.2000000476837158, 0, 368, "You should not do that. You will have to increase the resolution of your input audio file to a resolution that the audio recognition service supports.\nAlso, you have not explained what the problem is. For example, are you getting an error? Is the recognition accuracy low? If so, this is most likely due to the input files not being a good enough quality.", 0.30000001192092896, 0.05797101557254791, 0.15714286267757416, 0.04436833783984184], ["I have a labeled dataset and  I am going to develop a classifier for a multilabel classification problem (ex: 5 labels). I have already developed BERT, and CNN, but I was wondering if I could use RL for text classification as well.\nAs I know, using RL we can use a smaller training dataset\nI am looking for a python code for RL.", "Reinforcement learning is a different thing from BERT or CNN. It is not actually a technique or a model, it is a type of problem(hidden markov models), and the set of techniques used to solve that problem.\nMore precisely, Reinforcement Learning it the class of problems where you have\n\nAn agent\nwho has to chooses actions to take\nThose actions will change its state and give it a reward\nWhere your goal is to maximize the reward.\n\nThis fits very well with game AI, or robotics applications for example.\nBut in your case, you want to develop a classifier from a labeled dataset. That is not a reinforcement learning problem, it is supervised learning", "Are there examples of using reinforcement learning for multi label text classification?", 1.2000000476837158, 1, 369, "A Reinforced learning model is a model that learns a policy function to choose actions from a set of available actions. A reinforce learning agent will use its learned policy to predict what action to take in each state and then choose to follow that action. The state of the system is then updated and the agent receives a reward signal to learn from. In a multi-label classification setting, you will have to encode your labels in some representation, usually a vector. Then you can train a reinforeced model to perform classification by predicting the most probable label vector and maximizing the expected reward (reward is the value of your label). The training of such a RNN is usually done by maximization of expected return. There are different approaches for this, including Reward Shaping and Actor-Critic algorithms. I will include some references below.", 0.39230769872665405, 0.06201550364494324, 0.19230769574642181, 0.06864563375711441], ["I am working on an inventory forecasting model and I require specific data in order to train and test the model. Currently, I am trying to use one year worth of data to build a basic linear regression model to predict for the following year.\nWhat I am having trouble with is removing outliers from my dataframe that contains 2 different types of outliers (\"quantity\" and \"dates\"), and I am only trying to remove the outliers using \"quantity\".", "You can remove the outliers by comparing them to the mean or median (I suggest using the median). Divide the distance between each value and the median by the distance between the maximum and median values if it is greater than a threshold value (eg 0.98, It depends on your data and only you can select it) Delete that data.\nFor example, if you set your threshold to 1, the farthest data will be deleted.", "How do I remove outliers from a dataframe that contains floating integers in Y-axis and dates in X-axis?", 1.2000000476837158, 0, 370, "\\begin{code}\n# your dataframe\ndf\n           Quantity      Date\n2018-12-04    386.0 0.250095\n1998.10      332.5   0\n66740226078  3565.4 11.346281\n30544157751  1427.6   9.894553\n8593170883   4790  97213.7\n4871637391   3643  584.96924\n529    23      1    9\n9       1      4   5     1 3 6\n0       0    0   1     4     8\n7       6      6 4      7 7    7  8 8    8   8     0     9 9   7   6  7     6     7      8      0         9     3     5       7        4", 0.03846153989434242, 0.0, 0.025641025975346565, 0.008029078133404255], ["The following code is giving me some unexpected output. In summary, I am defining a Dictionary (dict2) and then creating a series out of it. Then, I am re-assigning a new value to the Math course and the Science course using the Series' method. Only the value for Science changes (and for Math it is unchanged). Can you please help me understand why? Thank you.\nEdit: My goal is to understand why this is not working as expected, rather than actually reassigning a value to Math. I've also added the code here instead of the screenshot. Thank you.\ndict2 = {'Maths': 60, 'Science': 89, 'English': 76, 'Social Science': 86}\nmarks_series = pd.Series(dict2)\nprint(marks_series)\nmarks_series.Maths = 65\nmarks_series.Science = 90\nprint (marks_series)", "I restarted my notebook and that fixed the issue. I still wonder why it happened in the first place. But that's for another day.", "Can't Change the First Value in a Series Using Index as a Method in Python", 0.0, 0, 371, "Your `dict` is a dictionary, not a `Series`.\nTry:\n`dict_ = dict(**dict1)`\nTo turn it into a pandas Series\nThis will result in: `{'Math': '66', 'Physics': ...}`\nYour code works for me\nBut you could also turn your `Dict` directly into the `DataFrame`:\ndf = df.assign(Math = marks_df['Math'] * 1.25, Science = (df.groupby('Math').agg('mean')['Sci'] + 0.5)*10)", 0.11627907305955887, 0.0, 0.06976744532585144, 0.02857142500579357], ["The following code is giving me some unexpected output. In summary, I am defining a Dictionary (dict2) and then creating a series out of it. Then, I am re-assigning a new value to the Math course and the Science course using the Series' method. Only the value for Science changes (and for Math it is unchanged). Can you please help me understand why? Thank you.\nEdit: My goal is to understand why this is not working as expected, rather than actually reassigning a value to Math. I've also added the code here instead of the screenshot. Thank you.\ndict2 = {'Maths': 60, 'Science': 89, 'English': 76, 'Social Science': 86}\nmarks_series = pd.Series(dict2)\nprint(marks_series)\nmarks_series.Maths = 65\nmarks_series.Science = 90\nprint (marks_series)", "According to replays and info , your code working correctly and the problem may be case of importing pandas and your Env", "Can't Change the First Value in a Series Using Index as a Method in Python", 0.0, 0, 372, "This is a really confusing way to do this.  You're building a list of lists, which is inefficient for data like this because it takes up more space in memory. But the list you built is mutable, so you can reorder the elements at will. You can even make a copy of this list with a deep copy. And then you rebuild the same list by mutating it again. This will be very ineffective and very confusing. If you need to make changes to a data structure like a dict, you want to use the dict's mutator methods. Or you could just use a dataframe. Here' a working solution:\n\\begin{itemize}", 0.10447761416435242, 0.0, 0.0746268630027771, 0.05049445852637291], ["I'm doing a convolutional neural network classification and all my training tiles (1000 of them) are in geotiff format. I need to get all of them to a numpy array, but I only found code that will do it for one tiff file at a time.\nIs there a way to convert a whole folder of tiff files at once?\nThanks!", "Try using a for loop to go through your folder", "Is there a way to convert multiple tiff files to numpy array at once?", 0.0, 0, 373, "If your tiffs are not very big, you can use the [PyTiff](https://pypi.org/project/pytiff/) library. It is easy to use and has support for reading and writing multiple files in a single step. You can then load the numpy arrays using the `asarray` method of the returned `PyArray` object. Here is an example using [matplotlib](http://matplotlib.com/) to plot the resulting images.  Note that I have not tested the code. Please let me know if you find any errors. The code is as follows:\n\\begin{itemize}\n<h4>Define a function to load all tif files</h3\ndef load_tif_files(file_list):\n    for file in file_lst: # read all files\n        img = numpy.asfarray(py.TIF.imread(str(filename)))\n  # print the dimensions of each image\n      print(img.shape)\n</body>\nThe following code will read 12 tifs (as shown in the directory above) and print out the shape of all images in that directory. Note the use of `str`", 0.05847953259944916, 0.0, 0.035087719559669495, 0.0], ["I want to make a face recognition for employees as work. I already have system that gets image from cameras and outputs face embeddings (128-dimensional vectors). So my next step, as far as I understand, is to compare these embeddings with the one stored somewhere in database and find one with nearest distance.\nThe problem is that I want to enable machine learning for this. Initially, on like every tutorial, only one photo of employee is used to create a reference embedding. But what if a want to store multiple embeddings for one person? For example, maybe this person came with glasses, or slightly changed appearance so that my system no longer recognises it. I want to be able to associate multiple embeddings with one person or another, creating a collection of embeddings for each employee, I think this would improve recognition system. And if in future my system will show me that there's unknown person, I could tell it that this embedding corresponds to specific person.\nIs there any database that can store (maybe as array) or associate multiple vectors per person? I've looked into Milvus, FAISS, but didn't find anything about that.\nI use Python 3.9 with OpenCV3, Tensorflow and Keras for creating embeddings.", "If your embeddings come from different dimensions of a person, such as a face and a voiceprint. Then it makes sense to store two vector fields in milvus, one for the face vector and one for the voiceprint vector.", "What is the best approach for storing multiple vectors per person for face recognition", 0.06656807661056519, 1, 374, "\\begin{blockquote}\nBut what I I do if I have multiple photos of one employee and I don' t know which one to use to get the embedding?\n\\end{itemize}", 0.2028985470533371, 0.0, 0.17391304671764374, 0.02499999850988388], ["I want to make a face recognition for employees as work. I already have system that gets image from cameras and outputs face embeddings (128-dimensional vectors). So my next step, as far as I understand, is to compare these embeddings with the one stored somewhere in database and find one with nearest distance.\nThe problem is that I want to enable machine learning for this. Initially, on like every tutorial, only one photo of employee is used to create a reference embedding. But what if a want to store multiple embeddings for one person? For example, maybe this person came with glasses, or slightly changed appearance so that my system no longer recognises it. I want to be able to associate multiple embeddings with one person or another, creating a collection of embeddings for each employee, I think this would improve recognition system. And if in future my system will show me that there's unknown person, I could tell it that this embedding corresponds to specific person.\nIs there any database that can store (maybe as array) or associate multiple vectors per person? I've looked into Milvus, FAISS, but didn't find anything about that.\nI use Python 3.9 with OpenCV3, Tensorflow and Keras for creating embeddings.", "Maybe you can store one id for one person with different vectors in milvus", "What is the best approach for storing multiple vectors per person for face recognition", 0.06656807661056519, 1, 375, "You have to use an embedding model that supports multi-person faces. For instance, the EfficientNet model from the [FaceForensics++](https://github.com/facebookresearch/faceforensicspy) library is trained on a large dataset and can recognize multiple faces at the same time.", 0.145454540848732, 0.0, 0.072727270424366, 0.016129033640027046], ["I want to make a face recognition for employees as work. I already have system that gets image from cameras and outputs face embeddings (128-dimensional vectors). So my next step, as far as I understand, is to compare these embeddings with the one stored somewhere in database and find one with nearest distance.\nThe problem is that I want to enable machine learning for this. Initially, on like every tutorial, only one photo of employee is used to create a reference embedding. But what if a want to store multiple embeddings for one person? For example, maybe this person came with glasses, or slightly changed appearance so that my system no longer recognises it. I want to be able to associate multiple embeddings with one person or another, creating a collection of embeddings for each employee, I think this would improve recognition system. And if in future my system will show me that there's unknown person, I could tell it that this embedding corresponds to specific person.\nIs there any database that can store (maybe as array) or associate multiple vectors per person? I've looked into Milvus, FAISS, but didn't find anything about that.\nI use Python 3.9 with OpenCV3, Tensorflow and Keras for creating embeddings.", "1- you can store many embeddings for a person. when you have a face to compare, then you will compare it to the many images of each person. then, find the average of the similarity and decide they are same person or different.\n2- if you have many facial images for a person, then you will find embeddings for each photo then find an average embedding. suppose that you have 10 images for a person, you find 128-d embeddings for all of 10 photos. thereafter, you will find the average of each dimension and finally you will have one 128-d embedding.\ni recommend you to store your embeddings in spotify annoy, facebook faiss, nmslib or elasticsearch. you can find implementations of deepface library for python with those vector databases with a basic google search.", "What is the best approach for storing multiple vectors per person for face recognition", 0.0, 0, 376, "\\begin{blockquote}\nBut what If I don'T want it to remember the person\n\\end{code}", 0.07894736528396606, 0.013333333656191826, 0.05263157933950424, 0.029411764815449715], ["I have 3 scripts at the end of each script i have a dataframe results and i want to run this 3 scrips from one script and to show results (3 dataframes) that i will regroupe in one dataframe.\nIf you know how to run this 3 scripts at the same time and get results in one file (Dataframe)", "In scripts make sure you run them inside if __name__ == __main__: block(so you don't run then while importing). Turn those scripts into functions (or classes, depending on the structure of your code) and then import them to the main python file. Then write their results to one file inside the main script.", "How run multiple scripts from one scripts and retrieve results", 0.0, 0, 377, "You can do it in a for loop like this:\n\\begin{itemize}\n  \\item Define a function that takes as input the dataframe and the script to be executed. For example: `def func(dataframe, script_name):`\n \\em{Note:} This function will be used to call each `script` with the `datafram`e passed as the first argument. `func(df, 'script_1')`\n   \\begin {code}  script1\n    <do_stuff>\n        dataframe1 = <use dataframe>    # you can get data here\n         <still doing some stuff>       # etc\n       <end do_all>                       ## do whatever you want with dataframe 1. for example,\n                                            # merge it with df_0 and return it\n      <return dataframe_to_pass_in_next_call_of_func>   # pass it to the next function call\n     <call next script>             # <======= 2nd script\n           <same stuff as  in script 0>     # do the stuff again\n              <more stuff to do with data> # add whatever other stuff you need", 0.1340206116437912, 0.010416666977107525, 0.09278350323438644, 0.09055119007825851], ["I am using the tf_Agents library for contextual bandits usecase.\nIn this usecase predictions (daily range between 20k and 30k predictions, 1 for each user) are made daily  (multiple times a day) and training only happens on all the predicted data from 4 days ago (Since the labels for predictions takes 3 days to observe).\nThe driver seems to replay only the batch_size number of experience (Since max_step length is 1 for contextual bandits). Also the replay buffer has the same constraint only handling batch size number of experiences.\nI wanted to use checkpointer and save all the predictions (experience from driver which are saved in replay buffer) from the past 4 days and train only on the first of the 4 days saved on each given day.\nI am unsure how to do the following and any help is greatly appreciate.\n\nHow to (run the driver) save replay buffer using checkpoints for the entire day (a day contains, say, 3 predictions runs and each prediction will be made on 30,000 observations [say batch size of 16]). So in this case I need multiple saves for each day\nHow to save the replay buffers for past 4 days (12 prediction runs ) and only retrieve the first 3 prediction runs (replay buffer and the driver run) to train for each day.\nUnsure how to handle the driver, replay buffer and checkpointer configurations given the above #1, #2 above", "On the Replay Buffer I don't think there is any way to get that working without implementing your own RB class (which I wouldn't necessarily recommend). Seems to me like the most straight forward solution for this is to take the memory inefficiency hit and have two RB with a different size of max_length. One of the two is the one given to the driver to store episodes and then rb.as_dataset(single_determinsitic_pass=true) is used to get the appropriate items to place in the memory of the second one used for training. The only thing you need to checkpoint of course is the first one.\nNote: I'm not sure off-the-top-of-my head how exactly single_determinsitic_pass works, you may want to check that in order to determine which portion of the returned dataset corresponds to the day you want to train from. I also have the suspicion that probably the portion corresponding to the last day shifts, because if I don't remember wrong the RB table that stores the experiences works with a cursor that once reached the maximum length starts overwriting from the beginning.\nNeither RB needs to know about the logic of how many prediction runs there are, in the end your code should manage that logic and you might want to keep track (maybe in a pickle if you want to save this) how many predictions correspond to each day so that you know which ones to pick.", "How to use the replay buffer in tf_agents for contextual bandit, that predicts and trains on a daily basis", 0.1973753273487091, 1, 378, "We have a similar problem, and what we do is that we use a re-indexing mechanism. So, we save each batch of observations in a new variable and then when we load the model, load all of them in one shot and reindex them. The reloading mechanism of tf.train is very robust so you won't have any problem with that. If you are looking for more details, please let me know.", 0.21341463923454285, 0.01840490847826004, 0.11585365980863571, 0.03666480630636215], ["I have a huge set of numbers with a defined order. The logic in simple terms would look like this:\n\ndata['values'] = [1,1,3,4,4,-9,10]\ndata['order'] = [1,2,3,4,5,6,7]\nExpectedSum = 0\n\nWhat I wish to return is the original order and values of biggest possible subset of values that we can get with total sum equal 0.\nFor this case one optimal solution would be\n\nsolution['values'] = [1,1,3,4,-9]\nsolution['order'] = [1,2,3,4,6]\n\nThe sum could be also achieved by replacing 4th order number with 5th order number, however, one optimal solution is enough. The goal is to reach maximum possible size of subset with total sum =0.\nWas looking for variations of Knapsack problem and maximum subarray algorithms but none met my needs.\nAny hints or directions appreciated.", "Maybe I'm missing something (it's getting late), but if we denote a subset with k elements as S(k) and we have N elements in total, you could:\n\nsee if S(N) sums to 0; if so, that's the largest subset\nif not, see if any of S(N-1) sums to 0; there are N such sets\nif not, see if any of S(N-2) does; there are N*(N-1) such sets\n\nand so on. This is a \"brute force\" solution and probably far from optimal, but if the largest subset is expected to be relatively large (close to N in size) it shouldn't be too bad. Note that each step can utilize the sums computed in the preceding step.\nYour solution[order] seems to be the indices to the solution subset. It can be done of course, but I'm not sure why you need to get both the values and their indices? It's kind of redundant.\nFinally, while doable in pure Python, the NumPy library might be useful for this kind of problem.", "Largest subset from set of numbers (also negative) having sum equal 0", 0.0996679961681366, 1, 379, "You can create a dictionary of all the possible combinations: `possible_combinations = dict(zip(data[\"order\"], data[\"values\"]))`\nthen you can iterate over all combinations and find the one with the lowest sum:  `min(possible, key=lambda x: sum(x))`", 0.1165919303894043, 0.0, 0.06278026849031448, 0.04186046123504639], ["I am totally new to machine learning, after going through many tutorials I am bit confused over which python version is most stable for libraries like tensorflow and keras ?\nSome are suggesting python 3.7 while some are telling to use latest one. Which one should I use, any suggestions? Please help!", "anywhere from Python 3.6\u20133.9 should work fine, the version doesn't differ too much.", "Which python version should I use for machine learning?", 1.2000000476837158, 2, 380, "The latest version of Python is 2.x (2021) and the most recent version for Python 1.3 is Python27.\nTensorflow is compatible with Python3, so you could use the latest Python version, but Keras is not compatible (yet) with python3 so it is best to stick to Python-2-only for now.", 0.11428571492433548, 0.0, 0.11428571492433548, 0.05628478527069092], ["I am running a classification model in Microsoft Azure using the pyspark.ml.classification library with RandomForestClassifier.\nMy question:\nI know in sklearn.ensemble.RandomForestClassifier you can specify the n_jobs parameter to configure number of jobs to run in parallel.\nWhen using pyspark.ml.classification.RandomForestClassifier in Azure, I find that each job is run separately. It first runs, Job 1, when done it runs Job 2 etc.\nIs there a way to specify the number of jobs to run in parallel in the pyspark.ml.classification.RandomForestClassifier function?", "The Spark job you're describing does not have the same meaning with sklearn's job (which defining the parallelism via n_jobs parameter).\nSpark does run your classifier in parallel (in the background). The \"Job 1\" and \"Job 2\" etc is more about running some sequential steps, one after another, and each of them still running with multiple executors behind the scene.", "Is Microsoft Azure running jobs in parallel automatically?", 0.0, 0, 381, "There is no way for you to control the amount of parallelism used when running in Spark.  Spark will automatically use however many cores it has access to.", 0.19780220091342926, 0.0, 0.08791209012269974, 0.06866893172264099], ["I have fit a GAM model in h2o with several gam variables (P-splines) using the h2o.estimators.gam package. I'd like to get a table with the factor loading for every level of each gam variable. For example, one of my variables is age, and I need a table of the coefficient for each age.", "Right now we do not support categorical columns in generating splines.  It is on our roadmap though.", "Is there a way to create a factor table for spline terms using GAMs in Python's h2o", 0.1973753273487091, 1, 382, "This is a common problem in machine learning.  You can find many different algorithms for doing it. The algorithm I will propose is based on the paper:\n\\begin{itemize}\nhttps://www.jstatsoft.org/article/view/v022i11/3506/\nThe paper is \"Estimation of factor loadings from a regression model with an interaction term\" by John D. Thompson and Diane L. Shafer. It is available on ArXiv.com. You could use the H2O.glm function to fit your model and then use ht.FactorLoadings to estimate the loaders. If you want a more automated solution, you can use my R package, FactorWrangler.", 0.0683760717511177, 0.017391303554177284, 0.0683760717511177, 0.060236528515815735], ["I am installing matplotlib with pip install matplotlib  on a juypter hub instance.\nWhen I go to import I get the following error:\n\n----> 4 import matplotlib.plot as plt\n5 import seaborn as sns\nModuleNotFoundError: No module named 'matplotlib.plot'\n\nI couldn\u2019t find anything on here. I have tried to manually install via command line on Linux from git but that didn\u2019t seem to work either. Any help would be great.\nThank you.", "The proper command is import matplotlib.pyplot. If everything is correctly installed it shoud work.", "Plot attribute missing from matplotlib", 1.2000000476837158, 1, 383, "matplotlib has two versions, one which is installed by default via pip, and one from which it can be installed using conda. The former is for Python 3, the latter is Python2.  This means that if you're using Jupyter notebook, you must specify the version of matplotlib you want to use when you call `import matplotlib`.  In your case, it's `matplot lib` not `pyplot`.\nThe problem you describe is an example of a common mistake, that of importing matplotlib twice.", 0.1855670064687729, 0.021052632480859756, 0.12371134012937546, 0.023213287815451622], ["filtered_df = df[~df.index.isin(df_to_remove)]\nWhat does this ~ reduction mean?\nFound it in answers to the task? Was written by smn smart", "~ Operator performs a not logical operation bitwise, which means it takes a set of data and negates the condition that you are performing on your dataframe.\nIn your case, df.index.isin(df_to_remove)  will return a certain set of values, like [True, False, True...]\nWith ~ operator, the output would be the logical  negation, [False, True, False...] or just the negation of the original condition.", "What ~ in isin pandas method means&", 1.2000000476837158, 1, 384, "It's called the [boolean indexing](http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-index-selectors). It selects rows that don't match given criteria.\nIn your case, it will select rows, that are not in `df` that you're passing as a second argument.", 0.2300885021686554, 0.054054055362939835, 0.14159291982650757, 0.0683760717511177], ["We have a pandas DataFrame df and a set of values set_vals.\nFor a particular column (let's say 'name'), I would now like to compute a new column which is True whenever the value of df['name'] is in set_vals and False otherwise.\nOne way to do this is to write:\ndf['name'].apply(lambda x : x in set_vals)\nbut when both df and set_vals become large this method is very slow. Is there a more efficient way of creating this new column?", "The real problem is the complexity of df['name'].apply(lambda x : x in set_vals) is O(M*N) where M is the length of df and N is the length of set_vals if set_vals is a list (or another type for which the search complexity is linear).\nThe complexity can be improved to O(M) if set_vals is hashed (turned into dict type) and the search complexity will be O(1).", "Check for a column in pandas dataframe for all elements if they are in a set of values", 1.2000000476837158, 1, 385, "\\begin{code}\nIn [1]: df = pd.DataFrame(columns=['A', 'B'])\n   ...: df\nOut[1]\n  A   B\n0  a  b\n1  c  d\n2  e  f\n3  g  h\n4  i  j\n5  k  l\n6  m  n\n7  o  p\n8  q  r\n9  s  t\n\\end{pre}", 0.13008129596710205, 0.01652892492711544, 0.08130080997943878, 0.020472319796681404], ["Premise\nI am working on a private project in python using numpy. My program works with hexagons, which edges have certain features.\nI currently represent these hexagons by numpy arrays with one entry for each edge.\nThere is a need to compare hexagons for equality, which is special because some differentiable features are considered equal. Also hexagons should be equal if they are the same just in another rotation (numpy array is barrel-rolled by some amount).\nTo solve the first problem, edges are represented by bytes, each bit marking an \"equal\" feature. Now two edges can simply be compared by bitwise_and and any value non zero prooves equality.\nTo solve the second problem equality is checked by comparing all six rotations of a hex aginst the other hex.\nAll arrays I describe here are of dtype=uint8, so 255 is the neutral element for bitwise_and.\nProblem\nEach arrangement of a hexagon has a value associated with it. What I mean by arrangement is how features of the hexagon are arranged, for example two adjacent edges have one feature, the other four an other. The specific features dont matter for this value. So the hex [4, 4, 2, 8, 8, 8] has the same value associated with it as hex [2, 4, 4, 1, 2, 2] (notice rotation by one and substitution of values).\nAdditionally edges can have \"don't care\" as a feature (represented by all ones in binary to make the equality check work as intended). In that case I need to find the associated values of all compatible hexagons, no matter the feature of that particular edge.\nWhat I need is a clever representation of these \"meta\"-hexagons, which just care about arrangement not type of features.\nI then need a way for a given hexagon to find all meta-hexagons that could describe its arrangement.\nIdeas\nOne idea is to represent the meta-hexes like every other hexagon, using numbers to differentiate each unique feature. A meta-hex like [1, 1, 2, 1, 2, 2] would need to match any hex [x, x, y, x, y, y].\nHow to find all meta-hexes for a given hexagon with \"dont care\"-edges?\nA: One possibility would be to create all possible meta-hexagons for a given hexagon. The problem with this approach is that the number of possibilities can be quite large. For example, a common hexagon in my application is one with five adjacent \"don't care\"-edges (only one important feature). The number of features is only about 5 (it's actually more, but some features are mutually exclusive so 5 independent features is a good approximation) but even then 5^5=3125 (minus a couple because of equality under rotation) seems quite a lot. The advantage of this approach would be that I don't need any equality checks against the meta-hexes and could use a dictionary to access the values (for example using the numpy bytes as key).\nB: Another possibility would be to save the meta-hexes in a numpy array, which allows fast comparisons against all meta-hexes at once. In that case one could leave \"don't care\"-edges as they were (all bits one) and would only need to transform the given features into meta-hex representation. So [2, 8, 8, 255, 255, 255] would become something like [1, 2, 2, 255, 255, 255] and comparison could be done with bitwise_and and a nonzero check again to make the \"dont care\" edges match anything. The problem in this case is that the hexagon wouldn't match meta-hexes like [2, 3, 3, 1, 1, 1] where the features are simply numbered differently, even though it should. So all possible numbering schemes and rotations would have to be created. Even with rules such that numbers are increased by one from one feature to the next, it would be a couple dozen possible representations.\nQuestions\n\nIn general, is there a way to represent polygons so that two polygons with different rotations but otherwise equal, can be identified as such without having to compare all possible rotations?\n\nWhich numpy functions should I look into to implement my idea in A. Replacing all \"dont cares\" with all possible features sounds like permutations to me?\n\nFor my approach in B, is there a way to further reduce the amount of hexagons I have to create?\n\n\nAny help is appreciated, I've thought about this for three days now, going back and forth in my head. So even remotely related links and reading material or just keywords to lookup and/or add to the tags of this question are happily received.\nAlso I'm new here, so any tips regarding stack overflow are welcome!\nIf you've read this far, thank you for your time!", "It seems like you are chasing performance here... I'll answer some of the questions keeping that in mind but be warned: premature optimization is always a bad idea. If it is optimization you are looking for it is almost always best to write any solution and then take it from there.\nIn general you can win in terms of time complexity if you are willing to deal with a bit of extra memory.\n\nFor each hexagon create a copy of it but permuted such that the smallest \"edge\" is first. This has a little bit of startup cost and a bit of extra memory but then the comparison is easy as you only need to compare one array. (if the smallest element is repeated you can come up with some heuristic to create a unique order)\n\n2.3. I would create \"mask\" arrays that each hexagon has. This mask array is then used by your comparison function to decide what comparison rules to check for. If you just want code neatness then this just means creating a hexagon object which contains all these extra arrays and then overloading the __eq__property.\nIf you want all this to be fast then you probably have no choice but to implement this \"comparison\" operation in C and then having your python code call it.", "How do I efficiently represent and compare my data-type (hexagons)?", 0.1973753273487091, 1, 386, "First of, what you call \"hex\" is not a valid object in Python. I think you mean \\em{hexagon}.\n\\begin{itemize}\n  \\item If you are willing to store a list of tuples in each hex, that is what I would do.  In this case, it is possible to do a [bitwise XOR](https://stackoverflow.com/questions/54515914/how-do-i-efficiently-compare-two-numpy-arrays-with-different-number-of-elements) operation on two arrays to get the features that are different. From there, you can find those hexa-tuples that have all the different features as well as the ones that only have the \"non-bit-set\" features and those that match exactly. If I understand correctly, this should give you what is needed. But if the list is long, then you would probably want to use [numpy.allclose()](http://docs.scipy.org/doc/numpy/reference/generated/ numpy .all close.html) (in Python 3) or [isinstance() function]((http: //docs-tr.opencv", 0.3864229619503021, 0.05249343812465668, 0.17232376337051392, 0.046632133424282074], ["I have two large datasets. Let's say few thousands rows for V dataset with 18 columns. I would need to find correlations between individual rows (e.g., row V125 is similar to row V569 across the 18 columns). But since it's large I don't know how to filter it after. Another problem is that I have B dataset (different information on my 18 columns) and I would like to find similar pattern between the two datasets (e.g., row V55 and row B985 are similar, V3 is present only if B45 is present, etc...). Is there a way to find out? I'm open to any solutions. PS: this is my first question so let me know if it needs to be edited or I'm not clear. Thank you for any help.", "Row V125 is a value, perhaps you meant row 125. If two rows are the same, you can use the duplicate function for pandas or find from the home menu in excel.\nFor the second question, this can be done using bash or the windows terminal for large datasets, but the simplest would be to merge the two datasets. For datasets of few thousand rows, this is very quick. If you are using a pandas dataframe, you can then use the append function to merge them and find the duplicates.", "Data science: How to find patterns and correlations in two datasets in python", 0.0, 0, 387, "You can use scikit-learn' for this purpose. You need a feature matrix for both the datasets and then use [scikit' learn'](https://scikitlearn.org/stable/auto_examples/applications/plot_regression_model.html) to calculate the correlation between two matrices. It is quite simple and it takes 2-3 mins to do the same.", 0.2777777910232544, 0.056338027119636536, 0.1805555522441864, 0.03272727131843567], ["I have a pandas dataframe such as:\n\n\n\n\ngroup\nmonth\nvalue\n\n\n\n\n1\n1\n2\n\n\n1\n2\n2\n\n\n1\n3\n3\n\n\n2\n1\n7\n\n\n2\n2\n8\n\n\n2\n3\n8\n\n\n3\n1\n9\n\n\n3\n2\n0\n\n\n3\n3\n1\n\n\n\n\nAnd I want to calculate a new column ('want' in the below) equal to the value where month == 2, per group, as shown below:\n\n\n\n\ngroup\nmonth\nvalue\nwant\n\n\n\n\n1\n1\n2\n2\n\n\n1\n2\n2\n2\n\n\n1\n3\n3\n2\n\n\n2\n1\n7\n8\n\n\n2\n2\n8\n8\n\n\n2\n3\n8\n8\n\n\n3\n1\n9\n0\n\n\n3\n2\n0\n0\n\n\n3\n3\n1\n0\n\n\n\n\nAnyone able to help?", "Guess I could just create a groupby df (groupby group where mth == 2) then merge back to it.\nWill just go with that instead of attempting to do via a groupby.apply route!", "How do I return a value based on another column per group in pandas", 0.0, 0, 388, "Use `np.select` with the `needs_mask` parameter to create a mask: `df.groupby('group').value.apply(lambda x: np.where(x == df.group.eq(2), x, 0))`\nThen apply `fillna` to `wanted` (the masked column).\n`df['wants'] = df['value'].fill(0).groupby(df, sort=False).apply('wanton', axis=1)`\nThis will create the wanted column for each group.\nTo summarize: use `select`, `where` and `apply` for this problem.", 0.27450981736183167, 0.07999999821186066, 0.23529411852359772, 0.05369127541780472], ["I used tf.stop_gradient() to turn off the gradient calculation for some of the weights in my neural network. Unfortunately, tf.GradientTape().gradient() assigns the gradient for those weights as None, which does not work with optimizer.apply_gradients. The workaround is to afterwards assign zeros to those gradients.\nIs there a better work around?\nIs it possible to have tf.GradientTape().gradient() automatically replace None with zeros? Alternatively, is there a way to get the optimizer to work with None in the gradients list?", "If you are turning off the gradients for some weights, GradientTape().gradient() automatically sets the gradient for those weights to be None which are not compatible with an optimizer. However, they can be replaced by zeros by setting unconnected_gradients as such:\ntape.gradient(loss, weights, unconnected_gradients=tf.UnconnectedGradients.ZERO)\nThis is then compatible with the optimizer.", "TensorFlow: Can \"None\" in gradients be automatically replaced with zeros or used in an optimizer?", 0.0, 0, 389, "There is a `stopgrad` argument in `tf.grad_checker` that allows you to check for a gradient tensor being `None`.\nSo, your code can look something like this:\n`tf_g_tensor = tf_n_op.outputs[0] # the actual gradient`\n `check_stop = (tfgrad.is_none(tfg)) # check if the tensor is None` `if check_stopp == True: tfg = 0 # zero the non-none gradient tensors`", 0.26446279883384705, 0.0, 0.13223139941692352, 0.04526500031352043], ["I want to reproduct the code of cross modal focal loss cvpr2021. But I ran into some difficulties and I don't know where to find the solution. The difficulties are  the following.\n\nFile \"/data/run01/scz1974/chenjiawei/bob.paper.cross_modal_focal_loss_cvpr2021/src/bob.io.stream/bob/io/stream/stream_file.py\", line 117, in get_stream_shape\ndescriptor = self.hdf5_file.describe(data_path)\nRuntimeError: HDF5File - describe ('/HOME/scz1974/run/yanghao/fasdata/HQ-WMCA/MC-PixBiS-224/preprocessed/face-station/26.02.19/1_01_0064_0000_00_00_000-48a8d5a0.hdf5'): C++ exception caught: 'Cannot find dataset BASLER_BGR' at /HOME/scz1974/run/yanghao/fasdata/HQ-WMCA/MC-PixBiS-224/preprocessed/face-station/26.02.19/1_01_0064_0000_00_00_000-48a8d5a0.hdf5:''", "The instructions assumes that you have obtained the raw dataset which has all the data channels. The preprocessed files only contains grayscale and swir differences. If you want to use grayscale and one of the swir differences as two channels you can skip the preprocessing part as given in the documentation.", "RuntimeError: HDF5File - describe", 0.0, 0, 390, "It looks like the H5 file cannot be found. You can use the command `h5ls` to list the files in the current directory.", 0.2702702581882477, 0.0555555559694767, 0.18918919563293457, 0.04036921262741089], ["I have a GeoDataframe of about 3200 polygons, and another GeoDataframe of about 26,000 points. I want to get a third GeoDataframe of only the polygons that contain at least one point. This seems like it should be a simple sjoin, but geopandas.sjoin(polygons, points, predicate='contains') returns a GeoDataframe with more polygons than I started with (and very near the number of input points). Examining this GeoDataframe shows that there seem to be some duplicate polygons, perhaps explaining why I have more polygons than I expected. How do I find only the polygons that contain any point without duplicates?", "I found a workaround, although I feel like it's not the best solution. My polygons have a unique ID column on which I was able to remove duplicates:\ngeopandas.sjoin(polygons, points, predicate='contains').drop_duplicates(subset=['UNIQUE_ID'], keep='first')", "How do I find all the polygons of a GeoDataframe that contain any point of another GeoDataframe in GeoPandas?", 0.0, 0, 391, "This is a bug in geo_join.  It should only return unique polylines.\nhttps://github.com/geopanda/GeoAlchemy/issues/40", 0.09677419066429138, 0.0, 0.09677419066429138, 0.02912621572613716], ["when I import statsmodels.api as sm it gives the error import statsmodels.api as sm\nBut if I only import statsmodels as sm then it does not give any error\nbut a few days ago import statsmodels.api as sm was also working\nand I also tried pip install statsmodels --force-reinstall --user But it did not fix the problem\nAnd also my python file is not named statsmodels.py or statsmodels.ipynb", "after I reloaded vs code after running pip install statsmodels --force-reinstall --user it fixed my problem", "Error While Importing statsmodels.api \"AttributeError: module 'scipy' has no attribute '_lib'\"", 0.0, 0, 392, "I was facing the same error and I tried the following solution which worked for me.\nI had a python project named `project1.zip` and there was a module in that folder named as `statsmodels`.\nSo I renamed that module to `mystatsmodel.mod` then I created a `new project` named it `Project2.ZIP`. I then unzipped the project2 and copied the `import module` into my new project. This solved the issue.", 0.09090909361839294, 0.0, 0.09090909361839294, 0.01084431167691946], ["So this may seem like a simple question, but every question I've checked isn't exactly approaching the problem in the same way I am.\nI'm trying to bin the timestamps of a dataframe into specific buckets. I want to be able to count every minute of a dataframe starting from the first row until the last. I then want to turn that counted minute into a bucket (starting from 1 going to n). I then want to count every row what second it was for the timestamp of that row until the end of the bin.\nHere is an example of what I want it to look like:\n\n\n\n\ntime_bin\nseconds_in_bin\ntime\n\n\n\n\n1\n1\n2022-05-05 22:12:59\n\n\n1\n2\n2022-05-05 22:13:00\n\n\n1\n3\n2022-05-05 22:13:01\n\n\n1\n4\n2022-05-05 22:13:02\n\n\n\n\nI'm currently working in python and am trying to do this in pandas with my data. I feel like this problem is much easier than I think it is and I'm just not thinking of the right solution, but some help would be appreciated.", "I am not sure I quite get what you are going for here but wouldn't this be equivalent to getting the rank of seconds?\nAs far as I understand it, binning has to do with putting together an interval (fixed or not) and counting the number of items in it. If you could please elaborate on this I'll do my best to help with a more plausible answer.", "Bin rows by time with pandas", 0.0, 0, 393, "You can use `pd.cut` with `labels` argument and the `bins` parameter to define the bins: https://pandas.pydata.org/docs/reference/api/pandas-cut.html\nThen you can do `df['bin'] = pd.Series(pd['time']).dt.seconds.map(df.index.get_level_values('bin').astype(int))` and finally you have a new column containing the corresponding bin number of each row.", 0.19696970283985138, 0.015384615398943424, 0.12121212482452393, 0.058631930500268936], ["What is the basic difference between these two loss functions? I have already tried using both the loss functions.", "BCEloss is the Binary_Cross_Entropy loss.\ntorch.nn.functional.binary_cross_entropy calculates the actual loss inside the torch.nn.BCEloss()", "torch.nn.BCEloss() and torch.nn.functional.binary_cross_entropy", 0.0, 0, 394, "From the [documentation](https://pytorch-docs.org/stable/nn_api/function_doc.html#torcht.loss.BinaryCrossEntropy):\n\\begin{itemize}\n \\item The `torchs.bnn.CategoricalCrosentropicLoss` uses the `n` as `1` (which is used as the denominator in the equation).\n  \\em{It is worth mentioning that `BinaryLearningLSTM` is not considered for `crosenentrophy` loss.}\n   \\strong{Reason:} In the above equation, the cross-entroy will become `0` when `y = 1`, which will cause `p(y) = p(0)`.\n      \\begin {code} # Example: When y=0, x=1, and x_hat=2, then the binary loss is equal to\n p_y= 0.5 * p_(0) * (0*2 + 2*0 +1*1) / 4 =0\n     # p = (1/4) + (2/16) (where 32 and 6", 0.13235294818878174, 0.014925372786819935, 0.0882352963089943, 0.037593986839056015], ["img_height,img_width=180,100 batch_size=32 train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir1,validation_split=0.01,subset=\"training\",seed=123,image_size=(img_height, img_width),batch_size=batch_size)\nOutput: Found 1376 files belonging to 4 classes.\nUsing 1363 files for training.\nhow can I get the total number of classes in a variable?", "label_map = (train.ds.class_indices)", "how to obtain the number of classes using tf.keras.preprocessing.image_dataset_from_directory?", 0.0, 0, 395, "You can get it by the size of `train_samples_per_class`\nfor example,\n`train.shape[1]`\nwhere `shape = (1505, 3)`\nthe number is 5 classes\nor\nby using `y_train[0].shape` where `0` is the index of the class number you want to get\nif `1`  then `5` the classes of your dataset\nOr\nyou can also get this information by using the `dataset.class_indices` and `class.ind`", 0.07792207598686218, 0.02666666731238365, 0.07792207598686218, 0.028825638815760612], ["A noob question.\nAs I understand, the pipeline library of scikit learn is a kind of automation helper, which brings the data through a defined data processing cycle. But in this case I don't see any sense in it.\nWhy can't I implement data preparation, model training, score estimation, etc. via functional or OOP programming in python? For me it seems much more agile and simple, you can control all inputs, adjust complex dynamic parameter grids, evaluate complex metrics, etc.\nCan you tell me, why should anyone use sklearn.pipelines? Why does it exist?", "I have used pipelines recently for data exploration purposes.\nI wanted to random search different pipelines.\nThis could be at least one reason to use pipelines.\nBut you are right pipelines aren't verry useful for many other purposes.", "Scikit learn pipelines. Does it make any sense?", 0.0, 0, 396, "I'll try to explain why I like it, and why it is useful. I think that the best way to answer your question is to show you a simple example, to give you an idea of the power of pipelining. So I made this very simple model to make it easy to understand:\n\\begin{itemize}\n  \\item Take a picture, do some image processing on it to get a list of vectors\n    - I.e. take the RGB values of each pixel and flatten the list\n      - If the image is very small, it won'y have any depth, so I flatted it\n   \\strong{Note:} This is the most complex step. It's the only one that requires\n     a lot of code, as opposed to the rest. If you don\u2019t need to do any\n       image transformation, there is no reason to use a pipeline\n        \\em{but} there are other benefits of using it that I will get to in a\n         while. For now, just imagine you have to take an image and\n          flaten it as the first step in your process. (and yes, I know\n           it\u2019s a very simplistic model, but", 0.13913042843341827, 0.017543859779834747, 0.09565217047929764, 0.06107712909579277], ["I'm running one kernel to learn a Tensorflow model and that's using my GPU. Now, in the same conda environment, I would like to evaluate another model learned before, and the model is also a Tensorflow one. I'm sure I can run two kernels with the same conda environment mostly but I'm not sure when using GPU. Now if I run a kernel using Tensorflow, can it affect a kernel running early somehow, especially in terms of GPU usage?\nMy environment: Windows10, tensorflow2.1, python3.7.9", "This is not the best answer though, I realized that I can evaluate my model in another conda environment that has another version of Tensorflow. In this environment, my CUDA and CUDNN versions are not compatible with the version of Tensorflow, so my GPU was not used. In this sense, I evaluated a model without stopping or affecting learning a model in the running kernel.", "Can two kernels access the same conda environment at the same time even when using GPU?", 0.0, 0, 397, "Yes, it is possible.  I use this approach in my work on GPU training.\n\\begin{itemize}\n  \\item Make a new condo environment (I have two, one for training and one\n    for inference).  The environment can be on your local machine or\n      a remote server.   I prefer to use the local one, because it\n     tends to be faster.    I keep the remote one available for\n       testing.     I make the conco environment in a directory that\n        is not in any of my home directories. (For example, the\n         directory might be called something like \"my_data_dir\" and\n          is located in one of the home subdirectories. The\n           conca environment is in \"~/mydata/conda/envs/ml_env\"\n            with a cona environment that is the parent of \"ml\n             env\".   The conden environment contains the following\n              (note that I have removed the \"mkl\" part, which\n               was a workaround that made sure that my mkl\n                 libraries were loaded by the python kernel).\n   ...\n \\end{code}", 0.28828829526901245, 0.027272727340459824, 0.14414414763450623, 0.058124788105487823], ["Good day,\nI am quite new to Bokeh and I'm making a few charts. I want to bold only part of the text in the title of the chart.\n\nAs an example, I want to take this:\n\n\"Number of schools in District B in 2022\"\n\nAnd turn it into this:\n\n\"Number of schools in District B in 2022\"\n\n\nIs there a way to do that with a Bokeh chart and maybe some LaTeX?\nThanks!", "Basic plot titles are rendered directly on the HTML canvas, which has very simple text rendering options. Bokeh only exposes one set of text properties for standard titles. Accordingly, what you are asking for is not possible using standard titles.\nFor now, your best option is to not use a plot title at all. Instead. Put a Div above the plot in a column, and use the Div for a \"fancy title\" (since can contain arbitrary HTML to look however you like).\nIn the near future (Bokeh 3.0) LaTeX for plot titles will also be an option.", "Bokeh bold some text in title", 1.2000000476837158, 1, 398, "\\begin{itemize}\n\\item Use `sans-serif` font. It has bold variant. This way you can keep your chart text bold, but make the rest of text to be not bold. You can use this font: https://fonts.google.com/specimen/Sans+Serif.  Example: `bokehp.title(\"Number\", bold=\"yes\", font_family=\"Segoe UI\", size=30)`\n<a href=\"https://i.stack.imgur.xkcd.net/4Y2Q7.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nIt has a nice bold effect. But, unfortunately, it doesn't have italic variant, so you will need to use a bit of trickery to make it appear to have an italics effect: you need some math, like `0.8` in `x = 0,8*y`. This is not ideal, because you have to recalculate this every time you want different font size. Also, you should avoid using `Scipy` and `numpy` functions in your bokehs. Instead, use [`math_transform`](https:/x.stackexchange.math", 0.25702810287475586, 0.024291498586535454, 0.14457830786705017, 0.054585155099630356], ["I'm trying to translate coordinates from one picture (Res: 311, 271) to another picture (Res: 1920, 1080).\nThe coordinates don't need to be accurate in the 2nd picture, it just needs to be the same vector relative to the center of the images\nDon't know if that makes sense...\nEdit:\nSo far I've tried to calculate the difference between the center of the first image and the coordinates and then apply them to the bigger image. However this doesn't seem to work very consistently.", "You'll need to use trigonometry.\nSay there's some object in the image you're trying to get the vector for. Given the x and y distances from the center of the original image, you can tabulate the angle and hypotenuse distance. Simply use the same angle and scale the hypotenuse distance with the new size image.", "Translating coordinates of two pictures", 1.2000000476837158, 0, 399, "A common method to find the image coordinates (x,y) in a bigger picture is to use the ratio of width and height. For example, the x coordinates would be: x = (300 / 4096) * x0. The same would go for the y coordinates. This would assume that the second image has a ratio equal to that of its parent. If the parent is 8:5 and its child is also 5:2, then you would want to scale down the child by 0:05 before doing the math.", 0.33103448152542114, 0.0699300691485405, 0.22068965435028076, 0.042946863919496536]]}