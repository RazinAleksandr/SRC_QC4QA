{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API TabularDataset.to_pandas_dataframe(), it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, TabularDataset.to_pandas_dataframe() completes within few minutes.\nBy empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).\nI discovered the root cause while working on another issue mentioned [here][1].\nMy question is how can make TabularDataset.to_pandas_dataframe() work even when there are empty parquet files?\nUpdate\nThe issue has been fixed in the following version:\n\nazureml-dataprep : 3.0.1\nazureml-core :  1.40.0", "Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.\nI could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.", "AzureML: TabularDataset.to_pandas_dataframe() hangs when parquet file is empty", 1.2000000476837158, 1, 240, "This is now fixed. Please update your Azure Machine Learning to the latest version (32.20231).", 0.11428571492433548, 0.029411764815449715, 0.11428571492433548, 0.04999999701976776], ["I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API TabularDataset.to_pandas_dataframe(), it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, TabularDataset.to_pandas_dataframe() completes within few minutes.\nBy empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).\nI discovered the root cause while working on another issue mentioned [here][1].\nMy question is how can make TabularDataset.to_pandas_dataframe() work even when there are empty parquet files?\nUpdate\nThe issue has been fixed in the following version:\n\nazureml-dataprep : 3.0.1\nazureml-core :  1.40.0", "You can use the on_error='null' parameter to handle the null values.\nYour statement will look like this:\nTabularDataset.to_pandas_dataframe(on_error='null', out_of_range_datetime='null')\nAlternatively, you can check the size of the file before passing it to to_pandas_dataframe method. If the filesize is 0, either write some sample data into it using python open keyword or ignore the file, based on your requirement.", "AzureML: TabularDataset.to_pandas_dataframe() hangs when parquet file is empty", 0.0, 0, 241, "This is an old issue, however the reason for the issue is that parquets have a feature that allows it to use data that has not been read yet. The feature is called \"lazy loading\" and it can be enabled using a flag. I believe this was done to allow parquery to run faster, but in my opinion it is not very smart. So here is what we need to do in order to fix this issue: disable lazy loading. How to disable it? I don'tt know yet, that is something I will have to research. But for now, this is the best solution.", 0.19209039211273193, 0.011428571306169033, 0.11299435049295425, 0.06073809415102005], ["I am very new to python and I'd like to ask for an advice on how to, where to start, what to learn.\nI've got this fantasy name generator (joining randomly picked letters), which every now and then creates a name which is acceptable, what I'd like to do though is to train an AI to generate names which aren't lets say just consonants, ultimately being able to generate human, elvish, dwarfish etc names.\nI'd appreciate any advice in this matter.\nEdit:\nMy idea is: I get a string of letters, if they resemble a name, I approve it, if not - reject. It creates a dataset of True/False values, which can be used in machine learning, at least that's what I am hoping for, as I said, I am new to programming.\nAgain, I don't mind learning, but where do I begin?", "Single characters are not really a good fit for this, as there are combinatorial restrictions as to which letters can be combined to larger sequences. It is much easier to not have single letters, but instead move on to bi-grams, tri-grams, or syllables. It doesn't really matter what you choose, as long as they can combine freely.\nYou need to come up with an inventory of elements which comply with the rules of your language; you can collect those from text samples in the language you are aiming for.\nIn the simplest case, get a list of names like the ones you want to generate, and collect three-letter sequences from that, preferably with their frequency count. Or simply make some up:\nFor example, if you have a language with a syllablic structure where you always have a consonant followed by a vowel, then by combining elements which are a consonant followed by a vowel you will always end up with valid names.\nThen pick 2 to 5 (or however long you want your names to be) elements randomly from that inventory, perhaps guided by their frequency.\nYou could also add in a filter to remove those with unsuitable letter combinations (at the element boundaries) afterwards. Or go through the element list and remove invalid ones (eg any ending in 'q' -- either drop them, or add a 'u' to them).\nDepending on what inventory you're using, you can simulate different languages/cultures for your names, as languages differ in their phonological structures.", "How to train AI to create familiar sounding, randomly generated names?", 0.0, 0, 242, "You can use the [Word2Vec](https://en.wikipedia.org/wiki/Word_embedding) method for this.  There are a number of Python libraries that implement this, such as [gensim](http://radimrehurek.com/gensims/) and [keras-embeddings](pypi.python.net/pype/keraser-word2vec/).  Word2Vectors are used to represent words in a vector space, so that similar words are close together and different words far apart.", 0.1253918558359146, 0.031545739620923996, 0.0815047025680542, 0.05681817978620529], ["I was recently asked to create an in-place sorting algorithm in an interview. The follow up was to code it up to make it faster than O(n logn), discuss the time complexity of every loop.\nI understand that insertion sort, bubble sort, heap sort, quicksort, and shell sort are all in-place, however which of these can be modified to have better time complexity?", "A comparison based sort cannot be faster than O(nlogn). Since all the algorithms you mentioned are comparison based, none of them can have better time complexity.\nThere are algorithms like bucket sort and radix sort that can achieve O(n) in some cases, namely if the input is uniformly distributed.", "How to create an in-place sorting algorithm with faster than O (nlogn) time complexity?", 0.0, 0, 243, "For a given set of comparator functions `comparators` and a data structure `ds` that is already in the correct order, I can sort it in time `O(ds.size + k log k)`, where `k` is the number of elements that are the \\em{most} different from their proper place. That is, we're sorting `DS` by comparing elements to their neighbours. This is a variant of \"merge sort\" that can handle a significant amount of out of order data. We can implement this in O(\\em{[`k`](https://en.wikipedia.org/wiki/Big_O_notation#Notation)}log\\em{`[k]`}) by using a binary heap. (We can also use a \"red-black\" tree in place of a heap, but I won't go into that.)\n\\begin{itemize}\n  \\item Initialize a `binaryHeap` data type to the same size as `data` (with comparators `*comparator`).\n    \\begin {code}", 0.1649484485387802, 0.02083333395421505, 0.12371134012937546, 0.03599260002374649], ["I need to run a model but it needs older version of gensim with DocvecsArray attribute.How can i run it?\nAttributeError: Can't get attribute 'DocvecsArray' on <module 'gensim.models.doc2vec'...", "The DocvecsArray class was removed by Gensim-3.3.0, released in February 2018. So your model, or the code-installation that created it, used a version of Gensim more than 3 years old.\nHowever, it's possible your model will still load in Gensim-3.8.3 \u2013 I'm not sure, you'd have to try it.\nYou can always choose to install an older version of Gensim, or any other library, instead of the latest, when setting up your environment from public package repositories.\nIf you've been installing Gensim using pip at your command-line in your working environment, you could try uninstalling the current Gensim & then installing a specific exact earlier version:\npip uninstall gensim\npip install gensim==3.8.3\nBut, some caveats:\n\nyou might need to roll-back to gensim==3.2.0\nbecause of all the other potential libraries involved, which might also need to be rolled-back to compatible versions, it may make sense to start a fresh virtual environment, building it up with each requirement (starting with the most-important ones or the ones with the strongest version limitations)\nif using some other Python distribution or environment-manager (like conda), the installation commands will change a bit\n\nAlso note, the later versions of Gensim have many bug fixes & performance improvements, so if at all possible you should try to either migrate your model forward to be used with the latest library, or re-train a fresh model for the same purposes with either the original or updated data.\nOnce you manage to load it into a working version, you can re-save it, and another version close should be able to re-load it. For example, if 3.8.3 manages to load your model, then you re-saved it, then current 4.x Gensim should be able to load that save.", "Run model that need gensim older vesion", 0.0, 0, 244, "You can install the required version using the [docs](https://radimrehurek.com/gensims-docvec-docs/latest/generated/ gensims.model.Doc2Vec.html#gensiums.documents.vector) here.\n`pip install -r https://github.gensimos-release-downloads/release/2.4.0/docs/`", 0.08022922277450562, 0.011527377180755138, 0.05730659142136574, 0.062240663915872574], ["Let's say I have a CSV file with several hundreds of million records.\nThen I want to convert that CSV into a Parquet file using Python and Pandas to read the CSV and write the Parquet file. But because the file is too big to read it into memory and write a single Parquet file, I decided to read the CSV in chunks of 5M records and create a Parquet file for every chunk.\nWhy would I want to merge all those of parquet files into a single parquet file?\nThanks in advance.", "In general, it's the small files problem; for companies working with big data, file count limits can be an issue if one does not consistently control this problem.\nIt's a problem to be solved as there is no benefit for read performance if you split up files to small files (each parquet file consists of multiple row groups that ensures good parallelism during FileScan operations by itself).\nHowever, jobs gravtitate towards small files problem because there is a benefit for write performance as creating too large of a parquet file with too many row groups before it is flushed as a file can be extremely memory intensive (cost in resources provisioned and duration wise).", "Why would I want to merge multiple pieces of parquet files into a single parquet file?", 0.0, 0, 245, "When the parquets are stored in a directory, you can't access them as a parqeet file because you have to specify the path to each file in the schema.  You can store them in s3, and then you could access each parqueet by name.", 0.21118012070655823, 0.012578615918755531, 0.12422360479831696, 0.043472349643707275], ["Basically; I messed up. I have pickled some data, a pretty massive dictionary, and while my computer was able to create and pickle that dictionary in the first place, it crashes from running out of memory when I try to unpickle it. I need to unpickle it somehow, to get the data back, and then I can write each entry of the dictionary to a separate file that can actually fit in memory. My best guess for how to do that is to unpickle the dictionary entry by entry and then pickle each entry into it's own file, or failing that to unpickle it but somehow leave it as an on-disk object. I can't seem to find any information on how pickled data is actually stored to start writing a program to recover the data.", "pickle is a serialization format unique to Python, and there is no user-level documentation for it.\nHowever, there are extensive comments in a standard distribution's \"Lib/pickletools.py\" module, and with enough effort you should be able to use that module's dis() function to produce output you can parse yourself, or modify the source itself. dis() does not execute a pickle (meaning it doesn't build any Python objects from the pickle). Instead it reads the pickle file a few bytes at a time, and prints (to stdout, by default) a more human-readable form of what those bytes \"mean\".", "How to unpickle part of a pickled dictionary?", 1.2000000476837158, 1, 246, "Pickle is not designed for data of this size.  This is a limitation of Python, not pickles. Pickles are designed to store small data structures in a small amount of space. If you have a dictionary of 500MB or so, then the pickling step is going to fail. You can find out more about the size limitations of picklings [here](http://docs.python.org/2/library/pickling.html#pickles).\n\\begin{itemize}\n \\item Pickling a very large dictionary will fail\n  \\em{OR} \\begin {code}", 0.24731183052062988, 0.043478261679410934, 0.1505376398563385, 0.05822765454649925], ["My requirement is to read an excel using Pyspark, while doing same getting below error.\nOr else alternatively is there any solution using Pandas to read excel and convert into Pyspark dataframe ? Any one is fine.\nlat_data=spark.read.format('com.crealytics.spark.excel').option(\"header\",\"true\").load(\"a1.xlsx\")\nerror:\nPy4JJavaError: An error occurred while calling o756.load.\n: java.lang.ClassNotFoundException: Failed to find data source: com.crealytics.spark.excel.\nThanks in advance.", "You need to install the crealytics library. You can do it via pip:\npip install xlrd", "How to read excel xlsx file using pyspark", 0.0, 0, 247, "Please try the below steps\n\\begin{itemize}\n  \\item Open the file in excel\n    \\begin {code}", 0.06451612710952759, 0.0, 0.06451612710952759, 0.058762770146131516], ["As far as I understand, it is recommended to use numpy.random.SeedSequence() when running parallel processes with random number generators that are independent of each other. This seems to be easy when using the python multiprocessing library. However, I couldn't come up with a way to use this functionality with GNU parallel. Is there a neat way to ensure independent random number generation running python scripts using GNU parallel?", "Your idea in the comment about using a process ID is indeed a valid idea. The problem is that you want enough entropy in your seed - the output of a Pseudo-Random Number Generator is solely defined by its seed. Using the start time as a seed will give different output over time, but often the same output for parallel jobs.\nUsing only the process ID is also not a good idea, because it's typically a fairly small number. There's often only 16 or 32 bits of data in there. Combining it with the time adds entropy.\nNow you mention that the process ID's might be \"linearly dependent\" - it is common enough on Linux to have incrementing process ID's. This in itself is not a real problem. Any decent PRNG should have be strong enough to handle such seeds.\nOne notable exception is cryptography. In that case, the independence of the various PRNG's might be a much bigger concern, but we'd need more details to be sure. That's why the common advice is to use existing crypto libraries.", "Independent numpy random numbers using GNU parallel processes", 1.2000000476837158, 1, 248, "One way would be to spawn a Python process for each of the parallel jobs. Then each process will have a fresh random seed and the `random` module will be happy with it.\n\\begin{itemize}\n  \\item On GNU/Linux you could use a script like:\n    `for job in joblist; do python -c 'import numpy; numpy.__init__(None); import random; random.seed(random_seed); numpy._randomstate = random'; python job.py >&2; done`\n   This will create a new Python instance for the current job, then call `numpy` with the new random seeds, and then execute the Python code in the background. `>&` redirects the output to the pipe of `stdin`, so the child Python will write to `stdout` (which will go to `/dev/null` in this case) and `stderr` will end up in `/var/log/jobs.log`.\n      I'm using `bash -x` to enable tracing. That will help you find out what's happening. You can replace `>` with `>>` if you want to capture all output. If you are using a different shell, you", 0.3154929578304291, 0.03966005519032478, 0.1352112740278244, 0.05426356568932533], ["No code - just requested for a project to build a tool with matlab code and wondering if i can use Matlab code as the backend for a Streamlit app?", "Generally, no. You could possibly set up MATLAB to work as an API of sorts to the Streamlit app using FastAPI in Python, but there is no out-of-the-box support for creating a Streamlit UI over MATLAB.", "Is it possible to build a streamlit app with a matlab back end?", 0.0, 0, 249, "Yes, you can. You can find examples in the Streamlink package.\nhttps://streamlink.readthedocs.io/en/stable/quickstart.html#matlab-experimental\nYou can also check the examples from the MatLab package: https://github.com/Streamlit/streamlit-matlib", 0.21052631735801697, 0.0, 0.1315789520740509, 0.03932584077119827], ["I am very new to python but I have this idea in mind. I'd like to create an AI fantasy name generator. I've got a simple one, generating random strings of 3-11 letters, and my idea is: I get a word and decide if it's acceptable (Bggtrkl isn't, while Koerth might be), and in this process I build a dataset on which I'd base the machine learning. My question is - is it possible? If so - where do I begin? What to learn? I am open to any suggestion / advice.\nThank you.", "Yes it is possible - the key is to decide the criteria of what is acceptable input. So a first step is to make some rules about what a name is. Everything has to has some kind of boundary or limit which as the coder you'd have to determine because really any combination of letters/numbers/symbols at all could make an acceptable name to someone. This is why in machine learning people are employed to get data from all over to find a pattern to start with.\nThen in python these rules translate into the conditions you need to process the input.\n(...added...)\nThere is also the aspect that a name can be recognised according to the context it is in, eg could recognise a word is a name if it only occurs if referring to a person. Once that pattern is determined then the person could be another object such as an organisation or item. Then it is about the patterns in language grammar.  At the start I would focus on that aspect of program design and practice coding how to process the input to fit the original pattern with all the basic data structures available. Then research machine learning, AI, NLP - natural language processing to see how to extend my basic code. So to begin - design first.", "advice for trainig my AI to generate random names", 0.0, 0, 250, "You can try using the NLTK library. It'll give you a lot of functions and classes that are related to Natural Language Processing. Here is the link to NLP library:\nhttps://www.nltk.org/book/ch04.html\nAlso, I think you're looking for the [Natural Language Toolkit](https:https//www-nlp.google.com/nlu/index.php). It has a [random word generator](http://nlptools.sourceforge.net/api/nlpi.random.RandomWordGenerator-class.htm) that you can use.", 0.1812080591917038, 0.013513513840734959, 0.08724832534790039, 0.040920715779066086], ["I have two parameters which I want a neural network to predict. What is the best or most conventional method to implement the loss function? Currently I just define the loss, torch.nn.L1Loss(), which automatically computes the mean for both parameters such that it becomes a scalar.\nAnother plausible method would be to create two loss functions, one for each parameter, and successively backpropagate.\nI don't really see whether both methods compute the same thing and whether one method is better (or plain wrong).", "The probelm could be seen as a Multi-task Probelm. For example, two parameters represents A-Task and B-Task respectively.\nIn Multi-task, two loss function is often used.\nThe usual form is as follows,\n$$total_loss = \\alpha * A_losss(\\hat{y_1},y_1) + \\bata * A_losss(\\hat{y_2},y_2)$$\nThe $\\alpha$ and $\\beta$ is the weight of the loss function.Usually they are both 1 or 0.5.", "Pytorch - predict multiple parameters in neural network", 0.0, 0, 251, "If you want to compute a loss for two different parameters, you can do it as you suggested. The reason why you get a single loss value in the first case is because the `L2L` loss only takes two input parameters: the target and the output. If it is used for more than two inputs, the other inputs will be ignored.", 0.31343284249305725, 0.0, 0.19402985274791718, 0.04879949614405632], ["Using SymPy module, is there a method to tell me if the equation is linear or non-linear? If not what's your recommendation to implement such a method. I noticed from documentation that solve() returned values change according to the coefficients.\nI just need it as a simple introduction to high school students. to answer a question I found to label whether the function is linear or not.\nthank you very much.", "The answer depends on what you want to do next. To just get a True/False answer try Poly(expr, x).is_linear. This will expand the expression and tell you if it is linear in x.\nlinear_coeffs will either raise an error if non-linear terms are encountered or else give you the linear coefficients of the given variables, solveset.linear_coeffs(2*x+3*y+z**2,x,y) -> [2, 3, z**2] (where the last element is the constant term in the expression).\nlinear_eq_to_matrix and solve_linear are also possibilities.", "SymPy module, identify equation as linear or non-linear?", 1.2000000476837158, 4, 252, "The answer is no. There is nothing built in to Sympy to do this.", 0.1621621549129486, 0.03669724613428116, 0.10810811072587967, 0.21568629145622253], ["Hi I am using LightAutoML on supervised data can someone help me with how to do preprocessing in this framework I am using it for the first time\nI have tried to use train split but it says that input contains null values", "I found out in the documentation of lightautoml that it takes care of data preprocessing and feature engineering too", "how to do LightAutoML data preprocessing especially NaN Values", 1.2000000476837158, 1, 253, "You can use the `skip_null` parameter to handle missing values.\nSee here:\nhttps://lightautoml.readthedocs.io/en/latest/train_split.html\n`train.split(x_train, y_val, epochs=1, skip_missing=True, batch_size=32)`\nThis will skip the rows that have null or missing data.", 0.12121212482452393, 0.0, 0.12121212482452393, 0.02884615585207939], ["I'm trying to run code that is supposed to identify different types of image categories.\nThe code is of VGG16 and I have the following error.\nOf course I tried to install and import directories (keras etc...)and the error still exists.\nWould appreciate help.\nThanks.\nThis is the line of code that is marked\nvgg16 = applications.VGG16(include_top=False, weights='data3/')\nAnd that's the error\nAttributeError: module 'keras.applications' has no attribute 'VGG16'", "It should be applications.vgg16.VGG16(...).", "AttributeError: module 'keras.applications' has no attribute 'VGG16'", 0.0996679961681366, 1, 254, "There is a problem with the syntax. Please correct the code as follows:\n\\begin{itemize}\n  \\item In `from keras import applications`, change it to `import kerast.vg`\n    `app = kerasp.models.load_model('vggi15/vga1.h5')`\n   \\strong{instead of} `load` it should be `model`.\n     \\begin {code} </strong>\n      from kerasm.modelfeatures import\n        conv_blocks\n       from   kerasma.layers import Conv2D,MaxPooling2d,\n           Activation\n          from  kerasa.preprocessing import image\n         import tensorflow as tf\n            from tensorflow import keratas\n                    import numpy as np\n             from    kerass.utils.data_utils import \\\n               get_file\n                from KerasApplications import Vgg\n              from models import vgg_19,vge_net\n from\n kerase.kerastools import to_categorical\n import matplotlib.pyplot as plt\nimport numpy\nfrom sklearn.metrics import f1_score\ntrain =", 0.06611569970846176, 0.03361344709992409, 0.04958677664399147, 0.0], ["I'm trying to run code that is supposed to identify different types of image categories.\nThe code is of VGG16 and I have the following error.\nOf course I tried to install and import directories (keras etc...)and the error still exists.\nWould appreciate help.\nThanks.\nThis is the line of code that is marked\nvgg16 = applications.VGG16(include_top=False, weights='data3/')\nAnd that's the error\nAttributeError: module 'keras.applications' has no attribute 'VGG16'", "I solved same issue with from tensorflow.keras import applications instead of from keras import applications", "AttributeError: module 'keras.applications' has no attribute 'VGG16'", 0.0, 0, 255, "You can check this [link](https://kerastudies.com/kerassamples/imagenet/custom_networks/vGG_19/) for more information.", 0.0, 0.0, 0.0, 0.03960396349430084], ["ERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\nERROR: No matching distribution found for tensorflow-addons", "The reason why you see that error message is because tensorflow-addons is in beta development and built only up to python 3.9\nPlease downgrade your python version to 3.9, that should do the trick (for any operating system).\nAfter that, please run:\npip install tensorflow-addons==0.15.0\nYou should not see any uncompatibilities or error messages.", "cant install tensorflow-addons i have tensorflow version 2.8 and python 3.10", 0.1973753273487091, 1, 256, "I think you have the same error that I had when I installed Tensorflow for the first time. I'm using windows 11.\nWhat you can do is uninstall tensorflow and install a new version of python that has the add-ons already installed. And then reinstall tensorflow.", 0.29357796907424927, 0.0, 0.1651376187801361, 0.032406002283096313], ["i have a dataset of images and built a strong image recognition model. now i want to add another label to my model.\ni am asking myself, if i have to label every single image in my dataset, which has the requested attribute:\nsimple example:\nlets say i have 500k images in total and i want to label all images which have a palm on it.\nlets imagine that around 100k images have a palm on it.\nwould my model be able to recognise the label palm 80%, 90% or better, if i only label around 20, 30 or 50k images with a palm on it? or do i have to label all 100k images with a palm to get acceptable performance?\nfrom my point of view this could be interpretated in two directions:\n\nmultilabel image classification model  ignores all 0 labeled attributes and these wont affect model accuracy -> 20k labeled palm images would be good enough for strong performance, because the model is only interested in the attributes labeled as 1. (even if 100k labeled images would result in better performance)\nmultilabel image classification model will get affected by 0 labeled attributes as well. if only 20k out of 100k palm images will be labeled, the model gets confused, because 80k images have a palm on it, but arent labeled as palm. result would be weak performance regarding this label. if thats the case, all 100k images have to be labeled for strong performance.\n\nAm I right with one of the two suggestions or does multilabel image classification work different?\nI have a very big dataset and I have to label all my images by hand, which takes a lot of time. If my first suggestion works, I could save myself weeks of work.\nI would appreciate a lot, if you share your expertise, experiences and whys!", "The training process uses the negative cases just as much as the positive cases to learn what a palm is. So if some of the supplied negative cases actually contain a palm tree, your model will have a much harder time learning. You could try only labeling the 20k images to start to see if the result is good enough, but for the best result you should label all 100k.", "Keras, TF: Do I have to label all images when adding an attribute to a mutilabel image classification model?", 0.0, 0, 257, "I think your understanding of how ML models learn is a little bit off. The model does not care about the unlabeled data, so you can label as few images as you want. In fact, you could label just a single example of each class. However, a model that learns from unseen data may not be a good model for your task. I would label a reasonable percentage of your data and then check to see how well it performs on your test set. You may have better luck using a different model architecture or changing the learning algorithm.", 0.3076923191547394, 0.023952096700668335, 0.1420118361711502, 0.050503890961408615], ["I am using h2o autoML on python.\nI used the autoML part to find the best model possible: it is a StackedEnsemble.\nNow I would like to take the model and retrain it on a bigger dataset (which was not possible before because I would explode the google colab free RAM capacity).\nBut AutoML does some preprocessing to my data and I don't know which one.\nHow can I get the preprocessing steps to re-apply it to my bigger data before feeding it to the model ?\nThanks in advance,\nGab", "Stacked Ensemble is a model that is based on outputs of other models. To re-train the SE model you will need to re-train the individual models.\nApart from that AutoML will not pre-process the data. It delegates the pre-processing to downstream models. There is one exception - target encoding.\nDid you enable TE in AutoML?", "h2o AutoML - retrain stacked ensemble from autoML - preprocessing the data", 1.2000000476837158, 0, 258, "You can always look at the code and see what happens.  For example, the stacking happens in the `Stacking` class. That is part of the Auto-ML code base, but if you are using python you can just open it up and read it, or you could run it locally. In this case you will see it uses the [`FeatureSubsetSelection`](https://github.com/h20data/auto-ml/blob/master/src/main/java/com/_heroiclabs/AutoML/FeatureSelection/Stacked/featureSubSet.py#L32) class to determine the subsets. To recreate that you would do something like:\n\\begin{itemize}\n`df.setColumn(\"name\", \"categorical\");`\n   `df = df.getSchema().transpose();`  // this will transform the schema into an RDD\n    `autoML.stacking.featureSelection.FeatureSet featureSet = autoMl.model.createFeatureSelector(df).train(10);` // 128 is the default\n  `for (int i = 0; i < 4; ++i) {`", 0.23157894611358643, 0.021276595070958138, 0.12631578743457794, 0.052229490131139755], ["I was working on wine data on kaggle. Where there was a column named price has values like $32, $17, $15.99, Nan\nwine_data.isnull().sum()--After applying this code, there were a lot of missing values so I wrote another code i.e.\nwine_data['designation'].fillna(wine_data['designation'].mode()[0], inplace = True)\nwine_data['varietal'].fillna(wine_data['varietal'].mode()[0], inplace = True)\nwine_data['appellation'].fillna(wine_data['appellation'].mode()[0], inplace = True)\nwine_data['alcohol'].fillna(wine_data['alcohol'].mode()[0], inplace = True)\nwine_data['price'].fillna(wine_data['price'].mode()[0], inplace = True)\nwine_data['reviewer'].fillna(wine_data['reviewer'].mode()[0], inplace = True)\nwine_data['review'].fillna(wine_data['review'].mode()[0], inplace = True)\nThen I wanted to do a correlation of alcohol with rating and price with rating but both alcohol and price column has '%' and '$' these characters.So, I applied this code.\nwine_data = wine_data.assign(alcohol_num = lambda row: row[\"alcohol\"].replace(\"%\", \"\", regex=True).astype('float'))\nwine_data = wine_data.assign(price_numbers= wine_data['price'].str.replace('$','',regex = True)).astype('float')\nIt's throwing me an error like--\ncould not convert string to float: 'J. Lohr 2000 Hilltop Vineyard Cabernet Sauvignon (Paso Robles)'\nThen I tried this code:\nwine_data = wine_data.assign(price_numbers= wine_data['price'].str.replace('$','',regex = True)).astype('int')\nIt's throwing me an error like--\ninvalid literal for int() with base 10: 'J. Lohr 2000 Hilltop Vineyard Cabernet Sauvignon (Paso Robles)'", "Your data is not clean. One of the elements in your price column keeps containing the string 'J. Lohr 2000 Hilltop Vineyard Cabernet Sauvignon (Paso Robles)', which is why the column cannot be converted to float, even though you did some other cleansing steps.\nYou want be a bit more structured in your data cleansing: Do one step after the other, take a look at the intermediate df, and do not try to do many cleansing steps at once with an apply() function. If you have a messy dataset, maybe 10 steps are required, no way you can do all of that with a single apply() call.", "How to convert a datatype of a column with both integer and decimal numbers in Python?", 0.0, 0, 259, "You can use `pd.to_numeric` to parse the data. It will convert all characters except comma, dot and dash. To remove these exceptions, you can pass a list of characters to the `errors` parameter: `parse_dates=['', ' ', '']`.\nFor example, for the first column,\n`df = pd.DataFrame({'price':['$12.50', '$13.16', '', '$59.73']})`\nthen, the following will work: \\begin{code}\ndf['_'.join(pd.__settings__['parseFloatError'])('price')] = df['prices'].map(lambda x: pd.__parsePrice(x, parse_all=False, errors=parsefloat_errors) if pd._isnumeric(str(float(np.nan)) else str(nan), errors='coerce'))`", 0.1909547746181488, 0.010152284055948257, 0.08040200918912888, 0.05304518714547157], ["Lets say we have a numpy array A with shape (m_1,m_2,...,m_n) where n can be variable. Given a list of  n integers [i_1,...,i_n] I want to slice A as follows: A[i_1][i_2]...[i_n]\nWhat is the  easiest way to do this?", "see comment of hpaulj: Use a tuple, not a list", "How to slice numpy array across multiple dimensions by passing a list?", 1.2000000476837158, 1, 260, "You can use list slicing:\n`A[np.arange(n)]`\nwhich will produce an array with the same shape as `A` and where the corresponding indices will be replaced with `np_range(m)`.\ne.g.\nIf `n=2` then you get `array([[0, 1], [2,3]])` where `0` is replaced by `1`, `2`, etc.", 0.1269841343164444, 0.0, 0.095238097012043, 0.04999999701976776], ["This is a typical python coding challenge. Many beginners have a hard time handling it.\nFor example, we have a test array as:\ntest = [[1,2],[1,3],[2,4],[2,1],[2],[5,1],[3,4]]\nQ1: count the number of pairs in the list.\nQ2: count the number of pairs for 1.\nI know I can use the least/greatest function in SQL to do the job, but I don't know how to do it in python, especially in 2 dimension arrays.\nExpected result for Q1 is 5 ([1,2],[1,3],[2,4],[5,1],[3,4]\uff09\nExpected result for Q2 is 3 (2,3,5)", "len([k in test if len(k)==2]) and len([k for k in test if 1 in k]) should get you there.", "count paired numbers in 2-dimension python", -0.0996679961681366, -1, 261, "You can do this with a combination of `itertools.groupby` and `collections.Counter`\n\\begin{itemize}\n  \\item First, using `groupby`, group the elements of the `test` array into pairs of elements (where the first element is paire to the second).\n  2. Then, for each pair, use `Counter.from_iterable` to create a `dict` that counts the frequency of each element in that pair. This can be achieved with the following `lambda` function: `(k,v)=(k[0],v[1]-v[-1])` (see https://stackoverflow.com/a/4506115/5218354 for the explanation)  - that is, each value of `(v,k)` will be treated as a (frequency, element). Finally, iterate over the pairs and sum the frequencies. Note that this approach works even if the array is not sorted. See code below.  If you want to keep track of how many pairs there are, you can add a counter. If the counter is zero, it means that you have already counted the total number, so you don\u2019t need to count again", 0.13684210181236267, 0.0, 0.08421052992343903, 0.012987014837563038], ["I have some questions about multiple regression models in python:\n\nWhy is it necessary to apply a \u201cdummy intercept\u201d vector of ones to start for the Least Square method (OLS)? (I am refering to the use of X = sm.add_constant(X). I know that the Least square method is a system of derivatives equal to zero. Is it computed with some iterative method that make a \u201cdummy intercept\u201d necessary? Where can I find some informative material about the detail of the algorithm est = sm.OLS(y, X).fit()?\n\nAs far as I understood, scale.fit_transform produce a normalization of the data. Usually a normalization do not produce value higher than 1. Why, once scaled I see value that exceed 1?\n\nWhere  is it possible to find a official documentation about python functions?\n\n\nThanks in advance", "In the OLS the function you are trying to fit is :\ny=ax1+ax2+ax3+c. if you don't use c term, your line will always pass through the origin. Hence to give more degrees of freedom to your line which can be offset by c from your origin you need c .\n\nYou can fit a line without constant term and you will get set of coefficients (dummy intercept is not necessary for iterative computation), but that might not be the best possible straight line which minimises the least square.", "regression OLS in python", 1.2000000476837158, 1, 262, "\\begin{blockquote}\n\\begin {itemize} \\item Why is necessary apply dummy intercept vector to leas square? I am referring to use X=sm.Add_Constant(x).\nI know least squares method make system derivative equal zero but I don't understand where and how the \"dummmy\" vector is used.\nPlease, can you provide some information about how this method works? (the algorithm).\\end{itemize }\n-\nThe method does not use a \"intercept\" in the usual sense of regression (where you have to predict Y from X using a constant). However, it uses a \\em{constant}, namely the mean of Y. You can think of it as a special case of using an intercept with a fixed constant of 0. This is what you do by calling\n`sm = make_minimizer_residuals(Y,X)`\ninstead of `sm=make_model( Y,x)`. In the first case, the intercept is 5, which is the average of your y values. The intercept will be used to estimate the y-values that you don\u2019t have, namely for example, in your case 20 and 40", 0.3208955228328705, 0.04511278122663498, 0.13432836532592773, 0.03999999910593033], ["I need to interpolate a 4D data set (four angles, one output value). Angles are cyclic but scipy.interpolate.LinearNDInterpolator can't seem to take this into account. Is there a tool that can do this?", "You could duplicate all of your data at \u00b12\u03c0 on each dimension (for 4 dimensions, the easy way of doing this would create 81 copies of each point; the slightly harder way would create 16 copies, by adding 2\u03c0 to the angles between 0 and \u03c0, and subtracting 2\u03c0 from the angles between \u03c0 and 2\u03c0). That should ensure that every point you query (with all of the angles between 0 and 2\u03c0) will have neighbors \"on both sides\" in each dimension, for the linear interpolation to work with.", "is there a way to do 4D (linear) interpolation of data with cyclic (angle) coordinates in python?", 0.0, 0, 263, "You can use the `cylindrical` and `linear` parameter of `PolynomialInterp`:\n`poly = Polynomial1D(my_coordinates, degree=2)`\nand then:\n\\begin{code}\npoly(t, a=0, b=1)\n#or\ninterp(poly, my_data, t, 'linear', 'cylinndrical')\nfor a in range(0.01, 10):\n    interp_poly_a = interpn(polynomial_interpn, [a], [my data], 'polynominal', order=5)  # order 5 = 2^5-1\n...\nplt.plot(inter_pol_1_vals, inter_cur_values, label='Poly interpo')  ## interpolated data\n....\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.scatter(coords, data, c='r', label=f'scat data') # I have to set the colors as otherwise it does not work.\nx1 = np.linspace(ax_min,axmax", 0.1734693944454193, 0.0, 0.10204081982374191, 0.03846153989434242], ["I entered following command in my jupyter notebook: !pip install -U ibm-watson-machine-learning and with I can see the package install with !pip list.\nBut when I try to import like so: import ibm_watson_machine_learning, I get following error: ModuleNotFoundError: No module named 'ibm_watson_machine_learning'.", "SOLVED: For me, I simply needed to update all my packages in conda with conda upgrade --all.", "ModuleNotFoundError for ibm-watson-machine-learning package", 1.2000000476837158, 0, 264, "The ibmi-* packages are all for python3. To install them, use `pip3` instead of `python3`. You can also use the `venv` command to create a virtual environment for your ipython session. This way, you won't have to worry about python versions.", 0.1355932205915451, 0.0, 0.06779661029577255, 0.025974029675126076], ["I have a dataframe where one of the columns contains a list of values:\nexample:\ntype(df['col_list'].values[0]) = list\nI saved this dataframe as csv file (df.to_csv('my_file.csv'))\nWhen I load the dataframe (df = pd.read_csv('my_file.csv'))\nthe column which contains list of values change to string type:\ntype(df['col_list'].values[0]) = str\nWhen converting to list (list(df['col_list'].values[0]) I'm getting list of characters instead of list of values.\nHow can I save/load dataframe which one of it's columns contains list of values ?", "Use JSON or HDF file format instead of CSV. CSV file format is really inconvenient for storing a list or a collection of objects.", "saving and loading list values?", 0.06656807661056519, 1, 265, "You should use a csv.DictReader with the `delim_whitespace=True` parameter to properly parse a CSV file with commas and whitespace. For example: `pd.DataFrame.from_dict(csv.reader(file, delim=',',  delimiter=' ',  skipinitialspace=False, skiprows=1,  lineterminator='\\r\\n', quoting=csv, names=['a', 'b', c], encoding='utf-8'))`.\nYou can read more about it here: https://stackoverflow.com/a/55720235/8124954\nHope this helps!", 0.17977528274059296, 0.02298850566148758, 0.15730337798595428, 0.01801801472902298], ["so i am trying to use the Logistic Regression classifier and apply it on the UniGram bag-of-words feature set\nmy code:\nclf = sklearn.linear_model.LogisticRegression()\nclf.fit(tf_features_train, train_labels)\nprint (clf)\nerror message: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\ncan someone please help me", "The message is explicit: check the content of train_labels, it contains only class 1. Normally it should contain at least two different classes, otherwise there's nothing to classify.", "I'm trying to use the Logistic Regression classifier and apply it on the UniGram bag-of-words feature set", 0.0, 0, 266, "`sklearn` is very good library but it is not good for this task.\nIf you have only 4491 words in your vocabulary, then this is a linear model and you should use `LinearSVC` instead of `LogitReg` class. But your data is too small to build a good linear classifiers. Linear models work best with large datasets. You should try with `RandomForestClassifier` or `KNeighborsClassifer`.", 0.1505376398563385, 0.0, 0.10752688348293304, 0.012802443467080593], ["Currently trying to write code to check for data quality of a 7 gb data file. I tried googling exactly but to no avail. Initially, the purpose of the code is to check how many are nulls/NaNs and later on to join it with another datafile and compare the quality between each. We are expecting the second is the more reliable but I would like to later on automate the whole process. I was wondering if there is someone here willing to share their data quality python code using Dask. Thank you", "I would suggest the following approach:\n\ntry to define how you would check quality on small dataset and implement it in Pandas\ntry to generalize the process in a way that if each \"part of file\" or partition is of good quality, than whole dataset can be considered of good quality.\nuse Dask's map_partitions to parralelize this processing over your dataset's partition.", "Data Quality check with Python Dask", 1.2000000476837158, 2, 267, "Here's a start. You'll want to do more checks, such as is this a numeric column, does this column have the same units, etc.\nI'm using a dask dataframe with a bunch of dummy data (it'd be better to use your data), but you get the idea. This code should be run as a separate process, not in the main process that will be checking the data. It will then pass a dataframe back to your main program which will use it to look at the different columns. The idea is that you are not using your memory to calculate a single dataframe, but rather to run multiple computations that are all done on a shared memory cluster. If you want more detail on how that works, check out the [documentation](https://docs.dask,org/en/latest/dataframe-api-reference.html).\nThis code can easily be expanded to handle your use case. Feel free to ask follow up questions if needed. Good luck!\n\\begin{itemize}\nFirst, we load the dummy dataframe\nimport d2\ndf = d.DataFrame({'a': [1, 2, np.nan], 'b': ['x', 'y', np_", 0.20472441613674164, 0.0, 0.10236220806837082, 0.03592332825064659], ["I have a very big data frame with the orders for some products with a reference. This reference has periodical updates, so for the same product there are a lot of rows in the dataframe. I want to choose the last update for each reference, but i dont know why.\nFor a reference, for example there are 10 updates, for another, 34, so there is not a patron...\nAny ideas?", "you can use func iget like this :\ndf['column'].iget(-1);\nor\ndf.iloc[-1:]", "how to select the last value in a irregular data frame", 0.0, 0, 268, "Try:\n\\begin{itemize}\n  \\item [`.sort_values()`](http://pandas.pydata.org/pandas-docs/stable/generated/series.sortvalues.html)\n    \\begin {code} for i in range(len(df)):\n      df.loc[i, 'id'] = df['id'].sort().values[df.iloc[-1]['id'], 'ref']\n          print df\n   \\end{code}.\nor\n2. Use a dictionary instead of a dataframe: `df = pd.Series(dict(ref=['A','B','C','D']))`\n3. Create a list with all the keys in your dictionary, sort the list and get the first element\n4. You can sort by key or value, depending on your preference\n5. For example, if you want the 2nd last element, you'd do `max(lst, key=lambda x:x.value)[1]`\n6. `lst` is your sorted list. (if you don't know how you can read `help(max)`) and `[1]`, to get only the second element. Note that `[0]` would give the", 0.1139240488409996, 0.03846153989434242, 0.06329113990068436, 0.014925369992852211], ["I have a df like this:\n\n\n\n\nmonth\noutcome\nmom.ret\n\n\n\n\n10/20\nwinner\n0.2\n\n\n10/20\nwinner\n0.9\n\n\n11/20\nwinner\n0.6\n\n\n11/20\nwinner\n0.2\n\n\n11/20\nwinner\n0.9\n\n\n10/20\nloser\n0.6\n\n\n10/20\nloser\n0.2\n\n\n10/20\nloser\n0.9\n\n\n11/20\nloser\n0.6\n\n\n\n\nI would like to add another column, which has 1 / by the counts of times the value \"winner\" or \"loser\" appears per each month on the column outcome. The expected output for the example df is:\n\n\n\n\nmonth\noutcome\nmom.ret\nq\n\n\n\n\n10/20\nwinner\n0.2\n1/2\n\n\n10/20\nwinner\n0.9\n1/2\n\n\n11/20\nwinner\n0.6\n1/3\n\n\n11/20\nwinner\n0.2\n1/3\n\n\n11/20\nwinner\n0.9\n1/3\n\n\n10/20\nloser\n0.6\n1/3\n\n\n10/20\nloser\n0.2\n1/3\n\n\n10/20\nloser\n0.9\n1/3\n\n\n11/20\nloser\n0.6\n1/1\n\n\n\n\nI thought of using the function count to count how many times the values are repeated, but then I need to specify that the \"count\" should be done per each date. Any ideas?", "Use df['q'] = 1/df.groupby(['month', 'outcome']).transform('count').", "Filling a column with the amount of duplicated values in another column", 0.06656807661056519, 1, 269, "\\begin{code}\nimport pandas as pd\ndf = pd.DataFrame({'month':['12/01','13/8','23','33'],\n                   'out_date':[1,1.3,2,3.5],\n              'ret': [2.14,0,6,7]})\n\\end{pre}", 0.1428571492433548, 0.0, 0.095238097012043, 0.015606715343892574], ["When I run the below code:\nfrom statsmodels.regression import rolling\nI get this error message:\n\nAttributeError                            Traceback (most recent call last)\n/var/folders/q9/_s10_9yx6k7gxt3w4t7j0hgw0000gn/T/ipykernel_56663/1581398632.py in \n----> 1 from statsmodels.regression import rolling\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in \n456\n457\n--> 458 class RollingRegressionResults(object):\n459     \"\"\"\n460     Results from rolling regressions\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in RollingRegressionResults()\n514\n515     @cache_readonly\n--> 516     @Appender(RegressionResults.aic.func.doc)\n517     def aic(self):\n518         return self._wrap(RegressionResults.aic.func(self))\nAttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func'\n\nI've never had this problem before and I'm unsure what has gone wrong. I'm running statsmodels version 0.12.2 and Python 3.8.12 on MacOS 11.4. I'm trying to use RollingOLS.\nThanks for your help.\nEDIT:\nOut of curiosity I just replaced all '.func' with '' in this file and this issue no longer exists and the results seem to be accurate. I don't really understand what this did however and since I'm using this in a professional capacity I need to be sure this is correct.", "I can't reproduce this issue with macOS 12.1 - it's likely a problem with your code / system setup.\nHowever, 0.13.2 seems to work.", "Problem when importing statsmodels.regression.rolling (AttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func')", 0.0, 0, 270, "This is a bug in the latest version of statsmodel. You can try to downgrade to the previous version.", 0.17391304671764374, 0.0, 0.1304347813129425, 0.012195122428238392], ["When I run the below code:\nfrom statsmodels.regression import rolling\nI get this error message:\n\nAttributeError                            Traceback (most recent call last)\n/var/folders/q9/_s10_9yx6k7gxt3w4t7j0hgw0000gn/T/ipykernel_56663/1581398632.py in \n----> 1 from statsmodels.regression import rolling\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in \n456\n457\n--> 458 class RollingRegressionResults(object):\n459     \"\"\"\n460     Results from rolling regressions\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in RollingRegressionResults()\n514\n515     @cache_readonly\n--> 516     @Appender(RegressionResults.aic.func.doc)\n517     def aic(self):\n518         return self._wrap(RegressionResults.aic.func(self))\nAttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func'\n\nI've never had this problem before and I'm unsure what has gone wrong. I'm running statsmodels version 0.12.2 and Python 3.8.12 on MacOS 11.4. I'm trying to use RollingOLS.\nThanks for your help.\nEDIT:\nOut of curiosity I just replaced all '.func' with '' in this file and this issue no longer exists and the results seem to be accurate. I don't really understand what this did however and since I'm using this in a professional capacity I need to be sure this is correct.", "I was getting the same error when I was trying to import statsmodels.tsa.arima_model that has been removed & replaced by statsmodels.tsa.arima.model.\nThe steps I followed to troubleshoot the error are:\n\nUpdating pandas using this command: pip install --upgrade pandas --user\n\nUpdating statsmodels using this command: pip install --upgrade statsmodels --user\n\nAfter that I got the below error:\nNotImplementedError:\nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen removed in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and statsmodels.tsa.SARIMAX.\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained. It also offers alternative specialized\nparameter estimators.\n\n\nThen I resolved the error by replacing statsmodels.tsa.arima_model with statsmodels.tsa.arima.model .", "Problem when importing statsmodels.regression.rolling (AttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func')", 0.0, 0, 271, "It seems that the regression.Regressors.ols.RollingOls class has a method `aics()` that is defined as: `def aics(endog, exog=None, hats=hat_diagonal):` which is the culprit. It's probably the reason why you are getting the error. The problem is that it doesn' t exist. You should probably file a bug with statsmodel.", 0.1458333283662796, 0.021052632480859756, 0.1145833358168602, 0.07777723670005798], ["When I run the below code:\nfrom statsmodels.regression import rolling\nI get this error message:\n\nAttributeError                            Traceback (most recent call last)\n/var/folders/q9/_s10_9yx6k7gxt3w4t7j0hgw0000gn/T/ipykernel_56663/1581398632.py in \n----> 1 from statsmodels.regression import rolling\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in \n456\n457\n--> 458 class RollingRegressionResults(object):\n459     \"\"\"\n460     Results from rolling regressions\n~/opt/anaconda3/lib/python3.8/site-packages/statsmodels/regression/rolling.py in RollingRegressionResults()\n514\n515     @cache_readonly\n--> 516     @Appender(RegressionResults.aic.func.doc)\n517     def aic(self):\n518         return self._wrap(RegressionResults.aic.func(self))\nAttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func'\n\nI've never had this problem before and I'm unsure what has gone wrong. I'm running statsmodels version 0.12.2 and Python 3.8.12 on MacOS 11.4. I'm trying to use RollingOLS.\nThanks for your help.\nEDIT:\nOut of curiosity I just replaced all '.func' with '' in this file and this issue no longer exists and the results seem to be accurate. I don't really understand what this did however and since I'm using this in a professional capacity I need to be sure this is correct.", "statsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA\nhave been removed in favor of statsmodels.tsa.arima.model.ARIMA (note the . between arima and model) and statsmodels.tsa.SARIMAX.\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and is both well tested and maintained. It also offers alternative specialized parameter estimators.", "Problem when importing statsmodels.regression.rolling (AttributeError: 'pandas._libs.properties.CachedProperty' object has no attribute 'func')", 0.0, 0, 272, "The class `RollingResults` inherits from `RegressResult` which has a `aics` property which is a function that takes a string. This means that the method is defined in the class, but the function is not. The reason is probably that you are using a version of Python that has moved to using properties. In Python3 the property is essentially a descriptor which looks like a method but is actually a property. You can read more about the differences in [this blog post](https://nedbatchelder.com/blog/2019031/how_to_implement_a_descriptor.html).\nAs the code is written, the `func` is missing which leads to the error you see. If you simply remove the `.func`, then it works.", 0.06896551698446274, 0.0, 0.06896551698446274, 0.041786156594753265], ["I need a TF-IDF value for a word that is found in number of documents and not only a single document or a specific document.\nFor example, Consider this corpus\ncorpus = [\n'This is the first document.',\n'This document is the second document.',\n'And this is the third one.',\n'Is this the first document?',\n'Is this the second cow?, why is it blue?',\n]\nI want to get TD-IDF value for word 'FIRST' which is in document 1 and 4. TF-IDF value is calculated on basis of that specific document, in this case I will get 2 score for both indiviual document. However, I need a single score for word 'FIRST' considering all documents at same time.\nIs there any way I can get score TF-IDF score of a word from all set of documents?\nIs there any other method or technique which can help me solve the problem?", "tl;dr\nTf-Idf is not made to weight words. You cannot compute the Tf-Idf of a word. You can compute the frequency of a word in a corpus.\nWhat is TfIdf\nThe Tf-Idf computes the score for a word according to a document ! It gives high scores to words that are frequent (TF) and particular (IDF) to a document. TF-IDF's goal is to compute similarity between documents, not\nweighting words.\nThe solution given by maaniB is essentially just the normalized frequency of words. Depending on what you need to accomplish you should find an other metric to weigh words (the frequency is generally a great start).\nWe can see that the Tf-Idf gives a better score to 'cow' in doc 5 because 'cow' is particular to this document but this is lost in maaniB's solution.\nExample\nFor example we will compare the Tf-Idf of 'cow' and 'is'.\nTF-IDF formula is (without logs): Tf * N / Df. N is the number of documents, Tf the frequency of word in document and Df the number of document in which word appear.\n'is' appears in every document so it's Df will be 5. It appears once in documents 1, 2, 3 and 4 so the Tf will be 1 and twice in doc 5.\nSo the TF-IDF of 'is' in doc 1,2,3,4 will be 1 * 5 / 5 = 1; and in doc 5 it will be 2 * 5 / 5 = 2.\n'cow' appears only in the 5th document so it's Df is 1. It appears once in document 5 so it's Tf is 1.\nSo the TF-IDF of 'cow' in doc 5 will be 1 * 5 / 1 = 5; and in every other doc : 0 * 5 / 1 = 0.\nIn conclusion 'is' is very frequent in doc 5 (appears twice) but not particular to doc 5 (appears in every document) so it's Tf-Idf is lower than the one of 'cow' which appear only once but in only one document !", "How to get TF-IDF value of a word from all set of documents?", 0.06656807661056519, 1, 273, "You can use the `tfidf_vectorizer` method of `CountVectorizer`. For example:\n`vector = CountVectorizr(train, encoding='utf-8')`\nThe `vector` object contains the tf-idfs for every word in the documents. It also contains a `vocab` that maps the words to their tfidfc.", 0.11518324911594391, 0.02631578966975212, 0.0890052393078804, 0.04385965317487717], ["For example I have multiple lines log file\nI have mapper.py. this script do parse file.\nIn this case I want to do my mapper it independently", "Hadoop Streaming is already \"distributed\", but is isolated to one input and output stream. You would need to write a script to loop over the files and run individual streaming jobs per-file.\nIf you want to batch process many files, then you should upload all files to a single HDFS folder, and then you can use mrjob (assuming you actually want MapReduce), or you could switch to pyspark to process them all in parallel, since I see no need to do that sequentially.", "How to distribute Mapreduce task in hadoop streaming", 1.2000000476837158, 0, 274, "You can use the following Hadoop streaming example to split the log files:\nhttp://wiki.apache.org/hadoop/WordCountWordSplitter\nThis example will split a log into 50Mb files (max size of a single split).\nIf you need to limit the split size (e.g. to 100MB) you can add this parameter to the conf file: `-splitSize 20480m`", 0.283687949180603, 0.08633093535900116, 0.1702127605676651, 0.04089219868183136], ["I have the following intent classification data (4 columns, 65 rows):\nColumns:  \u00a0  intent-A   \u00a0   intent-B  \u00a0    intent-C    \u00a0  intent-D\nrecords:   \u00a0\u00a0\u00a0 \u00a0  \u00a0  d1a \u00a0\u00a0\u00a0\u00a0\u00a0    \u00a0  \u00a0     d1b   \u00a0\u00a0\u00a0\u00a0\u00a0   \u00a0  \u00a0    d1c     \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  \u00a0     d1d\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0   \u00a0  \u00a0     d2a    \u00a0\u00a0\u00a0\u00a0\u00a0   \u00a0  \u00a0     d2b    \u00a0\u00a0\u00a0\u00a0\u00a0   \u00a0  \u00a0   d2c      \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  \u00a0    d2d\nI am attempting to combine the columns into two columns to look like this (2 columns, 260 rows):\ndata \u00a0   intent\nd1a  \u00a0 intent-A\nd1b  \u00a0   intent-B\nd1c  \u00a0   intent-C\nd1d  \u00a0   intent-D\n\nd2a  \u00a0   intent-A\n\nd2b    \u00a0   intent-B\n\nd2c   \u00a0    intent-C\n\nd2d    \u00a0   intent-D\nI am using pandas DataFrame and have tried using different functions with no success (append, concat, etc.). Any help would be most appreciated!", "You can use the following code, (here df is your data frame)-\npd.DataFrame({\"Date\":df.values.flatten(), \"intent\":df.columns.tolist()*65})", "Extracting selected columns to new DataFrame as a copy in python", 0.0996679961681366, 1, 275, "Here is a way you can do it using `concat`\n\\begin{itemize}\n  \\item Create an empty df\n    \\begin {code}", 0.2857142984867096, 0.05000000074505806, 0.1428571492433548, 0.03268253430724144], ["Let's suppose that I have n points, and a square numpy matrix where i,j'th entry is filled with the distance between the point i and point j. How can I derive an adjacency matrix in a way that, there is an \"edge\" between point a and point b if for every other point c, max(dist(c,a), dist(c,b)) > dist(a,b), in other words there is not any other point c such as c is closer to a and b than, a and b are to each other. I could write this in numpy with some for loops, but I wonder if there is any easier/faster way to do so.", "This is really hard to do without a concrete example, so I may be getting this slightly wrong.\nSo you have an nxn matrix (presumably symmetric with a diagonal of 0) representing the distances.  Let's call this matrix A.\nThen A[:,None,:] is an nx1xn matrix such that if you broadcast it to nxnxn, then A[i, j, k] is the distance from the i'th point to the k'th point.  Likewise, A[None, :, :], when broadcast to nxnxn, gives a matrix such that A[i, j, k] gives the distance from the j'th point to the kth point.\nSo B = np.maximum(A[:,None,:],A[None,:,:]) is an array such at b[i, j, k] is the maximum of the distance from i to k or from j to k.  Take the minimum of B along the last dimension, and you've got the value for the best possible k.  Edges are those values for which np.min(B, axis=2) <= A.\nAgain, try this out on your own computer.  I may have slight details wrong.", "Deriving an adjacency matrix wrt distance", 1.2000000476837158, 1, 276, "You could use a `scipy.spatial.distance.pdist` implementation that does not use kd-trees (like `manhattan_distance` or `euclidean_distances`) to compute the distances and then use `numpy.where` to find the pairs of points that are not neighbors. You can also use the [scikit-neighbors](http://scikits.org/sci-kit_learn/neural_networks.html) module for this.", 0.16806723177433014, 0.008474576286971569, 0.11764705926179886, 0.0569620206952095], ["I need to write a complicated function that will evaluate a new column for a DataFrame in pandas.\nThis function will have to use data from multiple (more than 10) columns of this DataFrame.\nIt won't fit into a lambda, to plug it in easily to the apply() function.\nI don't want to write a function that takes more than 10 arguments and plug it into apply(), because it would hurt readability of my code.\nI would rather not use for loop to iterate over rows, as it has poor performance.\nIs there a clever solution to this problem?", "If all column values are on the same row you can use apply(func, axis=1) to pass a row from your df as argument to function func. Then in func, you can extract all values from your row.", "Writing a complicated function that will by applied to a DataFrame", 0.0996679961681366, 1, 277, "For this kind of things, I usually create a class which will hold the data and the calculation needed. This way I can easily create the class and use it on a dataframe.", 0.1388888955116272, 0.0, 0.1111111119389534, 0.022058824077248573], ["I devlopped a flask app in which I use pandas.\nWhen I start the python environment using\nSource myenv/bin/activate\nAnd run.py\n=> everything is ok and the app run normally\nBut when I try to deploy the app using mod_wsgi it crushes with this importing pandas error\nPandas/init.py line 13\nMissing_dependencies.append(f\"{dependency}:{e}\")\nAm I missing something ?\nI use the standard mod_wsgi config that is working for with another app that doesn't use pandas\nThanks", "I found a solution for my problem:\nIt was due to python 2 used to compile mod_wsgi\nI changed that to python 3\nNow everything is working fine", "Issue with import pandas error using mod_wsgi", 0.0, 0, 278, "Found the problem. I have two environments that are very similar. In one of them, I had installed the numpy dependency in anaconda with \"conda install numpy\", and in the other with pip. Because the two envrionments are so similar, it was easy to confuse the dependencies. Once I removed numpy from the env that was using pip, the deployment worked.", 0.17777778208255768, 0.022727273404598236, 0.15555556118488312, 0.043103452771902084], ["I am trying to add the elements that are in a row vector to a 1X1 vector in python,\nProblem:\n[10 100]\nSolution:\n[110]\nIs there any way to achieve this?", "Example provided in question is more of a list.\nTo sum up all the elements in a list, sum() function can be used.\ne.g:\nsum([10 100])\n//output: 110", "Adding elements in a 1 D vector", 0.0996679961681366, 1, 279, "\\begin{code}\na = [1,2,3] # a is a list\nb = 42\nc = a + b\n\\end{pre}", 0.1702127605676651, 0.04444444552063942, 0.1702127605676651, 0.038335178047418594], ["Why is df.head() (withoutprint) enough to print rows from the dataframe in Google Colab but in PyCharm, I have to actually use print(df.head()) as df.head() alone will not print any rows?\nI am wondering if this could be due to different versions of Python between Google Colab and what I have in PyCharm on my local machine or if maybe it's due to a setting in PyCharm?\nNot a big deal to have to use print in PyCharm but just asking since I am just learning Python and I was stuck for a while trying to figure out why df.head() wasn't working in PyCharm like it did in Google Colab until I figured I had to use print.", "Google Colab uses Jupyter notebook. If you are using a Jupyter notebook in Pycharm, it should work similarly to Google Colab. If you are using a normal Python File with .py extension, you have to use the print statement.\np.s. I use VS Code since it supports Jupyter notebooks directly in the editor and works similar to Google Colab.", "Colab vs PyCharm - df.head() vs print(df.head())", 1.2000000476837158, 1, 280, "`df` is a dataframe and `head()` is the method for retrieving the first n rows of a data frame. You are trying it without printing the data. So it will print nothing. In Colaboratory, you can use `print(x)` instead of `x` and it is sufficient. But in your local environment, it has to be `import x, y, z`, then `y.method()` or `z.parameter()`.\nHope it helps!\nBest regards,\nKim\n<a href=\"https://i.stack.imgur.com/d19XZ.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nhttpss://www.youtube.colab.research.google.cn/video/9f4137a4c7549c51b17f66931810f168/blob?usp=drivesdk", 0.2292993664741516, 0.012903225608170033, 0.12738853693008423, 0.047269873321056366], ["I am receiving below error in Azure synapse Pyspark notebook\nTypeError: AutoMLConfig() takes no arguments\nwhile running below code:\nautoml_settings = {\n\"primary_metric\": 'r2_score',\n\"enable_early_stopping\": True,\n\"experiment_timeout_hours\": 0.5,\n\"max_cores_per_iteration\": 1,\n\"max_concurrent_iterations\": 2,\n\"enforce_time_on_windows\": True,\n\"exclude_nan_labels\": True,\n\"enable_tf\": False,\n\"verbosity\": 20\n}\nautoml_config = AutoMLConfig(\"task\": 'regression',\n\"label_column_name\": label,\n\"compute_target\": compute_target,\n\"featurization\": 'auto',\n\"training_data\": train_data\n**automl_settings)", "You're using the wrong version of Apache spark pool. If you change to a pool with version 2.4, then it should fix your problem.", "TypeError: AutoMLConfig() takes no arguments in Azure Synapse", 0.0, 0, 281, "This is a bug in the Pandas module in Jupyter.\nPlease follow the instructions here to update Panda: https://github.com/Azure/auto-ml/issues/438\n(For reference, the issue was reported as a \"Python 3.7 bug\" and was fixed in Python 4.0)", 0.11267605423927307, 0.0, 0.11267605423927307, 0.055045876652002335], ["I have a 0.4 KV electrical network and I need to use particle swarm optimization algorithm on it to find the optimal place and size for DGs but I'm new to optimization subject I tried a lot but I couldn't know how to do it could anyone help me with it please", "From the paper \"Prakash, D. B., and C. Lakshminarayana. \"Multiple DG placements in distribution system for power loss reduction using PSO algorithm.\" Procedia technology 25 (2016): 785-792\", PSO algorithm is given below\nStep 1: Input data such as line impedance, line power.\nStep 2: Calculate voltages at each node and total power loss in the distribution network using forward backward sweep method.\nStep 3: Initialize population size.\nStep 4: Initialize number of particles to be optimized.\nStep 5: Set bus count x=2.\nStep 6: Set generation count y=0.\nStep 7: Generate random position and velocity for each particle.\nStep 8: Calculate power loss for each particle using Active power loss minimization.\nStep 9: Initialize current position of each particle as \u2018Pbest\u2019.\nStep 10: Assign \u2018Gbest\u2019 as best amont \u2018Pbest\u2019.\nStep 11: Update velocity and position of each particle using  velocity and position update equations respectively.\nStep 12: If generation count reaches maximum limit, go to Step 13 or else increase the counter by one and go to Step 7.\nStep 13: If bus count reaches maximum limit, go to Step 14 or else increase the counter by one and go to Step 6.\nStep 14: Display the results.", "optimal location of DGs", 0.0, 0, 282, "If you're interested in finding the \\strong{optimal} size and location for the DER, you should look at \\em{multi-objective optimization}. This means that the objective function is a vector of objective functions. One of the most common objective is to minimize the total cost of generation (cost of capital, operation, maintenance, fuel,...). This can be expressed as:\n\\begin{equation}\nJ = J_{P} + J_C + ... +J_O\n \\end{aligned} $$\nThe objective vector $J$ is the sum of all cost functions, and $P$, $C$, ..., $O$ are the cost components.\nIn the multi-variable optimization, the constraints are expressed in terms of a set of constraints equations: $M$ and the vector $\\vec{x}$ is called the decision variable. These equations are then called \\textit{equality constraints} since they must be satisfied for any solution to be valid. The cost function and constraints together form the optimization problem. In this case, there are no equality constraints and only inequality constraints. Therefore, we have to define a \\textbf{Lagrangian function} which is an expression that represents the behavior of our", 0.1741424798965454, 0.010610079392790794, 0.11081793904304504, 0.041800644248723984], ["I am trying to run inference for a Tensorflow model on GPU, but it is using CPU. I can confirm it is using CPU as the inference time is very large and nvidia-smi shows no python process.\nTo debug this, I listed the physical and logical devices in Tensorflow. I can see that physical devices list contains GPU, but the logical devices list doesn't contain GPU. What can I do to fix this and run my model inference on GPU?\nI am using Tensorflow 2.4.4.\n\ntf.config.list_physical_devices()\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\nPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\ntf.config.list_logical_devices()\n[LogicalDevice(name='/device:CPU:0', device_type='CPU')]", "The reason why GPU was listed in physical devices but not in logical devices was because I had this line in my script. This line made my GPU not visible to the runtime.\n\ntf.config.set_visible_devices([], \"GPU\")", "GPU listed in physical devices in Tensorflow, but not in logical devices", 0.0, 0, 283, "You can's use gpu because you need to use the device assignment for the session. You need a session with a `tf1.train.Session` class. This can be done by:\n\\begin{itemize}\n1) Creating a new session\n2) Execute the tf.compat.v1 or tf1 call in the new created session, this will create a graph for you. The tensorflow team recommends to create the graph before the execution in a fresh session and then execute the tensorflow code. However, if you do not have enough time to do this you can also do it in this way. For this reason, the code you want to execute must be in that session to work with the GPU and the rest of the sessions must have a cpu. Here is the tutorial to know how to make the assignment: https://www.tensorflow.org/tutorials/using_gpu#assign_a_tensor_to_the_right_place_on_cpu_and_infer_with_cuda\n3) After that, execute it and use: tf.__internal__.tf_eager_execute\n4) When you are done, just create another session (or just reuse the same one) and", 0.13274335861206055, 0.008928571827709675, 0.08849557489156723, 0.06285612285137177], ["is it possible to create my own object detection script with YOLO or create a Neuron Network  to implement it in the NAO robot( iknow that there is a box of detection in choregraph but isn't very useful that's why i want to build an other one from scratch )  .. if there are any resources or something else that help me not hesitate to put them and thank you", "It is possible, but not easy.\nYou could use TensorFlow or PyTorch and run YOLO on a PC connected to NAO using PyNAOqi, to get images from the camera.\nThese Python packages are not available on NAO, because it is lacking pip and compilers.\nMost advanced developers should be able to compile neural networks into binaries that can run on the robot using the NAOqi C++ SDK, but honestly they must be rare.", "robot NAO object detection from scratsh", 0.0, 0, 284, "You should have a look at this tutorial about [image recognition with python](https://pypi.org/project/opencv-python/) which should give you some ideas.\nYou could also have look on the official [NAOqi documentation](http://doc.aldebaran.com/2-3/robot_control/robots/naoqi/py/doc/index.html#py) which has a chapter about the [vision subsystem](ftp://ftp.naos.fr/pub/al/docs/vis_naoseg/vision.pdf).\nI also recommend to have read the chapter of the documentation about  [Vision](ftp://naoshare.me/file/d1/4/8/6/V4869_V_EN.PDF) to get an idea about how to use the vision system with the Robot Operating System (ROS) and the Python programming language.", 0.17989417910575867, 0.04278074949979782, 0.11640211939811707, 0.041208792477846146], ["I am trying to use sklearn Linear Regression, however whenever I run my code it comes up with an error: Expected 2D array, got 1D array instead:\narray=[1.16 2.51 1.15 1.52 1.11 1.84 1.07 3.   2.   1.71 0.48 1.85 1.32 1.17\n1.48 2.59].\nAnyone know How I can fix this?", "You have only one element in the array because you didn't but \",\" between your numbers. Try it like this: array=[1.16, 2.51, 1.15, 1.52, 1.11, 1.84, 1.07, 3, 2, 1.71, 0.48, 1.85, 1.32, 1.17, 1.48, 2.59]. If this isn't what you wanted, describe your problem a bit more.", "How do I change the dimension of an array", 0.0, 0, 285, "If you are using the `.fit` method you can supply your array as a `X` parameter, the array should be of shape `(n_samples, n_features)` where `n` is the number of samples in your dataset.\nThis will create a new array with the same shape and you will be able to predict the output for any new sample.", 0.19512194395065308, 0.01652892492711544, 0.11382114142179489, 0.018097849562764168], ["I tried scipy.interpolate.RegularGridInterpolator but MATLAB and python give me results with tiny different (For example: python: -151736.1266937256 MATLAB: -151736.1266989708). And I do care about those different decimals.", "Those two functions are equivalent. However, MATLAB's griddedInterpolant has multiple interpolation methods, whilst RegularGridInterpolator only seems to support linear and nearest. With MATLAB, this gives you more possibilities to choose a proper method given your data.\nYour two results seem to be accurate to the 12th digit, which in most cases is a good accuracy. The difference between the results is probably due to different implementations of the interpolation method.\nIf you want accuracy beyond the 12th digit you should rescale your problem so that you only consider the decimals.", "What is equivalent to MATLAB griddedInterpolant function in python?", 0.0, 0, 286, "You should use `scipy` and `matplotlib` as they have native support for interpolation:\nhttps://docs.scikit-learn.org/stable/modules/generated/sklearn..interfaces.Transformer.html#sklearndotinterfacestransformer-class\nhttp://matlab.eng.ohio-state.edu/matwiki/index.php/Plot_Interp\nYou should not be using `MATlab` for machine learning.", 0.1304347813129425, 0.014705882407724857, 0.05797101557254791, 0.041800644248723984], ["While running the code az.plot_trace(result_final);\nfacing the below error\nTypeError: expected dtype object, got 'numpy.dtype[float64]'\nThe above exception was the direct cause of the following exception:\nSystemError: CPUDispatcher(<function histogram at 0x00000147106C8B88>) returned a result with an error set\nCan you please let me know how to solve this", "I suggest upgrading numba to the latest version, e.g. python3 -m pip install numba==[latest version]. This might require a manual update to llmvite as well by doing, python3 -m pip install --ignore-installed llvmlite. Hope that helps!", "In Arviz while ploting the plot_trace or plot_posterior getting the type error", 0.0, 0, 287, "\\begin{blockquote}\nwhile running a code to find the posterior I am getting this error: TypeError: \"expected dtype objects,got 'float' \"\n\\end{itemize}", 0.16129031777381897, 0.0, 0.09677419066429138, 0.023255812004208565], ["If I have a dataframe with a string column and I want to do some filtering, what's the difference between\ndf[\"string_column\"].str.startswith(...)\nand\ndf[\"string_column\"].startswith(...)\nBoth work fine for me. I'm just curious of why we use .str", "some methods are specific to string types only. such as contains(), lower(), replace()..", "Why use df[\"column\":].str as opposed to not", 0.0, 0, 288, "The `str` method on Series and DataFrames does a couple of things:\n\\begin{itemize}\n  \\item It converts any non-string values into a list, so `df['string'].str == df['bool'].astype(str).str`.\n    \\em{Note: This is not the same as `ast.literal_eval`, which is a bit of a can of worms.}\n   \\begin {code}", 0.05882352963089943, 0.0, 0.05882352963089943, 0.07888250797986984], ["I am training a model with different outputs in PyTorch, and I have four different losses for positions (in meter), rotations (in degree), and velocity, and a boolean value of 0 or 1 that the model has to predict.\nAFAIK, there are two ways to define a final loss function here:\none - the naive weighted sum of the losses\ntwo - the defining coefficient for each loss to optimize the final loss.\nSo, My question is how is better to weigh these losses to obtain the final loss, correctly?", "This is not a question about programming but instead about optimization in a multi-objective setup. The two options you've described come down to the same approach which is a linear combination of the loss term. However, keep in mind there are many other approaches out there with dynamic loss weighting, uncertainty weighting, etc... In practice, the most often used approach is the linear combination where each objective gets a weight that is determined via grid-search or random-search.\nYou can look up this survey on multi-task learning which showcases some approaches: Multi-Task Learning for Dense Prediction Tasks: A Survey, Vandenhende et al., T-PAMI'20.\nThis is an active line of research, as such, there is no definite answer to your question.", "optimizing multiple loss functions in pytorch", 0.1973753273487091, 2, 289, "It's very unclear what you are trying to do. In any case, you can do: \\begin{code}\nmodel = nn.ModuleList()\n\\end{Code} where `n` is the number of losses. Each loss will have a corresponding optimizer which will optimize each of those losses, separately. Then, the total loss is a sum over the separate losses and you will get your total accuracy.", 0.2210526317358017, 0.03191489353775978, 0.11578947305679321, 0.051538608968257904], ["I have imported data from excel to python and now want to draw multiple plots on a single figure but for that I will need separate variables like 'x' & 'y' etc because we know that plt.plot(x,y), basically I have two datasets in which I am doing Time series analysis. In first data set I have Monthly+Yearly data in which I combined both columns and formed one column having name Year-Month, In second dataset I have Daily+Yearly data in which I formed one column by merging both and named it as Year-Daily. Now the dependent variable in both datasets is the number of sunspots.\n\nNow I want to Plot Daily and Monthly sunspot numbers on a single Graph in Python, so how will I do that?", "What is the library that are you using to import the data?", "Imported data from excel and assigning variables in python", 0.0, 0, 290, "You could do this with `df.melt` (https://pandas.pydata.org/pandas-docs/stable/reference/api/ pandas.DataFrame.meltdf.html) and then plotting the data. It would look something like this:\nFirst you would import your data into pandas (if you don't have a dataframe yet):\n`import pandas as pd`\nThen you can melt your dataframe: `mdf = df.drop(['Date','Month', 'Year']).meld()`\nYou can then use mdf to plot your sun spots with the following code: \\begin{code}\nmpl = mldf[['Year-month', 2, 'Month']]\n\\end{mld} `fig, ax = plt_helpers.plt_subplots(1, 1) plt = ax.matplotlib.axes_grid1.TwinAxesGeneric(fig) ax1 = fig.add_axe(pl)\nax1 .plot_date(mpf, label='Monthly')` ax2 = figure.gca(projection='3d') ax3 =figure.new_marginal_", 0.08888889104127884, 0.015037594363093376, 0.05925925821065903, 0.05769231170415878], ["I'm trying to calculate the confidence of a random forest regression.\nI couldn't find a way to do so in sklearn library in python, thus I am trying to calculate it using the variance between the predictions of each tree, but I still couldn't find anything.\nHave you faced this problem and have a solution that helps me calculate the confidence?", "The answer was pretty simple, we have the ability to get each estimator in the forest (we can simply go through the forest in a loop) and this solves the problem.", "Trees' predictions in random forest regression", 0.0, 0, 291, "\\begin{blockquote}\nIn Random Forests, a bootstrap sample of the data is used to\ncreate a number of trees (usually a large number). A tree is a\ndecision tree that is built by recursively splitting the samples\ninto a training and test set. A random split of samples is made at\neach node. This is done to ensure that the trees are\nindependently created, so that no two trees share the same\ninformation. The process is repeated a few times (typically 100-1,0\n0) and the result is the ensemble of all the random trees. Each\ntree is then evaluated by its performance on the test sample and\nthe final tree which has the lowest error is chosen. To get a better\noverview of how the Random Forest method is implemented, have\na look at the code in the scikit-learn. RandomForest\nclass. There are two parameters of importance:\n\\begin {itemize}   \\item max_depth - maximum depth of tree (default = 3)\nThis determines the number trees to be created in\nRandomForests. For each node, there will be a tree.  \\em{The larger\nthis", 0.14018692076206207, 0.01886792480945587, 0.09345794469118118, 0.06766917556524277], ["I recently updated matplotlib and now I am consistently getting an error when I write from matplotlib import pyplot as plt.\nImportError: cannot import name 'artist' from 'matplotlib' (C:\\Users\\nenze\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\__init__.py)\nI've tried uninstalling and reinstalling matplotlib which didn't solve anything. I even tried to downgrade to an older version but I am still getting the same error.\nThis is with matplotlib version 3.5.1. This is with Python version 3.9.7. This is through Jupyter Notebooks.", "Ended up deleting Python\\Python39\\site-packages\\matplotlib_init_.py and it worked itself out.\nI also deleted files that started with ~ (ex: ~matplotlib).", "Cannot import name 'artist' from 'matplotlib' (Python)", 0.0, 0, 292, "You might want to check the Python Version. You can check that by opening an interactive interpreter and typing `import sys; print(sys.version)`\nYou can see what version of matplotlib is installed by typing\n`import matplotlib;print(matplotlib.__version__)`", 0.15625, 0.0, 0.09375, 0.06659674644470215], ["I have an example of a dataset (below). Column ['ID'] has values refering to customer codes A, B, C. Each customer-code has been to different locations (referred to as ['LON'] and ['LAT'].\nI am trying to group each ID and calculate the mean value of the corresponding LON and LAT values. After the calculation, I try to append the mean value in the same column or a new column but it doesn't seem to work (runs into an error that the column isn't defined).\nCould you please shed some light?\nThanks so much!\n\n\n\n\nID\nLON\nLAT\n\n\n\n\nA\n62.03755\n16.34481\n\n\nB\n-50.37181\n54.94410\n\n\nC\n16.95291\n50.35189\n\n\nB\n59.95044\n173.64574\n\n\nA\n31.31972\n-128.33218\n\n\nB\n-50.37181\n54.94410\n\n\nA\n23.11042\n157.43303\n\n\nB\n2.15615\n97.10632\n\n\n\n\nI tried this:\ndf.groupby('ID')['LON'].mean().append\nand\ndf['MEANLON'] = df.groupby('ID', as_index=False)['LON'].mean()", "Thank you for your help Ivan!\nTo answer your Qs.\n\nYes, I'm trying to calculate the mean values of the latitude & the longitude.\n\nI'll try to explained it better. The shape of the original df I deal with is (15, 30000).\nIt contians electric vehicles charging records in a City.\n\n\n\nEach ID refer to to a contract ID / User.\nsome users charge regularly, mostly at the same charging station or other close-by stations.\nI'm trying to Filter the charge records by Each ID ans calculate the mean lon & lat at which charge events happened.\nThis mean values of geocoordinates indicates where the User lives (roughly assumptions).\n\n\n\n\n\nID\nLON\nLAT\nStation\nSTARTTIME\nENDTIME\n\n\n\n\nA\n62.03755\n16.34481\nstationname\ntimestamp\ntimestamp\n\n\nB\n-50.37181\n54.94410\nstationname\ntimestamp\ntimestamp\n\n\nC\n16.95291\n50.35189\nstationname\ntimestamp\ntimestamp\n\n\nB\n59.95044\n173.64574\nstationname\ntimestamp\ntimestamp\n\n\nA\n31.31972\n-128.33218\nstationname\ntimestamp\ntimestamp\n\n\nB\n-50.37181\n54.94410\nstationname\ntimestamp\ntimestamp\n\n\nA\n23.11042\n157.43303\nstationname\ntimestamp\ntimestamp\n\n\n\n\nWhat I'm trying to get is like:\n\n\n\n\nID\nLON\nLAT\nStation\nSTARTTIME\nENDTIME\n\n\n\n\nA\nMean of LON values for A\nMean of LAT values for A\nstationname\ntimestamp\ntimestamp\n\n\nB\nMean of LON values for B\nMean of LAT values for B\nstationname\ntimestamp\ntimestamp\n\n\nC\nMean of LON vakues for C\nMean of LAT values for C\nstationname\ntimestamp\ntimestamp\n\n\n\n\nOr\n\n\n\n\nID\nLON\nLAT\nStation\nSTARTTIME\nENDTIME\nLONMEAN\nLATMEAN\n\n\n\n\nA\n62.03755\n16.34481\nstationname\ntimestamp\ntimestamp\nMean of LON values for A\nMean of LAT values for A\n\n\nB\n-50.37181\n54.94410\nstationname\ntimestamp\ntimestamp\nMean of LON values for B\nMean of LAT values for B\n\n\nC\n16.95291\n50.35189\nstationname\ntimestamp\ntimestamp\nMean of LON vakues for C\nMean of LAT values for C\n\n\nB\n59.95044\n173.64574\nstationname\ntimestamp\ntimestamp\nMean of LON values for B\nMean of LAT values for B\n\n\nA\n31.31972\n-128.33218\nstationname\ntimestamp\ntimestamp\nMean of LON values for A\nMean of LAT values for A\n\n\nB\n-50.37181\n54.94410\nstationname\ntimestamp\ntimestamp\nMean of LON values for B\nMean of LAT values for B\n\n\nA\n23.11042\n157.43303\nstationname\ntimestamp\ntimestamp\nMean of LON values for A\nMean of LAT values for A", "How to calculate mean value and appending it to a column? #python", 0.0, 0, 293, "There are multiple ways to approach this problem.  Here is one approach:  `pandas` provides a [rolling function](http://pandas.pydata.org/pandas-docs/stable/generated/series.rolling.html).  The rolling function is applied on a subset of data with a specified window size (in this case, 3).   The first parameter of rolling is the window (3) and the second parameter is a keyword argument that is `True` for the data frame and `False` to use column-wise rolling.\n\\begin{itemize}\n  \\item `df = pd.DataFrame({'ID': ['A','A', 'A'], 'Lon': [1,2,3], 'Lat':[4,5,6]})`\n  2.`df_result = (df[['ID','LOn']]\n     .rolling(window=3,center=True).mean())\n     3.`print(df)`\n   4.`[('A' 'mean'), ('A ' 'Mean'), ( 'B ' )]`\n      5.`[(1  '1'),(2  ''),(3  '')]`  # row 1: A", 0.13779526948928833, 0.011857707053422928, 0.09842519462108612, 0.047999996691942215], ["I want to make a calculator for my data.\nBasically I have multiple measurements in different .csv files that are named as their physical representation (temperature_1, current_1, voltage_1 ecc.) and I am trying to make a calculator in python that given a certain expression [e.g. (current_1 * voltage_1) + (current_2 * voltage_2)] is able to load the data from each file and evaluates the result of the expression on the dataframes.\nI already made simple functions in order to sum, subtract, multiply and divide dataframes but I am stuck on how to handle complex expressions like the sum of many multiplications [e.g. (current_1 * voltage_1) + (current_2 * voltage_2) + (current_3 * voltage_3) ecc.].\nI tried to use a parser but still got no result.\nSomebody has any idea on how to handle this?\nNote: all the .csv have 2 columns, time and measurement, the number of rows are the same and the acquisition time is at the same timestamp.", "I have solved the issue. For anybody who will need similar functions i report here the solution in steps\n\nWrite your equation with the name of your files. E.g. current_1 * voltage_1 (you need file current_1.csv, voltage_1.csv)\n\nparse your equation with any parser. I used py_expression_eval.\n\nExtract the variables from the equation (variables = parser.parse(equation).variables())\n\niterate over the variables and at each step:\n\nload the data in a dataframe\ninsert the column of the measurement in general dataframe\nchange the name of that column to the name of your file (e.g. current_1)\n\nby doing this you will obtain a dataframe with columns: time, measurement_1, measurement_2 ecc.\n\nUse df.eval('result= ' + expression, inplace=True) to evaluate your initial expression using the columns you have added to the general dataframe\n\n\nHope this helps somebody", "Calculator for Dataframes", 0.0, 0, 294, "Here is a solution using the [pandas](https://pandas.pydata.org/docs/reference/api/pandas-read_csv.html) package:\n\\begin{itemize}\n  \\item Import the library: `import pandas as pd`\n    \\strong{Note}: The library is very useful and can be used to solve many problems. I recommend you take a look at its documentation. For example, if you want the solution to your problem, you can also use the `read` function. You can check the documentation here: [Pandas read function](http://www.tutorialspoint.com/data_science/pd_read.htm)\n      \\begin {code}", 0.2118644118309021, 0.017094017937779427, 0.1355932205915451, 0.06652360409498215], ["I wondered if there is any way to reproducibly draw random numbers when using parallel==True with jitted functions in numba. I know that for singlethreaded code, you can set the random seed for numpy or the standard random module within a jitted function, but that does not seem to work for multithreaded code. Maybe there is some sort of workaround one could use?", "In parallel, each worker need to have its own seed as a random number generator cannot be both efficient and and thread safe at the same time. If you want the number of threads not to have an impact on the result, then you need to split the computation in chunks and set a seed for each chunk (computed by one thread). The seed chosen for a given chunk can be for example the chunk ID.", "Random seeds and multithreading in numba", 0.0, 0, 295, "There is a random number generator (RNG) [in Numba](https://numba-doc.readthedocs.io/en/stable/user/rngs.html#numbapro-ranges) that works in both single-thread and parallel modes.  You can use this in parallel mode like so:\n\\begin{itemize}\nFirst, define a seed in the function signature: `num=num(seed=seed)`.\nNext, when you create your parallel context, seed the RNG:  `context.parallel(num, random.seed(your_seed))`.\nFinally, call `yourfunc(n)` where `n` is an array of the appropriate size.\nYou can then call this `func` from other threads in a parallel way, and the results will be reproducible.", 0.31460675597190857, 0.05681818351149559, 0.17977528274059296, 0.03264095261693001], ["I am stuck on a very simple problem but more I try to solve it the harder it becomes. Or maybe there is no better solution than O(N^2). The problem is simple. I have N data points in first set and M data points in second. I have a NxM similarity matrix A such that A[i, j] (range is between 0 and 1) gives score about similarity of Ni and Mj data-points.\nI want to find out which the points from first set match the best in the second. i.e the output is list with N elements each one corresponding to unique indices of M set which they match the most.\nI am using numpy. I sort matrix on second axis but the issue is argsort will not give me unqiue indices. And with indices logic it becomes really confusing.", "np.argmax(A, axis=1) does exactly what you describe (assuming that 1 means most similar).", "How to find most optimal match between data points given the similarity matrix?", 0.1973753273487091, 1, 296, "Here is one possible approach:\n\\begin{itemize}\n  \\item sort by similarity\n    \\begin {code} <\n    0\n     1\n   A\n>\n<\n0.8\n1.14\n.42\n2.023\n3.94129\n7.755899019685938e-16\n4.2867977181046982e+003926095\n8.56507063615357884e051132080e917\n-434749.62252448727e1e831 2 5 4 8 9 7 6 3  -\n6.330  <  .40,0>  /\n5.  =\nN\nM\nO\nP\nC\nB\nA\nQ\nR\nE\nS\nT\nU\nV\nW\nX\nY\nZ", 0.0731707289814949, 0.0, 0.04878048598766327, 0.02591422013938427], ["I have the following data frame structure:\n\n\n\n\nid_trip\ndtm_start_trip\ndtm_end_trip\nstart_station\nend_station\n\n\n\n\n1\n2018-10-01 10:15:00\n2018-10-01 10:17:00\n100\n200\n\n\n2\n2018-10-01 10:17:00\n2018-10-01 10:18:00\n200\n100\n\n\n3\n2018-10-01 10:19:00\n2018-10-01 10:34:00\n100\n300\n\n\n4\n2018-10-01 10:20:00\n2018-10-01 10:22:00\n300\n100\n\n\n5\n2018-10-01 10:20:00\n2018-10-01 10:29:00\n400\n400\n\n\n\n\nAnd I would like to check, using python, how often a trip starts and ends in a given season. The idea was to do these average intervals per day, per hour and then in intervals of a few minutes.\nWhat would be the best approach to doing this?\nMy desired output would be something to inform  eg: for station 100 on 2018-10-01, a travel starts, on average, every 4 minutes", "In order to do that you could group your DataFrame by different travels. Firstly, I would make a new column with a travel id, so travels starting and ending in the same stations can be grouped.\nThen you can easily group those rows by travel id and get all the information you need.\nPlease note that your data sample does not include any \"same travel\". Also, consider providing a code sample for your data, it would be easier for us to work with and run tests.", "Check average travel intervals for each station - pyhton", 0.0, 0, 297, "Let's say that you have a dataframe that looks like this: `df`\n`df = pd.DataFrame({'start':['07/03/21', '08/12/52'], 'end':[0, 0], 'id': [14,13]})` and you want to calculate the average time for a journey, you can use this code: (the average function is [here](https://www.geeksforgeeks.org/average-function-in-python/))\n    `time_avg = df['start'] - df.apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%y'), axis=1) + 59 * 60 * df[['end']].apply('datetime'.datetime).apply(-1)`\nThis code calculates the time between the start and end of the journey. It then adds 365 * (69*61 *62) to get the total time of year. Finally it multiplies the result by 864 and subtracts 7 to take into account the difference between UTC and GMT.", 0.2211981564760208, 0.01860465109348297, 0.12903225421905518, 0.022499999031424522], ["Haven't been able to find this answer online, so I'm asking the stackoverflow community...\nI'm wondering if DataSpell can connect to a SageMaker instance and use the EC2 instance hardware (i.e. virtual CPUs, GPUs, RAM, etc.) to run data transformations and machine learning model training on python and Jupyter notebook files?\nI.e. I want all the advantages of DataSpell on my local computer (git, debugging, auto-complete, refactoring, etc.), while having all the advantages of a SageMaker instance on AWS (scalable compute hardware, fast training, etc.) to run python and Jupyter notebook files.\nThank you.", "This can not be done. You can't bring your own IDE to SageMaker. You can use SageMaker's native IDE - SageMaker Studio which will give you an integrated experience with all of SageMaker's capabilities.\nI work at AWS and my opinions are my own.", "DataSpell & AWS Sagemaker Connection", -0.1973753273487091, -1, 298, "This is possible to do with Data Spell. You can use ECs in Data spell as well as the instances in S3. It would be a bit hard to explain in a comment, but I can give a link to the documentation where you can see a demo. Please note that we have an SLA of 30 minutes. So you have to wait 2 hours to see the results.", 0.260869562625885, 0.05309734493494034, 0.156521737575531, 0.07491929084062576], ["I have a list of texts. I turn each text into a token list. For example if one of the texts is 'I am studying word2vec' the respective token list will be (assuming I consider n-grams with n = 1, 2, 3) ['I', 'am', 'studying ', 'word2vec, 'I am', 'am studying', 'studying word2vec', 'I am studying', 'am studying word2vec'].\n\nIs this the right way to transform any text in order to apply most_similar()?\n\n(I could also delete n-grams that contain at least one stopword, but that's not the point of my question.)\nI call this list of lists of tokens texts. Now I build the model:\nmodel = Word2Vec(texts)\nthen, if I use\nwords = model.most_similar('term', topn=5)\n\nIs there a way to determine what kind of results i will get? For example, if term is a 1-gram then will I get a list of five 1-gram? If term is a 2-gram then will I get a list of five 2-gram?", "Generally, the very best way to determine \"what kinds of results\" you will get if you were to try certain things is to try those things, and observe the results you actually get.\nIn preparing text for word2vec training, it is not typical to convert an input text to the form you've shown, with a bunch of space-delimited word n-grams added. Rather, the string 'I am studying word2vec' would typically just be preprocessed/tokenized to a list of (unigram) tokens like ['I', 'am', 'studying', 'word2vec'].\nThe model will then learn one vector per single word \u2013 with no vectors for multigrams. And since it only knows such 1-word vectors, all the results its reports from .most_similar() will also be single words.\nYou can preprocess your text to combine some words into multiword entities, based on some sort of statistical or semantic understanding of the text. Very often, this process converts the runs-of-related-words to underscore-connected single tokens. For example, 'I visited New York City' might become ['I', 'visited', 'New_York_City'].\nBut any such preprocessing decisions are separate from the word2vec algorithm itself, which just considers whatever 'words' you feed it as 1:1 keys for looking-up vectors-in-training. It only knows tokens, not n-grams.", "Retrieve n-grams with word2vec", 1.2000000476837158, 1, 299, "A 5-word term will match all 4- and 6-words in the vocabulary. So you'll get all of those ngram'ed words in your  results.\nIf you only want  to get  the  nGram'd words that exactly match the term, you can use [.vectors](https://scikit-learn.org/stable/modules/generated/sklearn-vocabularies.html#skl-vect) to do this.", 0.21739129722118378, 0.014598540030419827, 0.10144927352666855, 0.0555555522441864], ["I tried updating conda, and I got this message:\nERROR conda.core.link:_execute_actions(337): An error occurred while uninstalling package 'defaults::requests-2.14.2-py36_0'. PermissionError(13, 'Permission denied').\nAnd if I try updating just matplotlib on conda, I get: ERROR conda.core.link:_execute_actions: An error occurred while installing package", "The first question when you update conda is that PermissionError, maybe you just do not have enough system permission. The second question, you did not give enough error tips.", "Having trouble installing matplotlib in python", 0.0, 0, 300, "To solve this problem, you need to run: `conda install -n base python=3.6`\nin terminal. This will install python 3 and it will work for you.", 0.10526315867900848, 0.0, 0.10526315867900848, 0.0593021996319294], ["I am collecting time series data, which can be separated into \"tasks\" based on a particular target value. These tasks can be numbered based on the associated target. However, the lengths of data associated with each task will differ because it may take less time or more time for a \"task\" to be completed. Right now in MATLAB, this data is separated by the target number into a MATLAB cell, which is extremely convenient as the analysis on this time-series data will be the same for each set of data associated with each target, and thus I can complete data analysis simply by using a for loop to go through each cell in the cell array. My knowledge on the closest equivalent of this in Python would be to generate a ragged array. However, through my research on answering this question, I have found that automatic setting of a ragged array has been deprecated, and that if you want to generate a ragged array you must set dtype = object. I have a few questions surrounding this scenario:\n\nDoes setting dtype=object for the ragged array come with any inherent limitations on how one will access the data within the array?\n\nIs there a more convenient way of saving these ragged arrays as numpy files besides reducing dimensionality from 3D to 2D and also saving a file of the associated index? This would be fairly inconvenient I think as I have thousands of files for which it would be convenient to save as a ragged array.\n\nRelated to 2, is saving the data as a .npz file any different in practice in terms of saving an associated index? More specifically, would I be able to unpack the ragged arrays automatically based on a technically separate .npy file for each one and being able to assume that each set of data associated with each target is stored in the same way for every file?\n\nMost importantly, is using ragged arrays really the best equivalent set-up for my task, or do I get the deprecation warning about setting dtype=object because manipulating data in this way has become redundant and Python3 has a better method for dealing with stacked arrays of varying size?", "I have decided to move forward with a known solution to my problem, and it seems to be adapting well.\nI organize each set of separate data into it's own array, and then store them in a sequence in a list as I would with cells in MATLAB.\nTo save this information, when I separated out the data I stored the subsequent index value in a list. By this I mean that:\n\nI identify the location of the next separate set of data.\nI copy the data up until that index value into an array that is appended to a list.\nI store the index value that was the start of the next separate set of data.\nI delete that information from a copy of my original array.\nI repeat steps 1-4 until there is only one uniquely labelled sequence of data left. I append this set of data. There is no other index to record. Therefore the list of indices is equal to the length of the list of arrays -1.\nWhen saving data, I take the original array and save it in a .npz file with the unpacking indices.\nWhen I want to use and reload the data into it's separate arrays for analysis, I can 'pack' and 'unpack' the array into it's two different forms, from single numpy array to list of numpy arrays.\n\nThis solution is working quite well. I hope this helps someone in the future.", "Python: Is there a better way to work with ragged arrays than a list of arrays with dtype = object?", 0.1973753273487091, 1, 301, "\\begin{blockquote}\n\\begin {itemize}", 0.0, 0.0, 0.0, 0.060606054961681366], ["I want to make a binary classifier that classifies the following:\nClass 1. Some images that I already have.\nClass 2. Some images that I create from a function, using the images of class 1.\nThe problem is that instead of pre-creating the two classes, and then loading them, to speed up the process I would like the class 2 images to be created for each batch.\nAny ideas on how I can tackle the problem? If I use the DataLoader as usual, I have to enter the images of both classes directly, but if I still don't have the images of the second class I don't know how to do it.\nThanks.", "You can tackle the problem in at least two ways.\n\n(Preferred) You create a custom Dataset class, AugDset, such that AugDset.__len__() returns 2 * len(real_dset), and when idx > len(imgset), AugDset.__getitem__(idx) generates the synthetic image from real_dset(idx).\nYou create your custom collate_fn function, to be passed to DataLoader that, given a batch, it augments it with your synthetic generated images.", "How can I create images for each batch using Pytorch?", 1.2000000476837158, 1, 302, "You can use `train_on_batch` and `eval_generator` to feed your data to the model. If you want a new batch every epoch, you can call `next(iter(batches))` and you will get a `BatchIterator` which you have access to.", 0.2222222238779068, 0.01886792480945587, 0.14814814925193787, 0.06555764377117157], ["I have trained a Scikit Learn model in Python environment which i need to use it for inference in GoLang. Could you please help me how can i export/save my model in python and then use it back in GoLang.\nI found a solution for Neural Network model where i can save Tensorflow model in ONNX format and load it using Onnx-go in GoLang. But this is specific for Neural Network models. But I am unable to figure it out for scikit-learn models.", "You can develop an REST json API service to expose your scikit-learn model and  communicate with go client.", "How to use trained Scikit Learn Python model in GoLang?", 0.0, 0, 303, "You can use the `sklearn.tree` library to build a decision tree and export it as a file in the `.npz` format. After this, you can load the decision trees file into Go using the [numpy package](https://golang-packages.org/packages/pkg/numpy/):\n\\begin{itemize}\n<h4>1. Import the numpy package</h3>\n`import (</li>`\n</ol>", 0.13333334028720856, 0.027397260069847107, 0.13333334028720856, 0.07863122224807739], ["i'm totally new in NLP and Bert Model.\nWhat im trying to do right now is Sentiment Analysis on Twitter Trending Hashtag (\"neg\", \"neu\", \"pos\") by using DistilBert Model, but the accurazcy was about 50% ( I tried w Label data taken from Kaggle).\nSo here is my idea:\n(1) First, I will Fine-tunning Distilbertmodel (Model 1) with IMDB dataset,\n(2) After that since i've got some data took from Twitter post,  i will sentiment analysis them my Model 1 and get Result 2.\n(3) Then I will refine-tunning Model 1 with the Result 2 and expecting to have Model (3).\nIm not really sure this process has any meaning to make the model more accuracy or not.\nThanks for reading my post.", "I'm a little skeptical about your first step. Since the IMDB database is different from your target database, I do not think it will positively affect the outcome of your work. Thus, I would suggest fine-tuning it on a dataset like a tweeter or other social media hashtags; however, if you are only focusing on hashtags and do not care about the text, that might work! My little experience with fine-tuning transformers like BART and BERT shows that the dataset that you are working on should be very similar to your actual data. But in general, you can fine-tune a model with different datasets, and if the datasets are structured for one goal, it can improve the model's accuracy.", "Does Fine-tunning Bert Model in multiple times with different dataset make it more accuracy?", 0.1973753273487091, 1, 304, "\\begin{blockquote}\nI'll just do the training for DistillBerta and make a small dataset\nof it's training set and use it as my training dataset for my\nmodel. I'd like to know if this is possible. Also, should I do\nsomething like that for the rest of the models? I mean, take the\ntraining dataset of each model and then make my own dataset by\nadding the data that I got from my testing dataset. Is that a\ngood practice? Or should i just use the test data of those model? If\nit' s the case, how should the dataset look like? How do I combine\ndata from those models with my test dataset? Thanks for your help\n\\end{itemize}", 0.34008097648620605, 0.016326529905200005, 0.17004048824310303, 0.0766085684299469], ["I want to train my YOLOv4 detector on 5 classes [Person,Car,Motorcycle,Bus,Truck]. I used around 2000 images for training and 500 for validation.\nThe dataset I used is from OID or from COCO.\nThe main problem is that, when the training is over, the detector finds only one class in the image every time. For example, if it's a human in a car, it returns only the Car or the Person bounding box detection.\nI saw that the .txt annotation on every image is only for one class.\nIt's difficult to annotate by myself 10.000 images.\nAll the tutorials usually detect only one class in the image.\nAny ideas on how to train my model on all 5 classes?", "i finally found the solution.\nThe problem was that OID dataset downloads images with one specific class, like person, car etc.\nAS Louis Lac mentioned i must train my model on dataset with all relevant classes", "Train multi classes object detector (YOLOv4)", 0.0, 0, 305, "First of all, please read the tutorial of yolov4 to get the idea of how yolo v4 works. [Yolo V4 tutorial](https://github.com/pjreddie/darknet/wiki/Yolovo4-Tutorial) . Also check this link for more details: [Training Yolow4 from scratch](http://blog.csdn.net/?p=489528).\nPlease find the answer to your question below:\n\\begin{itemize}\n    \\item In your dataset, you have 3 types of boundings boxes: person, car and motorcycle. You can only find one of them at any given time since the model is designed in this way. The yolu v3 model can detect multiple classes, but it is not the case of v1 and v2. YoloV4 is a different architecture and is more powerful than v 4. So you can't have the same model and expect to have a better result. I would recommend you to use YoLO V3 to detect all the 8 classes. Also, make sure that your images are not overlapping. If you still have issues, then please share your code and", 0.1170731708407402, 0.009852216579020023, 0.07804878056049347, 0.055865924805402756], ["My yolov5 model was trained on 416 * 416 images. I need to detect objects on my input image of size 4008 * 2672. I split the image into tiles of size 416 * 416 and fed to the model and it can able to detect objects but at the time of stitching the predicted image tiles to reconstruct original image, I could see some objects at the edge of tiles become split and detecting half in one tile and another half in another tile, can someone tell me how to made that half detections into a single detection in the reconstruction.", "Running a second detection after offseting the tiles split would ensure that all previously cut objects would be in a single tile (assuming they are smaller than a tile). Maybe you could then combine the two results to get only the full objects", "Detect Objects on high resolution image by splitting the image into tiles and reconstruct the tiles into single image", 0.0, 0, 306, "I guess you are using [Spatial Transformer Networks](https://github.com/YOLO90/yolox) which was a great improvement to [YoloV3](http://www.cs.toronto.edu/~kriz/binary_classification_coco_2017.html) but I don't know how it works on YOLo9k. This is a feature of the network that detects objects in different locations of your image. In fact, in Yolo, this is the main reason for the success of Yolobox. It detect multiple objects and combines the bounding boxes to form a final boundings box. The problem here is that the boxes are not bound to a specific location.", 0.1818181872367859, 0.0, 0.1118881106376648, 0.018264835700392723], ["So I have a file with 6 different people, each person having 2 photos (in different angles) thus there are 6 * 2 = 12 images (in B&W).\nEach image is 140 (ht) x 120 (width)\nWhen I read the file, all the info is read so I have 12 columns (corresponding to each image) and 16,800 rows (corresponding to the image size).\nHow do I plot the image on matplotlib ?\nI tried extracting each column e.g. df.loc[:,0] and then reshaping it to (140,120). But plotting it gives some abstract art looking output instead of the face. am I doing something wrong?", "I've solved it. I think it was a bug/glitch.\nSo I tried with plt.imshow() and plt.show() but it did not work.\nI then tried various methods plt.plot() <- which was giving me the weird colour output.\nEventually, going back to plt.imshow() somehow worked", "How to show image that's been given as 1-D array", 0.0, 0, 307, "If the images are not too large, it'd be better to store them as a single numpy array instead. Then, the data should look like this:\n\\begin{itemize}\n<a href=\"https://i.stack.imgur.com/y5pKP.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nThen, we can use `imshow` and `subplots` to plot them, like so: \\begincode{code} plt.subplot(2, 3, <idx>)\nplt.imread(<filename>) # read in the ndarray\nim = plt.<array>.imresize(im, im.shape[1] / 4) # reshape\nax.axis('off')\nfig.colorbar(pl.contourf(np.uint8(image), cmap='gray'))\nfor i in range(4): plt.(<subplottype>) <image, contour> # subplot types\nendfig = fig.get_figures()[0].subfigs[<idx>]\nshow()\nsubs = [] # save subplots in an array so we don't have to recreate", 0.19883041083812714, 0.011834319680929184, 0.10526315867900848, 0.0277777761220932], ["Having trouble getting Pandas data reader to retrieve price quotes from Yahoo\u2019s API. The most up to date answer seems to be:\n\n\"pip install --upgrade pandas pip install --upgrade pandas-datareader\n\nHowever, for the time being I will be using Google Collab and its Python platform, does anyone know how to update the pandas here? Or has the API truly just been discontinued?", "In Colab you need to put a ! before pip", "Python, Pandas, Yahoo Finance API", 0.06656807661056519, 1, 308, "I haven't seen this specific API discontinuation mentioned anywhere, so I suspect that it's just a temporary issue.  I think the simplest solution would be to use [Google Colab](https://colab.research.google.com/) for a bit.", 0.125, 0.0, 0.0833333358168602, 0.09090908616781235], ["We're developing custom runtime for databricks cluster. We need to version and archive our clusters for client. We made it run successfully in our own environment but we're not able to make it work in client's environment. It's large corporation with many restrictions.\nWe\u2019re able to start EC2 instance and pull image, but there must be some other blocker. I think ec2 instance is succefully running, but I have error in databricks\n\nCluster terminated.Reason:Container launch failure\nAn unexpected error was encountered while launching containers on\nworker instances for the cluster. Please retry and contact Databricks\nif the problem persists.\nInstance ID: i-0fb50653895453fdf\nInternal error message: Failed to launch spark container on instance\ni-0fb50653895453fdf. Exception: Container setup has timed out\n\nIt should be in some settings/permissions inside client's environment.\nHere is end of ec2 log\n\n-----END SSH HOST KEY KEYS----- [   59.876874] cloud-init[1705]: Cloud-init v. 21.4-0ubuntu1~18.04.1 running 'modules:final' at Wed, 09\nMar 2022 15:05:30 +0000. Up 17.38 seconds. [   59.877016]\ncloud-init[1705]: Cloud-init v. 21.4-0ubuntu1~18.04.1 finished at Wed,\n09 Mar 2022 15:06:13 +0000. Datasource DataSourceEc2Local.  Up 59.86\nseconds [   59.819059] audit: kauditd hold queue overflow [\n66.068641] audit: kauditd hold queue overflow [   66.070755] audit: kauditd hold queue overflow [   66.072833] audit: kauditd hold queue\noverflow [   74.733249] audit: kauditd hold queue overflow [\n74.735227] audit: kauditd hold queue overflow [   74.737109] audit: kauditd hold queue overflow [   79.899966] audit: kauditd hold queue\noverflow [   79.903557] audit: kauditd hold queue overflow [\n79.907108] audit: kauditd hold queue overflow [   89.324990] audit: kauditd hold queue overflow [   89.329193] audit: kauditd hold queue\noverflow [   89.333125] audit: kauditd hold queue overflow [\n106.617320] audit: kauditd hold queue overflow [  106.620980] audit: kauditd hold queue overflow [  107.464865] audit: kauditd hold queue\noverflow [  127.175767] audit: kauditd hold queue overflow [\n127.179897] audit: kauditd hold queue overflow [  127.215281] audit: kauditd hold queue overflow [  132.190357] audit: kauditd hold queue\noverflow [  132.193968] audit: kauditd hold queue overflow [\n132.197546] audit: kauditd hold queue overflow [  156.211713] audit: kauditd hold queue overflow [  156.215388] audit: kauditd hold queue\noverflow [  228.558571] audit: kauditd hold queue overflow [\n228.562120] audit: kauditd hold queue overflow [  228.565629] audit: kauditd hold queue overflow [  316.405562] audit: kauditd hold queue\noverflow [  316.409136] audit: kauditd hold queue overflow", "This is usually caused by slowness in downloading the custom docker image, please check if you can download from the docker repository properly from the network where your VMs are launched.", "AWS Databricks Cluster terminated.Reason:Container launch failure", 0.0, 0, 309, "This is because the ec  instance was not started. Try to follow the steps as per the link below. This might help you. The steps are mentioned in the below link. You might have to contact your client to get more details. Also, please do let me know if you still face the same issue. Thank you!\nhttps://aws.amazon.com/premiumsupport/knowledge-center/databr\n\\begin{itemize}\n=============================\nIf the issue persisits you might need more help from your Client.", 0.2181818187236786, 0.03703703731298447, 0.1818181872367859, 0.09424261748790741], ["I have a dataframe and am trying to set the index to the column 'JnlNo'. Currently the index is just a row number. The JnlNo are integers. But it keeps returning this error:\n\nKeyError                                  Traceback (most recent call last)\n~\\AppData\\Local\\Temp/ipykernel_52996/1782638178.py in \n----> 1 journals=journals.set_index('JnlNo')\n~\\Anaconda3\\lib\\site-packages\\pandas\\util_decorators.py in wrapper(*args, **kwargs)\n309                     stacklevel=stacklevel,\n310                 )\n--> 311             return func(*args, **kwargs)\n312\n313         return wrapper\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in set_index(self, keys, drop, append, inplace, verify_integrity)\n5449\n5450         if missing:\n-> 5451             raise KeyError(f\"None of {missing} are in the columns\")\n5452\n5453         if inplace:\nKeyError: \"None of ['JnlNo'] are in the columns\"\nI have initially ran these codes\nimport pandas as pd\njournals = pd.read_csv('Journals.csv')\njournals.head()\nbut when I then went ahead to set_index\njournals=journals.set_index('JnlNo'), it returned the error.", "Look at journals.columns. You can only use columns that are in the DataFrame. JnlNo is not in the dataframe as the error message tells you. Maybe you're confusing an uppercase i with a lowercase L or something like that.", "Dataframe set_index function returning error", 0.0, 0, 310, "When you use `.setindex` it will create an index from a column. In your case, you are not using any column for indexing so you get the key error as it is not able to find any key to index the data. You can try `setterows` function to create index. Below code should work. I created a dummy dataframe with only 2 columns. Hope it helps. Let me know if it works. Cheers.", 0.35087719559669495, 0.0357142873108387, 0.17543859779834747, 0.027451125904917717], ["I'm using gensim to create a Word2Vec model. I'm wondering if there is a way to feed the gensim class Word2Vec with my examples [(target, context1), (target, context2), ...] instead of feeding it with sentences.\nThanks in advance", "The Gensim Word2Vec class expects a re-iterable sequence where each item is a list of string word tokens. It then does the construction of the inner 'micro-examples' (context-word -> target-word in skip-gram, or context-window -> target-window in CBOW) itself.\nThere's not an alternate interface, or easy extension-hook, for changing the micro-examples. (Though, as the source code is available, it's possible even when not easy to change it arbitrarily.)\nIf you only ever need single-word contexts to single-word targets, and are OK with (as in standard word2vec) every A B pair to imply both an A -> B prediction and a B -> A prediction, you may be able to approximate your desired effect by the proper preprocessing of your corpus, completely outside Word2Vec code.\nSpecifically, only ever provide 2-word texts, of exactly the word pairs you want trained, as if they were full texts.", "How to inject training examples in gensim Word2Vec?", 0.1973753273487091, 1, 311, "The WordVector class will read in a file or corpus (list of tuples) of words and contexts (a list of lists).  There is no way for you to directly provide a list or tuple of examples to the class.  I would think of the WordVectors as just a pre-processing step for a particular document or set of documents.", 0.27230048179626465, 0.028436018154025078, 0.15023474395275116, 0.06529715657234192], ["I have a df like that:\n\n\n\n\nmonth\nstock\nMV\n\n\n\n\n1994-07\nA\n50\n\n\n1994-07\nB\n60\n\n\n1994-07\nC\n70\n\n\n1994-07\nD\n80\n\n\n1994-08\nA\n90\n\n\n1994-08\nB\n60\n\n\n1994-08\nC\n70\n\n\n1994-08\nD\n95\n\n\n1994-08\nE\n100\n\n\n1994-08\nF\n110\n\n\n\n\nI would like to subset my df in a way that I only have in it the 50% of the highest MV per month. For July/1994 I only have 4 stock, so 50% will be the 2 highest MV. For the month after, I have 6 stocks, which gives me 3 highest values:\n\n\n\n\nmonth\nstock\nMV\n\n\n\n\n1994-07\nC\n70\n\n\n1994-07\nD\n80\n\n\n1994-08\nD\n95\n\n\n1994-08\nE\n100\n\n\n1994-08\nF\n110\n\n\n\n\nI have tried:\ndf = df.groupby(pd.Grouper(freq=\"M\")).nlargest(2, \"MV\")\nBut I got the error: AttributeError: 'DataFrameGroupBy' object has no attribute 'nlargest'\nIn addition, the value of n will need to be a different value for every month. I am not sure how to handle that as well.", "df.groupby('month').apply(lambda monthly_data: monthly_data[monthly_data['MV'] >= monthly_data['MV'].median())", "Subset dataframe based on large values of a column per month", 0.07982976734638214, 2, 312, "You can use the following: `df.drop_duplicates(subset=['stock', 'Mv'])`\nThe `subset` parameter can be any column(s) of your dataframe. The `dropdup` method will drop duplicates by the `stock` and `M` columns. You could also just remove duplicates as long as you have the same number of rows.", 0.06060606241226196, 0.0, 0.06060606241226196, 0.02818734385073185], ["For example, if I want to see only one sentence in the dataframe in row 21, how can I type in the head function?\ndf.head(20)? df.head(19:20)", "you can access elements with their integer position df.iat[row,col]\nor an integer row with df.iloc([row])", "How can I type to see only one row in df.head()", 0.0, 0, 313, "Use the `iloc` method. This will return the values you want:\n`df['col'].ilocs[2]`, where `col` is the column name.\nFor example: `df[['Age']].illocs[3]`.\nThis will select the 3rd value in column `Age`.\nNote that this will work for the entire dataframe. If you only want the value from the first row, use `head`.", 0.15584415197372437, 0.0, 0.12987013161182404, 0.032958827912807465], ["How can I mock existing Azure Databricks PySpark codes of a project (written by others) and run them locally on windows machine/Anaconda to test and practice?\nIs it possible to mock the codes or I need to create a new cluster on Databricks for my own testing purposes?\nhow I can connect to storage account, use the Databricks Utilities, etc? I only have experience with Python & GCP and just joined a Databricks project and need to run the cells one by one to see the result and modify if required.\nThanks", "You can test/run PySpark code from your IDE by installing PySpark on your local computer.\nNow, to use Databricks Utilities, in fact you would need a Databricks instance and it's not available on local. You can try Databricks community Editionfor free but with some limitation\nTo acess a cloud storage account, it can be done locally from your computer or from your own Databricks instance. In both cases your will have to set up the end point of this storage account using its secrets.", "How to mock and test Databricks Pyspark notebooks Locally", 0.0, 0, 314, "You can run your own noteblocks locally without the need for a cluster. You'll need the [PySparkexec](https://docs.databrics.com/spark/latest/python/languageapi/pysparkee.html#pypyspex) package, and the notepad will need access to the local environment. This can be done with the following code:\n\\begin{itemize}\na. Create a temporary folder, for example `c:\\temp\\test`\nb. Add your local pyenv to that folder: `cd c:\\test; pip install --user --upgrade --prefix env_local py39` and then `cp .bashrc .env_python3.9/bashren` (for example)\nc. Start the databrick notecd: `$ notemodules.py -n databrick_notebook -e envlocal_p3_9 -d databriks-cluster-test-notebrick` This will create the folder `/databrick/test/databri_cluster_test_envlocal` for the environment to be stored in\nd. Edit the `.env` file and add the path", 0.24434389173984528, 0.04566210135817528, 0.13574661314487457, 0.054631829261779785], ["I'm looking at python code working with numba, and have some questions. There is less tutorial on numba, so that I come here to ask.\nIn numba, data type is pre-declared to help processing. I'm not clear on the rule to declare data type. One example is numba.float64[:,::1]. I feel it's declaring a 2D array in float type. However, I'm not sure what ::1 means here. Another example is nb.types.NPDatetime('M')[::1]. Is it slicing the 1D array?\nI still have questions on ListType(), which is imported from numba.types. Only one element is allowed here? In my code, one class type is saved and passed to ListType() as single argument. What if I need to explicitly define this class type, and pass it here? Thanks.\nI feel there is few tutorial or documents on numba module. If ok, please share some resources on numba. That's very appreciated.", "One example is numba.float64[:,::1]. I feel it's declaring a 2D array in float type. However, I'm not sure what ::1 means here\n\nExactly. ::1 means that the array axis is contiguous. This enable further optimizations like the use of SIMD instructions.\n\nAnother example is nb.types.NPDatetime('M')[::1]. Is it slicing the 1D array?\n\nnb.types.NPDatetime('M') is a Numpy datetime type (where 'M' is meant to specify the granularity of the datetime. eg. months) here and [::1] means that this is a 1D contiguous array (containing datetime objects).\nOne should not be confused between object instances and object types. In Python, this is quite frequent to mix both but this is due to the dynamic nature of the language. Statically-typed compiled languages like C or C++ clearly separate the two concepts and types cannot be manipulated at runtime.\n\nOnly one element is allowed here?\n\nListType is a class representing the type of a typed list. Its unique parameter defines the type of the item in the resulting type of list. For example nb.types.ListType(nb.types.int32) returns an object representing the type of a typed list containing 32-bit integers. Note that it is not a list instance. It is meant to be provided to Numba signature or other types.", "How to understand [] in data type definition in numba", 1.2000000476837158, 1, 315, "`::` means \"taking the slice of the array\", and `[:]` is \"creating a view on this slice\".\nThe former is used when you want to operate on a subarray of an array. The latter is useful for creating views on an existing array, e.g. to make it possible to pass the same array to multiple functions without copying it.", 0.1942446082830429, 0.014492753893136978, 0.1366906464099884, 0.04559940844774246], ["I am analyzing a consumer survey and there are both Dutch (NL) and French (FR) respondents. Depending on the answer they gave when we asked about their mother language they got the same questionnaire but translated in Dutch or French. The problem is that the output of Qualtrics (the survey software) gave us the following output:\n\n\n\n\nUser_Language\nQ1_NL\nQ2_NL\n...\nQ1_FR\nQ_FR\n...\n\n\n\n\nNL\n1\n3\n...\n\n\n...\n\n\nNL\n4\n4\n..\n\n\n...\n\n\nNL\n1\n3\n...\n\n\n...\n\n\nNL\n2\n5\n...\n\n\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nFR\n\n\n...\n3\n2\n...\n\n\nFR\n\n\n..\n4\n3\n...\n\n\nFR\n\n\n...\n2\n5\n...\n\n\nFR\n\n\n...\n1\n2\n...\n\n\n\n\nAs you can see the answers for the Dutch-speaking participants were recorded in the first n columns, while the French answers were recorded in the following n columns.\nHow can I cut the French answers from the last n columns and append them on the bottom of the DataFrame as those are answers to the exact same questions?\nThanks!\nEDIT: Solutions that make use of grouping by the strings \"Q1\" or \"Q2\" are unfortunately not viable as the column names are actually the questions, this was just an example value. I do know the exact range of the French answers and the Dutch answers.", "If there aren't any overlapping values you could simply split the data frame into two data frames based on where the dutch values \"stop\" and the french \"start\" rename the value columns of those two data frames   Q2_NL and so on to simply Q2 ... and then concatenate those frames again into one.", "Transform DataFrame: place values in right columns as new rows", 1.2000000476837158, 1, 316, "The solution that comes closest to solving the problem was to use\n`pandas.concat(pd.Series(df['User Language'].str.extract('(?<=Q\\d+)_FR', expand=True), index=df.index, axis=1).stack().unstack(0), axis='columns')`\nwhich is the closest thing to a one-liner as I could find. It works, but does require to explicitly state the indices of each column and can't be generalized to more columns with different formats.", 0.17322835326194763, 0.0, 0.11023622006177902, 0.03846153989434242], ["I`m trying to make a research in which the observations of my dataset are represented by matrices (arrays composed of numbers, similar to how images for deep learning are represented, but mine are not images) of different shapes.\nWhat I`ve already tried is to write those arrays as lists in one column of a pandas dataframe and then save this as a csv\\excel. After that I planned just to load such a file and convert those lists to arrays of appropriate shapes and then to convert a set of such arrays to a tensor which I finally will use for training the deep model in keras.\nBut it seems like this method is extremely inefficient, cause only 1/6 of my dataset has already occupied about 6 Gb of memory (pandas saved as csv) which is huge and I won't be able to load it in RAM (I'm using google colab to run my experiments).\nSo my question is: is there any other way of storing a set of arrays of different shapes, which won`t occupy so much memory? Maybe I can store tensors directly somehow? Or maybe there are some ways to store pandas in some compressed types of files which are not so heavy?", "Are you storing purely (or mostly) continuous variables? If so, maybe you could reduce the accuracy (i.e., from float64 to float32) these variables if you don't need need such an accurate value per datapoint.\nThere's a bunch of ways in reducing the size of your data that's being stored in your memory, and the what's written is one of the many ways to do so. Maybe you could break the process that you've mentioned into smaller chunks (i.e., storage of data, extraction of data), and work on each chunk/stage individually, which hopefully will reduce the overall size of your data!\nOtherwise, you could perhaps take advantage of database management systems (SQL or NoSQL depending on which fits best) which might be better, though querying that amount of data might constitute yet another issue.\nI'm by no means an expert in this but I'm just explaining more of how I've dealt with excessively large datasets (similar to what you're currently experiencing) in the past, and I'm pretty sure someone here will probably give you a more definitive answer as compared to my 'a little of everything' answer. All the best!", "How to store a set of arrays for deep learning not consuming too much memory (Python)?", 0.0, 0, 317, "In general, you should use sparse matrices for this sort of thing. The reason is that the memory used is not proportional to the number of elements, it is proportional with the size of the largest element. For instance, for a matrix with a 250x200 shape, the total memory is around 512 megabytes, regardless of how many elements it has. This means that a dataset with 30 0.01 size elements will take the same memory as one with only one element of size 99.9, or one million elements of 4 bytes. I am not sure about the exact memory footprint of pandas.csv, if you are able, try to use scikit-learn's `SparseCSVLoader` to read the file as you create it. You will have to change the `max_features` argument to `1`, but you can use the whole data as an input to your network. If you want to train a network, I would suggest to save the data in a binary format and load them as numpy arrays. It is faster and will consume less memory.", 0.3193717300891876, 0.02631578966975212, 0.14136125147342682, 0.03880128264427185], ["I`m trying to make a research in which the observations of my dataset are represented by matrices (arrays composed of numbers, similar to how images for deep learning are represented, but mine are not images) of different shapes.\nWhat I`ve already tried is to write those arrays as lists in one column of a pandas dataframe and then save this as a csv\\excel. After that I planned just to load such a file and convert those lists to arrays of appropriate shapes and then to convert a set of such arrays to a tensor which I finally will use for training the deep model in keras.\nBut it seems like this method is extremely inefficient, cause only 1/6 of my dataset has already occupied about 6 Gb of memory (pandas saved as csv) which is huge and I won't be able to load it in RAM (I'm using google colab to run my experiments).\nSo my question is: is there any other way of storing a set of arrays of different shapes, which won`t occupy so much memory? Maybe I can store tensors directly somehow? Or maybe there are some ways to store pandas in some compressed types of files which are not so heavy?", "Yes, Avoid using csv/excel for big datasets, there are tons of data formats out there, for this case I would recommend to use a compressed format like pd.Dataframe.to_hdf, pd.Dataframe.to_parquet or pd.Dataframe.to_pickle.\nThere are even more formats to choose and compression options within the functions (for example to_hdf takes the argument complevel that you can set to 9 ).", "How to store a set of arrays for deep learning not consuming too much memory (Python)?", 0.0996679961681366, 1, 318, "There are two ways, you can use Pandas dataframe or you use numpy arrays. I prefer numpy array. If you are storing your data in the numpy.array format, then you have to be aware of some things\n\\begin{itemize}\n \\item The size of the array will be large. So, if you want to use a specific memory then, it is better to save your array in a compresed format. For example,\n [scipy.io.savemat](https://docs.scikit-learn.org/stable/modules/generated/sklearn/utils/sparsetools.html#scikits-learnsparsemat)\n You can compress the data to sparse matrix by using this command. The comressed array size will not be more than 20% of your original array (based on my experience) .\n If the size is more, compress your numpy file to scipy sparse format and save it. Then, read the comresed file again and load the file in numpy format as per your requirement. There is no other option. You cannot use the original file as it will occupies much of space. However, I suggest to create a new file. It will save", 0.22489960491657257, 0.04048582911491394, 0.1204819306731224, 0.031020013615489006], ["I have written a script which deploys a Dashboard using plotly-dash. It has graphs, data for which is coming from the excel file located on the PC. This data is stored in the excel file which will be updated on a daily basis. What can I do for the app to get updated with the new data without me redeploying it every day? Maybe you can give some advice or ideas?\nP.S. The dashboard is currently deployed using Heroku.", "If your app reads the file in as part of a callback based on something like the pathname (dcc.Location), then you could just refresh the page.", "Plotly/dash Dashboard is not live updating - Python", 0.0, 0, 319, "If you want to use the plot data from your excel, you should use this [example](https://dash-core-components.org/docs/v0.5.1/charts/experimental_data_table) from DASH. In this example, the data in your dash comes from a REST endpoint (which you could also use in this case).\n\\begin{itemize}\nThe following code should give you a good start. I added a small example of the REST API for this. Note that the `data` parameter of your `DataTable` is a dictionary of data which comes in an JSON object. You can then use that data to create the graphs. (I would recommend using `pandas` or `numpy` for that). The following example is just a basic example:\nhttps: //codesandbox.io/s/live-updating-data-table-example-0-c1032\nIf you are using this in a project, make sure to read the docs about how to connect to a database (or other) REST endpoints. That is what I did in my example above. To create your graphs you need to pass the corresponding data (the data that comes as a response from that REST", 0.12206573039293289, 0.009478673338890076, 0.0845070406794548, 0.12598426640033722]]}