{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I was importing a Swin-Transformer like this, which used to work the last week:\npip install tfswin\nfrom tfswin import SwinTransformerLarge224\nSince today I get the following error:\n\"----> 3 from keras.mixed_precision import global_policy\nImportError: cannot import name 'global_policy' from 'keras.mixed_precision' (/usr/local/lib/python3.7/dist-packages/keras/mixed_precision/init.py)\"\nI tried pip installing those packages as well as setting global policy with set_global_policy('float 32'). Nothing seems to work. Is it likely this is going to work again tomorrow ? Im a bit time pressured because it's a master thesis and this was the first Swin import that worked for me.\nTF version is 2.10.0", "Fixed it with !pip install keras==2.9.0.", "cannot import name 'global_policy' from 'keras.mixed_precision' since today", 0.1973753273487091, 1, 640, "`from kerast.kerast_migated_model import GlobalPolicy`\n(in `init_kerasmixedprec_v2_py37.cpython-38-x86_64-linux-gnu.so` for example)", 0.0, 0.0, 0.0, 0.05263158306479454], ["I am working on with 'ERA5-Land monthly averaged data from 1950 to present' for total precipitation and evaporation.\nI found out that the number of values in the dataset is 1,306,863,104.\n[1306863104 values with dtype=float32]\nHowever, the total dimension of the dataset is 86418013600 (time, latitude, longitude respectively) =5,601,830,400.\nHave you guys ever worked with such data?\nI want to calculate SPEI index (using climate_indices package), but the error code is like that they cannot reshape 1,306,863,104 data into (864,1801,3600). Because of this issue, I am stuck in..\nPlease help me.", "I solved the problem by myself.\nIt was because of the dtype.\nDefault dtype of built-in function was int type.\nThe number of value exceeds the range that integer can represent.\n5,601,830,400 > 2^32\nTherefore, I set the dtype to float.\nThen, It worked well.", "What if the number of values in dataset is different from total dimension of dataset in python?", 0.0, 0, 641, "The number you have is the shape of a numpy array, it doesn't mean that it's the dimension, as you are using a 3D array. You can do:\n\\begin{itemize}\n  \\item to reshaping the array: `x.reshape(8, 6)`\n    \\begin {code}", 0.239130437374115, 0.02222222276031971, 0.1304347813129425, 0.05583309009671211], ["I'm trying to get the ProXAS_v2.43 running for the evaluation of QEXAFS data. I installed the necessary packages the manual provided, but when I try to start the program I get the following error: ImportError: cannot import name 'donaich' from 'lmfit.lineshapes' (C:\\Users\\sq0346\\Anaconda3\\lib\\site-packages\\lmfit\\lineshapes.py)\nAll packages required listed by conda search  , should be present.\nMainly: Pandas, Scipy, Numpy-indexed, Xraylarch\nFull error:\n\nFile\n~\\Anaconda3\\envs\\py38\\Lib\\site-packages\\ProQEXAFS-GUI-master\\ProXAS-2.43\\ProXAS_v2.43.py:9\nin \nimport tkinter, time, os, psutil, subprocess, sys, shutil, ast, codecs, re, larch, gc, peakutils.peak, itertools\nFile ~\\Anaconda3\\lib\\site-packages\\larch_init_.py:47 in \nfrom . import builtins\nFile ~\\Anaconda3\\lib\\site-packages\\larch\\builtins.py:21 in \nfrom . import math\nFile ~\\Anaconda3\\lib\\site-packages\\larch\\math_init_.py:4 in\n\nfrom .utils import (linregress, realimag, as_ndarray,\nFile ~\\Anaconda3\\lib\\site-packages\\larch\\math\\utils.py:11 in\n\nfrom .lineshapes import gaussian, lorentzian, voigt\nFile ~\\Anaconda3\\lib\\site-packages\\larch\\math\\lineshapes.py:16 in\n\nfrom lmfit.lineshapes import (gaussian, lorentzian, voigt, pvoigt, moffat,\nImportError: cannot import name 'donaich' from 'lmfit.lineshapes'\n(C:\\Users\\sq0346\\Anaconda3\\lib\\site-packages\\lmfit\\lineshapes.py)\n\nUpdating XRaylrach to version 0.9.60 resolved it, but produced a new error:\n\nFile\n~\\Anaconda3\\Lib\\site-packages\\ProQEXAFS-GUI-master\\ProXAS-2.43\\ProXAS_v2.43.py:9\nin \nimport tkinter, time, os, psutil, subprocess, sys, shutil, ast, codecs, re, larch, gc, peakutils.peak, itertools\nFile ~\\Anaconda3\\lib\\site-packages\\larch_init_.py:48 in \nfrom .version import date, version, release_version\nImportError: cannot import name 'release_version' from\n'larch.version'\n(C:\\Users\\sq0346\\Anaconda3\\lib\\site-packages\\larch\\version.py)", "Update xraylarch to its latest version.  That will fix the misspelled import.", "ProQEXAFS: ImportError: cannot import name 'donaich' from 'lmfit.lineshapes'", 0.0, 0, 642, "As explained in the [release announcement](https://www.lars-koch.de/release-announcement/2018-08/):\n\\begin{itemize}\n  \\item ProQuest Exafs does not support Python 3 yet\n    (https:/www-xray.lbl.gov/software/proquest-exaf-gui/troubleshooting.html#python3) -> try Python2\n  2) you are using Python3, hence the imports fail\n     -> please use Python version \\strong{2} instead. Note that X-ray labs also provide a Python package (XrayLarch) that you can use for Python\n     2 only (and not 2 & 6).\n       https://github.com/lks-lab/X-Rays-Labs\n        Note: there is a bug in Python which causes this. See\n           https:/github/io/issues/6633 for more details. You can try\n         https:githubcom-XR-Py-3-6-5-8 to fix this, or you could also\n          try running Python from anaconda", 0.08510638028383255, 0.0, 0.042553190141916275, 0.0416666641831398], ["I created a model with CatBoostRegressor. My dataset is 74274 rows \u00d7 24 columns.I am using encoding and min-max scaler.\nThe more I raise the n_estimators value in my model, the better the model score. What is the end of this? How do I decide where to stop? That way I guess it goes forever. Is being high good or bad? Where should the stopping point be?\nmodel = CatBoostRegressor(n_estimators=3000,verbose=False)\nmodel = CatBoostRegressor(n_estimators=10000,verbose=False)\nmodel = CatBoostRegressor(n_estimators=20000,verbose=False)\n.\n.\n.", "Which set are you checking the score upon?\nIf that's a train set, the score will likely keep increasing because of overfitting.\nFor a validation set, the score should stop increasing at a certain point once the order of model complexity becomes comparable to the sample size.\nSklearn/skopt cross validation routines such as GridSearchCV() should aid you in automating the best hyperparameters' selection.", "Are high n_estimators good?", 0.0, 0, 643, "If you are using 3K, 1K and 5K for the number of estimators, then you should be getting different scores. If they are the same then it means you may be over-fitting. So, it depends on your data and the problem you want to solve. In this case, I would suggest you to use 4K as the maximum number estimator.", 0.2222222238779068, 0.0, 0.1428571492433548, 0.04683730751276016], ["I have two byte arrays - one from mic and one from soundcard of same duration (15 seconds). They have different formats (sample rate of mic = 44100, n_frames = 1363712; sample rate of stereo = 48000, n_frames=1484160). I had assumed resampling would help (16k desired) but they are still of differing lengths and can't simply be combined (added - am assuming adding tensors will result in mixed audio).\nI can't see a built in method for mixing audio, but perhaps I'm overlooking something.\nI see that sox_effects is included, but none of the effects listed seem relevant - although I know sox can mix audio.\nAm I barking up the wrong tree with torchaudio?", "Mixing audio is simply taking sum or average of source waveforms, so TorchAudio does not provide a specialized method, but users are expected to do the operation with pure PyTorch Tensor operation.\nNow the problem you need to think is how to handle the different lengths, i.e. how to make them the same length.\nYou can cut the long one to align it to the short one, or zero-pad the short one to align it to the long one.", "Is it possible to mix two mono audio tensors of different length (number of frames) in torchaudio?", 1.2000000476837158, 1, 644, "You can use the `concat` method on the AudioTensor, it will add the two AudioSamples together.  For example:\n`tensor.concat([sample, other_sample])`\nBut, this will take a bit of work to get it into the correct format for your needs.", 0.2295081913471222, 0.01666666753590107, 0.13114753365516663, 0.06185566633939743], ["I'm using ArangoDB 3.9.2 for search task. The number of items in dataset is 100.000. When I pass the entire dataset as an input list to the engine - the execution time is around ~10 sec, which is pretty quick. But if I pass the dataset in small batches one by one - 100 items per batch, the execution time is rapidly growing. In this case, to process the full dataset takes about ~2 min. Could you explain please, why is it happening? The dataset is the same.\nI'm using python driver \"ArangoClient\" from python-arango lib ver 0.2.1\nPS: I had the similar problem with Neo4j, but the problem was solved using transactions committing with HTTP API. Does the ArangoDB have something similar?", "Every time you make a call to a remote system (Neo4J or ArangoDB or any database) there is overhead in making the connection, sending the data, and then after executing your command, tearing down the connection.\nWhat you're doing is trying to find the 'sweet spot' for your implementation as to the most efficient batch size for the type of data you are sending, the complexity of your query, the performance of your hardware, etc.\nWhat I recommend doing is writing a test script that sends the data in varying batch sizes to help you determine the optimal settings for your use case.\nI have taken this approach with many systems that I've designed and the optimal batch sizes are unique to each implementation. It totally depends on what you are doing.\nSee what results you get for the overall load time if you use batch sizes of 100, 1000, 2000, 5000, and 10000.\nThis way you'll work out the best answer for you.", "Query execution time with small batches vs entire input set", 0.0, 0, 645, "The reason is that the batch size of the underlying index is one. That means if you have a dataset that is greater than the index size then you will need to go through all the data. If the size is smaller, then the query can be done in one step.", 0.2293577939271927, 0.02777777798473835, 0.15596330165863037, 0.06398696452379227], ["I have a dataframe which includes both numeric and non numeric values (It includes some special characters like -, space etc). I want to encode that Non Numeric value to run corr(). Non numeric Column name eg: 'Department', 'Location', etc. I used Label Encoder(). But it shows a TypeError;\nTypeError: '<' not supported between instances of 'int' and 'str'\nI used this code :\nle = preprocessing.LabelEncoder()\nX_train['Department'] = le.fit_transform(X_train['Department'])", "If the data is not ordinal, I wouldn't use LabelEncoder with corr(), as that will yield false insight.\npd.getdummies(X_train['Department']) has been adequate for using pd.DataFrame.corr() for me. It will create as many columns as there are classifications, and mark 1 for each row where the classification matches the column label, otherwise 0.\nThe other issue is possibly mixed datatypes in 'Department', which can be fixed with df['Department'] = df['Department'].astype('str'). It's probably most efficient to do this before your train-test split.", "Is there any way to encode Non Numeric values in a dataframe column", 0.0, 0, 646, "You should try to use `pandas.to_numeric()` method for all your numeric data:\nhttps://pandas-docs.github.io/pandas/reference/api/generated/pd.DataFrame.astype.html\nExample: `X.reset_index().astore_numpy()`\nAlso you can use function `drop_duplicates()` to remove duplicated values.\nAnd for non-numeric values you should check them by using `isnumeric()`.", 0.1756756752729416, 0.013698630034923553, 0.0810810774564743, 0.04716981574892998], ["Is it correct to use 'accuracy' as a metric for an imbalanced data set after using oversampling methods such as SMOTE or we have to use other metrics such as AUROC or other presicion-recall related metrics?", "You can use accuracy for the dataset after using SMOTE since now it shouldn't be imbalanced as far as I know. You should try the other metrics though for a more detailed evaluation (classification_report_imbalenced combines some metrics)", "imbalabced data set score after smote", 0.0, 0, 647, "Yes. Accuracy is the correct metric.", 0.08695652335882187, 0.0, 0.08695652335882187, 0.19354838132858276], ["i have a MILP with ~3000 binaries, 300000 continuous variables and ~1MM constraints. I am trying to solve this on the VM how long could it potentially take on a 16 core 128 gig machine? also what are the general limits of creating problems using  pulp that cplex solver can handle on such a machine? any insights would be appreciated", "It is impossible to answer either question sensibly. There are some problems with only a few thousand variables that are still unsolved 'hard' problems and others with millions of variables that can be solved quite easily. Solution time depends hugely on the structure and numerical details of your problem and many other non-trivial factors.", "scaling MILP using pulp and cplex", 0.0, 0, 648, "The solution time depends on how much time it takes to get the MIP formulation and on your objective function. The objective can be solved using the \\em{minimize} command. If your problem is a \\strong{single objective} problem with a linear objective, then you can do the following:\n\\begin{itemize}\n1) Write the objective and constraints in MPL\n2) Construct the model using \\begin {code}", 0.28099173307418823, 0.08403361588716507, 0.1652892529964447, 0.04152148589491844], ["i have a MILP with ~3000 binaries, 300000 continuous variables and ~1MM constraints. I am trying to solve this on the VM how long could it potentially take on a 16 core 128 gig machine? also what are the general limits of creating problems using  pulp that cplex solver can handle on such a machine? any insights would be appreciated", "The solution time is not just a function of the number of variables and equations. Basically, you just have to try it out. No one can predict how much time is needed to solve your problem.", "scaling MILP using pulp and cplex", 0.0, 0, 649, "You can run the solvers directly on your VM and you can see how they behave.\nThere is no direct link between the number of variables/constraints and the solution time. The time is more related to the size of the problem, i.e. the amount of data to be processed. As an example, I have run a problem with 20 million variables, over 5 million constraints and 80 days of solution run time on Cplex. There are many factors that affect the running time, but the most important ones are:\n\\begin{itemize}\na) number and type of constraints\nb) time to construct the model (in the form of cmodels)\nc) memory requirements of Cmodels (and therefore the input file size) for the particular model. If the memory requirement is large, it may require too much memory to fit in the system. You should be able to check this by running the example model and see the message \"unable to allocate memory for variable\" when you try to run. Clients typically allocate ~25% more memory than they need. Too little memory and a part of it will be used for garbage and swapping. This will reduce the run times. But if the", 0.21008403599262238, 0.05932203307747841, 0.11764705926179886, 0.012987014837563038], ["Is there a difference between pandas sum() function and SQL SUM(...) function. I'm using tables with around 100k rows. My current test runs were not good. The runtime was always different with both being not predictable (problem might be my bad wifi...)\nIt will run on a server later, but maybe someone knows it already and I don't have to pay for my server now.\nThanks in advance!", "It might be hard to get a clear answer without actual tests because it depends so much on what machines are used, what you are willing to pay for each part, ...\nHowever, aggregating the data in SQL gives you less network traffic, which can be valuable a lot of the time.", "Pandas sum vs. SQL sum", 0.0, 0, 650, "I had the same problem and found the solution in this article: https://www.pydata.org/blog/pandas-dataframe-sum-vs-pandasdataframesum-performance/\nThere's a lot of information about the differences in performance.", 0.1411764770746231, 0.048192769289016724, 0.1411764770746231, 0.07853402197360992], ["I have a network that's pretty much UNet. However, the model crashed when I feed in input size of 3x1x1 (channel =3, height =1, width=1) since the first max pooling (with kernel size =2 and stride =2) will reduce the dimension into 3x0x0.\nHow do I modify Unet model such that it can take my 3x1x1 input and handle arbitrary number of poolings? Any help is appreciated!", "One must normalize sizes of images with preprocessing, see torchvision.transforms.functional.resize.", "Modify UNet to take an arbitrary input dimension?", 0.0, 0, 651, "It'd be best if you could give some more info, but I guess your problem is that you're using the wrong input shape. The default input of an U-Net is (100,150) which is the default shape of the pre-activations in the 1st convolutional layer. You could modify the input to (3032, 540).", 0.02857142873108387, 0.0, 0.02857142873108387, 0.010894178412854671], ["I am new to using reinforcement learning, I only read the first few chapters in R.Sutton (so I have a small theoretical background).\nI try to solve a combinatorial optimization problem which can be broken down to:\nI am looking for the optimal configuration of points (qubits) on a grid (quantum computer).\nI already have a cost function to qualify a configuration. I also have a reward function.\nRight now I am using simulated annealing, where I randomly move a qubit or swap two qubits.\nHowever, this ansatz is not working well for more than 30 qubits.\nThat's why I thought to use a policy, which tells me which qubit to move/swap instead of doing it randomly.\nReading the gym documentation, I couldn't find what option I should use. I don't need Q-Learning or deep reinforcement learning as far as I understood since I only need to learn a policy?\nI would also be fine using Pytorch or whatever. With this little amount of information, what do you recommend to chose? More importantly, how can I set my own value function?", "There are two categories of RL algorithms.\nOne category like Q-learning, Deep Q-learning and other ones learn a value function that for a state and an action predicts the estimated reward that you will get. Then, once you know for each state and each action what the reward is, your policy is simply to select for each state the action that provides the biggest reward. Thus, in the case of these algorithms, even if you learn a value function, the policy depends on this value function.\nThen, you have other deep rl algorithms where you learn a policy directly, like Reinforce, Actor Critic algorithms or other ones. You still learn a value function, but at the same time you also learn a policy with the help of the value function. The value function will help the system learn the policy during training, but during testing you do not use the value function anymore, but only the policy.\nThus, in the first case, you actually learn a value function and act greedy on this value function, and in the second case you learn a value function and a policy and then you use the policy to navigate in the environment.\nIn the end, both these algorithms should work for your problem, and if you say that you are new to RL, maybe you could try the Deep Q-learning from the gym documentation.", "How to set your own value function in Reinforecement learning?", 0.1973753273487091, 1, 652, "The value-function is usually the target function that the policy tries to optimize. In your case, the value could be something like the objective function, e.g. the mean-squared error in the cost- or reward-value function (see [here](https://www.machinelearningmastery.com/q-learning-reinforced-qnetwork-python/)). The policy should try and minimize the error of the estimated value. So you could use [optimizer](http://pytorch.org/docs/stable/optim.html) (e. g. [Adam](//arxiv. org/pdf/1412.6980v2) optimizer) to minimze the loss of your cost value to the real value, and the optimizing policy would be your policy network.", 0.3076923191547394, 0.0773809552192688, 0.1834319531917572, 0.06227106228470802], ["I have two dataframes, df1 and df2, and a fairly complicated set of logical statements that I have to run as a separate function to merge them. That function returns a pair of indices for the row in df1 and the row in df2, that looks right now like\nmatches = [[1,2,7,14], [1,2,7,14], [3,8]]\nsomething like that so that matches[idx] has a list of indices in df2 to merge with the row df1.loc[idx], so rows 0 and 1 in df1 would merge with rows 1,2,7,14 in df2, and on.\nHow would I merge df1 with df2 on these lists? The logic is prohibitive to try to run through pandas in terms of speed, so I have to start with these lists of matches between the dataframes.", "Per @MYousefi, this was the solution:\n\nTry\npd.concat([df1, pd.Series(matches, name='match')], axis=1).explode('match').merge(df2, left_on='match', right_index=True)\nShould work for numerical indices.", "Merging pandas dataframes based on lists of paired indices", 1.2000000476837158, 0, 653, "You can use `merge` with `indicator` parameter, where you can set `left_on` and `right_off` to use for merge:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/merge.html\n`left=matches`\nand\nright=df2\nshould do the trick\nIf you want to avoid index duplication, you need to set the merge options `on=None`", 0.22727273404598236, 0.023255813866853714, 0.11363636702299118, 0.049716766923666], ["I am working on a project where I am given a round 200 readings/featured columns and based on those reading there are some attributes about 60(columns) of them ranked from 0-5. now I have about 1000 rows from the featured readings and only 100 from the attributes. I am looking to use a model that I can train the data with the 100 attributes filled out and then predict on the remaining 900 attributes rows from the featured data given.\nAre there are any recommendations for the best approach or even better a similar project I can reference?", "I was able to figure it out I just ran a loop to train on each dependant var separately. if you have a big dataset like 300,000 using random forest take about 2.5- 3 seconds per dependant var and then used the missing data as a df to find predictions and append. if you need more explanation let me know", "Predict Values based on readings Best approach", 1.2000000476837158, 0, 654, "This is what you call \\strong{classification} problem. In the simplest case, you can try to fit a single model and use it to predict the values of the new data. However, this may not be a good idea as the model will be underfit and will have very low accuracy. You may try out this approach and see how it works. If it doesn't work well, then you may split the dataset into training and testing set and fit the single classifier on training set. Then you test it on testing data to see if it performs well. This will give you an idea how well it will perform on new dataset. Next step is to try multi-class classifiers and evaluate them.", 0.2139037400484085, 0.0, 0.12834224104881287, 0.053278692066669464], ["In a data frame, when the value of a given column is below 10, I need to change all the values till the end of the column to 5.\nso let's say these are the values of the column:\n\n\n\n\nA\n\n\n\n\n134\n\n\n413\n\n\n12\n\n\n81\n\n\n9\n\n\n483\n\n\n93\n\n\n30\n\n\n\n\nI would need it to become:\n\n\n\n\nA\n\n\n\n\n134\n\n\n413\n\n\n12\n\n\n81\n\n\n5\n\n\n5\n\n\n5\n\n\n5\n\n\n\n\nI apologize if I didn't explain this well, I'm new to coding. Thanks for all the help!", "you coutd try:\nmyDataFrame = np.where(myDataFrame < 10, 5, myDataFrame)\nthis checks where the value is lower than ten. if it is it sets it to five, else it just sets the value to what it already was.", "How do I change the value in a column given a Boolean (in Python)?", -0.049958374351263046, -1, 655, "\\begin{code}\nIn [1]: import pandas as pd\n    In [2]: df = pd.DataFrame({'a': [4, 2, -1,0], 'b': [-2,-2.221,-1.11,'-2']})\n     ...: df\nOut[26]:\n   a     b\n0  4   -2  # <--- a should be > 0 to be 4 and < -0.5 to get 3 (1-0)\n\\end{pre}", 0.06818182021379471, 0.0, 0.04545454680919647, 0.06128450110554695], ["I am reading a parquet file with polars and would like to convert a column called datetime from type datetime[ms, America/New_York] to datetime[ns,UTC].\nI can take the column out and do it in pandas, use tz_convert and add the column back to polars dataframe but would be nice if there was a way to do it in polars :)", "As of polars 0.14.14 there is:\npl.col(\"datetime\").dt.with_time_zone which sets a timezone without modifying the underlying timestamp.\npl.col(\"datetime\").dt.cast_time_zone which modifies the underlying timestamp by correcting from the current timezone the the given timezone.", "Is there a way to convert a pl.Series of type pl.Datetime from one timezone to another?", 1.2000000476837158, 1, 656, "This is not currently possible. However, this is something we are looking into adding to the framework.\nIt would involve a slight change to how datetime fields are handled. We are considering having a default timezone, which can be overridden. For example, if you are reading in a file, the default time zone can remain the local time. If you want to change it, then you would have to pass in the timezone you wanted to use. You would then be able to specify a timezone when writing out the data.", 0.21897810697555542, 0.029629629105329514, 0.16058394312858582, 0.06161763519048691], ["I want to find the root of a 2D nonlinear equation.\nI wrote: res = mpm.findroot(f=func1, x0=x01, tol=1.e-5, solver='MDNewton', J=JAKOB1)\nbut I get this message: ValueError: could not recognize solver\nAs per the documentation of findroot, MDNewton is an acceptable solver.\nWhere is my mistake?\nscipy.optimize.fsolve works fine, but I need the increased accuracy of mpmath.\nThanks a lot for any help!", "All I had to do was to write mdnewton, all small letters. No idea, why but it worked.", "How to use solver in mpmath.findroot", 0.0, 0, 657, "The solvers for `find_root` are implemented using `mpmath`'s `solvers` function, which you can see at https://mplib.org/mp/docs/reference/solver/index.html.  It does not seem that `MDnewton` is implemented as a solvertion, and `f_jac` does seem to be required.", 0.095238097012043, 0.0, 0.0317460335791111, 0.013333332724869251], ["The title pretty much says it all, I have a df with 40+ dimension which I'd like to process into the Umap algorithm in order to have a 2-d output.\nI would like to know if it is possible to weight the input columns differently for the purpose of studying the possible different Umap outcomes.\nThank you for your time\nP.S. I work in python", "Why not simply applying UMAP to A:\nA = X*W\nwhere X is your Nx40 matrix and W=diag(w) is a 40x40 diagonal matrix of weights w=[w1, w2,..., w40]?\nConsider using normalized weights wi, i=1,2,...,40 such that sum(w) == 1, to distribute normally your information.", "How to perform Weighted dimensionality reduction with Umap", 1.2000000476837158, 1, 658, "UMAP's default behaviour is to assign equal weights to all dimensions, i.e. if you have 10 dimensions then all of them will have the same weight. To change the default weights you need to use the `weights` argument of `uMap.fit`.\nMore information about this can be found in the [UMAP documentation](https://github.com/tidyverse/tibble/blob/master/inst/doc/articles/umap.md).\nFor example:\n`uMAP(x, weights=c(3, 5,2,4,1))`", 0.18897637724876404, 0.0, 0.11023622006177902, 0.04651111736893654], ["We currently have a working application that is ready to post and get data from an API which displays results of predicted disease (purpose of the ML model). Right now we don't have an exact idea on how to make the .ipynb communicate with the application provided we have large data for training the model.\nWe have 2 .ipynb files Model.py and Predict.py. One performing the required pre-processing, split (for train, test and validation), train and save the model. Predict uses the saved model and classifies the user input.\nThe main concern is how do we send the data from User's end-point(Flutter Application) to Predict.py and get the result data back to the user on the application.\nWe have considered the idea of hosting the model with prediction somewhere, but do not know on how to proceed further.\nThis is my first encounter with handling Deep Learning with Flutter Application. Any kind of information on proceeding forward will be very helpful.", "First, .ipynb files are Jupyter Notebooks.\nSecond, do you have your API ready ? Is there a server dedicated to it or is there only the flutter app, that is ony the front of your application.\nIf you do not have an API, you have to create one (using whatever framework you want).\nTo facilitate things, create it in Python, and you can directly import your model as a Python module and use it.", "How do I integrate a Flutter application with ipynb/colab notebook?", 0.0, 0, 659, "I found a workaround to this problem by creating a new Flask API with 3 endpoints\n\\begin{itemize}\nGet Predicted Labels\nPost Prediction Results\nPredict Label\nIt does not use the existing API because it is very large and the performance is not good. We have implemented the same logic in the new API and it works as expected.", 0.28787878155708313, 0.0, 0.1666666716337204, 0.04511278122663498], ["I'm not used to the TextVectorization Encoder Layer. I created my vocabulary manually before. I was wondering how one can save a Keras Model which uses the TextVectorization layer. When I tried to do it with simply model.save() and later models.load_model() I was prompted with this error:\nAssertionError: Found 1 Python objects that were not bound to checkpointed values, likely due to changes in the Python program. Showing 1 of 1 unmatched objects: [<tensorflow.python.ops.lookup_ops.MutableHashTable object at 0x7fb9602df7c0>]", "I've solved my problem by using another version of Keras. If someone faces a similar issue I can recommend to use a different (most of the time newer) version of Keras.\nAs I already said in my comment. I can't really recommend Keras and or Tensorflow right now. I've started a big NLP project some time ago (half a year). And since then Keras had multiple updates. Their documents changed like 2 times. And the old examples are not there anymore. The new way to create Text Tokens is quite nice but their example uses Masking_zero=True. Which basically means that It will pad the sequences for you and following layers will ignore the zero. That sounds nice but masking is not compatible with Cuda which makes training larger models a time consuming job because it's not hardware accelerated with the GPU. And most NLP models are quite large.", "Save TextVectorization Model to load it later", 1.2000000476837158, 0, 660, "The problem is not with `TextVectorizer` but with the `MultiHeadDotProduct` layer in between.  The layer is using the same dictionary and changing it will cause the problem.\nYou can resolve the issue by using `tf.compat.v1.train.Saver` and `model.saver.restore()`\nThere are some more details on this issue [here](https://github.com/tensorflow/models/issues/3688).\nThis is my code\n\\begin{code}\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, InputLayer\ntf_version = '2.1'\nmodel = tf.contrib.learn.KerasClassifier(build_fn=model_builder, model_dir='model',\n                                          batch_size=32,\n#      epochs=1, # 20 epoch training\n      verbose=0,  # training verbose\n        class_weight='auto', # weights for each class\n         input_shape=(1008,),\n          )\nencoder = model._layers['input_embedding']", 0.25992780923843384, 0.02909090928733349, 0.08664260059595108, 0.04169827699661255], ["I'm trying to calculate the correlation between 2 multi-index dataframes(a and b) in two ways:\n1)calculate the date-to-date correlation directly with a.corr(b) which returns a result X\n2)take the mean values for all dates and calculate the correlation\na.mean().corr(b.mean()) and I got a result Y.\nI made a scatter plot and in this way I needed both dataframes with the same index.\nI decided to calculate:\na.mean().corr(b.reindex_like(a).mean()) and I again achieved the value X.\nIt's strange for me because I expected to get 'Y'. I thought that the corr function reindex the dataframes one to another. If not, what is this value Y I am getting?\nThanks in advance!", "I have found the answer - when I do the reindex, I cut most of the values. One of the dataframes consists of only one value per date, so the mean is equal to this value.", "Weird correlation results", 0.0, 0, 661, "In your second case, you are doing a mean calculation with `reIndex_Like`, which is not the case in the first one, as you used `corr` which calculates the difference between the two data frames. The correlation is actually between data frame and not data points.", 0.2222222238779068, 0.0, 0.14814814925193787, 0.05714285001158714], ["I have a dataframe similar to this yet, but a lot bigger (3000x3000):\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nW\n3\n1\n8\n3\n4\n\n\nX\n2\n2\n9\n1\n1\n\n\nY\n5\n7\n1\n3\n7\n\n\nZ\n6\n8\n5\n8\n9\n\n\n\n\nwhere the [A,B,C,D,E] are the column names and [W,X,Y,Z] are the rows indexs.\nI want to compare every cell with its surrounding cells. If the cell has a greater value than its neighbor cell value, create a directed edge (using networkX package) from that cell to its smaller value neighbor cell. For example:\nexamining cell (X,B), we should add the following:\nG.add_edge((X,B), (W,B))  and G.add_edge((X,B), (Y,C)) and so on for every cell in the dataframe.\nCurrently I am doing it using two nested loops. However this takes hours to finish and a lot of resources (RAM).\nIs there any more efficient way to do it?", "If you want to have edges in a networkx graph, then you will not be able to avoid the nested for loop.\nThe comparison is actually easy to optimize. You could make four copies of your matrix and shift each one step into each direction. You are then able to vectorize the comparison by a simple df > df_copy for every direction.\nNevertheless, when it comes to creating the edges in your graph, it is necessary for you to iterate over both axes.\nMy recommendation is to write the data preparation part in Cython. Also have a look at graph-tools which at its core is written in C++. With that much edges you will probably also get performance issues in networkx itself.", "Compare every cell in a dataframe with its surrounding cells", 0.0, 0, 662, "You can use this\n`df.assign(g = df.iloc[:,0].ne(df['B']).astype(int))`\nto assign the result of the comparison of each row with column B to the row itself. You then use `g` as the condition to add a new edge to `G`.", 0.2248520702123642, 0.011976048350334167, 0.1420118361711502, 0.0225988682359457], ["I have a 8x10 2D array, A. I need another 2D array that stores only the row index of each element of A. How do I go about doing so? Appreciate your help!", "I think I got it!\nC = np.array(np.where(A))[0].reshape(10, 8)", "How to return the row index of each element from a 2D array?", 0.0, 0, 663, "You can do it by this way.\n\\begin{itemize}\n  \\item Create a new 1D Array called B. It will be 400*1 (1 for each row).\n  2. For loop:\n       for(int i=0;i<A.length; i++)\n           B[A[i][0]] = i;\n    \\strong{B is now an array with each entry being the index value of the corresponding value in A} .\n    3. for loop :\n        for (int j=1; j<11 ;j++) {\n         for  ( int k= 0 ; k < A[j-1][j].length ;k++)  { // loop to get the value at the k'th position of  A\n            System.out.println( B [A [j][k]]);\n          }\n      } // end of second for\n \\end{code}", 0.09836065769195557, 0.0, 0.08196721225976944, 0.07508958876132965], ["I have a three-layer MLP classifier model.(Input layer - hidden layer - output layer).\nAnd I'd like to calculate the signed distance of data points from the decision boundary.\nIn the case of SVM or logistic regression, getting the signed distance is not that difficult.\nBut how about the MLP?\nI'd like to check \"how far is a new data from the decision boundary\", without the true label of the new data.", "Basically, the output of the classifier model represents the signed distance from the decision boundary if you don't use sigmoid activation in the output layer. If you do use sigmoid activation in the output layer, you can just use inverse sigmoid to find the signed distance using the following formula.\nIf p is the output of the classifier model with sigmoid activation in the final layer,\nsigned_distance = -ln((1 / (p + 1e-8)) - 1)", "Can I get signed distance from decision boundary for MLP model?", 1.2000000476837158, 0, 664, "If you use sklearn.tree.DecisionTreeClassifier, you can get the distance to the nearest node by calling\n\\begin{itemize}\n`tree._tree_.to_json()`\nand then\n`-` `infile.readlines()` `tree = Tree._deserialize(json_string)`.\nThen you get a tree object that you have access to. `node_toos` is the array of nodes and you should be able to get each nodes distance by doing\nnode.get_distance_from_parent(leaf).get(0)\nIf you want to iterate over all nodes, do a `for i in range(len(node._tobs)): node_i = node._nodes[i]` and then iterate again `i` through the tree and get all the distances.", 0.23333333432674408, 0.056179776787757874, 0.14444445073604584, 0.021333331242203712], ["How to get all the duplicate values of one specific column in dataframe?\nI want to only check values on one column but it's getting output with table or data.\nI want to count the number the times each value is repeated", "Use df['column'].value_counts().", "Regarding counting of duplicate values in data", 0.0, 0, 665, "You can do it with a for loop:\n\\begin{itemize}\n \\item create an empty dataframe\n2.loop through your data frame and if the values are the same you add the index to your empty df\n3.return the df with the indexes\n  \\strong{Note:} make sure your columns are unique\n    df = pd.DataFrame()\nfor i in df.index: if df['col'][i] == df[col][i+1]: df=df.append(df['index'], col)\ndf\n   col   index\n0  a    2\n1  b    3\n...\n4  e    4\n5  f    5\n6  g    6\n7  h    7\n8  i    8\n9  j    9\n[10 rows x 2 columns]\nOut[23]:\n     col    index   \\\na       2      NaN   NaN\nb       3     3       NaN NaN...etc\nf       5     7      7     9     NaN  NaN..etc... 12    14     etc\ng       6     8      8     11     ...\nh       7     np.", 0.0139860138297081, 0.0, 0.0139860138297081, 0.03112572804093361], ["I am using a Keras model to calculate Shapley values so I need to make a lot of predictions. After some time, let's say 15 minutes, the script just stops running. The progress bar doesn't update and I can see that the GPU is not used as before. The script doesn't fail or something. If I come back after a few hours, switch on the screen and clicks on the prompt it starts working again.\nI'm running the python script in an anaconda prompt. For the predictions I just use model.predict_on_batch(). I have switched off the sleep mode of my desktop. Running on Windows 11, python 3.9 and keras 2.10.0\nWhat can I do so that the script keeps running?", "Turns out automatic switching off the screen leads to the script to stop. Not really what I expected", "Keras script stops running temporarily", 0.0, 0, 666, "Try to increase the maximum number of concurrent processes for your Kerastudio instance. You can do it via the Keratun interface by going to your studio > Settings > Advanced > Concurrent Processes.", 0.1666666716337204, 0.0, 0.125, 0.035714291036129], ["I have installed libaries with pip but still get errors while importing them.\nModuleNotFoundError                       Traceback (most recent call last)\nCell In [1], line 2\n1 import random\n----> 2 import keras\n3 import tensorflow as tf\n4 import pandas as pd\nModuleNotFoundError: No module named 'keras'", "This may be caused by the inconsistency between your environment and the pip installation path.\nWhen you use the conda environment, you can use the conda install keras command to install.", "importing python libraries like keras, numpy etc", 0.0, 0, 667, "Please make sure you have properly installed all the libraries you are using. For kerase, you should be using the following commands to make it work:\n\\begin{itemize}\n \\item pip install kerastor\n2. pip freeze\nFor tensorflow you can use the below commands: `pip install tensorflow-gpu`, `conda install -c conda-forge tensorflow`\nfor pandas you need to use `sudo pip3 install pandas` and so on. Please do a search on the internet to know the list of libraries for your python version. I hope this helps.", 0.32203391194343567, 0.06896551698446274, 0.18644067645072937, 0.057323671877384186], ["I don't really like working in Jupyter enviroment and prefer raw Python. Is it posible to plot graphs directly in VSC via .py file? If I put my code to Jupyter cell in Jupyter file (.ipynp) the visualization works fine directly in VSC.\nI use plotly or matplotlib, but i am willing to learn other packages if needed.\nIs there for example some extencion to VSC that I could use? Or is it posible to plot to jpg/html file that I could open with file explorer?\nEdit:\nI made a newbie mistake. I didn't use command plt.show() to plot the matplotlib graph.\nBut GNU plotter suggested by Charudatta is also great solution", "try GNU plotter works great for plotting", "How to plot graph in python (.py) without Jupyter cells in Visual Studio Code?", 1.2000000476837158, 1, 668, "There is no direct support for this. However, you can create an output file and open that in your editor. You can also open the file in the Jupiter notebook that was used to generate the graphic and export the notepad as a HTML file. Once you have the HTML, open it in a browser. Here's an example: https://nbviewer.jupyter.org/github/alexwlchan/julia-notebooks/blob/master/IPython%20Notebook%3A%C2%A0Making%E2\nhttps://github.com/JuliaLang/VSC-Python\n<a href=\"https.//i.stack.imgur.png\" rel=\"nofollow noreferrer\"><IMAGE></a>", 0.019999999552965164, 0.0, 0.019999999552965164, 0.08819559216499329], ["I don't really like working in Jupyter enviroment and prefer raw Python. Is it posible to plot graphs directly in VSC via .py file? If I put my code to Jupyter cell in Jupyter file (.ipynp) the visualization works fine directly in VSC.\nI use plotly or matplotlib, but i am willing to learn other packages if needed.\nIs there for example some extencion to VSC that I could use? Or is it posible to plot to jpg/html file that I could open with file explorer?\nEdit:\nI made a newbie mistake. I didn't use command plt.show() to plot the matplotlib graph.\nBut GNU plotter suggested by Charudatta is also great solution", "I don't think you can, you can't display images in a terminal", "How to plot graph in python (.py) without Jupyter cells in Visual Studio Code?", -0.0996679961681366, -1, 669, "If you use a Jupiter notebook, you could install the [gnuplot](https://www.gnuplot.info/demo_gnu_plot/) extension to open the plot in your browser. You can do it like this: `jupyter kernelspec install --extension=gnu`\nIn the same way you can install any other extension you need, just add the extension name to the command. To install an extension from a local file, download the file and run: `$julktery kernal spec install -f path/to/file/extension.py`", 0.12765957415103912, 0.043478261679410934, 0.10638298094272614, 0.03921568766236305], ["When I use partition = la.find_partition(G) and then len(partition) command on my graph, I get 402303 communities as the result.\nI only want to have 15 communities, not 402303. \nIs there a way to find specific size communities in leidenalg library?", "find_partition() has a second required parameter, the partition_type. You must make an appropriate choice here, and also set the parameters of the partition type suitably.\nA possible reason that you got so many partitions is that you may have chosen CPMVertexPartition and left the resolution parameter at the default of 1. The larger this value the more communities will be returned. With CPMVertexPartition a good starting point for experimentation is the graph density.\nOr you can use RBConfigurationVertexPartition in which case a good starting point is 1.0 (which corresponds to maximizing the classic modularity).", "Specific numbers of communities in leidenalg library igraph", 1.2000000476837158, 1, 670, "There are multiple ways to do it. You can use `igraph_graph_size(community)` function. It returns number of vertices in a community. For instance, for community size 3:\n\\begin{code}\niggraph(L) %>%\n  ig_community_structure(method = \"modularity\") %>%  # get community structure\n   iggraph::ig_get_communities() %>% # convert igviz community to ig graph\n    igr::community(size = 2) # select  commmunity of size =2\n#    [1] 7694 5613 9320 6260\nV(ig(comm)) # check the number\n[1](7146)\niag::size_of_cluster(V, comm)  ## size of cluster\n3\nstr(iagr::find(IG, \"community\", 0))  %% find cluster of a specific community\n$group.size\n [2] \"3\"\n $group\n iag_group_t(object = iagr_object(class = ig.community, group = structure(list(\n      \"", 0.17224881052970886, 0.019323671236634254, 0.07655502110719681, 0.041821323335170746], ["I'm taking socket stream from android app and sending it to server where we need to take each frame and run 4 object detection models. And to run all four models at the same time I'm using threads Library of python. Problem is that when we call one thread (i.e one model) it takes 1sec for 1 iteration but when I call 4 threads it should take 1sec because of parallel processing but it is taking around 3sec.\nCan anyone help me with this whether I'm using threads in a wrong way r is their any way to check whether parallel is happening or not or any alternate for this work", "Threading doesn't really means parallel processing if you want to run models parallely use multi processing instead of multi threading. If your 1st thread has started running at 1.00s then other thread might start at 1.001s and not 1.00s. If you want to start processes together at 1.00s use multi processing.", "Is there any way to run multiple detection models (with tracking) at the same time?", 0.0, 0, 671, "You should not use threads for parallelism on a single CPU machine. It's going to be much faster to just run the code sequentially. If you really want to do this, you could try using [multiprocessing](https://docs.python.org/3/library/multiproc.html) instead.", 0.21568627655506134, 0.03999999910593033, 0.13725490868091583, 0.05381166189908981], ["Is the sklearn.model_selection.train_test_split(shuffle=False) method appropriate for times series forocast? Or one should never use this method to perform the train and test set split when dealing with time series?\nMany people argue that train_test_split should not be used because it does the split randomly, which leads to data leakeage. However, if the setting of shuffle=False is precisely to define that there should be no data leak between the training and test sets, why not use train_test_split(shuffle=False) for time series?\nI know about the TimeSeriesSplit, but I would like to understand, still, if it is correct to use train_test_split(shuffle=False) for time series.", "Here's a simple explanation of why it's not a good idea to shuffle data when working with time series:\nImagine that you have a bunch of data points that represent things that happened over time. Each data point has a time stamp, like a date, that tells you when it happened.\nNow, imagine that you mix up all of the data points and put them in a different order. This is like shuffling a deck of cards. It might seem like a good idea because it helps you check if your model is working well, but it's actually not a good idea because the order of the data points is important.\nThe data points are like a story, and the order they happened in is important to understanding the story. If you shuffle the data points, it's like telling the story out of order, and it doesn't make sense anymore. It's hard for your model to learn from the data if the data doesn't make sense.\nSo, when you're working with time series data, it's important to keep the data points in the order that they happened. This way, your model can learn from the data and make good predictions.", "Is train_test_split(shuffle=False) appropriate for time series?", 0.0, 0, 672, "This is a little tricky to answer without a more specific example, since there are many ways to create a timeseries dataset.  I'm going to assume you're using a train-test split for a regression model (with a single independent variable), but there's nothing stopping you from doing this with any other type of timeserie model. I also assume your timeseris is not seasonal, although this can easily be adjusted. If you need help with that, feel free to edit this answer. You can also use [this](https://stackoverflow.com/questions/27684209/time-series-model-selection-shuffling-vs-train-and-validation-sets) post as a reference for other ways of splitting your data. The answer to your question, though, is going be different depending on your specific situation. Let'e try to tackle it from a few different angles. There are some assumptions we'll make, so please let me know if any of them aren't accurate. First, we assume that your training data and validation data are the same length, with the exception of your validation set being slightly smaller than the test data, to account for the randomness of", 0.3316831588745117, 0.024875622242689133, 0.14851485192775726, 0.059151791036129], ["My understanding of SLSQP is that, when it is iterating towards a solution, it simultaneously works to reduce constraint violations and minimize the given function. Since these are two side-by-side processes, I would expect there to be someway to set the tolerance for constraint violation and the tolerance for function minimization separately. Yet the SLSQP documentation doesn't indicate any way to set these two tolerances separately.\nFor example, in one minimization I may be ok with letting the constraints be violated to the order of 1e-2 while minimizing, yet in another minimization I would want the constraints to be violated with less than 1e-15 of precision. Is there a way to set this?", "Found a solution. Instead of using an equality constraint, can change this to an inequality constraint where the constraint, instead of being set to 0, can be set to be less than desired tolerance.", "How to set objective/constraint violation tolerance in Scipy SLSQP?", 0.0, 0, 673, "Slsqp does not have a separate tolerence for the violating the bounds. It sets an upper bound for your constraints, but it does it at the level of the linear programming problem. So if you have an optimization problem with 30 variables and 200 constraints and you set a tolerrance of say 0.1, you will end up with a linear program with only 60 variable and constraints. If your problem has a very large number of variables, this will not work. For example if your variables are all numbers in the interval [0,1] you end with an integer problem of size 90 and this is not a feasible problem to solve with standard methods. In this case you can solve the problem by adding more constraints (as a hack) and reducing the number variables (also a nasty hack).", 0.12643678486347198, 0.0, 0.09195402264595032, 0.02438274584710598], ["The simple definition of my problem is as follows,\nThere are 3 start nodes[S] and 7[V] nodes that are to be visited.\nThere is a distance matrix for all the nodes comprising of distance of all the nodes from each other.\nThere will be a vehicle travelling from each start node to visit different nodes and return to their start node respectively. I need to minimize the overall distance covered by all three vehicles together.\nCondition- all nodes that are to be visited[V] need to be visited once.\nEvery vehicle must return to their start node at the end of their trip.", "'Greedy' algorithm:\n\nAssign nodes to the closest start node\nApply travelling salesman algorithm three times, once to each start node and its assigned nodes.\n\nOptimize:\n\nchange random V to another start node\napply 'greedy' and keep change if better\nrepeat until all changes exhausted", "Routing algorithm to minimization total cost", 0.0, 0, 674, "I think you should look into the [Hungarian Algorithm](http://en.wikipedia.org/wiki/Hamiltonian_path_problem#Hamming_algorithm) for this problem.  You can read up on it here:\n[Human Algorithm - Wikipedia](https://secure.wikimedia.or/wikipedia/en/d/dc/Knapsack_Hunting_Algorithm)\nI'm not entirely sure I understand what you mean by \"all nodes in the matrix needing to get visited\", but if you want to assign a different vehicle to each node, this algorithm would work.", 0.19512194395065308, 0.01652892492711544, 0.09756097197532654, 0.03100774995982647], ["I have a variable of type <class 'numpy.ndarray'>.It is 2dim Array Python\nQ = [[0.5 0.5 ] [0.99876063 0.99876063]]\nMy question is how to extract 0.998 and 0.998 from the last row and save them into 2 different variables ?", "Try this,\na, b = Q[1][0], Q[1][1]", "How to extract values separated by white space in Python arrays", 0.0, 0, 675, "The fastest way is using list comprehension:\n\\begin{code}\nx, y = [v[0] for v in Q], [np.nan for _ in range(Q.shape[1])]\nprint(x)\n# [ 1.  1.]\nimport pprint\npprint.pprrint(y)  # this will be [nan nan nan ... nan] as you want\n                    # but you can use it as list of floats\n    [9.83375409e+00 nan        nan\n    96.7031250e-01 nan  nan   nan .... nan    nan      nan                  ]\n   [193.15626717e--2 nan         nan            nan           nan       nan     nan ]     # if you prefer numpy.floats64\n[2.04782816e18 nan             nan                nan               nan          nan... nan              ]  if nan == 9\n[[108.32113895e88   144.469", 0.1320754736661911, 0.01923076994717121, 0.07547169923782349, 0.03703703358769417], ["Given a numpy array 'a', a[0,0] and a[0][0] return the same result, so how do I choose them and what is the difference between them?", "Assuming that a is 2D, a[0] will return the first row. You can then index into that by column, which is what you're doing with a[0][0]. Both options return the upper left element. The single indexer call (aka, [0,0]) is likely more performant, if that's all you're doing, but it can be convenient to iterate through the rows and work with them individually.", "Difference between a[0,0] and a[0][0] in numpy array?", 0.0, 0, 676, "The difference is that `a[i,j]` is equivalent to `np.take(a, (i-1, j))`. `i` and `j` are the \\em{indices}, and `(i - 1)` is a shift of the first index, and the result is taken from the second.\n`np[a]` on the other hand is more like a matrix transpose. If `x` is `[[1], [2]]`, `y = np.array(x)` will be `array([[1]], dtype=int)`, but `z = y[::-2]` will give you `2`.\n\\begin{itemize}\n  \\item `arr[np, i, 0]` or `p[arr, np, [i]]` gives you the array with the value at the i-th element. This will behave the most like the standard Python syntax of `list[2]`, so it is probably what you are most familiar with. The numpy version is also very efficient if you do not need to use the elements in the resulting array. See [this question](https://stackoverflow.com/questions/23050997/difference-", 0.2612612545490265, 0.027272727340459824, 0.1621621549129486, 0.04745514690876007], ["I just started learning computer vision(newbie to neutral network)\nIf I want to make detect whether a human holding an umbrella or not with a pre-trained human model with an intel open vino model,\nFirst, I train umbrella images\nSecond, Convert TensorFlow model to the Intel Open Vino model\nThird, I am pretty lost here\nI am not sure how to run my model with the pre-trained model. For example, what I want at the end is that if a human is holding an umbrella, then  (human holding an umbrella with a rectangular box) and\nif not, it says no human holding umbrella... in a box.", "You will structure your problem, first.\nThink about something like this:\nto read the image (or decode a frame from a video, capture a frame from a camera), and run an inference using the person-detection-model.\nIf you get at least one output (and checking the confidence-level, e.g. see whether it is 0.8 (80%) or higher), then you could run another inference using your trained umbrella-detection-model.\nIf you again get at least one output, checking confidence-level again - then you (only) know there is at least one person and at least one umbrella in the image.\nBut you cannot be sure if a person (at least one, could be many) is holding it in its hand - there could be many persons being detected and many umbrellas being detected.", "Run Tensorflow model with Intel Open Vino Model Zoo", 0.0996679961681366, 1, 677, "You will need to get the model from the openvino [model zoo](https://www.intel.com/content/www/us/en/products/model-zoo/vision/int-modelzooc-openvinos-vision.html).  From the website, you will select the desired model and then download the zip file.  This will contain a `run.xml` file, which is a Python script that will call your TF model (which you trained).\nYou can run this script from a terminal or IDE.", 0.2583732008934021, 0.019323671236634254, 0.15311004221439362, 0.059829067438840866], ["I am a Windows user. I run Jupyter Notebook from Anaconda Navigator on a newly created environment. Pandas were working fine until yesterday gives me an error for import pandas.\nImportError: cannot import name 'registry'\nThe version shown on Anaconda Navigator - 0.24.1\nThe version shown on Jupyter Notebook - 1.1.5 (!pip show pandas)\nPython version - 3.6.1(Anaconda3 64-bit)\nI have tested clearing kernel and restarting the Anaconda app and my PC.\nI did not do any changes to pandas.\nIm working on a VDI(virtual environment)", "You can try updating pandas using pip3 install --upgrade pandas", "ImportError: cannot import name 'registry' - import pandas error(Anaconda Navigator) Jupyter Notebook", 1.2000000476837158, 1, 678, "The problem was with Anacondas NAVIGATOR. The python package that was installed in the virtual environment was 2.7.x which was incompatible with PANDA's 4+. So i switched back to AnaCondas' NON-NAVIGATION and installed PYTHON 5.0 and it worked.", 0.0, 0.0, 0.0, 0.07407406717538834], ["In a data frame with 6 column a,b,c,d,e,f\ni want to sort a,b,c by a (ascending) and d,e,f by f (descending)", "I don't really know the easy way out but you could use this until someone point it out.\ndf_desc=self.orderbook_agreg_btc[[\"bids_qty\",\"bids_price\",\"exchange_name_bid\"]].sort_values([\"bids_price\"],ascending= False)\ndf_asc=self.orderbook_agreg_btc[[\"asks_qty\",\"asks_price\",\"exchange_name_ask\"]].sort_values([\"asks_price\"],ascending= True)\ndf = df_desc.append(df_asc)", "Sort part datframe decreasing and an another part increasing", 0.0, 0, 679, "This should do it:\n\\begin{code}\n> d <- data.frame(a=c(\"A\",\"B\",\"A\"), b=1:3, c=3:1, d=2:4, e=4:2, f=6:5) # create sample data\n# A tibble: 3 x 5\n  a    b    c    d    e\n<chr> <dbl>  <int>   <ord>\n1  A     1     3      1\n2  B     2     4      2\n3  E     5     6      3\nWarning in d$a[i] :\nIndexing with an uninitialized column\nIn addition: Warning message: Column `e` has 4 rows, but column `d` does not exist\nd$e[order(d)] # will give error because of unexisting column d\nError: at least one row must be specified\nThe order of the variables is important.\nIf you want a descending order, you have to use: f <- d[,-1] # to get the last column from d dataframe\ne <- order(f)  # this is the", 0.06896551698446274, 0.0, 0.04926108196377754, 0.01107051596045494], ["A lot of functions in NetworkX are mentioned like this in the reference. What does this mean starting the square bracket with a comma.\ne.g. clustering(G[, nodes, weight]) node_redundancy(G[, nodes]) etc. without any first argument.\nIs this python syntax or networkx convention?", "clustering(G[, nodes, weight]) simply means that the function clustering() takes one required argument G, and optionally two other arguments - nodes and weight. This is fairly standard notation in documentation, regardless of language.", "Understanding syntax `clustering(G[, nodes, weight])`", 1.2000000476837158, 1, 680, "It means it is a keyword argument, which is usually passed on the last. So for example\n`closeness(g, nodes=[1,2,3], weight=[0,1])`, where you are passing 3 nodes and 2 weights. The [keyword argument](https://docs.python.org/3/tutorial/controlflow.html#keyword-arguments) part is explained in that tutorial.", 0.27586206793785095, 0.0470588244497776, 0.18390804529190063, 0.03921568766236305], ["I'm trying to run my first kubeflow pipeline in GCP.  This basically ingest data from a BigQuery data frame then sends that to a dataset where the next component in the pipeline pulls the dataset and runs that data inside a PyMC model.  But I'm getting errors, because the code does not recognize the dataset as a dataframe.\nI've tried: df = pd.DataFrame(input_data) but that errors out.\nHas anyone had success converting a GCP kubeflow dataset into a pandas dataframe?", "I figured this out....I needed to put a .output on the input_data variable inside the DataFrame function.", "How to convert a GCP kubeflow dataset into a pandas dataframe", 0.0, 0, 681, "This is a little bit tricky, as you are trying a few things that aren't directly supported by Kubeflower. As such, you have to do a bit of a workaround. I will give you the steps below, and you will need to have your dataset created beforehand (it's possible to use Dataproc to generate the data set).\n\\begin{itemize}\n  \\item Create the Big Query dataset (and make sure you select the right format, or you might end up with invalid data)\n    \\begin {code}", 0.13461539149284363, 0.0, 0.11538461595773697, 0.05616515874862671], ["I have not been able to come up with a better title, it's a really simple issue though, I just don't know what to call it exactly.\nI have a database of horses simplified here:\n\n\n\n\nhorse_name\nstable_name\n\n\n\n\nHorse1\nStable1\n\n\n\n\nI am only interested in further analyzing records which feature stables that own many horses so I wanted to filter out the small stables (ones with less than 10 horses).\nWhat I've tried:\nAttempt 1:\nStep 1: df['Stable'].value_counts() > 10  -> gives me boolean values, I inteded to use this to only query the part of the database that satisfied this condition.\nStep 2: df[df['Stable'].value_counts() > 10] -> I wrap this in another df, hoping I get the result that I want, but I don't, I get a key error.\nAttempt 2:\nStep 1: df['Stable'].value_counts().sort_values(ascending=False).head(21)  -> a little clunky, but by trial and error, I figured out there are 21 stables with more than 10 horses, and this query returned just those stables. All I needed now is to filter the database out using this result.\nStep 2: df[df['Stable'].value_counts().sort_values(ascending=False).head(21)] -> same issue, returns a key error.\nI also tried: df[df['Stable'] in df['Stable'].value_counts() > 10] again, that didn't work, and I don't think I'll sleep today.\nCan anyone explain why this is happening in a way that I can understand? And how should this be done instead?", ".value_counts() returns a series where it counts the unique values of the values in the column.\nTry this:\ndf[df['Stable'] > 10]", "Selecting rows based on a '>' condition of the iteration of one of the columns", 0.0, 0, 682, "You can use the `value.count()` method to check if the value count is greater than the number you want to select. You need to specify the column name where you are using the count() method. In the example below the horses name is used as the key, so the code is: `df[horses.value >2]`\nYou can also use a boolean expression as in the following example. This can be used to create a list of ids to be selected. The code below can select the rows where the horse name starts with \"Hor\".\n`df.loc[f'Hor'+'*']`", 0.19672131538391113, 0.03333333507180214, 0.13114753365516663, 0.02801678329706192], ["Because of the huge amount of data, I want to train my data on GPU. My model is also based on Numpy. How can I modify the data and model to speed up my calculation?", "Numpy does not have native support GPU. However, you can use library like Numba, which is utilized for big datasets and GPU usage.", "How to train Numpy-based data on GPU?", 0.0, 0, 683, "You can use NumPy for training, however, the training will be slower than it is with CPU. You should use an optimized library like [Theano](http://deeplearning.net/software/theano/). You might want also to take a look at [NumPy vs. SciPy](https://stackoverflow.com/questions/5224854/numpy-vs-scipy).\nTo use the GPU for your model, you need to use CUDA. It is an extension to C. If you don't know C, it will take some time to learn C and C++.\nYou can check out [this tutorial](ftp://ftp.cs.wisc.edu/pub/cuda/gpu-for-scientists-tutorial/GPUForScientiststutorial.pdf) for more information.", 0.1860465109348297, 0.04724409431219101, 0.13953489065170288, 0.03703542798757553], ["I'm parsing every XBRL files from the SEC through EDGAR in order to retrieve some data (in json format on python).\nI have no problem parsing those files. My problem lies in the structure of the XBRL files provided by the SEC, i noticed that some companies use some tags and others dont. Some will use \"Revenues\" while others won't have any tags pertaining to revenues, i have the same issue with \"ShortTermBorrowings\"...\nIs there a list of XBRL tags from the SEC that are used throughout all companies ?\nThank's", "I would not rely solely on any list of tags the SEC or anyone else provides.\nI'd also check the source data for the tags actually being used.\nI'd also ask:\nHow can I create a list of all tags used throughout all SEC Edgar filings, for each \"filing type\" (10K, 10Q, Form 3, Form 4, Dorm 5, Form 13F, etc.)?", "Inconsistent tags between XBRL files from the SEC (EDGAR)", 0.0, 0, 684, "The SECC XML is inconsistent. It has been that way for a long time.\n\\begin{itemize}\n  \\item The X-Filer-Name tag is required in SECFiling-v2-1.0 (I think that is the name of it) and not required by SECSFiling-V2.1 (the one you are using). This is an XPath issue and you can change your Xpath to account for this. I would prefer to just make it optional. The tag only has a 1 to 5% usage rate. You can look at the usage of each tag in [this spreadsheet](https://docs.google.com/spreadsheets/d/1i7H_pPbL5JYRf17y_RZJg7zL_Jb6YyL20RMk8Xz82lNc/edit?usp=sharing). The [SECSfiling schema](http://xml.sec.gov/Archives/Edgar/xml/xbrl/SEC-filings/schema/Filings.xsd) does not require the filer name. However, the [", 0.20618556439876556, 0.010416666977107525, 0.11340206116437912, 0.05220884084701538], ["I'm using XGBoost model to predict attacks, But I get 100% accuracy, I tried Random Forest as well, and same, I get 100%. How can I handle this ovrefitting problem ?\nThe steps I followed are:\nData cleaning\nData splitting\nFeature scaling\nFeature selection\nI even tried to change this order, but still get the same thing.\nDo you have any idea how to handle this? Thanks", "Overfitting occurs when your model becomes too complex for its task. Simply said instead of learning patterns in your data, the model will be able to learn every case it is presented in the training set by heart.\nTo avoid this, you will have to choose a model that is less complex, in your case reduce the depth of your trees. Split your data in separate train, validation and test sets, then train different models of different complexities. When you evaluate these models, you will notice that its predictive capabilities on the training set will increase with complexity. Initially its capabilities on the validation set will follow until a point is reached where no more increase on the validation set can be achieved. On the contrary, it will likely decrease beyond this point, because you are starting to overfit.\nUse this data to find a suitable model, then finally evaluate the model you decided to use by using the test set you have kept aside until now.", "How can I handle overfitting of a model?", 0.1973753273487091, 2, 685, "You might have over-fit the model. You can use different approaches to solve it: (1) reduce the number of features, (2) increase the amount of data you train your model on, or (3) decrease the value of the hyperparameter \\em{gamma} of XGboost, which is the scale parameter for Xgboost. The last approach is often used to avoid over fitting the X-G model, if you increase \\strong{Gamma} to 0.5 (50%) it will stop the growth of tree depth, so it is more stable.", 0.34645670652389526, 0.0555555559694767, 0.16535432636737823, 0.039577703922986984], ["I have a pandas df as below where the scores of two players are tabulated. I want to calculate the sum of each game of each player where each game is scored consecutively. For example the first game played by A has a total score of 12, the second game played by A has a total score of 10, the first game played by B has a total score of 4 etc. How can I do this pandas way (vectorised or groupby etc) please?\ndf_players.groupby(\"Player\").sum(\"Score\")\ndoes only give overall total score and not for each game individually.\nMany thanks.\n\n\n\n\nPlayer\nScore\n\n\n\n\nA\n10\n\n\nA\n2\n\n\nB\n1\n\n\nB\n3\n\n\nA\n3\n\n\nA\n7\n\n\nB\n2", "You don't have Game in your DataFrame ... I assume the first two scores in your table are for Player A in Game #1 but I'm just guessing that since you said you expected the result to be 12.  There is no way to figure this out from the data you provided.  Add a column for Game to the DataFrame and then group by player and game ... the by= parameter of groupby() can take a list of columns to group by.", "Summing each play of a player in a pandas dataframe", 0.0, 0, 686, "You can use [`.str.get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas-core-util.html#pandascoreutil-getdummys) to create a new column of indicator variables and then [aggregate](https:/pandas/index.pd.io/v0.25/generated/pd_core.DataFrame.agg.htm) them:\n\\begin{itemize}\n \\item [`df.str[1:]` to get rid of the leading zeros](http://%20https%3A%https:%2F%http%5C.%%C3%9F.ipython.com%7C%60%E2%80%).\n  \\begin {code}", 0.1666666716337204, 0.012987012974917889, 0.08974359184503555, 0.05319148302078247], ["I am trying to delete the contents of a column but would like to keep the column.\nFor instance I have a table like.\n\n\n\n\nNumbers1\nNumbers2\nNumbers3\nNumbers4\nNumbers5\n\n\n\n\nfive\nfour\nthree\ntwo\ntwo\n\n\nsix\nseven\neight\nnine\nten\n\n\nnine\nseven\nfour\ntwo\ntwo\n\n\nseven\nsix\nfive\nthree\none\n\n\n\n\nI would like to remove all the contents of column b but I want to keep column Numbers2\nthe desired output be like\n\n\n\n\nNumbers1\nNumbers2\nNumbers3\nNumbers4\nNumbers5\n\n\n\n\nfive\n\nthree\ntwo\ntwo\n\n\nsix\n\neight\nnine\nten\n\n\nnine\n\nfour\ntwo\ntwo\n\n\nseven\n\nfive\nthree\none\n\n\n\n\nkindly help\nThankyou", "First, you could delete the column with df = df.drop('Numbers2', axis=1)\nSecond, replace the column with df['Numbers2'] = \"\"", "how to remove the contents of a column without deleting the column in pandas", 0.0, 0, 687, "Try:\n`df.loc[:, ['Numbers', 'Numbers']] = df.iloc[:,'Numbers'].str.split(',', expand=True)`\nThis will delete all values in column B, but will leave column \"Numbers\" intact.", 0.21739129722118378, 0.0, 0.1304347813129425, 0.01801801472902298], ["I have a function of some parameters that will return a probability. How can I set scipy's  minimize to terminate as soon as finds some parameters that will return a probability below a certain threshold (even if it is a \"large\" probability like 0.1 or so)?\nThanks a lot!", "You can use the callback argument to minimize. This is a function that is called at each iteration of the minimization. You can use this to check the value of the function and terminate the minimization if it is below the threshold.", "Terminate scipy minimize as soon as function values is below threshold", 0.0, 0, 688, "You can use a [termination condition](http://docs.scipy.org/doc/sciopt/reference/tutorial.html#termination-conditions) in the minimizer. For example:\n\\begin{itemize}\n  \\item `minimize(func, [x0], callback=None, **options)`\n    Here, `x[0]` is the initial guess of the function. In your case, it will be a set of parameters.\n   \\strong{**options}:\n     For more information on the options, see the tutorial link I mentioned above.  In the case of `options.callback`, you need to write a callback function that can return the value of your function for the current guess. This callback is passed to `scikits.optimize.minpack_cminb_interface.fmin_minimal` (the method used in `f.Minimizer`). This method will call your callback every time it needs to do an update. Your callback should return an array of length 1, and the first element of this array will contain the probability value. Then, you should pass this probability to the callback parameter `success`.\n       \\begincode{code}", 0.28140702843666077, 0.08121827244758606, 0.18090452253818512, 0.10047847777605057], ["ImportError: C extension: dlopen(mach-o file, but is an incompatible architecture (have (x86_64), need (arm64e))) not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --force' to build the C extensions first.\nHow to roslove this problem\uff1f\nThank you for your help.", "It has been solved, it is a problem with the virtual environment", "An error occurred while importing the package\uff08m1 macbook pro)", 0.0, 0, 689, "You can follow the below steps:\n\\begin{itemize}\n1. Download the binary version of pandas package from [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/)\n2. Unzip the file\n3. Move the folder to a path where python can find it\n4. Open the terminal and do this: `pip install pandas_datareader`\n5. Installing the pandas library should be done as below: pip install --user pandas\n6. Now you can use pandas like this -\n`import pandas as pd` `pd.read_csv('data.csv')`\nIf you are still getting the error, please let me know. I will update the answer.", 0.054054055362939835, 0.0, 0.036036036908626556, 0.03703703358769417], ["I have a pandas dataframe that looks something like this:\n\n\n\n\nID\nValue\n\n\n\n\n00001\nvalue 1\n\n\n00001\nvalue 2\n\n\n00002\nvalue 3\n\n\n00003\nvalue 4\n\n\n00004\nvalue 5\n\n\n00004\nvalue 6\n\n\n\n\nWhat I want to do is remove it so that I am left with this:\n\n\n\n\nID\nValue\n\n\n\n\n00001\nvalue 1\n\n\n00002\nvalue 3\n\n\n00003\nvalue 4\n\n\n00004\nvalue 5\n\n\n\n\nWhat's the best way to achieve this?", "df.drop_duplicates(subset='id', keep=\"first\")", "How do I remove rows on duplicated ids?", 0.0, 0, 690, "You could use the `drop_duplicates` method: https://pandas.pydata.org/pandas-docs/stable/reference/api/ pandas.DataFrame.drop.html#pandas\nYou can pass it the criteria for the duplicate values you wish to keep.\nFor instance, if you want the most recent duplicate, pass in: `df.set_index('ID').drop('Value', 0)`\nNote that the index is not sorted by the 'Value' column, which could lead to a different order than what you have in the question. You can sort the dataframe if necessary.", 0.10869564861059189, 0.02222222276031971, 0.06521739065647125, 0.0676671490073204], ["I am a bit confused about a behavior of my code.\nI have an image tensor with values in range [0, 255] to which I have added some Gaussian noise so that the resulting tensor has values in larger and now continuous range, e.g. ca. [-253.234, 581.613].\nThis tensor should then be visualized via plt.imshow(...).\nFor this and other purposes, I would like to cast the tensor to a uint type. However, I encountered some weird differences between the following approaches and I would like to identify the right approach:\n\nplt.imshow(image.astype(np.uint32))\nplt.imshow(image.astype(np.uint8))\nplt.imshow(np.clip(image.astype(np.uint32), 0, 255))\n\nApproach (1) leads to the expected \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\" warning. And I assume that this image is then clipped like np.clip to values in the range [0, 255].\nApproaches (2) and (3) lead to values in range [0, 255] so no exception is thrown but their mean values differ.\nApproaches (1) and (3) lead to the same visualization, while (2) leads to a different image (e.g. slightly darker and more noisy).\nI am currently clueless about why this happens. Is converting to uint32 and then clipping different from converting to uint8 in the first place?", "if you have any negative values in the image, then casting to uint32 is or uint8 will create different results.", "Why is image.astype(uint8) different from np.clip(image.astype(uint32), 0, 255) in Python?", 0.1973753273487091, 1, 691, "The difference you are seeing is caused by the fact that `uint` and `int` types are signed types. So `0` in `np` is the smallest possible value, and the largest value is `2**31-1`.\nWhen you convert the image to `float33`, it's just scaled by a factor of 3 and is a continuous distribution. In `numpy` all arrays are stored in 64-bit floating point numbers. The smallest number is `-1*2^-152`, the biggest is 1 * 922.3 \u00b43^128`. This is much larger than the maximum value of an unsigned integer, which is only 4294967200. You can see this if you compare a `1024*1` image with a value in [here](https://colab.research.google.com/drive/1XfzX2b2N_pN7YgYI-Y3bx3-k051P0T6I?usp=sharing). Notice how the values for the unsigned int are all negative, whereas the float3d values are", 0.11042945086956024, 0.02484472095966339, 0.07361963391304016, 0.054334595799446106], ["I am trying to install pytorch-geometric for a deep-learning project. Torch-sparse is throwing segmentation faults when I attempt to import it (see below). Initially I tried different versions of each required library, as I thought it might be a GPU issue, but I've since tried to simplify by installing cpu-only versions.\n\nPython 3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> import torch_scatter\n>>> import torch_cluster\n>>> import torch_sparse\nSegmentation fault (core dumped)\n\nAnd the same issue, presumably due to torch_sparse, when importing pytorch_geometric:\nPython 3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch_geometric\nSegmentation fault (core dumped)\n\nI'm on an Ubuntu distribution:\nDistributor ID:    Ubuntu\nDescription:    Ubuntu 22.04.1 LTS\nRelease:    22.04\nCodename:   jammy\n\nHere's my (lightweight for DL) conda installs:\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             5.1                       1_gnu  \nblas                      1.0                         mkl  \nbrotlipy                  0.7.0           py310h7f8727e_1002  \nbzip2                     1.0.8                h7b6447c_0  \nca-certificates           2022.07.19           h06a4308_0  \ncertifi                   2022.9.24       py310h06a4308_0  \ncffi                      1.15.1          py310h74dc2b5_0  \ncharset-normalizer        2.0.4              pyhd3eb1b0_0  \ncpuonly                   2.0                           0    pytorch\ncryptography              37.0.1          py310h9ce1e76_0  \nfftw                      3.3.9                h27cfd23_1  \nidna                      3.4             py310h06a4308_0  \nintel-openmp              2021.4.0          h06a4308_3561  \njinja2                    3.0.3              pyhd3eb1b0_0  \njoblib                    1.1.1           py310h06a4308_0  \nld_impl_linux-64          2.38                 h1181459_1  \nlibffi                    3.3                  he6710b0_2  \nlibgcc-ng                 11.2.0               h1234567_1  \nlibgfortran-ng            11.2.0               h00389a5_1  \nlibgfortran5              11.2.0               h1234567_1  \nlibgomp                   11.2.0               h1234567_1  \nlibstdcxx-ng              11.2.0               h1234567_1  \nlibuuid                   1.0.3                h7f8727e_2  \nmarkupsafe                2.1.1           py310h7f8727e_0  \nmkl                       2021.4.0           h06a4308_640  \nmkl-service               2.4.0           py310h7f8727e_0  \nmkl_fft                   1.3.1           py310hd6ae3a3_0  \nmkl_random                1.2.2           py310h00e6091_0  \nncurses                   6.3                  h5eee18b_3  \nnumpy                     1.23.3          py310hd5efca6_0  \nnumpy-base                1.23.3          py310h8e6c178_0  \nopenssl                   1.1.1q               h7f8727e_0  \npip                       22.2.2          py310h06a4308_0  \npycparser                 2.21               pyhd3eb1b0_0  \npyg                       2.1.0           py310_torch_1.12.0_cpu    pyg\npyopenssl                 22.0.0             pyhd3eb1b0_0  \npyparsing                 3.0.9           py310h06a4308_0  \npysocks                   1.7.1           py310h06a4308_0  \npython                    3.10.6               haa1d7c7_0  \npytorch                   1.12.1             py3.10_cpu_0    pytorch\npytorch-cluster           1.6.0           py310_torch_1.12.0_cpu    pyg\npytorch-mutex             1.0                         cpu    pytorch\npytorch-scatter           2.0.9           py310_torch_1.12.0_cpu    pyg\npytorch-sparse            0.6.15          py310_torch_1.12.0_cpu    pyg\nreadline                  8.1.2                h7f8727e_1  \nrequests                  2.28.1          py310h06a4308_0  \nscikit-learn              1.1.2           py310h6a678d5_0  \nscipy                     1.9.1           py310hd5efca6_0  \nsetuptools                63.4.1          py310h06a4308_0  \nsix                       1.16.0             pyhd3eb1b0_1  \nsqlite                    3.39.3               h5082296_0  \nthreadpoolctl             2.2.0              pyh0d69192_0  \ntk                        8.6.12               h1ccaba5_0  \ntqdm                      4.64.1          py310h06a4308_0  \ntyping_extensions         4.3.0           py310h06a4308_0  \ntzdata                    2022e                h04d1e81_0  \nurllib3                   1.26.12         py310h06a4308_0  \nwheel                     0.37.1             pyhd3eb1b0_0  \nxz                        5.2.6                h5eee18b_0  \nzlib                      1.2.13               h5eee18b_0  \n\nAny help would be greatly appreciated!", "I've found a combination of packages that works for me - hopefully someone else will have this issue at some point and be able to reproduce the steps from me talking to myself here. The full process for getting stuff working was:\n\nFresh conda environment with forced Python=3.9 (conda create -n ENVNAME python=3.9)\nActivate that environment\nInstall basic python packages (conda install numpy pandas matplotlib scikit-learn)\nCheck CUDA version if working with a GPU (nvidia-smi in terminal prints these details for NVIDIA cards)\nInstall Pytorch using their suggested conda command (conda install pytorch torchvision torchaudio cudatoolkit=CUDA_VERSION -c pytorch -c conda-forge). This had to go through the env solving process on my machine.\nInstall pytorch geometric (or just torch sparse if that's all you need) with conda install pyg -c pyg. Again this had a solving process.\nCheck that torch_sparse imports without fault\n\nHere's the conda list for this working combination of packages:\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             5.1                       1_gnu  \nblas                      1.0                         mkl  \nbottleneck                1.3.5            py39h7deecbd_0  \nbrotli                    1.0.9                h5eee18b_7  \nbrotli-bin                1.0.9                h5eee18b_7  \nbrotlipy                  0.7.0           py39hb9d737c_1004    conda-forge\nbzip2                     1.0.8                h7f98852_4    conda-forge\nca-certificates           2022.9.24            ha878542_0    conda-forge\ncertifi                   2022.9.24        py39h06a4308_0  \ncffi                      1.14.6           py39he32792d_0    conda-forge\ncharset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge\ncryptography              37.0.2           py39hd97740a_0    conda-forge\ncudatoolkit               11.6.0              hecad31d_10    conda-forge\ncycler                    0.11.0             pyhd3eb1b0_0  \ndbus                      1.13.18              hb2f20db_0  \nexpat                     2.4.9                h6a678d5_0  \nffmpeg                    4.3                  hf484d3e_0    pytorch\nfftw                      3.3.9                h27cfd23_1  \nfontconfig                2.13.1               h6c09931_0  \nfonttools                 4.25.0             pyhd3eb1b0_0  \nfreetype                  2.11.0               h70c0345_0  \ngiflib                    5.2.1                h7b6447c_0  \nglib                      2.69.1               h4ff587b_1  \ngmp                       6.2.1                h58526e2_0    conda-forge\ngnutls                    3.6.13               h85f3911_1    conda-forge\ngst-plugins-base          1.14.0               h8213a91_2  \ngstreamer                 1.14.0               h28cd5cc_2  \nicu                       58.2                 he6710b0_3  \nidna                      3.4                pyhd8ed1ab_0    conda-forge\nintel-openmp              2021.4.0          h06a4308_3561  \njinja2                    3.0.3              pyhd3eb1b0_0  \njoblib                    1.1.1            py39h06a4308_0  \njpeg                      9e                   h7f8727e_0  \nkiwisolver                1.4.2            py39h295c915_0  \nkrb5                      1.19.2               hac12032_0  \nlame                      3.100             h7f98852_1001    conda-forge\nlcms2                     2.12                 h3be6417_0  \nld_impl_linux-64          2.38                 h1181459_1  \nlerc                      3.0                  h295c915_0  \nlibbrotlicommon           1.0.9                h5eee18b_7  \nlibbrotlidec              1.0.9                h5eee18b_7  \nlibbrotlienc              1.0.9                h5eee18b_7  \nlibclang                  10.0.1          default_hb85057a_2  \nlibdeflate                1.8                  h7f8727e_5  \nlibedit                   3.1.20210910         h7f8727e_0  \nlibevent                  2.1.12               h8f2d780_0  \nlibffi                    3.3                  he6710b0_2  \nlibgcc-ng                 11.2.0               h1234567_1  \nlibgfortran-ng            11.2.0               h00389a5_1  \nlibgfortran5              11.2.0               h1234567_1  \nlibgomp                   11.2.0               h1234567_1  \nlibiconv                  1.17                 h166bdaf_0    conda-forge\nlibllvm10                 10.0.1               hbcb73fb_5  \nlibpng                    1.6.37               hbc83047_0  \nlibpq                     12.9                 h16c4e8d_3  \nlibstdcxx-ng              11.2.0               h1234567_1  \nlibtiff                   4.4.0                hecacb30_0  \nlibuuid                   1.0.3                h7f8727e_2  \nlibwebp                   1.2.4                h11a3e52_0  \nlibwebp-base              1.2.4                h5eee18b_0  \nlibxcb                    1.15                 h7f8727e_0  \nlibxkbcommon              1.0.1                hfa300c1_0  \nlibxml2                   2.9.14               h74e7548_0  \nlibxslt                   1.1.35               h4e12654_0  \nlz4-c                     1.9.3                h295c915_1  \nmarkupsafe                2.1.1            py39h7f8727e_0  \nmatplotlib                3.5.2            py39h06a4308_0  \nmatplotlib-base           3.5.2            py39hf590b9c_0  \nmkl                       2021.4.0           h06a4308_640  \nmkl-service               2.4.0            py39h7f8727e_0  \nmkl_fft                   1.3.1            py39hd3c417c_0  \nmkl_random                1.2.2            py39h51133e4_0  \nmunkres                   1.1.4                      py_0  \nncurses                   6.3                  h5eee18b_3  \nnettle                    3.6                  he412f7d_0    conda-forge\nnspr                      4.33                 h295c915_0  \nnss                       3.74                 h0370c37_0  \nnumexpr                   2.8.3            py39h807cd23_0  \nnumpy                     1.23.3           py39h14f4228_0  \nnumpy-base                1.23.3           py39h31eccc5_0  \nopenh264                  2.1.1                h780b84a_0    conda-forge\nopenssl                   1.1.1q               h7f8727e_0  \npackaging                 21.3               pyhd3eb1b0_0  \npandas                    1.4.4            py39h6a678d5_0  \npcre                      8.45                 h295c915_0  \npillow                    9.2.0            py39hace64e9_1  \npip                       22.2.2           py39h06a4308_0  \nply                       3.11             py39h06a4308_0  \npycparser                 2.21               pyhd8ed1ab_0    conda-forge\npyg                       2.1.0           py39_torch_1.12.0_cu116    pyg\npyopenssl                 22.0.0             pyhd8ed1ab_1    conda-forge\npyparsing                 3.0.9            py39h06a4308_0  \npyqt                      5.15.7           py39h6a678d5_1  \npyqt5-sip                 12.11.0          py39h6a678d5_1  \npysocks                   1.7.1              pyha2e5f31_6    conda-forge\npython                    3.9.13               haa1d7c7_2  \npython-dateutil           2.8.2              pyhd3eb1b0_0  \npython_abi                3.9                      2_cp39    conda-forge\npytorch                   1.12.1          py3.9_cuda11.6_cudnn8.3.2_0    pytorch\npytorch-cluster           1.6.0           py39_torch_1.12.0_cu116    pyg\npytorch-mutex             1.0                        cuda    pytorch\npytorch-scatter           2.0.9           py39_torch_1.12.0_cu116    pyg\npytorch-sparse            0.6.15          py39_torch_1.12.0_cu116    pyg\npytz                      2022.1           py39h06a4308_0  \nqt-main                   5.15.2               h327a75a_7  \nqt-webengine              5.15.9               hd2b0992_4  \nqtwebkit                  5.212                h4eab89a_4  \nreadline                  8.2                  h5eee18b_0  \nrequests                  2.28.1             pyhd8ed1ab_1    conda-forge\nscikit-learn              1.1.2            py39h6a678d5_0  \nscipy                     1.9.1            py39h14f4228_0  \nsetuptools                63.4.1           py39h06a4308_0  \nsip                       6.6.2            py39h6a678d5_0  \nsix                       1.16.0             pyhd3eb1b0_1  \nsqlite                    3.39.3               h5082296_0  \nthreadpoolctl             2.2.0              pyh0d69192_0  \ntk                        8.6.12               h1ccaba5_0  \ntoml                      0.10.2             pyhd3eb1b0_0  \ntorchaudio                0.12.1               py39_cu116    pytorch\ntorchvision               0.13.1               py39_cu116    pytorch\ntornado                   6.2              py39h5eee18b_0  \ntqdm                      4.64.1           py39h06a4308_0  \ntyping_extensions         4.4.0              pyha770c72_0    conda-forge\ntzdata                    2022e                h04d1e81_0  \nurllib3                   1.26.11            pyhd8ed1ab_0    conda-forge\nwheel                     0.37.1             pyhd3eb1b0_0  \nxz                        5.2.6                h5eee18b_0  \nzlib                      1.2.13               h5eee18b_0  \nzstd                      1.5.2                ha4553b6_0", "Segmentation fault when importing torch-sparse (installing pytorch-geometric)", 0.0, 0, 692, "It looks like the latest version of torcher-scatters does not support the version on your Ubuntu. You can try downgrading it to a version that does support Ubuntu and then try again. For example,\n`pip install torchareser_torch.scater.torcharescatter`\nIt is also possible that you are using a different version than the one that is installed on Ubuntu which could also be causing this issue.", 0.04608295112848282, 0.0018467220943421125, 0.02211981639266014, 0.06097561493515968], ["I have a large dataframe which combines data from multiple excel (xlsx) files. The problem is every column with decimal values is seperated with a dot.I need to replace every dot with a comma. I have already tried using the replace function, but the issue some columns also contains string values. So my question is, how do I replace dot with comma on each column in my dataframe and also keep the string values?\nExample:\nColumn a: \n14.01 -> 14,01 \nNo data (keep)", "This is probably your default language setting for Office tool is US or UK where . is used a decimal denoter where as in languages like German it is a ,. If  you are using Libre Office, you can go to Tools -> Language -> For all text -> More and change the default decimal separator key. If you are using Microsoft excel, there should be something similar. Afterwards save the excel and then open it back in pandas. Voila.", "Replace dot with comma using pandas on dataframe", 0.0, 0, 693, "You can do this with regular expressions: `str.replace('.','','')`\nThe `','` is for the comma and `''` is to match any characters.\nSee the [docs](https://pandas.pydata.org/docs/reference/api/pandas-series.html) for more information.", 0.23423422873020172, 0.01834862306714058, 0.12612612545490265, 0.02427184209227562], ["I have a 2d numpy array psi with shape (nx,ny). I want to create a new array phi of the same shape where for each element phi[i][j] I need to evaluate an expression containing psi[i][j] and neighboring elements psi[i-1][j],psi[i+1][j],psi[i][j+1] and psi[i][j-1],except for edge cases where any of these neighbors are not in the bounds of psi, treat that element as 0 in the expression.\nI can implement this using nested for loops and checking for boundary conditions, but I would like to perform this operation as time efficient as possible. I've tried by assigning\nphi[1:-1,1:-1] = f(psi[1:-1,1:-1], psi[0:-2,1:-1], psi[2:,1:-1], psi[1:-1,0:-2], psi[1:-1,2:])\nbut this does not cover edge cases which get messy, so if there were some conditional way to only reference when within bounds else just be 0 it might work. Or, of course, if there is an even more time efficient way that would be better.", "This problem smells like finite differences. Your best bet is to write a (fast, possibly recursive) loop for the inner points, and then loop over the boundary points separately, imposing the desired boundary conditions there. Obviously, the other way around also works: start by assigning boundary points, then loop over inner points.\nThat said, if you are having issues with speed (probably because your grid is gigantic), you may want to do a few optimizations, as 2d arrays in python are S L O W:\n\ntry reversing the order of looping: in python (NumPy, in case you are using that), 2d arrays are traversed by rows first. You may want to experiment with that at least.\n\ntry allocating your 2d thing as a big 1d chunk where its unique index is n = i + nx * j, with i,j your original 2d indices. Again, experiment with running the new index n along rows vs columns first.\n\n\nThese two suggestions combined should give you a massive speedup.", "Indexing numpy arrays only if within bounds", 1.2000000476837158, 0, 694, "You can do it by a [boolean indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#boolean-array-index-expressions):\n\\begin{itemize}\nCopy the array and assign a `False` value to the boundary elements:\n`phi = p.copy()`\nAssign the values using a boolean array with the mask of elements inside the board: `phi[:, :, ph[:-2:-4]] = 1`\nRemove the `True` elements from the boolean mask:  `mask = ph > -4` and then, apply it to `psi`: `ph[:,-1:] = mask` .\nIn the end, we obtain: ph[mask] == 4.420710825\nSo we have the indices of `4`:\n[3, 3]\nTo check the condition, I suggest you to use a [`where`](https:/numpy/numpy-doc.pdf#g:17.16.7) statement: \\begin {code} where = (phi >= -5)\n(phi <= -3) & (mask == -1)  # <-- this is", 0.2184300273656845, 0.013745704665780067, 0.11604095250368118, 0.035830624401569366], ["I have a 2d numpy array psi with shape (nx,ny). I want to create a new array phi of the same shape where for each element phi[i][j] I need to evaluate an expression containing psi[i][j] and neighboring elements psi[i-1][j],psi[i+1][j],psi[i][j+1] and psi[i][j-1],except for edge cases where any of these neighbors are not in the bounds of psi, treat that element as 0 in the expression.\nI can implement this using nested for loops and checking for boundary conditions, but I would like to perform this operation as time efficient as possible. I've tried by assigning\nphi[1:-1,1:-1] = f(psi[1:-1,1:-1], psi[0:-2,1:-1], psi[2:,1:-1], psi[1:-1,0:-2], psi[1:-1,2:])\nbut this does not cover edge cases which get messy, so if there were some conditional way to only reference when within bounds else just be 0 it might work. Or, of course, if there is an even more time efficient way that would be better.", "I've realized that using numpy array operations is definitely the way to go to make the code faster. Pairing this with np.pad to add zeros to the edges of a matrix makes this fairly simple.", "Indexing numpy arrays only if within bounds", 0.0, 0, 695, "The following will work for your case.  You could generalize it to handle an arbitrary number of elements. The first three lines are just the way you already had it, which is the \"naive\" way. For the fourth line, we use `np.where` to fill in `phi` with a default value for the edge-case when an element is not within the padded array, i.e., `0` in this case, or something else if `psi` is `None` or `NaN` there. Note that the condition is tested both forwards and backwards from the edges. In general, you could also test the neighbours on both sides of a cell as well.", 0.21917808055877686, 0.0416666679084301, 0.13698630034923553, 0.04516034200787544], ["I am using gym==0.26.0 and I am trying to make my environment render only on each Nth step. So that my nn is learning fast but that I can also see some of the progress as the image and not just rewards in my terminal. With the newer versions of gym, it seems like I need to specify the render_mode when creating but then it uses just this render mode for all renders.\nHow to make the env.render() render it as \"human\" only for each Nth episode? (it seems like you order the one and only render_mode in env.make)", "My solution is to create a new 'human' env to be used on Nth step.", "gym env.render() on Nth step", 0.0, 0, 696, "What you need is called `reset_render_count` option. You can set it in your `env.set_up()`.\nExample:\n`env = gin.env_class()\nenv:add_option(\"reset-render-count\", 100) `\nIn that way, env will render 3 images after every 50 steps, so it will take 60 seconds to render one image.", 0.08955223858356476, 0.0, 0.08955223858356476, 0.0535714253783226], ["I need to find the minimum distance from a point (X,Y) to a curve defined by four coefficients C0, C1, C2, C3 like y = C0 + C1X + C2X^2 + C3X^3\nI have used a numerical approach using np.linspace and np.polyval to generate discrete (X,Y)  for the curve and then the shapely 's Point, MultiPoint and nearest_points to find the nearest points, and finally np.linalg.norm to find the distance.\nThis is a numerical approach by discretizing the curve.\nMy question is how can I find the distance by analytical methods and code it?", "You need to differentiate (x - X)\u00b2 + (C0 + C1 x + C2 x\u00b2 + C3 x\u00b3 - Y)\u00b2 and find the roots. But this is a quintic polynomial (fifth degree) with general coefficients so the Abel-Ruffini theorem fully applies, meaning that there is no solution in radicals.\nThere is a known solution anyway, by reducing the equation (via a lengthy substitution process) to the form x^5 - x + t = 0 known as the Bring\u2013Jerrard normal form, and getting the solutions (called ultraradicals) by means of the elliptic functions of Hermite or evaluation of the roots by Taylor.\n\nPersonal note:\nThis approach is virtually foolish, as there exist ready-made numerical polynomial root-finders, and the ultraradical function is uneasy to evaluate.\nAnyway, looking at the plot of x^5 - x, one can see that it is intersected once or three times by and horizontal, and finding an interval with a change of sign is easy. With that, you can obtain an accurate root by dichotomy (and far from the extrema, Newton will easily converge).\nAfter having found this root, you can deflate the polynomial to a quartic, for which explicit formulas by radicals are known.", "Finding the minimum distance from a point to a curve", 0.0996679961681366, 1, 697, "You can do this using analytic geometry. If you can write your curve as a linear function of a variable, then you have an equation of the form $y=mx+b$ where $m$ is the slope and $b=y_0$ the y intercept. The distance between your point and the line can be found by using the Pythagorean Theorem:\n$$d^ 2 = (x_1-x)^2+(y-y_{0})^2$$\nThe distance is then $d=\\sqrt{(x-a)^  (2)+(y-\\beta)^(2)}$ where $\\beta$ and a are the x and y cooridinates of your line and $\\alpha$ denotes the point.", 0.30821916460990906, 0.03448275849223137, 0.15068493783473969, 0.057934507727622986], ["I have a dataframe containing 1440 rows and 8 columns. I have a column names 'number' taking values from 0 to 12. And I have another column named \"correct\" which takes value of True or False.\nWhat I would like to do is a line chart with on my x axis'number' which as I said are ranging from 0 to 12 and on my y axis the number of \"correct\" taking the value True.\nWhat I want is to regroup all of the cases when the 'number' is 0 and number of true, for example.\nI tried to create a small example but I think I'm not precise enough.\nI tried a lot of things with grouby and other things", "Thanks a lot, I have been able to draw what I wanted but just for my graph to be a bit more precise. How could I add my axis like on x I would like 'number' and on the top left of my graph something saying what's being presented like the name 'correct' because I'm plotting the number of 'correct' ?", "I want to draw a line chart with a sort of condition", 0.0, 0, 698, "You can use `pandas.crosstab()` and specify the columns as `columns=['number', 'correct']`\nThen use the `aggfunc` parameter to specify a function that will calculate the mean and `values` will be the result:\nhttps://pandas-docs.github.io/pandas/generated/series.cross_table.html\nExample: `crs_c.agg({'correct': 'mean'})`", 0.1785714328289032, 0.0, 0.0892857164144516, 0.05263158306479454], ["I am new to Fasttext I am using python-wheel(v0.9.2) module with python3.10\nI trained a text classification model\nwhen I run a\nmodel.test(\"datasets\\dataset1.txt\")\nI except an output like:\n(nbr of samples, precision, recall)\nI get\n(1, 1.0, 1.1146408069999442e-05)", "The most simple explanation may be: (1, 1.0, 1.1146408069999442e-05) are the accurately-reported (sample_count, precision, recall) for your model & file.\nWhat makes you sure it's not? Are you sure training succeeded on properly-formatted data?\nHow did you train the model, on what data? What progress/success was reported in training?\nWhat's inside your dataset1.txt file - such as type & quantity of data? Are you sure it's formatted correctly for the test() operation \u2013 with the proper delimiters of fields, tokens, and lines?\nCan you show a few representative lines of the training & test data?\n(If you need to add such details, it'll be best to edit your question, so there's plen ty of space/formatting-options to make them clear.)", "model.test output is not (nbr of samples, precision, recall)", 0.0, 0, 699, "Please check your precision and recall. You can check it using below code:  `model._outputs[0][\"precision\"]`\nor\n`model.__output_dict__[\"output\"][\"recall\"]`.\nPlease check the code below: `from sklearn.metrics import precision_recaller_f1_score` and then do `print(model)`.", 0.12941177189350128, 0.0, 0.08235294371843338, 0.07438015937805176], ["I have multiple variables in my data frame with negative and positive values. Thus I'd like to normalize/scale the variables between -1, 1. I didnt find a working solution. Any suggestions? Thanks a lot!\nI scaled other variables with the sklearn MinMaxScaler 0, 1. Didn't find an additional -1, 1 solution there.", "Min max scaler uses a mathematical formula that converts values between 0,1 not -1,1\nif you want values between -1,1 try sklean's StandardScaler.\nHope this helps.", "Data frame normalization center = 0 solution (-1, 1)?", 0.0, 0, 700, "The problem you describe is the same as [Center a Dataframe](https://stackoverflow.com/questions/19675911/center-a-dataframe).\nSo the solution is:\n\\begin{itemize}\n \\item normalizing the columns,\n  \\begin {code} <-- use the Min-Max Scaler with 'center'=True and 'norm'='l2'\n    df = df.astype(float)\n   df_mms = mms.fit_transform(df)  # call the fit method of the msm\n     \\end {pre}    \\em{with the code above you have to cast the column to float} (if it's int or string the normalizer will return an error) \\strong{OR} \\sout{you could also use `normalize` from the [sklearn.preprocessing.normalization](http://scikit-learn-contrib.readthedocs.org/en/latest/preprocessors/normalizer.html) package} -- \\textbf{EDIT:} It seems like it doesn' t work as expected. In my case it returned an exception and the values got smaller.", 0.09756097197532654, 0.02469135820865631, 0.0731707289814949, 0.04081632196903229], ["how to use pretrained model on 512x512 images on real images with different sizes like Yolo object detection on real image ?\nCNNs require fixed image sizes, so how do you manage to use the models on real images larger than the inputs?", "If it is just about the image size, you could resize your image to have same size as model input. When you receive the output, assuming that you have bounding boxes or locations etc, you can always rescale them back to original image size. Many ML/DL frameworks provide this functionality", "how to use pretrained model CNN with real images various size?", 0.0, 0, 701, "I suggest you take a look at [pytorch](https://pyca.org/pych/install/). It can be used to resize images to fit the pre-trainded CNN model.\nThere is a lot of documentation available. If you are familiar with keras, you should be able to make the transition to pyTorch quite easy.", 0.25242719054222107, 0.0, 0.13592232763767242, 0.07296138256788254], ["I know I can change the data type of columns by passing the column names, for example\ndf = df.astype({'col1': 'object', 'col2': 'int'})\nbut what if I want to change multiple columns by a given range? My df contains 50+ columns so I don't want to change them all by name. I want to set columns 17 to 41 as ints and I've tried a few variations, for example:\ndf = df.astype([17:41], 'int64')\nbut can't get the syntax to work. Any ideas?", "You can access columns by index (position).\ndf.iloc[:,16:42] = df.iloc[:,16:42].apply(pd.to_numeric)", "Changing datatype of multiple columns by range", 0.1973753273487091, 2, 702, "Here is one way using a list comprehension.\nNote that I have replaced `16:38` with `range(15,39)` because the former is not an array. If you want the latter you could use a slice. `astypes` is a scalar. A list is used to index the array, which is why the `[]` are not necessary. You could also use `df.columns[14:18]` or any other combination of indexes. Note that using `pandas.Index.tolist()` with a sequence of integers and indexing is faster than using the list. The `tol` function is slow. [See here](https://stackoverflow.com/questions/22332087/how-to-use-pandas-index-tol-fast).\nThe first argument of `apply` can be a function, so we can use the function `lambda` to define the new dtype. Then we call `numpy.where` in the lambda function to return the values of the columns of interest. Lastly, we apply this function across the entire dataframe. We can add a `return_type` keyword to the apply function so that we don\u2019t need to specify the", 0.0824742242693901, 0.0, 0.05154639109969139, 0.010952911339700222], ["I am finetuning a transformer model and during the training cycle, evaluating it at each epoch. The best model is selected based on the highest evaluation accuracy among all epochs. Once the training cycle is completed and the best model is dumped to the disk, I try to regenerate that validation accuracy. I am unable to regenerate the exact validation accuracy reported by the training phase. I am getting a 3% to 4% drop in accuracy on the same evaluation data.\n(For regeneration, I am calling the same evaluation function and passing it model and dataset. Nothing else changed for evaluation accuracy regeneration)", "Are you sure you are saving a checkpoint at each time you evaluate during training? At the end of training, when loading the best model, you will load the best saved checkpoint. If there is no checkpoint for the best model version, you will end up loading some other version, which might explain the drop in accuracy.", "Why accuracy of finetune transformer model is less when evaluated after loading from disk, than during training?", 0.0, 0, 703, "When you do fineteuning, you retrain the model with the validation set. So, your model will be different from the one trained in the previous phase, that is why you can't get the original evaluation results.", 0.2978723347187042, 0.0, 0.21276596188545227, 0.05232558026909828], ["I am using FastAPI for Machine Learning model deployment in two cases based on the nature of the data we feed as inputs (features) which it an array of json. Thus, if inputs are matching method1 we execute the according model to it otherwise we apply method2 and execute the trained model for this case.\nHow can I achieve this process using FastAPI ? (process of verifieng the input data and apply the matching model for that data)", "You can create a pydantic scheme as a dependency, that includes all the possible fields (Optional) for both data types and check incoming data by special field into it. Also you can use different routes. Could you show JSON samples?", "FastAPI inputs check format and execute according method for that given format", 1.2000000476837158, 1, 704, "For this particular use case, I would recommend using the [FastAPI router](https://fastapi.tiangolo.com/tutorial/routers/).\nYou could then use a `match` endpoint like this:\n\\begin{itemize}\n<h4>Code</h3>\n`\n{\n  \"status\": \"ok\",\n    \"data\": {\n      \"match\": true\n        }\n}`,\nwhere `data` is a dictionary mapping the endpoint to the result. For example, `request.method` will return the method. If you want to return a JSON object or a list, you can use `json()` or `list()` functions. `status` can be any string value. You can also have nested dictionaries. In this example the match is True, but you could have a different status for the different cases. Here is another example that will use different models for different requests. The code below is not tested, so make sure you test it for yourself. This will also give you a better idea on how to use routers. Please note that the code above is just an example. There are a lot of different ways to structure the router and the application. However, this is one", 0.18009479343891144, 0.038277510553598404, 0.09478673338890076, 0.055276378989219666], ["I am working on an inventory simulation model. I have this global list variable called current_batches which consists of objects of a custom class Batch and is used to keep track of the current inventory. While running the simulation, a number of functions use this current_batches variable and modify it following certain events in the simulation.\nInside one function, I need to copy this variable and do some operations with the objects of the obtained copy, without modifying the objects of the original list. I used copy.deepcopy() and it works, but it is very slow and I will be running the simulation for many products with many iterations. Therefore, I was wondering if there is a way to copy this (global) list variable without using copy.deepcopy().\nI briefly looked at the pickle module, but it was unclear to me whether this module was useful in my situation.", "If you need a full deepcopy which takes time because the copied object is large I see no way to avoid it.\nI suggest you speed things up creating a current_batches_update object where you save the modifications only and adjust the logic of the code to get values not present in current_batches_update object from the current_batches one. This way you can avoid making a full copy keeping the ability to get all the values.\nAnother option would be to equip current_batches with the ability to store two versions for some of its values, so you can store the special modified ones as a second version in current_batches and allow version (=1 or =2)` as parameter in the function for retrieving the values designed to deliver version 1 if there is no requested version 2.", "Copying a list of custom class objects without using .deepcopy()", 0.0, 0, 705, "You can use `copy.copy()` instead of `deep copy`.\n\\begin{itemize}\n  \\item `pickle.dump(obj, f)`\n    serializes `obj` to a file descriptor `f`, or a\n    2-tuple of file descriptors `(f1,f2)`.  The file is written in\n   pickled format.  (The `dump` function is an alias for `cPickle`)\n         - `os.fdopen(file)` can be used in case of pickling to\n       files. (Python 3.3)\n      \\begin {code} (<type 'os'>,\n                  <type\n'file'>)\n        <class 'file'>\n          <file '...'>  # pickles to file\n             <_IO_fd_write object at 0x7f9077498230>\n               <function write>  Python 2.7.6\n                <built-in method write of object file at\n0xf539650c> (CPython 70.0 (default, Dec 13 5:58:", 0.22131147980690002, 0.00826446246355772, 0.10655737668275833, 0.026907935738563538], ["Below is all of the error, I think it must be some config or version wrong\n2022-11-01 19:43:58 [scrapy.crawler] INFO: Overridden settings:\n{'BOT_NAME': 'spider2022', 'NEWSPIDER_MODULE': 'spider2022.spiders', 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['spider2022.spiders'], 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor', 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' '(KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'}\npackages/scrapy/downloadermiddlewares/retry.py\", line 25, in \nfrom twisted.web.client import ResponseFailed\nFile \"/Users/zhangyiran/opt/anaconda3/lib/python3.9/site-packages/twisted/web/client.py\", line 24, in \nfrom twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS\nFile \"/Users/zhangyiran/opt/anaconda3/lib/python3.9/site-packages/twisted/internet/endpoints.py\", line 63, in \nfrom twisted.python.systemd import ListenFDs\nFile \"/Users/zhangyiran/opt/anaconda3/lib/python3.9/site-packages/twisted/python/systemd.py\", line 18, in \nfrom attrs import Factory, define\nModuleNotFoundError: No module named 'attrs'\n(venv) (base) zhangyiran@zhangyirandeair spider2022 % ``\n\"", "In one code I used import - 'from scrapy.crawler import CrawlerProcess'. Then, in other spiders, I began to have the same problem as you. When I commented out the import CrawlerProcess - the problem went away.", "why my scrapy not run? I'm a new user of scrapy and it cannot create csv", 0.0, 0, 706, "I am not sure if this is your issue, but I came across the same error message and I was able to solve it by uninstalling and reinstalling `attr` using `pip uninstall attr` and then `sudo pip install attr`.\n`pip install --upgrade attr`, however, did not work for me.\nThis is how I solved the problem: https://github.com/ScrappyPy/attr/issues/45#issuecomment-650976713", 0.18000000715255737, 0.040816325694322586, 0.14000000059604645, 0.05171004310250282], ["I am relatively new to web development and very new to using Web2py. The application I am currently working on is intended to take in a CSV upload from a user, then generate a PDF file based on the contents of the CSV, then allow the user to download that PDF. As part of this process I need to generate and access several intermediate files that are specific to each individual user (these files would be images, other pdfs, and some text files). I don't need to store these files in a database since they can be deleted after the session ends, but I am not sure the best way or place to store these files and keep them separate based on each session. I thought that maybe the subfolders in the sessions folder would make sense, but I do not know how to dynamically get the path to the correct folder for the current session. Any suggestions pointing me in the right direction are appreciated!", "I was having this error \"TypeError: expected string or Unicode object, NoneType found\" and I had to store just a  link in the session to the uploaded document in the db or maybe the upload folder in your case. I would store it to upload to proceed normally, and then clear out the values and the file if not 'approved'?", "Using temporary files and folders in Web2py app", 0.0, 0, 707, "You can create a temporary folder on your filesystem, which will be unique for each user session, use it to put your files, close it, delete it and the folder will disappear.\nYou can then get a path of that folder by using `os.path.join`\nFor example:\n`os_path = os.getcwd()`\nThen: `f = open(ospath + 'temp/file_name', 'w')`\nThe 'file' is an example, you can name it anything you want. In this example I have created a file and saved it in 'osPath + '/temp' folder. Then I opened it for writing. You will need this function to get this folder path: https://web2python.com/tutorials/sessions.html\nIn the `routes.py` file, when you get your `file` (which is a string in this case), you will get something like this: 'C:/Users/your_username/temp', and you should be able to save it anywhere you like.", 0.23076923191547394, 0.03883495181798935, 0.10576923191547394, 0.0416666641831398], ["I am relatively new to web development and very new to using Web2py. The application I am currently working on is intended to take in a CSV upload from a user, then generate a PDF file based on the contents of the CSV, then allow the user to download that PDF. As part of this process I need to generate and access several intermediate files that are specific to each individual user (these files would be images, other pdfs, and some text files). I don't need to store these files in a database since they can be deleted after the session ends, but I am not sure the best way or place to store these files and keep them separate based on each session. I thought that maybe the subfolders in the sessions folder would make sense, but I do not know how to dynamically get the path to the correct folder for the current session. Any suggestions pointing me in the right direction are appreciated!", "If the information is not confidential in similar circumstances, I directly write the temporary files under /tmp.", "Using temporary files and folders in Web2py app", 0.0, 0, 708, "If you are generating files dynamically in your application, you might want to consider using a separate folder structure for your temporary data.\nIn your `app.py` create a `temporary` folder in `sessions`.\nFor example:\n`/sesssions/<app-name>/temproary/`\nThen you can use that folder to save all your files. You could also write some code to delete the folder when the application is done. This will allow you to remove all files when your session is closed.", 0.16842105984687805, 0.0, 0.10526315867900848, 0.04122810438275337], ["I am trying to read in a large amount of Avro files in subdirectories from s3 using spark.read.load on databricks. I either get an error due to the max result size exceeding spark.driver.maxResultSize, or if I increase that limit, the driver runs out of memory.\nI am not performing any collect operation so I'm not sure why so much memory is being used on the driver. I wondered if it was something to do with an excessive number of partitions, so I tried playing around with different values of spark.sql.files.maxPartitionBytes, to no avail. I also tried increasing memory on the driver and using a bigger cluster.\nThe only thing that seemed to help slightly was specifying Avro schema beforehand rather than inferring; this meant the spark.read.load finished without error, however memory usage on the driver was still extremely high and the driver still crashed if I attempted any further operations on the resulting DataFrame.", "I discovered the problem was the spark.sql.sources.parallelPartitionDiscovery.parallelism option. This was set too low for the large number of files I was trying to read, resulting in the driver crashing. Increased the value of this and now my code is working.", "Why am I getting out of memory error on spark driver when trying to read lots of Avro files? No collect operation happening", 1.2000000476837158, 0, 709, "You can try to run spark in an embedded mode, then you can use the following command to start spark with the given memory size:\n`spark-submit --driver-memory 1g`\nThen you should get the output like this: (I just run with 2GB):\n<a href=\"https://i.stack.imgur.com/hKfYQ.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nYou can also try `spark.executor.memory` as the alternative.", 0.19819819927215576, 0.0, 0.12612612545490265, 0.05429355055093765], ["I've trawled through stack overflow, several youtube videos and can't for the life of me work this out.\nI've unpackaged and pulled from git, all files are where they need to be as far as the installation for Stable Diffusion goes - but when I go to run I get two errors, one being the pip version. I upgraded via 'pip install --upgrade pip' and though the version updated, I'm still getting the below error.\nThe other issue is that pytorch doesn't seem to have installed. I've added it to the requirements.txt and run 'pip install -r requirements.txt' which doesn't seem to work either. I also downloaded 1.12.1+cu113 and ran pip install \"path/\" and received the error \"ERROR: torch-1.12.1+cu113-cp39-cp39-win_amd64.whl is not a supported wheel on this platform.\"\nError received below:\nstderr: ERROR: Could not find a version that satisfies the requirement torch==1.12.1+cu113 (from versions: none)\nERROR: No matching distribution found for torch==1.12.1+cu113\nWARNING: You are using pip version 20.1.1; however, version 22.3 is available.\nYou should consider upgrading via the 'C:\\Users\\XXX\\Downloads\\STABLE\\stable-diffusion-webui\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\nAny help would be greatly appreciated, I've tried my best to be self-sufficient so I'm putting it to the people who may know how to help.", "Same problem with python 3.8. I install python3.10 and fixed.\nFor mac:\nbrew install python@3.10", "Attempting to install Stable Diffusion via Python", 0.0, 0, 710, "The issue here was that I had a Python 3.6 and a 6.5 installed on my system. For some reason, the 7.x pip that came with Stabile Difusion wasn'torch compatible with the one installed via conda. So I simply had to uninstall the latter, and everything was fine after that.", 0.16438356041908264, 0.028169013559818268, 0.10958904027938843, 0.03692465275526047], ["I have a huge \".csv\" file which corresponds to an one-day data file.\nEvery half-hour of the day, data was recorded during ten minutes. In the file, each half-hour is separated by a text tag, such as \"zzzz\" or \"system sleep\", depending on the .csv.\nI would like to split the CSV breaking it into the 48 half-hour blocks, and save each half-hour .csv in a new foder that includes 48 smaller csv. files. I am sure there are ways to do this but I cannot find the way. Because each half-hour values do not have exactly the same number of rows, I cannot split this data according to row numbers.\nThe file will look something like the following (I made a shortened example):\n\n\n\n\nID\nDay\nTime\nRec\nvalue\n\n\n\n\nA1\n2018/1/30\n00:00\n1\n251\n\n\nA1\n2018/1/30\n00:01\n2\n368\n\n\nA1\n2018/1/30\n00:02\n3\n430\n\n\nsystem sleep.\n\n\n\n\n\n\nA1\n2018/1/30\n00:30\n1\n195\n\n\nA1\n2018/1/30\n00:31\n2\n876\n\n\nA1\n2018/1/30\n00:32\n3\n864\n\n\nsystem sleep.\n\n\n\n\n\n\nA1\n2018/1/30\n01:00\n1\n872\n\n\nA1\n2018/1/30\n01:01\n2\n120\n\n\nA1\n2018/1/30\n01:02\n3\n208\n\n\nsystem sleep.\n\n\n\n\n\n\n(...)\n(...)\n(...)\n(...)\n(...)\n\n\nA1\n2018/1/30\n23:39\n10\n002\n\n\n\n\nAnd so it goes for the whole day. Please note my actual data has up to 7000 values per half-hour*.\nI would like to split it for each \"system sleep\" (or each time such text appears in the first column); and save the new .csv files in a new folder. Also, if possible, I would like to keep the header (first row) for all the half-hour blocks/new csv'. Ideally, I'd also like the file to be saved after the first time value/row of each block (\"Time\") -but I guess it would still work if it was saved as 1, 2, 3, 4.\nCan anyone help me? I usually work with R language, but if it's easily done in another language such as python (I found many answers in python but not exaclty what I need), I wouldn't mind giving it a try eventhough I have no experience with it (but if I know R, it should be doable). Thank you very much..!", "What I did was to use the function\nwhich(startsWith(df$treeID, \"system sleep.\")),\nthis retrieved me all the columns that started with this value.\nThen using the function slice and the previous column I can cut the dataframe. However, I can only do it one by one (selecting with [1], [2], etc. the row and slicing the rest). So I am trying to build a loop now.\nIf you want more details send me a message.\nThank you.", "Split a huge csv based on time intervals depending on inner text tags", 0.0, 0, 711, "With R:\n\\begin{itemize}\n \\item split into half hour chunks with the `split()` function\n  \\begin {code}", 0.08421052992343903, 0.0, 0.06315789371728897, 0.04542689025402069], ["I have xyz axis for gyro and accelerometer data, and i want to detect between whether the travel path was circular or square\nHave not tried anything, want initial ideas", "The way I would approach this would be:\nFirst compute some characteristics about my data such as:\n\ncentroid of my points\nmax distance between two points\n...\n\nThen create reference shapes with that data, for example a circle with its center being the computed centroid of my points, and its diameter being the max distance,...\nThen try to find how close to each reference shape is every point in my path to compute the standard deviation between each reference shape and my path. This might be more or less difficult depending on how complicated your reference shapes are.\nFinally I would just pick the shape with the smallest standard deviation.\nThis might not be very optimal though, since it involves quite a lot of computation, especially if you have a lot of point in your path.", "How can i distinguish between square and circular shapes from accelerometer and gyroscope sensor data using python?", -0.1973753273487091, -1, 712, "\\begin{itemize}\n\\item Get the mean x,y,z for the first few samples\n`mean_x = np.mean(x)`\netc\nYou should then look at how close the y axis is to the line y = x\nAnd the z axis to line z = y\nIf both are close to 0 then the path is circular.\nOtherwise it is a square. You could also look to see if the distance of the x axis and z axes is close. And the distances of x and y.", 0.2454545497894287, 0.01834862306714058, 0.145454540848732, 0.036018963903188705], ["I will like to delete all empty row in a column\nprices_dataframe[prices_dataframe['postcode'].isnull()]\nThis only seems to be showing me the empty rows not deleting it.", "Is Null only returns the empty rows it does not drop them. Your empty rows should contain NaN so you can use `prices_dataframe.dropna(inplace=True)\nTo drop them.\nIf your rows don't contain NaN you can first replace the empty rows with NaN\nprices_dataframe.replace('', np.nan, inplace=True) and then drop them", "how to delete for an empty row in a column in python csv", 1.2000000476837158, 0, 713, "`pd.dropna()`\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pd_drop_duplicates.html", 0.0555555559694767, 0.0, 0.0555555559694767, 0.031578946858644485], ["I have multiple csv files present in hadoop folder. each csv files will have the header present with it. the header will remain the same in each file.\nI am writing these csv files with the help of spark dataset like this in java\ndf.write().csv(somePath)\nI was also thinking of using coalsec(1) but it is not memory efficient in my case\nI know that this write will also create some redundant files in a folder. so need to handle that also\nI want to merge all these csv files into one big csv files but I don't want to repeat the header in the combined csv files.I just want one line of header on top of data in my csv file\nI am working with python to merging these files. I know I can use hadoop getmerge command but it will merge the headers also which are present in each csv files\nso I am not able to figure out how should I merge all the csv files without merging the headers", "coalesce(1) is exactly what you want.\nSpeed/memory usage is the tradeoff you get for wanting exactly one file", "merge multiple csv files present in hadoop into one csv files in local", 1.2000000476837158, 1, 714, "The `coalesce()` method takes an optional parameter that allows you to specify whether or not you want the coalesced file to have headers, if there are headers in all of the input files:\n\\begin{itemize}\n  \\item `true` - If there is a header line in at least one input file, the output file will be written with a new header containing the column names from the first input, followed by the data. Otherwise, no header is written. If any of your input CSV files do not have a column header, this option will not work. (default is false) (This is the behaviour you describe.)\n  2. `false` (only available since Hadoop 3.1.0) - if the number of columns in any input is different than the next largest number, an error will occur and the job will fail. This is to avoid the situation where one CSV file has a different number columns than another and causes the result to be invalid. In this case, you will need a join with `null`s. Note that if you use this method and have input columns that are missing from one or more input data files, Hive will assume that these are null values, and they will appear in", 0.08888889104127884, 0.017937220633029938, 0.07111111283302307, 0.06521347910165787], ["I am porting the Matlab code to Python. In Matlab, indices start at 1, but in python, they start at 0. Is there any way to set the first index as 1 through a command line flag?\nIt will be very useful for programming during index iteration.", "As far as Python is concerned, there cannot be changes in the Indexing part. It always starts with 0 (Zero) only and progresses onwards.\nHope this helps you.", "Python index starts at 0. Any possibility to set index value as 1 for any terminal?", 0.03997867926955223, 1, 715, "`range(start, end)` is inclusive, so the answer to your question is \"no\" (there is no way of changing that in Python).  If you want to start your range at `1` rather than `0`, you need to subtract `start` from the value returned by `len(range())` (or the length of the list returned if you call `list(x)` instead):\n\\begin{code}\n>>> len(list([x for x in range(5)]))\n5\n# result is a list of 5 values\nrange_length = len([1] + [x + 2 for (x,) in enumerate(r)])\nprint(len([[x] * i for i in xrange(-range[0], range[1], 3)] for\n(y, x)\nin zip(itertools.cycle(frozenset(map(str,range))),\n        itertools._count(0)) for _ in\n    range_xrange])[range:range + range])  # 60\n1 4 7 8 9 \n                    # ^ start index = ndx\n                ^", 0.08805031329393387, 0.0, 0.0628930851817131, 0.04614117369055748], ["I have got an excel file from work which I amended using pandas. It has 735719 rows \u00d7 31 columns, I made the changes necessary and allocated them to a new dataframe. Now I need to have this dataframe in an Excel format. I have checked to see that in jupyter notebooks the ont_dub works and it shows a dataframe. So I use the following code ont_dub.to_excel(\"ont_dub 2019.xlsx\") which I always use.\nHowever normally this would only take a few seconds, but now it has been 40 minutes and it is still calculating. Sidenote I am working in a onedrive folder from work, but that hasn't caused issues before. Hopefully someone can see the problem.", "Usually, if you want to save such high amount of datas in a local folder. You don't utilize excel. If I am not mistaken excel has a know limit of displayable cells and it wasnt built to display and query such massive amounts of data (you can use pandas for that). You can either utilize feather files (a known quick save alternative). Or csv files, which are built for this sole purpose.", "Writing dataframe to Excel takes extremely long", 0.0, 0, 716, "You can use below code to read excel and write to excel. You may try once. Below code is working fine with my system. Make sure that you are using right sheet name. Also check your file size. Once file is very big, it takes more time to process. Thanks\n\\begin{code}\nfrom openpyxl import Workbook\nimport pandas as pd\npath = r'C:\\Users\\user\\Desktop\\data.csv'\ndf = pd.read_csv(path)\nwb = WorkBook()\nws = wb.active\nfor x in df.index:\n    ws.cell(row=x, column=0).value = df[x].iloc[0]\n        ws['A'] = list(df[df['Date'] == df['D'].max()])\n       ws[['C','D','E','F']] = ['1','2','3','4']\n      ws[:, 0:3]  = [df.ilocs[i].values for i in range(4)]\n  wf = open_workbook(w) #create the workbook, pass in the filename\nwriter = ExcelWriter(filename =", 0.1990521401166916, 0.028708133846521378, 0.09478673338890076, 0.05937499552965164], ["Create a row that sums the rows that do not have a data in all the columns.\nI'm working on a project that keeps throwing dataframes like this:\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n\n108.864\n\nINTERCAMBIADORES DE\n1123.60      210.08     166.71     1333.68\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n147.420       5.000\nPZ\n1A0181810000\n81039.25       15149.52    19237.754880        96188.77\n\n\n\n147.420\n\nINTERCAMBIADORES DE\n3882.25      725.75     921.60     4608.00\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n566.093      12.000\nPZ\n1A0183660000\n66187.40       12374.29     6546.806709        78561.68\n\n\n\n566.093\n\nINTERCAMBIADORES DE\n3170.76      592.80     313.63     3763.56\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n3.645       1.000\nPZ\n1A0185890000\n836.64          159.69      996.330339          996.33\n\n\n\n3.645\n\nINTERCAMBIADORES DE\n40.08        7.65      47.73       47.73\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n131.998       3.000\nPZ\n1A0190390000\n32819.41        6135.17    12984.858315        38954.57\n\n\n\n131.998\n\nINTERCAMBIADORES DE\n1572.24      293.91     622.05     1866.15\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n123.833       3.000\nPZ\n1A0190790000\n54769.36       10238.84    21669.402087        65008.21\n\n\n\n123.833\n\nINTERCAMBIADORES DE\n2623.77      490.50    1038.09     3114.27\n\n\n\n\n\nCALOR 8419500300\n\n\n\n\n115.214       2.000\nPZ\n1A0195920000\n54642.66       10215.05    32428.851279        64857.70\n\n\n\n115.214\n\nINTERCAMBIADORES DE\n2617.70      489.36    1553.53     3107.06\n\n\n\n\nThis is going to insert a sql database, I don't know how to add the empty rows with the row that has all the information.\nNOTE: Spacing Empty cells is variable", "Question is unclear: please provide code you've tried, the error message you're getting, and expected output.", "Create a row that sums the rows that do not have a data in all the columns pandas", 0.0, 0, 717, "For the first data frame, you can use [`np.cumsum`](https://numpy.org/doc/stable/reference/generated/numpy/cumsums.html) along the `columns` axis: `df.sum(1)`\nFor the second dataframe, if `np_df` is the dataframe above, we can write:  `pd.DataFrame(np_.csums(df, 1), columns=df)`", 0.08955223858356476, 0.0, 0.05970149114727974, 0.03960396349430084], ["Short description: two computers in the same network, in the new one only those python scripts work that use native packages.\nI have Pycharm in my old computer and it has worked fine. Now I got a new computer, installed the most recent version of Python and Pycharm, then opened one of my old projects. Both the old and the new computer are in the same network and the project is on a shared folder. So I did the following:\n\nFile - Open - selected the project. Got a message that there is no interpreter\nAdd local interpreter - selected the latest Python 311 exe. So location of the venv is the same as in the old computer (because it's a network folder) but Base interpreter is pointing to the C drive of my new computer.\nPyCharm creates a virtual environment and the code runs fine.\nI select another project which uses imported packages such as pandas. Again, same steps as above, add local interpreter. Venv is created.\nI go to File - Setting - Project and see that pip, setuptools and wheel are listed as Packages. If I double click one of these, I can re-install and get a note that installation is succesful, so nothing seems to be wrong in the connection (after all, both the old and the new computer are in the same network.\nI click the plus sign to add a new one, search pandas. Installation fails. Same thing if I try e.g. numpy.\n\nError message has lots of retrying, then \"could not find the version that satisfies the requirement pandas (from versions: none\", \"not matching distribution found for pandas\" (pip etc. have the latest versions).\nAfter few hours of googling for solutions, I have tried the following:\n\nComplety uninstall and reinstall python and PyCharm. Checked that PATH was included in the installation.\nTried launching pip command from shell\nChanged http proxy to auto-detect\nTyped 'import pandas' in PyCharm, then used the dropdown in the yellow bulb but there is no install option\nStarted a new project in the new computer, tried to install pandas\n\nAll failed. I'm surprised that changing computers is this difficult. Please let me know if there are other options than staying in the old computer...", "If you want to use venv in the network, please use SSH interpreter. Pycharm supports this method. Shared folders are not a recommended usage, For pycharm, it will consider this as a local file. If the file map is not downloaded locally, it will make an error.\nAnother way is to reinstall the project environment on the new computer through requirement.txt. Reasonable use of requirements.txt can effectively avoid many project bugs caused by environment migration or different dependent versions.  Before installing some scientific module such as pandas, it is recommended to install visual studio build tools, such as gcc ...", "PyCharm cannot install packages", 0.0996679961681366, 1, 718, "After repartitioning my hard drive, all my problems disappeared and I could use the \"pip install\" command in PCh.", 0.06666667014360428, 0.0, 0.05000000074505806, 0.04210526496171951], ["Short description: two computers in the same network, in the new one only those python scripts work that use native packages.\nI have Pycharm in my old computer and it has worked fine. Now I got a new computer, installed the most recent version of Python and Pycharm, then opened one of my old projects. Both the old and the new computer are in the same network and the project is on a shared folder. So I did the following:\n\nFile - Open - selected the project. Got a message that there is no interpreter\nAdd local interpreter - selected the latest Python 311 exe. So location of the venv is the same as in the old computer (because it's a network folder) but Base interpreter is pointing to the C drive of my new computer.\nPyCharm creates a virtual environment and the code runs fine.\nI select another project which uses imported packages such as pandas. Again, same steps as above, add local interpreter. Venv is created.\nI go to File - Setting - Project and see that pip, setuptools and wheel are listed as Packages. If I double click one of these, I can re-install and get a note that installation is succesful, so nothing seems to be wrong in the connection (after all, both the old and the new computer are in the same network.\nI click the plus sign to add a new one, search pandas. Installation fails. Same thing if I try e.g. numpy.\n\nError message has lots of retrying, then \"could not find the version that satisfies the requirement pandas (from versions: none\", \"not matching distribution found for pandas\" (pip etc. have the latest versions).\nAfter few hours of googling for solutions, I have tried the following:\n\nComplety uninstall and reinstall python and PyCharm. Checked that PATH was included in the installation.\nTried launching pip command from shell\nChanged http proxy to auto-detect\nTyped 'import pandas' in PyCharm, then used the dropdown in the yellow bulb but there is no install option\nStarted a new project in the new computer, tried to install pandas\n\nAll failed. I'm surprised that changing computers is this difficult. Please let me know if there are other options than staying in the old computer...", "This took a while but here is what happened. Package installation did not work in project settings. Neither did it work when you select Python Packages tab at the bottom of the screen. The only thing that worked was to select the Terminal tab and manually install (pip install) there. We use a trusted repository but for other users, the easier package installation methods work. Not sure why they do not for me but at least there is this manual workaround.", "PyCharm cannot install packages", 0.0, 0, 719, "This has been a known issue with the current release of PyCharms 2019.3.1. You may have to downgrade to an older version, and/or do the workarounds mentioned in [this issue](https://youtrack.jetbrains.com/issue/PY-46443).", 0.1818181872367859, 0.0, 0.13223139941692352, 0.09375]]}