{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["When i have python 3.5 and python 3.6 on ubuntu .I entered some alternate commands to use python 3.5 only (when I type python -V and python3 -V the same output is 3.5.2)\nAnd then i install virtualenv and virtualenvwrapper \u2014 these packages allow me to create and manage Python virtual environments:\n$ sudo pip install virtualenv virtualenvwrapper\n$ sudo rm -rf ~/get-pip.py ~/.cache/pip\nTo finish the install of these tools, I updated our ~/.bashrc file.I added the following lines to your ~/.bashrc :\nexport WORKON_HOME=$HOME/.virtualenvs\nexport VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3\nsource /usr/local/bin/virtualenvwrapper.sh\nNext, source the ~/.bashrc file:\n$ source ~/.bashrc\nAnd final I created your OpenCV 4 + Python 3 virtual environment:\n$ mkvirtualenv cv -p python3\ni have created the virtual environment but had some problems in the back end and i guess it was due to the presence of python3.6. In the end i decided to uninstall python 3.6 and rerun the steps above from scratch and had a problem at the last step that I mentioned above.When i enter command \"mkvirtualenv cv -p python3\"  i get an ERROR:\nFileExistsError: [Errno 17] File exists: '/usr/bin/python' -> '/home/had2000/.virtualenvs/cv/bin/python'\nAt the same time when i enter the command \"update-alternatives --config python\" python3.6 is no longer there,but i get a warning:\nupdate-alternatives: warning: alternative /usr/bin/python3.6 (part of link group python) doesn't exist; removing from list of alternatives\nThere is 1 choice for the alternative python (providing /usr/bin/python).\nLooking forward to your help, thank you", "From the commands you've shared, the error arises from the mkvirtualenv cv being run twice - i.e. the environment already exists. To remove the environment you created, do: rmvirtualenv env-name-here which in this case will become rmvirtualenv cv. This shouldn't be done with that environment active, BTW. An alternate route is to delete $WORKON_HOME/env-name-here. By default, $WORKON_HOME is usually .virtualenvs.", "FileExistsError: [Errno 17] File exists: '/usr/bin/python' -> '/home/had2000/.virtualenvs/cv/bin/python'", 1.2000000476837158, 0, 160, "It is because you have two python installation, python2 and  python-3 in your system.\n\\begin{itemize}\n \\item Delete python folder from /home folder\n `sudo rm /opt/pyhton`\n  \\strong{OR}  you can run below command\n`sudo apt-get remove python*`\nto remove all the python. But this is a risky solution. I suggest you to delete python from your home folder. You can follow the below steps\na. Open terminal\nb. Run the commands\n       `cd /` to get to root\nc. `rm -r /python` (here I am deleting python in my home, you replace / with your own home directory) d. Finally run the `exit` command. (your system will restart) \\em{Remember to backup all your data in that directory before running the rm command.}\nAfter this you will have only python version 2.7\nNow you run\n    `pip install --user virtualenvin` and it will work. This is what I did. If you are facing any other problem please let me know.", 0.235807865858078, 0.026431718841195107, 0.12227074056863785, 0.028063498437404633], ["I encountered a problem while doing my ML project. Hope to get some advice from you!\nI fit logistic LASSO on a dataset with only 15 features trying to predict a binary outcome. I know that LASSO is supposed to do feature selection and eliminate the unimportant ones (coefficient = 0), but in my analysis, it has selected all the features and did not eliminate any one of them. My questions are:\n\nIs this because I have too few features, or that the features are not correlated with each other(low co-linearity?)\nIs this a bad thing or a good thing for a classification model?\nsome coefficients of the features LASSO selected are less than 0.1, can I interpret them as non-important or not that important to the model?\n\np.s. I run the model using the sklearn package in python.\nThank you!", "Lasso did not fail to perform feature selection. It just determined that none of the 15 features were unimportant. For the one's where you get coefficients = 0.1 this just means that they are less important when compared to other more important features. So I would not be concerned!\nAlso 15 features is not a large amount of features for Lasso to determine the important one's. I mean it depends on the data so for some datasets, it can eliminate some features from a dataset of 10 features and sometimes it won't eliminate any from a dataset of 20. It just depends on the data!\nCheers!", "Whay did LASSO fail to perform feature selection\uff1f", 0.0, 0, 161, "It's not possible to give a full answer without seeing the data and the code, but here are a few thoughts that might help you.  In general, LA is a \\em{regularization} technique that aims to shrink the coefficients on the variables to make them more interpretable and/or reduce the number of variables.   In your case, if you have no relevant variables, then LA will not be useful.    That said, there are two main uses for LA: (1) if there is collinear information among the predictors, which is likely to cause over-fitting, you can use LA to reduce that colinearity, and (2) you could use it to try to extract relevant predictor variables from a large set of potential predictive variables (but see below).  Here are some things to keep in mind:  If you are using LA with the lasso penalty (which is the default in scikit-learn), it will be conservative, so the penalized coefficients will all be nonzero (though some of these may be close to  zero).    You can then try different penalties to see if the results are more or less interpreTABLE with different levels of regularization. For example", 0.28289473056793213, 0.03311258181929588, 0.1381578892469406, 0.06223944202065468], ["I'm struggling to install Spyder (5.1.5) after installing Keras and Tensorflow.\nHere are the steps I have taken so far:\n\nInstall Anaconda\nWithin Anaconda Navigator create a new environment named 'tensorflow'\nInstall tensorflow and keras within Anaconda Navigator in the 'tensorflow' environment.\nattempt to install Spyder from Anaconda Navigator in the 'tensorflow' environment. I get the following error message when I do this:\n\n'spyder cannot be installed on this environment. Do you want to install the package in an existing environment or create a new environment?'\nThe other thing I've tried, from the Anaconda prompt:\n\nconda activate tensorflow (activate tensorflow environment)\nconda install spyder\n\nI get the following error:\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\nCollecting package metadata (repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nSolving environment: -\nFound conflicts! Looking for incompatible packages.\nThis can take several minutes.  Press CTRL-C to abort.\nThanks for any help!", "My solution was to install Spyder first in a clean environment and then use pip.\npip install tensorflow\nwhich installed tensorflow.keras as well.", "Can't install Spyder after installing Tensorflow and Keras", 0.0, 0, 162, "You should use the spyderm package instead of the normal Spyders. The spyrder package is designed specifically for Anacondas tensorflow enviroment and it should work.", 0.12244898080825806, 0.0, 0.08163265138864517, 0.047999996691942215], ["Encountering an error when running this cell, does any one know how to fix it? Thank you.\ncfos and autofluo images have been resampled align with the template/reference file in atlas. Is it necessary to debug this file?\nFile \"/homeanaconda3/envs/ClearMapStable/lib/python3.6/site-packages/tifffile/tifffile.py\", line 4696, in open\nself._fh = open(self._file, self._mode)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/ClearMap2/Documentation/Example/Haloperidol/haloperidol/1268/debug_resampled.tif'", "Thanks for all responding.\nI've solved this problem. It simply need to turn on the debug mode at the beginning of the script. In my case, I turned on the change the code to 'ws.debug = True' in the Initialize workspace cell.", "ClearMap2 - Cell alignment section - file not found error", 0.0, 0, 163, "The error message indicates that the file is not there. The image is being created in `cell/alignment/resampling` and the `debug` file needs to be created before the image file. I can see that you are using the same prefix for the debug file as the images. So, if you change the prefix to `136` instead of `610`, then it should work fine.", 0.3177570104598999, 0.03809523954987526, 0.1682243049144745, 0.043243248015642166], ["On the column that I\u2019d like to filter, the column contains data from two different sources. I\u2019d like to normalize this data. We collected some data a certain way and the other rows of data contain data that was collected another way. There are rows that contain 1.2 2.3 3.4 and nothing over 5. I would like to multiply these numbers by 1,000 to match up with the others and remove the comma from the numbers above 1,000.\n\n\n\n\ncol1\ncol2\n\n\n\n\n1\n1.641\n\n\n2\n1.548\n\n\n3\n1,807.000\n\n\n4\n1,759.000", "I have only tried one solution of those given.\nFloat64. What you talked about is accurate @fmarz10. I wanted to filter the rows and apply a transformation, then remove something. This first row of code works perfect, it just worked.\ndf.loc[df[\u2018col2\u2019]<=5,\u2019col2\u2019] = df[\u2018col2\u2019]*1000\nI did, however, refrain from using the second suggestion to as some numbers are not just whole numbers and contain values at least two places into the decimal. To complete it, I did something like this and it looks good from just scanning the first few rows.\ndf[\u2018col\u20192\u2019] = df[\u2018col2\u2019].replace(\u2018,\u2019,\u2019\u2019)\nvs the original suggestion:\ndf[\u2018col\u20192\u2019] = df[\u2018col2\u2019].str.replace(\u2018,\u2019,\u2019\u2019)\nNOTE: This works, but this is weekly data, and each row is a week\u2019s worth of data and there are about 15,000 rows, so I need to just graze a few before making an assessment, but the first few look good.", "I have a data frame that I\u2019d like to filter", 0.0, 0, 164, "This should work\n\\begin{itemize}\n  \\item `gsub(\"\\\\D*\", \"\", col2)`\n    - removes the 0s, blanks and numbers\n   \\begin {code}", 0.06936416029930115, 0.0, 0.04624277353286743, 0.03958577290177345], ["On the column that I\u2019d like to filter, the column contains data from two different sources. I\u2019d like to normalize this data. We collected some data a certain way and the other rows of data contain data that was collected another way. There are rows that contain 1.2 2.3 3.4 and nothing over 5. I would like to multiply these numbers by 1,000 to match up with the others and remove the comma from the numbers above 1,000.\n\n\n\n\ncol1\ncol2\n\n\n\n\n1\n1.641\n\n\n2\n1.548\n\n\n3\n1,807.000\n\n\n4\n1,759.000", "One other thing I thought about was the type mixing in Python. Based on what you have above, my guess is either you have col2 as string or float. If string, then go through the replace method to get rid of the string. If float, then you shouldn't need to replace the comma (that may be how Python shows thousands and millions but I can't remember specifics).\nRun print(df.dtypes) to check.", "I have a data frame that I\u2019d like to filter", 0.049958374351263046, 1, 165, "The code below should help you achieve this:\n\\begin{itemize}\n  \\item Created a new column with a function: `col3 = col2*10^3`\n    \\begin {code} col3 <- col1* 0,1*(1/1e3)\n   \\end{code}.\nThis function converts the values in the first column to a \n      decimal number with 4 decimal places. For instance, if  the\n       column has a value of 6.789, it will be converted to  `6790` (the\n        6th digit is removed and multiplied by a thousand).\n \\em{This is done by multiplying the number by `1 /  (1 *  e^(3))` and\n     then rounding the result to `4` decimal digits} .\nThe value `0.,1` is used to get the right number of decimals. You can\ncheck the value in `?formals`.\nIt also removes the commas in numbers that are greater than 99\nand removes leading zeros. This is achieved by doing the following:  a) if the character is `\",\"` it", 0.19819819927215576, 0.00909090880304575, 0.11711711436510086, 0.037528425455093384], ["On the column that I\u2019d like to filter, the column contains data from two different sources. I\u2019d like to normalize this data. We collected some data a certain way and the other rows of data contain data that was collected another way. There are rows that contain 1.2 2.3 3.4 and nothing over 5. I would like to multiply these numbers by 1,000 to match up with the others and remove the comma from the numbers above 1,000.\n\n\n\n\ncol1\ncol2\n\n\n\n\n1\n1.641\n\n\n2\n1.548\n\n\n3\n1,807.000\n\n\n4\n1,759.000", "It sounds like you want to filter some rows (col2 < 5), apply a transformation (col2 * 1000) then remove something (,).\ndf.loc[df['col2']<=5,'col2'] = df['col2']*1000\nNext would be to remove the comma but if you know all the values in col2 are whole numbers (no decimals) then I think you can just\ndf['col2'] = int(df['col2'])\nBut its safer to apply a replace but only if the values are string (if not, df['col2'] = str(df['col2']))\nThen you can apply the following:\ndf['col'2'] = df['col2'].str.replace(',','')", "I have a data frame that I\u2019d like to filter", 1.2000000476837158, 1, 166, "\\begin{code}\ndf = df.withColumn(\"col3\", F.expr(\"1024*col\")).na.fill(\"0\")\n\\end{pre}", 0.055045872926712036, 0.0, 0.055045872926712036, 0.0], ["I have some indices that I have to apply retention policies to.\nIndice-a-date_of_creation 30 days\nIndice-b-date_of_creation 180 days\nIs there a way to set retention policies to those Indices on Kibana?\nIf not, how can I set them on elasticsearch?", "Since ELK 6.6 (XPack) there is index lifecycle management.\nIn the ELK 7.16 you can use Index Lifecycle Policies in kibana\n\nStack Management > Index Lifecycle Policies. Click Create policy.\n\n\nIn older versions as your indexes contain timestamp you can write script to generate list of indexes to delete and then run loop over such list and call\ncurl -XDELETE", "Set retention days to a Elasticsearch indices, using kibana or Elasticsearch itself", 1.2000000476837158, 2, 167, "You can apply the policy by issuing the below command. You have two options, you can either apply it from k-elasticsearch or from the elastic-cli\n\\begin{itemize}\nSet Retention Policy\nRetention Policies can be applied to an index directly in Elastic\nSearch. If you want to implement a retentions policy, consider\nsetting up a master index, and creating a separate index for each\nretention policy. This way, each index will have a policy that is\napplicable to it, but the master will not be subject to that policy\nbecause the retribution policy is applied at the index level. Here is a\nsample query that applies the setting policy to the user index. The\npolicy will set the default retiration value for all documents created\nfrom now on to 900,0.1.2.3.4.5.6.7.8.9.0 and all future data in the\nindex will be set to this value. Setting this retirement policy in\nthis manner means that all existing data will remain untouched. Use\nthe settings API to create a global retire policy on all indexes in a cluster:\ncurl -XPUT", 0.1927710771560669, 0.03238866478204727, 0.08835341036319733, 0.04682273790240288], ["i'm doing k-mean clustering on an image (fruit tree image) with k=4 clusters. when i display 4 clusters seperately, fruits goes to cluster1, stem goes to cluster 2, leaves goes to clster3 and background goes to cluster4. i'm further interested in fruit clutser only. the probelm is when i change image to another fruit tree image, fruit cluster goes to cluster2 or sometimes to clsuter3 or 4. my wish is to not change the cluster for fruit, means if fruit is in cluster1 it should be in cluster1 in all images of fruit tree. how can i do that? 2ndly if its not possible i want to select that cluster automatically which contains fruit. how can i do that? thanks in advance.", "K-means clustering is unsupervised, meaning the algorithm does not know any labels. That is why the clusters are assigned at random to the targets. You can use a heuristic evaluation of the fruit cluster to determine which one it is. For example, based on data about the pixels (color, location, etc), and then assign it a label by hand. In any case, this step will require human intervention of some sort.", "How to choose required cluster after k-means clustering in python opencv?", 0.1973753273487091, 1, 168, "I think you are missing the fact that the k means algorithm is based on the assumption that each cluster is unique. So you can't simply assign an already assigned cluster to a new image. What you need to do is assign the new images to the nearest cluster and then check if that's the correct cluster, if not reassign it.", 0.4000000059604645, 0.09022556245326996, 0.2222222238779068, 0.04510095715522766], ["I'm running a python script in VSCode on a remote server and I want to save a dataframe that is generated in that script locally. Is this somehow possible? Thanks!", "You can save the dataframe to a directory (maybe in .csv) on the remote server and download it from the explorer in VSCode by right-clicking on that file.", "Locally save a dataframe from a remote server in VSCode", 1.2000000476837158, 1, 169, "You can do it by using the `local_path` in the [remote source](https://code.visualstudio.com/docs/editor/integrated-terminal#_integrating-with-remote-sources) option.\n<a href=\"https: //i.stack.imgur.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nWhen you write a command in a terminal in `remote_source` option, it will be executed on the remote machine and the output will appear in your terminal. By specifying the local path in this option you can save the data locally on your machine.", 0.2678571343421936, 0.09090909361839294, 0.2321428507566452, 0.06293706595897675], ["Until now, I have always used SAS to work with sensitive data. It would be nice to change to Python instead. However, I realize I do not understand how the data is handled during processing in pandas.\nWhile running SAS, one knows exactly where all the temporary files are stored (hence it is easy to keep these in an encrypted container). But what happens when I use pandas data frames? I think I would not even notice, if the data left my computer during processing.\nThe size of the mere flat files, of which I typically have dozens to merge, are a couple of Gb. Hence I cannot simply rely on the hope, that everything will be kept in the RAM during processing - or can I? I am currently using a desktop with 64 Gb RAM.", "If it's a matter of life and death, I would write the data merging function in C. This is the only way to be 100% sure of what happens with the data. The general philosophy of Python is to hide whatever happens \"under the hood\", this does not seem to fit your particular use case.", "Are temporary files generated while working with pandas data frames", 0.0, 0, 170, "\\begin{blockquote}\nBut what is happens, when using pandas?\n\\end{itemize}", 0.08955223858356476, 0.0, 0.08955223858356476, 0.0625], ["I have a retail dataset that consists of uncleaned mobile phone numbers. I have data like this\n\n\n\n\nPhone Number\n\n\n\n\n03451000000\n\n\n03451000001\n\n\n03451010101\n\n\n03451111111\n\n\n03459999999\n\n\n03459090909\n\n\n\n\nNow there is a very high probability that the above phone numbers are fakely entered by cashier. The genuine number looks like this for example 03453485413.\nThere are two important things:\n\nThe length of the string is always fixed 11 characters\nThe phone number always starts with 03*********\n\nNow how do I eliminate phone numbers based on the rule that, for example, character repetition of more than 5 times eliminated?", "You should use regex to find such patters.\nFor example:\n(\\d)\\1{4,}\nThis will match a digit and check if it repeats itself 4 more times. This is the case in examples 1, 2, 4 & 5\nAnother example is: (\\d\\d)\\1{2,}\nThis will match 2 digits and checks if it repeats itself 2 more times. This is the case in examples 1, 3, 4, 5 & 6", "Python Pandas phone numbers cleaning by eliminating consecutive repeated characters", 0.0, 0, 171, "Here's one way using `numpy.unique` to get all unique phone-number strings, and then using a list comprehension to filter out those with more-than-five repetitions: `num_repeats` is the number of characters in the phone string: so for a string of length 7, `len(phone_num)` is 4, while `5` would be the threshold. Finally, we use `sum()` to count the total number occurrences of each string, then use that to eliminate all but the few with the smallest occurrencies. (If you wanted to use a threshold of, say, 8, just change the `<= 6` in `range(len(...))` to `== 9`.)\n\\begin{itemize}\n  \\item Get the unique strings (one-character strings will be in a different list):\n<a href=\"https://i.stack.imgur.com/XR1Xe.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n <a\n href=\n\"https:/i\n.s\ntack\n .co\nm/pictu\nre.ph\notos/2021/06/1", 0.18433180451393127, 0.009302325546741486, 0.1105990782380104, 0.03663003817200661], ["W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\nI tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nAfter this, there comes a traceback error which says on the last line: \"from tensorflow.summary import FileWriter\nImportError: cannot import name 'FileWriter' from 'tensorflow.summary' (C:\\Users\\HP\\tetris-ai\\venv\\lib\\site-packages\\tensorboard\\summary_tf\\summary_init_.py)\nAfter installing tensoflow gpu again, I got this error\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.6.2 requires keras<2.7,>=2.6.0, but you have keras 2.7.0 which is incompatible.\ntensorflow 2.6.2 requires tensorflow-estimator<2.7,>=2.6.0, but you have tensorflow-estimator 2.7.0 which is incompatible.\nSuccessfully installed keras-2.7.0 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.0\nBut my issue with the dll and traceback error continued.In Vscode and in pycharm.", "It could be that you need a Nvidia GPU, CUDA is the language NVIDIA uses.\nYou can check if you have one following these steps: Windows -> Task Manager.", "Could not load library cudart64_110.dll with tensor flow gpu installation", 0.0, 0, 172, "You should install Tensorflow using the pip3 command. If you use the \"pip\" command, it will install the latest version. Try this:\npip3 install tensorflow\nand then try the ckpt command again\n(This is for python 3, you need to install pip and pip  install for Python 1)", 0.21052631735801697, 0.054054055362939835, 0.10526315867900848, 0.07874016463756561], ["I am currently working on a deep neural network, but i am confused about how we can compute the training time of a deep neural network. How i will know that my neural network takes less time compared to other deep neural networks.\nI am looking forward to your help and any article recommendation.", "If you are using a jupyter notebook or any notebook using a .ipynb file then you can use the: %%time, to calculate the time to run the cell.\nIf you are planning to use a .py code and just want to calculate the time the code runs you can utilise the time library before and after the training portion of the code, you could use the following method\n\nfrom time import time\nstart = time()\n\"Your code\"\nprint(time()-start)", "how we can compute the training time of deep neural networks?", 1.2000000476837158, -1, 173, "Computation time is not the only thing that matters when comparing different neural net architectures. For example, if one neural nets has a big number of parameters and another has less, the second one would probably be more efficient. Also, you can make a neural-net architecture that is more computationally efficient and requires less training steps, by for example using a convolutional neural architecture.", 0.1944444477558136, 0.028169013559818268, 0.1388888955116272, 0.051575928926467896], ["I am scraping reviews off Amazon with the intent to perform sentiment analysis to classify them into positive, negative and neutral. Now the data I would get would be text and unlabeled.\nMy approach to this problem would be as following:-\n1.) Label the data using clustering algorithms like DBScan, HDBScan or KMeans. The number of clusters would obviously be 3.\n2.) Train a Classification algorithm on the labelled data.\nNow I have never performed clustering on text data but I am familiar with the basics of clustering. So my question is:\n\nIs my approach correct?\n\nAny articles/blogs/tutorials I can follow for text based clustering since I am kinda new to this?", "I have never done such an experiment but as far as I know, the most challenging part of this work is transforming the sentences or documents into fixed-length vectors (mapping into semantic space). I highly suggest using a sentiment analysis pipeline from huggingface library for embedding the sentences (in this way you might exploit some supervision). There are other options as well:\n\nUsing sentence-transformers library. (straightforward and still good)\nUsing BoW. (simplest way but hard to get what you want)\nUsing TF-IDF (still simple but may simply do the work)\n\nAfter you reach this point (every review ==> fixed-length vector) you can exploit whatever you want to cluster them and look after the results.", "Clustering text data based on sentiment?", 0.1973753273487091, 1, 174, "It's a bit hard to say anything without seeing some data, but you seem to be going in the right direction. I'd recommend the [HDBSCAN](https://en.wikipedia.org/wiki/Hierarchical_clusters_with_application_to_scalable_graph_matching) algorithm, because it is able to cluster even with very few observations. If you're working with Amazon reviews, I think that you can get quite a few for each product. For example, the review page for [this product](http://rads.stackoverflow.com/amzn/click/B00A27K4CU) shows 116 reviews. But I still recommend starting with a very small dataset to make sure that everything works. Once you have a good algorithm working, you should be able (with a larger dataset) to find out which features of the reviews are important for your classification. This will allow you to tune the algorithm to maximize its performance.", 0.25954198837280273, 0.023076923564076424, 0.12977099418640137, 0.042414359748363495], ["I need to download NSE Futures data since 2012 for my strategy backtesting. I tried NSEpy and jugaad-data libraries but they are giving one day's data at a time.\nI tried Getbhavcopy as well but the data is not accurate there.\nIs there any other free source to download the same.\nThanks,\nMohit", "I've used NSEpy, this is basically scraping from the NSE website, better to use some API which actually has the right to provide the data. e.g: Samco, angel one APIs.\nthey are free as well.", "Download Historic NSE Futures Data", 0.0, 0, 175, "You can download historical data from Nifty.com:\nhttps://niftypedia.nseindia.co.in/content/historical-niftynse-futures-daily-close-prices\nIt has the last 15 years of data. You have to register first, but it'll be free.", 0.1599999964237213, 0.027397260069847107, 0.13333334028720856, 0.04117647185921669], ["Is there a possible approach for extracting sentences from paragraphs / sentence tokenization for paragraphs that doesn't have any punctuations and/or all lowercased? We have a specific need for being able to split paragraphs into sentences while expecting the worst case that paragraph inputted are improper.\nExample:\nthis is a sentence this is a sentence this is a sentence this is a sentence this is a sentence\ninto\n[\"this is a sentence\", \"this is a sentence\", \"this is a sentence\", \"this is a sentence\", \"this is a sentence\"]\nThe sentence tokenizer that we have tried so far seems to rely on punctuations and true casing:\nUsing nltk.sent_tokenize\n\"This is a sentence. This is a sentence. This is a sentence\"\ninto\n['This is a sentence.', 'This is a sentence.', 'This is a sentence']", "This is a hard problem, and you are likely better off trying to figure out how to deal with imperfect sentence segmentation. That said there are some ways you can deal with this.\nYou can try to train a sentence segmenter from scratch using a sequence labeller. The sentencizer in spaCy is one such model. This should be pretty easy to configure, but without punctuation or case I'm not sure how well it'd work.\nThe other thing you can do is use a parser that segments text into sentences. The spaCy parser does this, but its training data is properly cased and punctuated, so you'd need to train your own model to do this. You could use the output of the parser on normal sentences, with everything lower cased and punctuation removed, as training data. Normally this kind of training data is inferior to the original, but given your specific needs it should be easy to get at least.\nOther possibilities involve using models to add punctuation and casing back, but in that case you run into issues that errors in the models will compound, so it's probably harder than predicting sentence boundaries directly.", "Sentence tokenization w/o relying on punctuations and capitalizations", 0.0996679961681366, 1, 176, "I'm not sure of the technical feasibility of your idea, but I would start by trying to detect sentences using punctation and case, and see how you do. The problem here is that the sentence is still a collection of words, so you have to have some way to decide that one group of 3 words is \"a sentence\".\nYou could start with a simple heuristic: split on spaces, then on commas, stop when you find the end of a paragraph. Then you can look at the words in each chunk and determine if it's a full sentence or not. If it has at least a noun, verb, object, or subject, it is probably a complete sentence, otherwise it may be a fragment. For example, \"This, is, a, sentence.\" is not a valid sentence but \"I, love, apples.\" probably is. You can then use some sort of regex to make sure that it looks like a real sentence (if you don' t have capitalization, you could just make all the sentences have uppercase n, v, o, s).\nIf you are concerned about the case of sentences with lowercase letters, I suggest you use a regular expression to convert the whole thing to", 0.3980099558830261, 0.08500000089406967, 0.15422885119915009, 0.05062713101506233], ["Is there a possible approach for extracting sentences from paragraphs / sentence tokenization for paragraphs that doesn't have any punctuations and/or all lowercased? We have a specific need for being able to split paragraphs into sentences while expecting the worst case that paragraph inputted are improper.\nExample:\nthis is a sentence this is a sentence this is a sentence this is a sentence this is a sentence\ninto\n[\"this is a sentence\", \"this is a sentence\", \"this is a sentence\", \"this is a sentence\", \"this is a sentence\"]\nThe sentence tokenizer that we have tried so far seems to rely on punctuations and true casing:\nUsing nltk.sent_tokenize\n\"This is a sentence. This is a sentence. This is a sentence\"\ninto\n['This is a sentence.', 'This is a sentence.', 'This is a sentence']", "The only thing I can think of is to use a statistical classifier based on words that typically start or end sentences. This will not necessarily work in your example (I think only a full grammatical analysis would be able to identify sentence boundaries in that case), but you might get some way towards your goal.\nSimply build a list of words that typically come at the beginning of a sentence. Words like the or this will probably be quite high on that list; count how many times the word occurs in your training text, and how many of these times it is at the beginning of a sentence. Then do the same for the end -- here you should never get the, as it cannot end a sentence in any but the most contrived examples.\nWith these two lists, go through your text and work out if you have a word that is likely to end a sentence followed by one that is likely to start one; if yes, you have a candidate for a potential sentence boundary. In your example, this would be likely to start a sentence, and sentence would be likely to be the sentence-final word. Obviously it depends on your data whether it works or not. If you're feeling adventurous, use parts-of-speech tags instead of the actual words; then your lists will be much shorter, and it should probably still work just as well.\nHowever, you might find that you also get phrase boundaries (as each sentence will start with a phrase, and the end of the last phrase of a sentence will also coincide with the end of the sentence). It is hard to predict whether it will work without actually trying it out, but it should be quick and easy to implement and is better than nothing.", "Sentence tokenization w/o relying on punctuations and capitalizations", 0.0, 0, 177, "I think you're looking for [nltkv3.Tokenizer](https://nLTK.readthedocs.io/en/latest/n-grams.html#nldoc-nntk-Tokenizers). The [sentence_delimit](http://%20https%3A//nLtk.org/api/tokenizers/sentencetokenize.htm#sentencedelimit) method does the trick: it relies on the spaces between the sentences. It works in both cases: with or without capitalization, with punctations or not.", 0.12054794281721115, 0.027548208832740784, 0.07671232521533966, 0.03488371893763542], ["Can someone explain me, how I can recover the previous state of my table in jupyter notebook? For example, I have a table with a column \"prices\" and I accidentally had set all this numbers to 0. How I can make a stepback to recover previous values of numbers in \"prices\". Thank you in advance", "I'm not entirely sure if it will work but you can try navigating to your ipython directory and checking the history.sqlite file there might be a previous state of the table stored there somewhere!\nIf you're on windows just enter ipython locate, navigate to that directory and you will find it inside of profile_default.", "How I can recover the previous state of my table in jupyter notebook?", 0.0, 0, 178, "You can always go to previous cells by pressing \"Ctrl+A\" key. Then you can delete any cell(s) and cells below will be recovered to the state they were in before.\nIf you want to restore the table to its original state, just go back to where you had it and select cells in the range you are interested in. In the toolbar you should see a button which says \"Reset to default\". Click it. You will see all the cells recovering to their original value.", 0.30344828963279724, 0.0559440553188324, 0.17931035161018372, 0.07138577848672867], ["I was trying to train a model using tensorboard.\nWhile executing, I got this error:\n$ python train.py  Traceback (most recent call last): File \"train.py\", line 6, in <module> from torch.utils.tensorboard import SummaryWriter   File \"C:\\Users\\91960\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\", line 4, in <module> LooseVersion = distutils.version.LooseVersion \nAttributeError: module 'setuptools._distutils' has no attribute 'version'.\nI'm using python 3.8.9 64-bit & tensorflow with distutils is already installed which is required by tensorboard.\nWhy is this happening ? Please help !", "This command did the trick for me:\npython3 -m pip install setuptools==59.5.0\npip successfully installed this version:\nSuccessfully installed setuptools-60.1.0 instead of setuptools-60.2.0", "AttributeError: module 'setuptools._distutils' has no attribute 'version'", 1.0, 22, 179, "According to this [post](https://github.com/pytorcheval/torchtb/issues/88#issuecomment-993090011) you have to add `pip install setuptool`", 0.11538461595773697, 0.03999999910593033, 0.11538461595773697, 0.0], ["I have found the optimal results for 7 hyperparameter namely:\n\nNumber of layers,\nNode size,\nActivation functions,\nlearning rate,\nmomentum,\nbatch size,\noptimizer\n\nUsing Optuna multiobjective optimization. I minimized the training and validation loss as my objectives. Since the number of tuning parameters are more I reduced the number of epoch per trail as 50. Then I got the best parameters, post Optuna optimization. I increased the epoch size and build the same model with torch.manual_seed. But the results obtained after the same 50th epoch is different from what I got in the Optuna results.\nWhat is the reason am I missing anything? I want to reproduce the same results for the same condition!", "Finally, I was able to find the reason for improper reproducibility. In my code I used two different objective functions; def train(trial) and def layer(trial). I pivoted the second objective function into the train(trial). Also, specifying a manual seed is also important. Anyway there will be slight deviations of 0.0001%.", "Results reproducibility using Pytorch and Optuna for DNN", 0.0, 0, 180, "You should be using the exact same seed for each model.  This is to ensure the randomness is consistent.", 0.19178082048892975, 0.0, 0.10958904027938843, 0.08138976246118546], ["I have a list of tuples with the start and end node ID of a segment. I want to rearrange the tuples (segments) in a way that constructs a continuous path. There can be a loop in the path i.e., one node can be traversed multiple times. I do know the origin and destination of the entire path. There may be some segments that cannot be included in the path as their start or end node cannot be connected to the previous or next node to form that continuous path. These segments should be removed from the rearranged list of tuples.\nExample:\nThe list of tuples may look like the following. Also, I know that the path should start at 101 and end at 205.\n[(101, 203), (104, 202), (203, 104), (104, 208), (185, 205), (202, 185)]\nExpected Results:\n[(101, 203), (203, 104), (104, 202), (202, 185), (185, 205)]\nI would like to solve this in Python. My understanding is that this can be solved using a loop that looks at the end node ID of each segment, finds a segment with a similar start node ID, and adds that append that segment to the list of rearranged tuples. I am not quite sure how it can be solved efficiently. So, any hints or example codes would really help.", "The main challenge in this question is probably just choosing between tuples with the same starting/ending node, e.g. choosing between whether (104, 202) or (104, 208) should come after (203, 104). You didn't really make it clear of what conditions the resultant path must satisfy so I assume any continuous path is fine.\nIn such a case, this question can simply be framed as a shortest path question, so you just need to find the shortest path between node 101 and 205, and the weight of each vertice is just 1 if the node tuple exist in the set, else infinity. So for example. the vertice between 203 and 104 is 1 since (203, 104) is in the original list, on the other hand the weight between 203 and 208 is infinity, since the vertice doesn't exist. You can then apply the standard Dijkstra shortest path algorithm to obtain the nodes traversed and therefore the tuples.\nEdit\n\nI think I misunderstood what you are trying to do, so I think you are actually trying to include as many nodes as possible for the path created. In such a case, perhaps something like breadth first search would be possible, where you keep track of all possible paths and keep deleting those that have no further possible connections.", "How to rearrange a list of tuples (start node ID, end node ID) to create continuous path in Python?", 0.0, 0, 181, "You can do it with [itertools.groupby](https://docs.python.org/3/library/itertool.html#iterable-combinations) and the help of [collections.deque](http://www.programiz.com/python-programming/collections-deques)\nIt will iterate over the tuple lists and get the next one that can follow and append it to a deque list. Once the dequeue list has all of your tuple sets it will loop and pop the first tuple to be the head of queue and add it at index 0. Then it removes the last element of deQueue. The result will be in sorted order\nHere is a code: (note that I've removed the \"origin and dest\" code, it was not relevant to your problem) :\nYou can find the [output here](www1.cs.colostate.edu/~donglei/papers/sicomp19/code/my_code.py) (in python 3.6)", 0.24444444477558136, 0.016759777441620827, 0.12222222238779068, 0.042319752275943756], ["I'm training a neural network on stimuli which are being developed to mimic a sensory neuroscience task to compare performance to human results.\nThe task is based on spatial localization of audio. I need to generate white noise audio in python to present to the neural network, but also need to alter the audio as if it were presented at different locations. I understand how I'd generate the audio, but I'm not sure on how to generate the white noise from different theoretical locations.", "You can add a delay to the right or left track, to account for the arrival time at the two ears. If I recall correctly, it amounts to up to about 25 or 30 milliseconds, depending on the angle. The travel distance disparity from source to the two ears can be calculated with basic trigonometry, and then multiplied by speed of sound in air to get the delay length. (IDK what python has for controlling delays or to what granularity delay lengths can be specified.)\nMost of the other cues we have for spacial location are a lot harder to quantify. Most commonly we use volume, of course. Especially for higher-pitched content (wavelengths smaller than the width of the head) the head itself can block and cause some volume differences, based on the angle.\nBut a lot comes from reverberation for environmental cues, from timbrel roll-off as a function of distance (a quiet sound with lots of highs in the mix can really sound like they are right next to your ear), from moving the head to capture the sound from different angles, and from the filtering effects of the pinna of the ear. Because everyone's ear shape is different, I don't know that there is a universal thumbnail algorithm for what causes a sound to be sensed as originating from a particular altitude for a given angle. I think to some extent we just all learn by experiencing the sounds with our own particular ears while observing the sound source visually.", "Generating Spatial White Noise audio in Python", 0.0, 0, 182, "You can use [this](https://www.pygame.org/docs/tut/sounds.html) to get started. The code should be something like:\n\\begin{itemize}\n  \\item Load the sound (you can get the sample rate by using `pyautogui.pixel_to_screen_coords(100, 150)`)\n    \\begin {code} p.load_sample(filename, samplesize=None, channels=2, samplerate=sample_rate, stereo=False) # sample_size is optional\n        p = pygame.mixer.Music(sound, looping=True, volume=volume)  # loop the music\n   \\end{code}.\n \\em{You can load any sound file with this.}\n2. Load and play the sinewave.  You will need a sinesweave with a frequency of `1` to represent silence. This is done by creating a `sineswave` and setting the `frequency` of the wave to `freq=1`.\n3. You need an array of size `samplesize` containing samples of amplitude. For example, if the", 0.21938775479793549, 0.025641025975346565, 0.12755101919174194, 0.04838709905743599], ["So I am doing a face recognition project in a Raspberry Pi. It is a django project and when I run the server I get this error: AttributeError: module 'cv2' has no attribute 'face'\nI searched for the error and I came up with that I needed to install opencv-contrib-python\nThe problem is that when I try to install it the download gets to 99% and I get this: 'pip3 install opencv-contrib-pyt\u2026' terminated by signal SIGKILL (Forced quit).\nDoes anyone know why this happens? how can I fix this? help is much appreciated", "I got the same error and fixed it like this.\nTry:\n\nsudo apt install python-opencv libopencv-dev\n\ny/n --> y\n\nsudo apt install python python-pip\n\ny/n --> y\n\npip install opencv-python", "Can't install opencv package in a Raspberry pi", -0.1973753273487091, -1, 183, "You have to use the correct python version for your RPI, it seems you are using python 3.8, which is not supported by openCV.\nYou should use python3 instead.", 0.15625, 0.0, 0.125, 0.038759686052799225], ["I have a dataset of a disease with about 37 features, all of them are categorical variables except for two which are \"Age\" and \"Age_onset\". The number of entries is 219.\nI am working on developing a binary classification model that predicts whether patients have this disease or not.\nThe current problem I am facing is deciding what suitable model to select giving the categorical nature and volume of my data.\nNow the categorical variables are not high-cardinality, even after applying one-hot encoding the number of variables increases from 37 to 81 therefore it is still considered low in dimensionality. Thus the feature selection methods are not needed.\nMoreover, the data is not large in terms of the number of entries (219) and dimensionality (81), therefore there is no need to go for complex models such as neural network or ensemble methods.\nThis rules out a large number of models and by far I think the best candidate is the Logistic regression classification model.\nMy question: is this line of reasoning valid? or should I attempt to use complex models and through trial and error I can arrive at the best model in terms of performance and results?\nI have gone through many articles and papers with regard to handling categorical data in classification problems, however, my data contains no continuous variables (except for two) and it is not high in cardinality meaning all of the categorical variables have two or three possible answers (highlighted by the number of features after applying one-hot encoding which is 81). So I am not sure that the solutions discussed in those articles applies to my problem.", "Is this line of reasoning valid?\n\nYou do not have a low number of variables. This number of variables might be considered high, especially considering your low number of entries. As you likely know, scikit-learn is built to handle huge numbers of entries, large datasets. I theorize (not assume) that you have a low number of entries because you cleaned the data too much. You might try another cleaning method that interpolates missing values.\nPurely from an academic perspective with statistics and later studies in data science, I suggest that even better than interpolation with the cleaning, you could gather more data.\nOn another note, no, the reasoning is not valid (see above).\n\nShould I attempt to use complex models and through trial and error I can arrive at the best model in terms of performance and results?\n\nI would, in your position, try all the models you can, with as many different parameters as possible.", "Handling small dataset of categorical data in Binary classification problem", 0.0, 0, 184, "Yes, that line is valid. A model with one hidden layer and no regularization will not work well with 100 features. You will need a more complex model, either a multilayer perceptron or a random forest.", 0.15544041991233826, 0.0, 0.10362694412469864, 0.060240961611270905], ["A question regarding filtering using a list of values. I want to do the following:\n\nfilter a dataframe based on certain criteria\ncreate a list of (one column, containing ID's) of this dataframe\nnext i want to exclude this list from another dataframe.\n\nall individual steps are working using the following code:\ndf3 = df2.loc[df2['value']  < parameter] (1)\nmy_list = df3['ID'].tolist() (2)\nfinal_df = df[~df['column'].isin(my_list)] (3)\nyet somehow filtering the frame using the first step results in the final step NOT working (so not filtering anything). When i remove the first step it again works like a charm. Does anybody know what i am doing wrong?\nkind regards,\nAlex", "Changed datatype to 'integer' between step 1 and 2.", "How do i filter using a list created from filtered dataframe?", 0.0, 0, 185, "This is because you are setting the `in` parameter to `True`. You are doing this by passing the list as a parameter. If you remove that parameter, it should work.", 0.05128205195069313, 0.0, 0.05128205195069313, 0.11624661833047867], ["I have a dataset with dates encoded as strings formatted as %B %d, %Y, eg September 10, 2021.\nUsing:df['sale_date'] = pd.to_datetime(df.sale_date, format = '%B %d, %Y')\nproduces this error ValueError: time data 'September 10, 2021' does not match format '%B %d, %Y' (match)\nManually checking with strptimedatetime.strptime('September 10, 2021', '%B %d, %Y') produces the correct datetime object.\nIs there something I missed in the pd.to_datetime?\nThanks.", "Upon further investigation, I found out that the error only happens on the first element of the series. It seems that the string has '\\ufeff' added to it. So I just did a series.str.replace() and now it is working. Sorry for the bother. Question is how did that BOM end up there?", "String to date in pandas", 0.0, 0, 186, "The issue is in `format = %`, it should be `%` instead. `to-date` method does a string to datetime conversion with a default format of `'%d/%m/%Y'`\nSee the docs [here](https://pandas.pydata.org/docs/reference/api/pandas._libs.tslibs/strftime.html?highlight=strtotime#pandas-tslib-strf-%25d-%5Cd%2F%5D%7B%60-%61-%7D)", 0.14678898453712463, 0.0, 0.11009174585342407, 0.05999999865889549], ["I am building ARIMA/Sarima Model in percentages but getting following error\n1-\nmodel = SARIMAX(np.asarray(train), order = (0, 1, 1), seasonal_order =(1, 1, 1, 12))\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n2- If i don't covert pandas data frame to numpy array i get following error\nmodel = SARIMAX(train, order = (0, 1, 1), seasonal_order =(1, 1, 1, 12))\nValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\nthough few days back same code was working which I am using in step 2", "issue was with the input data format, few rows had commas so it was reading it as string. Once i removed it is working fine.", "when I run Arima/Sarima Model getting errors - ufunc 'isnan' not supported for the input types or Panda on numpy cast", 0.0, 0, 187, "There are two errors in your code. First, in line 4, the type of `train` is a `pandas.core.series.Series` object, which cannot be passed to `SARMAX` directly. So, you need to cast it to a numpy.array first:\n`train = np_array_from_series(pandas_dataframe)`.\nThis is because `Pandas` has its own set of functions for handling different types of data.\nSecond, `isna` takes the values of the array as input. In your case, it is `np_arrange_train`, so you should pass that. `pd.notnull(df_input)` would work as well.", 0.13445378839969635, 0.017094017937779427, 0.10084033757448196, 0.01923076994717121], ["In the sklearn documentation for sklearn.cross_validation.ShuffleSplit, it states:\nNote: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.\nIs this an issue? If so, why?", "Contrary to the most often used KFold cross validation strategy, the Shuffle Split uses random samples of elements in each iteration. For a working example, let's consider a simple training dataset with 10 observations;\nTraining data = [1,2,3,4,5,6,7,8,9,10]\n\nKFold (k=5)\n\n\nShuffle the data, imagine it is now [6,9,1,4,10,5,7,2,3,8]\nCreate folds; Fold 1 = [6,9], Fold 2 = [1,4], Fold 3 = [10,5], Fold 4 =\n[7,2] and Fold 5 = [3,8]\nTrain keeping one fold aside each iteration for evaluation and using all others\n\n\nShuffle split (n_iter=3, test_size=0.2)\n\nIt works iterative manner where you specify number of iterations (default n_iter=10 in sklearn)\n\nEach iteration shuffle the data; [6,9,1,4,10,3,8,2,5,7], [6,2,1,4,10,7,5,9,3,8] and [2,6,1,4,10,5,7,9,3,8]\nSplit into specified train and evaluation dataset as chosen with the hyper-parameter (test_size); Training data are [6,9,1,4,10,3,8,2], [6,2,1,4,10,7,5,9] and [2,6,1,4,10,5,7,9] respectively. Test data are [5,7], [3,8] and [3,8] respectively.\n\nAs you can notice, although the shuffle is different (technically it can be same), the training and testing data for the last two iteration are exactly same. As the number of iterations increase, your chance of fitting the same dataset increases which is counter-intuitive to the cross-validation idea where we would like get an estimate of generalizability of our model with limited amount of data. On the other hand, the datasets usually contains numerous observations so that having the same (or very similar) training and test datasets is not an issue. Keeping number of iterations high enough improves the generalizability of your results.", "Shuffle split cross validation, what are the limitations?", 1.2000000476837158, 2, 188, "This is not an [issue](https://en.wikipedia.org/wiki/Issue_(computer_science)). This statement means that the foldings generated by `ShuffledSplit` are not guaranteed to be \\em{different}.\n\\begin{itemize}\n_{This is a different question than the one you asked.}\nFor example, if you use a dataset with 100 observations and split it into 5 splittings, you can have a 2-fold split with splited dataset containing 99 observations, and the remaining 3 splitted dataset can also be a two-folds split dataset. This can happen since a random splitting algorithm can generate the same splitting in multiple runs. However, this can still be valid cross validations, as long as the datasets are different. As stated in the [documentation](http://scikit-learn-tutorial.readthedocs.io/en/latest/cross-validated-model-selection/strategies.html#shuffle-split-strategy),\nThe number of cross folding is usually set to 4 or 6. In general, for a given dataset and a crossvalidation strategy, a small number", 0.2921810746192932, 0.05371900647878647, 0.1316872388124466, 0.04215456172823906], ["hi am working on a project which is detecting heart failure and now I want to use the k_means algorithm for clustering and SVM for classification.\nI need to know if I can split the dataset into training and testing? since am using k_means is it ok??\nplease help...thanks", "Yes, you can cut randomly in two sets. You can cut in sequential sets. You can cut in large temporally-adjacent tests. That is what the ANOVA tests are all about.", "can we split the dataset in k means into testing and training?", 0.1973753273487091, 1, 189, "The short answer is yes, you can use k-meals on the training data and use SVC on test data. It is not a good idea to split a data set in such a way that you have to do the same operation multiple times. But this is a common practice.", 0.19512194395065308, 0.05000000074505806, 0.1463414579629898, 0.05252827703952789], ["In fact, I use a neural network consisting of four layers of input and two hidden one for exit and I had 17 features to enter in order to classify or predict something, but the range of weights in the network should be between 1 and -1 and I used the pygad library but when I print the solutions it gives me the range  Between 9 and -9, I used the activation function ReLu for the two hidden layers and sigmoid strong text for the exit layer. Please help", "The range of the weights exceeds the initial range (-1 to 1) because of the mutation. You can control the mutation in PyGAD using these 2 simple ways:\n\nSet the mutation_by_replacement parameter to True. In this case, no gene will exceed the -1 to 1 range.\nSet  init_range_low=-0.5  and  init_range_high=0.5 but also set the 2 parameters random_mutation_min_val and  random_mutation_max_val to small values. For example, random_mutation_min_val=-0.2 and  random_mutation_max_val=0.2. This option just tries to lower down the values created out of mutation. But there is possibility that the values get outside the -1 to 1 range.", "Is the range of weights in the optimized neural network in the genetic algorithm between 9 and -9 correct?", 0.0, 0, 190, "I can't see the code that you are using, however the fact that the weights are in a range between -8 and 8 suggests that they have been constrained to be in this range. This is typically done using a bias parameter, and the bias is generally used to give an overall mean of the output, which is why it has a much greater weight than the other weights.", 0.2526315748691559, 0.042553190141916275, 0.1473684161901474, 0.03435078263282776], ["In a part of my research work, I am in need to split a deep Learning model(CNN) into two in order to run the model in two different devices in a distributed way. Here, I split a CNN model into two, A and B where the output of A will be the input for B. If anyone has an idea, please.", "What model are you splitting?\nIn PyTorch it is possible to do so. Check the children function of a model in pytorch and reconstruct the layers for two sub models.", "Splitting a CNN model to run in two devices", 0.0, 0, 191, "This is a common practice in CNN research. You can do it using a single model with the following steps:\n\\begin{itemize}\n  \\item Define a split point on the first layer, that is, one of the layers that does not have fully connected layers. This layer usually has convolutional layers, max-pooling layers and fully-connected layers in some order. So if the layer has 10 convolutions, 5 max pooling and 2 fullyconnected then split the 7th layer into 3 (the first convolution, the maxpool, and the fullyconected).\n \\end{blockquote}", 0.23333333432674408, 0.016949152573943138, 0.11666666716337204, 0.05262550711631775], ["I'm using cartopy to draw global histograms with matplotlib.\nThere is a part in the map which I don't want to show, but it is in the longitude and the latitude that I need (so setting an extent can't help here).\nI know there is a way to create a mask for an area, but I'm having a little trouble with it.\nThe area that I want to crop isn't a country or anything specific, just a coastline...\nCan I create a mask using the Long and the Lat values only?\nThe goal is to show only the Mediterranean sea, and not anything else, so I don't want to see the northern coasts of France and Spain that aren't a part of the Mediterranean coastline.\nThank you guys,\nKarin.", "I used ax.add_patch(rectangle) to cover that part in the plot.", "cropping unwanted map part in cartopy", 0.1973753273487091, 1, 192, "This should work\n[See this example](https://github.com/Pierro1/cartopy/issues/146)\n\\begin{code}\nimport cartopry as cp\nfrom matplotlib import pyplot as plt\nfig, ax = plt.subplots()\ncp.crosshair(ax)  # make a cross hair\n# create the world map\nm = cp.Map(width=100, height=70) #, projection='robin',\nax.add_subplot(m)   # add the subplot to the axes\nplt.title('Plotting Cross Hair on the Map')\nlons = np.linspace(0.,360.01,120).reshape(-1)    # lons\nlat =  np_from_array(np.random.uniform(15,45),dtype=np_float_as_numpy_integer_dtypes)[:,None].reshuffle(25)                                 # lat\nlon = m(lats) + 0\nx, y = ax.transData.transform_point((lon,lat))\nc", 0.06504064798355103, 0.01652892492711544, 0.06504064798355103, 0.017543859779834747], ["I have student data which contains column 10_grade which consists both percentage and cgpa values mix.. I need to convert 10_grade column into percentage. A python code will be helpful", "Its been sorted.. now i want to plot these 3 categorical variables together 1) Graduation_Location (Mumbai region,south region,north region,others)\n2) Course_Country (us and canada, asian countries, European countries, others)\n3) status (hold reject , others)\nI was able to plot two wiith the help of pd.crosstab\npd.crosstab(df.Grad_Location,df.Course_Country)", "how to convert a column in percentage which contains both cgpa and percentage values", 0.0, 0, 193, "You can convert the values in your column by using the `astype` method.\nFor example:\n`df['column_1'] = df['coloumn_2'].astypes(int)`\nHere, you can use any type to the value in the column and the other column will automatically get the type as well.", 0.13333334028720856, 0.0, 0.07619047909975052, 0.04626292362809181], ["I have recently got started with spark. I am using Python as the language for the spark application. What happens if we execute pure python code as a spark application (Spark). Does it will be executed on an executor or the driver itself.\nSince the main function runs on spark, I think it should be on driver, but I have also read that spark does not do any computation, so will it run on executor then ? I may be missing something here. Would appreciate if anyone could explain this.", "The code you write (your \"main\" function) will be executed on the driver, and when you will operate on distributed data (e.g. RDDs and others), that executor will coordinate with the workers and handle that operation on them (in a sense, those operations will be executed on the workers).", "Pure Python / Python specific code in Pyspark application", 0.5370495915412903, 3, 194, "It runs in the cluster, on the executors. Spark doesn't have any specific \"driver\" or \"executor\".\nThe \"master\" is just the server that is controlling the executors. It's not the same thing.", 0.2142857164144516, 0.024390242993831635, 0.190476194024086, 0.04430379718542099], ["Today when I launched my Spyder the IPython console immediately showed me a note:\n\nraise ValueError(f\"Key {key}: {ve}\") from None\nValueError: Key backend: 'qt4agg' is not a valid value for backend; supported values are ['GTK3Agg', 'GTK3Cairo', 'GTK4Agg', 'GTK4Cairo', 'MacOSX', 'nbAgg', 'QtAgg', 'QtCairo', 'Qt5Agg', 'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']\n\nI tried to update matplotlib in anaconda command line, but after the updating it still appeared. How can I cope with this problem?", "(Spyder maintainer here) This is a bug in Spyder because we still offer the possibility to set the Qt4 backend, but Matplotlib removed it some versions ago. So please change your backend to other value.\nWe'll fix this problem in our 5.3.0 version, to be released in March 2022, by removing that option.", "I met an matplotlib backend error just after Spyder launched", 0.0, 0, 195, "As the error message says, the key 'backend' has to be one of the values listed in the `supported_backend` [doc](https://matplotlib.org/api/_as_gen/matplotlibs.backend_info.html?highlight=backend#matplolibs.BackendInfo.supported) of `matplots` module.\nMake sure you're using a supported backend.", 0.17475728690624237, 0.019801979884505272, 0.11650485545396805, 0.02631578966975212], ["Is there a method in graph-tool through which checking whether two nodes are connected (as first neighbours) or not without having to iterate?\nFor example, something like graph_tool.is_connected(v,u) which returns a boolean depending on whether v and u are or not connected vertices. Something like a function to check just whether a certain edge exists.\nThanks", "It is solved by checking the result of the function g.edge(v,u). If add_missing=False it just returns None whenever the edge does not exist. Thanks to @NerdOnTour for the comment", "Check whether two vertices are connected using graph-tool on Python", 0.1973753273487091, 1, 196, "You can use `graph_graph.adjacent_vertices` to get a list of adjacent vertices for a given vertex. It's not really clear what you're trying to do here. You'll need to clarify.", 0.14492753148078918, 0.0, 0.11594203114509583, 0.05442177504301071], ["I trained and tested a model in python with Keras and save it in SavedModel format.\nThen I imported it with the C-API and TF_LoadSessionFromSavedModel().\nI predict the same set of data in python and C but get different results.\nThe predictions in python are ok, the predictions in C are not ok, but also not fully nonsense.\nThings I have checked:\n\nTensorflow Version 2.5 in Python and C\nuse of the same model\nuse of the same data with same format\nload the SavedModel in Python again\ntry different arcitectures\ntrain without keras in low-level tensorflow\n\nEvery time python results are good, C results are different and worse.\nIs there something wrong with the SavedModel Format regarding Python and C?\nOre any other tipps to solve this problem?", "The Problem was, that the dataset was normalized in python as type float64\nand in C as type float32.\nSame type with same normalization gives the same result.\nThanks @GPhilo for your comment. It was the right direction!", "Different prediction of Tensorflow in Python and C-API with SavedModel", 0.0, 0, 197, "In the case of a saved model, there are two different data structures that the Kerase API uses: the graph and the model. The graph is a graph of operations (e.g. a list of nodes), the nodes are the operations and edges are links between the different nodes. It is the internal representation of Kerasa and is not a Tensoflow API. When you load a SavieModel, you are loading the node graph. You can find more information here: https://www.tensorflow.org/guide/saved_model#loading_savedmodel_graph\nA model is an abstraction of this graph, which is actually the result of compiling the TFLearner into the tensorflow graph that you get from `tf.get_default_session().graph`. You are getting different predictions because you do not have the exact same graph (nodes and connections) in both cases. I recommend to look at the `graph` variable in the code, that's the most important.", 0.11398963630199432, 0.010471204295754433, 0.07253886014223099, 0.022093886509537697], ["I have a SQL Server v2017 at work.  When they installed machine learning it installed Python 3.5 with Pandas 0.19.  I am trying to use read_excel on a file on a network drive.  I can run the script on my local machine, but I have Python 3.9 and Pandas 1.35.  The Script works fine locally but not when executed through the Server using EXECUTE sp_execute_external_script.  I realize there could be a huge number of things that coul dbe causeing problems, but I need to rule out Pandas version first.  The server is locke own adn it takes a lot of red tape to change something.\nCan Pandas 0.19  read_excel access excel files on a UNC address.  I know the newer version can, but this would help me rule out the Pandas library as a source for the issue.", "(I work for MS and I support SQL ML Services)\nThe short answer to your question is -\nYou will have a hard time accessing a UNC path in ML Services. It is technically possible, but the complications make it a no-go for many. You didn't show your code or errors, but I can assure you that your problem isn't with pandas, and perhaps you got an error about not being able to 'connect' because we disable outbound network traffic from ML services by default... but if you got past that, then you probably got an authentication error.\nThe long answer to your question is -\nSQL 2016 and 2017 - We use local 'worker' accounts. The default names (they are based on your instance name) are MSSQLSERVER01,02,03... 20. (There are 20 by default... there is also a MSSQLSERVER00, but we'll ignore that one).\nThe Launchpad service is ran by it's service account (default: NT Service\\MSSQLLaunchpad), and it can be ran as a domain account. But, it is not launchpad that is actually executing your R/Python code. Launchpad kicks off the R process, and it does this under the MSSQLSERVERXX users. It is THAT user that is technically running your code, and therefore, it is that user that is trying to connect to your UNC path and not YOUR user that you are logged into SQL as. This user is a local user - which cannot authenticate across a UNC share. This issue comes down to a design limitation.\nIn Windows, there is no way to provide a username/password in your UNC path (whereas, in Linux, you can). Using a mapped drive will not work because those are local-to-your-user-and-login-session. Therefore, a mapped drive of one logged in user will not be accessible to other users (and therefore the MSSQLSERVERXX users).\nIn short, if you absolutely wanted to make it work, you would have to disable authentication entirely on your network share. In Windows, this is more than just adding \"EVERYONE\" permissions to the file. You would also have to allow GUEST (or in the *nix world, ANONYMOUS) access to file shares. This is disabled by default in all recent Windows versions and you would have to modify various gpos/registry settings/etc to even allow that. It would not be my recommendation.\nIf this were in an AD environment, you could also theoretically allow the COMPUTER account of your SQL host so that ALL connections from THAT \"COMPUTER\" would be allowed. Again, less than ideal.\nIn SQL 2019 - we got rid of the local user accounts, and use appcontainers instead. This removes the need for local user accounts (many customers in large organizations have restrictions on local user accounts), and offers additional security, but as always, with more security comes more complexity. In this situation, if you were to run the launchpad service as a domain user, your R/Python processes ARE executed as the LAUNCHPAD account (but in a very locked down appcontainer context). Theoretically, you could then grant THAT service account in AD access to your remote UNC share... BUT, appcontainers provide a far more granular control of specific 'permissions' (not file level permissions). For example, at least conceptually, when you are using an app on your phone, or perhaps a Windows store UWP app, and it asks 'do you want to allow this to access your camera?\" - those layer of permissions are something that appcontainers can provide. We have to explicitly declare individual 'capabilities', and we do not currently declare the ability to access UNC shares due to several other security implications that we must first consider and address. This too is a design limitation currently.\nThe above possibilities for SQL 2016/2017 do not apply, and will not work, for SQL 2019.\nHowever, for all of them, while it may not be ideal, my suggestion and your best option is:\n\nReconsider which direction you are doing this. Instead of using your SPEES (sp_execute_external_scripts) code to access a network share, consider sharing out a directory from the SQL host itself... this way, you at least don't have to allow GUEST access, and can retain some level of permissions. Then you can drop whatever files you need into the share, but then access it via the local-to-that-host path (ex: C:\\SQL_ML_SHARE\\file.xel) in your SPEES code.", "Pandas 0.19 Read_Excel and UNC Addresses", 0.0, 0, 198, "As per the [pandas docs](https://pandas.pydata.org/docs/reference/api/pandas._io.excel.read_csv.html) it looks like `read.csv` requires a local path, not a remote one. You should be able to do this with `pandas_readers`:\nhttps//github.com/pydml/pandar-read-excel/tree/master/read\n`pandas-xlsx` also looks promising:\nhttp://xlsxtra.net/python-docs/#pandas\nI think I would start with one of these two options as they're more established.", 0.07090464234352112, 0.007352941203862429, 0.04156479239463806, 0.04627249017357826], ["I encountered an issue that when I use Series.str.len() in pandas query method, and actually all the functions for Series.str is not supported in some of my envs, but work in other envs, and all these envs have almost same version of pandas and numpy. (I'm sure Series.str.xxxxx could work in all my envs before)\nEnv1\nPython 3.9.7\nnumpy==1.21.4\npandas==1.3.4\nWhen I ran pd.DataFrame(columns=['core_text']).query(\"core_text.str.len()>1\"), it raised\n\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/frame.py\", line 4060, in query\nres = self.eval(expr, **kwargs)\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/frame.py\", line 4191, in eval\nreturn _eval(expr, inplace=inplace, **kwargs)\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/computation/eval.py\", line 353, in eval\nret = eng_inst.evaluate()\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/computation/engines.py\", line 80, in evaluate\nres = self._evaluate()\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/computation/engines.py\", line 120, in _evaluate\n_check_ne_builtin_clash(self.expr)\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/computation/engines.py\", line 36, in _check_ne_builtin_clash\nnames = expr.names\nFile \"/Users/huhon/miniconda3/envs/venv_dev/lib/python3.9/site-packages/pandas/core/computation/expr.py\", line 834, in names\nreturn frozenset(term.name for term in com.flatten(self.terms))\nTypeError: unhashable type: 'numpy.ndarray'\n\nEnv2\nPython 3.9.9\nnumpy==1.21.4\npandas==1.3.4\nIt works perfected.\nAnyone can help? Thanks in advance!\nHong", "The problem is probably with the versions of numexpr. This module evaluates all string expressions for Pandas like query or pd.eval.\nThe solution is to upgrade your version of numexpr (or remove and reinstall it).", "TypeError: unhashable type: 'numpy.ndarray' when I use Series.str.len() in pandas query?", 0.0, 0, 199, "Your issue seems to be with your environment. This should work:\n\\begin{itemize}\n  \\item `pandas-13`\n    \\begin {code} <-- Install this\n        pip install pandas==2.0.5\n     \\end {itemize\n   \\strong{or}  `numpy-22` and `pip install numpy==3231`  (This is the latest version available) [numpy is a dependency of Pandas]\n       \\em{For more info, read [this](https://towardsdatascience.com/the-complete-guide-to-installing-and-upgrading--1b0c1c86f52c)}\n      \\section{Note: for windows users, you should try to install conda} [here](http://docs.conda.io/projects/conda/versions/1/user-guides/install/windows.html) is how to do it\n \\textbf{This should solve the issue}", 0.24637681245803833, 0.0, 0.1304347813129425, 0.06681100279092789], ["Firstly I guess this question might be duplicated, but I couldn't find it.\nQuestion. It's actually in the title.\n\nCan I get Essential Matrix from Fundamental matrix with few matching keypoints, but without intrinsic parameters?\n\nSituation. I am trying to find Essential matrix. I use my phone camera to take photos, and then extract keypoints using SIFT(or ORB). I have 2 images of an object and it's matching points. I could get F, Fundamental Matrix but I have no idea how to get Essential Matrix from this.\nI don't have camera intrinsic parameters, such as Fx, Fy, Cx, Cy.\nI am stuck to this situation. I googled but couldn't get answers.\nLet me know if it's duplicated then I'd delete this.\nPLUS: I also don't have camera coordinates or world coordinates.", "No you can't, you need to know the metric size of the pixel and the focal because essential matrix is in the real world.", "Can I get Essential Matrix from Fundamental matrix with few matching keypoints, but without intrinsic parameters?", 1.2000000476837158, 0, 200, "The essential matrix will be of the form\n\\begin{equation}\nE = \\left(\\begin {array}{cc}a&b\\\\c&d\\end {\narray}\\right),\n \\end{eqnarray} where a and b are the fundamental matrix parameters and the entries of d are zero. So the essential matix is given by the following:\n$$\n  E =\n    \\begin\n        {bmatrix} \\frac{F_{11}}{||F_1||}& -\\frac{(F^T_{21})}{\\sqrt{\n   ||F||^2}}\\\\\n     \\sqrt{\\frac{|F|^2}{||G||_2^4}} & \\quad\n       \\dfrac{G_{33}}{|G|} \\\\\n      \\qquad \\ \\  \\text{where}  |F| =  ||G ||_4 \\ |G|^T \\\n \\\\ \\hline\n\\\\ \\noalign{\\hrule}  0 &\n-\\frac{\\sqrt{|G_3|^4} G_{44}}{\\sqrt{{||\nG |_\n|^3}}}\n         \\\\ 0\n & 1\n                  \\\\   0   &  1\\\\", 0.15748031437397003, 0.04800000041723251, 0.12598425149917603, 0.009615383110940456], ["I think this is a very basic question, my apologies as I am very new to pytorch. I am trying to find if an image is manipulated or not using MantraNet. After running 2-3 inferences I get the CUDA out of memory, then after restarting the kernel also I keep getting the same error: The error is given below:\nRuntimeError: CUDA out of memory. Tried to allocate 616.00 MiB (GPU 0; 4.00 GiB total capacity; 1.91 GiB already allocated; 503.14 MiB free; 1.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nThe 'tried to allocate memory(here 616.00 MiB) keeps changing. I checked the GPU statistics and it shoots up while I try to do the inferencing. In tensorflow I know we can control the memory usage by defining an upper limit, is there anything similar in pytorch that one can try?", "So during inferencing for a single image I realized there was no way I was able to fit my image in the gpu which I was using, so after resizing the image to a particular value I was able to do the inferencing without facing any memory issue.", "pytorch cuda out of memory while inferencing", 1.2000000476837158, 1, 201, "You can use `torch.set_device(0)` to force it to use the CPU.\nYou can also use [`torchaudio.AudioProcessing.infer_with_cpu(n_samples, n_channels)`](https://pytorchorchaudio.readthedocs.io/en/stable/reference/torchanimproved.html#torchat.pyaudioapi.torchy.audio_processing.PyAudioInference.cpu_inference) which is equivalent to using `insp.float32_to_int1`, but will allocate less memory and be much faster. You should use this when you have small batch sizes, less than 30, otherwise it might take 8-10 minutes to process a 70 second audio file.", 0.11678832024335861, 0.0, 0.08759123831987381, 0.031111110001802444], ["I'm working on merging two datasets on python; however, I'm running into a sorting issue while preparing the excel files for processing.\nExcel 1 sorts A-Z of project ID's as:12.a2.b3\nHowever, excel 2 sorts A-Z as:132.a2.b\nHow do I make sure  they both sort as excel 1?\nI've changed format of columns from General to number for both and it's still similar outcome.", "IMHO, sorting is unnecessary. u want:\n\nmerging two datasets on python\n\nThus, just import/merge both data 1st.. then sort in python.. just from looking in the output file you can see if some of the row label IS actually different. Eg : \"2.a\" vs \"2.a \"", "A-Z sorting is different between two excel files", 0.0, 0, 202, "This is a very old question, but I thought I would post the answer. I ran into the same problem when I had to merge two large excel data files that were sorted differently. The solution was to convert the data to a csv and then sort it on the first column.", 0.2222222238779068, 0.020618556067347527, 0.12121212482452393, 0.054176390171051025], ["I have a numpy array of shape (10, 10, 1024, 1024, 3). This represents an 10x10 grid of images, each of shape (1024, 1024, 3) (1024x1024 color images). I want to reshape this into one array of shape (10*1024, 10*1024, 3), where the 1024x1024 patch of pixels in the upper left of the new image corresponds to the [0, 0] index of my original array. What's the best way to do this using numpy?", "This should do the job: np.swapaxes(arr,1,2).reshape(10*1024, 10*1024, 3). Note that swapaxis generates an array of shape (10, 1024, 10, 1024, 3).", "Reshaping a 2D array of images using numpy", 0.0, 0, 203, "Use:\n\\begin{code}\narr = np.reshape(arr, (a.shape[0], a.size//a[...,0].size, a[...]\n    np_new = arr.copy()\nnp_reshaped =  np_.new_axis(np_.resample(a, np_, np_(newshape=arr.newaxis()))\nprint( np[np.argsort( a[:,0]), :,np._new[:-1]] )\n# array([[ 48,  53,   0]], dtype=uint8)\na[:] = a._sort_indices()[a._indptr[1:-1]:, :]\nnew  = array( [np_, arr[::- ndim]], shape = new_shape )#array([<xarray.core.dimension.Dimension object at 95677409272208>,\n                                       #  <xray.array object (x, y, z, c)>],\n                dtype = <class 'numpy.ndarray'>, shape=<(1,), dtype='int64'>", 0.1034482792019844, 0.0, 0.08620689809322357, 0.0703125], ["I want to calculate maximum diameter of a 3D binary mask of a nodule (irregular shape).\nI have implemented a function that calculates the distance of all boundary points between each other. This method is very computational expensive while dealing with tumors or larger volume.\nSo my Question is what can be the possible methods to calculate maximum diameter of a 3d binary mask which is less computationally expensive.", "Something similar to a Gradient Descent could be implemented.\n\nStart with 2 points (A and B), located randomly around the 3D mask\nFor point A, calculate the direction to travel on the 3D mask that will most increase the distance between him and point B.\nMake point A take a small step in that direction.\nFor point B, calculate the direction to travel on the 3D mask that will most increase the distance between him and point A.\nMake point B take a small step in that direction.\nRepeat until it converges.\n\nThis will very likely find local maxima, so you would probably have to repeat the experiment several times to find the real global maxima.", "Calculate maximum diameter in 3D binary mask", 0.1973753273487091, 1, 204, "You can't. A 2D mask is a set of points. If you use 1D distance, you'll get a distance that is not necessarily symmetric. So you need to use the Euclidean distance. The Euc. distance is symmetric and well defined, but is computation expensive because you have to find the closest point from all the others.", 0.2298850566148758, 0.04651162773370743, 0.18390804529190063, 0.03984064236283302], ["in a linear regression model let's say we have 3 independent variable (age, height, gender) and one dependent variable (diabetic) and then we split the model as, X train- i.e. (say 70%) data of independent variables for training, X test-> i.e. 30% data of independent variables for testing\ny train-> i.e. (say 70%) data of dependent variable for training, y test-> i.e. 30% data of dependent variables for testing\nso when we do predict X-test, or predict X-test, are we predicting values of independent variables or are we predicting the dependent variable (diabetic?)", "So our goal in training is given some features or dependant variables or X ind a model that predicts the independent variable or y from X. We mostly do that by minimizing some loss function f(M(X), y). When it comes to testing, we want to see how good our model is when applied to examples that were not part of the training set. So we take our trained model M, feed it with our features X_test and check how good it predicts y_test.", "i need clarity with prediction of X test, X train, y test, y train", 0.0, 0, 205, "In linear modeling, you can have your training set (train) as well as test set. This is important because you want to test your model on the test data before you use it on any other data. Otherwise you could end up overfitting the data and your test performance could be better than your actual performance in real life.\nSo, for example, say you have a dataset of 100 observations (x,y) where x is height and y is diabetes. You split your data into train and test sets. Now, your train data could have the following distribution:\n<a href=\"https://i.stack.imgur.com/3QPzX.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nHere, there are 2 peaks. So your algorithm would have to choose a model that can predict the height well (i,e., in the range 40 to 90) but predicts diabetics correctly as 0 or 60. If you train your x, and it performs well on training data but fails on test (68% accuracy), you know that your X is not the reason for your poor performance. Instead, it'd be your y that", 0.27106228470802307, 0.036900367587804794, 0.12454212456941605, 0.062322940677404404], ["in a linear regression model let's say we have 3 independent variable (age, height, gender) and one dependent variable (diabetic) and then we split the model as, X train- i.e. (say 70%) data of independent variables for training, X test-> i.e. 30% data of independent variables for testing\ny train-> i.e. (say 70%) data of dependent variable for training, y test-> i.e. 30% data of dependent variables for testing\nso when we do predict X-test, or predict X-test, are we predicting values of independent variables or are we predicting the dependent variable (diabetic?)", "We are predicting the dependent variable i.e diabetic.\nYou can compare your results with Y test to get accuracy of your model.", "i need clarity with prediction of X test, X train, y test, y train", 0.0, 0, 206, "The predictive model is built on the training set. If you have a large enough training dataset, you can use all the available data to build the predictor. You then use the built predictors to predict the outcome on a new test set and measure the error. This is a supervised learning model.\nhttps://www.kdnuggets.com/2017/07/-supervised-vs-unsupervise...\nIn your example, the 25% of the data that is used to train the classifier is split randomly into a training and test dataset. The test data is the \"unknown\" data. It'll be used for measuring the accuracy of your model on this data set, and the train data will be the source for the predictions.", 0.14084507524967194, 0.05714285746216774, 0.1267605572938919, 0.05713222175836563], ["I have 2 lists\n\n[A,B,C,D,E,F] - first list\n[X,X,X,X,X,X] - second list\n\nI would like to take last element of the first list and if there are any elements in the second list move them to the left and add the element as last.\n\n[A,B,C,D,E,F]\n[X,X,X,X,X,X]\n[A,B,C,D,E]\n[X,X,X,X,X,F]\n[A,B,C,D]\n[X,X,X,X,F,E]\n\nTill there is only first element in the first array, so it would stop at:\n\n[A]\n[X,F,E,D,C,B]\n\nI'm quite new to Python, I would really appreciate some help", "You can use for loop for this.\nAnd you can access the last elements by using -1 as index values.", "Python changing last elements of list", 0.02856365591287613, 1, 207, "You can use [`zip()`](https://docs.python.org/3/library/functions.html#zip) to iterate over the two lists in parallel and build up the new list: `[item[0] for item in zip(first_list, second_lits)]`.\nIn Python 3.5, you can also use [list comprehension](https:/docs/python/reference/expressions.assignment.list#list-comprehension).\nIf the lists are too long, it may be better to do the operation in a [generator](http://wiki.ycombinator.com/Python_Generators) using a `yield` statement: `for item, i in enumerate(zip(your_long_first, your_second_short)): yield item[i]`.\nThis will build a list of tuples of (item, position), and you just need to reverse the list to get the desired result.", 0.1560283750295639, 0.057553958147764206, 0.12765957415103912, 0.10227272659540176], ["Let's say I have a table that has 5 rows and 10 columns:\n\nRow 1 has 3 missing values\nRow 2 has 2 missing values\nRow 3 has 8 missing values\nRow 4 has 5 missing values\nRow 5 has 2 missing values\n\nI would like the function to return me row 2 & 5", "df.isnull().sum(axis=1) will return the number of missing values per rows.\nmin(df.isnull().sum(axis=1)) will return the minimum missing values for a row\ndf[df.isnull().sum(axis=1) == min(df.isnull().sum(axis=1))] will return the rows that have the minimum amount of missing values", "Return rows that have the minimum amount of missing values pandas", 0.0, 0, 208, "I think you need: `df.ix[np.argsort(df['Column'].isnull())[0]]`\n\\begin{itemize}\n    \\item `is null` to check if a value is null\n        `np` is numpy package, `argsor` takes a list of values, returns the indices of the position of that value in the list\n          `ix` selects the rows/columns according to the index\n           `[0]` selects only the first element from the array\n      \\em{Note that you can also use `None` for missing data.}\n   \\begin {code}", 0.23076923191547394, 0.015625, 0.13846154510974884, 0.02823973074555397], ["Would there be \u201cdilution\u201d of accuracy if I train the same text classification model with multiple training datasets? For example, my end users would be providing (uploading) their own tagged CSVs to train the model and use the trained model in the future. The contexts of datasets would be different - L&D, Technology, Customer Support, etc.\nIf yes, how do I have a \u201cseparate instance or model\u201d for each user?\nI am using Python and would possibly use Gradio or Streamlit as the UI. Open to advice.", "I ended up using huggingface's zero-shot classification.", "Text Classification - Multiple Training Datasets", 0.1973753273487091, 1, 209, "This depends on your dataset, but if it is sufficiently large and varied, then there is a chance of dilution of your accuracy, yes. This is because your model will have to learn a lot of different concepts to correctly label the texts in your datasets, and this may dilute the power of the labeling. In other words, it may not be able to pick out the concepts that are most important. That said, if your data is large enough, you can also use a larger number of training data to avoid diluting your power. You can even use all your training text to build your first model, which will learn to label a subset of it. Then, use those results to select your best labels for your second model. Repeat this process until you get your desired accuracy. It is an iterative process. I hope this helps.", 0.012738853693008423, 0.0, 0.012738853693008423, 0.03985726088285446], ["I have a task about clusterization in python. When I did this clusterization I need to check the result with business logic.\nI dont see the pattern in solved clusters. Next, I decided to do post analysis with correlation. I take one cluster and calculate a correlation pairwise. In calculation I used whole feature unlike a clusterization when I used only 3.\nI got a high level of correlation from 0.99 to 1 in whole cluster. For me it means that algorithm watched the logic in cluster.\nBut, i did this clusterization to solved a problem with banks data (i wont to see the client's pattern like (issued amount > 50.000,age < 22, salary < 80.000 - this client, for instance bad)). And I cant see the business logic, for me it's random data.\nWith this description I have a question. How can i check the logic in the clusters except a simple self-checking ?\nI think there are 2 reasons. First, my clusterization is bad and I need to write a new one. Second, the data is bad and I need to check data and do a post analysis\nI did a BIRCH cluster with StandardScaler.", "All of verification methods are 'empirical'.\n\nYou can compare the different methods of clusterization and choose the best one.\nThe correlation comparison methods:\na) If correlation approximately 1. You need to calculate a row's average and median. Next step you compare this two value and drop bads row.\nb) If corr are different in whole matrix. Calculate averages for all rows and compare the value and the mean average; choose god one like this 'value > mean(avg)'", "Are there any verification methods how good cluster I got?", 1.2000000476837158, 0, 210, "If you are going to use this for prediction, then I would suggest to start from a good model. If your data already exists and you want to work with it, try to find a way to transform the original data into a predictable one that you can use to build a model (if possible).\nIf you have the possibility to change the input data, you should transform it in a form that can be used to create a very good predictive model, so that the cluster analysis is more reliable. The easiest way is to scale the variables. You can also do some feature selection to remove redundant features or features with little influence on the model performance. See this [link](https://scikit-learn.org/stable/modules/feature_selection.html) for more info. Also, as said by others, it would be better to take a random sample of the dataset and run the clustering algorithm to evaluate the results.", 0.19327731430530548, 0.008474576286971569, 0.1260504275560379, 0.05137892812490463], ["i am new in tensorflow i have a question : Can i put in my conv2d an input with shape not fixed ?\ninputs.shape =(?, 1568)\nwhen i train my neural network i get this message :\nraise ValueError(f'Input {input_index} of layer \"{layer_name}\" ' ValueError: Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (Dimension(None), Dimension(1568))\nmy con2d layer is like this :\n x = Conv2D(32, (3,3), padding=\"same\",input_shape=input_shape[1:])(inputs)", "I resolved that by using tf.expand_dims", "can I use conv2d with input dimension not fixed?", 1.2000000476837158, 0, 211, "If you don't have fixed input size, then you can use `Conv3D` with `padding=\"valid\"`.\n`Convolution with padding \"valid\"` will not produce any change in the input shape.", 0.0, 0.0, 0.0, 0.058713316917419434], ["I am quiet certain I installed everything correctly, and the required file is clearly in my path. I am not sure what I can do at this point. Please Help.\necho %path%\nC:\\Users\\idvin\\anaconda3\\envs\\3.7;C:\\Users\\idvin\\anaconda3\\envs\\3.7\\Library\\mingw-w64\\bin;C:\\Users\\idvin\\anaconda3\\envs\\3.7\\Library\\usr\\bin;C:\\Users\\idvin\\anaconda3\\envs\\3.7\\Library\\bin;C:\\Users\\idvin\\anaconda3\\envs\\3.7\\Scripts;C:\\Users\\idvin\\anaconda3\\envs\\3.7\\bin;C:\\Users\\idvin\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\MATLAB\\R2020b\\bin;C:\\Users\\idvin\\Downloads\\elastix-4.9.0-win64;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2022.1.0;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\\cudnn_cnn_infer64_8.dll;D:\\dll_x64\\dll_x64\\zlibwapi.dll;C:\\Program Files\\MySQL\\MySQL Shell 8.0\\bin;C:\\Users\\idvin\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\idvin\\AppData\\Local\\Programs\\Julia-1.6.4\\bin;D:\\dll_x64\\dll_x64\\zlibwapi.dll;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\bin\\cudnn_cnn_train64_8.dll;.", "solved it in my path i needed to add the directory of zlibwapi.dll not zlibwapi.dll itself", "Could not load library cudnn_cnn_infer64_8.dll. Error code 126 Please make sure cudnn_cnn_infer64_8.dll is in your library path", 0.1973753273487091, 1, 212, "This is most likely a permission issue. Check your directory permissions and try running CUDNN from an administrator command prompt.", 0.05263157933950424, 0.0, 0.05263157933950424, 0.01315789483487606], ["[Redacted]\nIn this example, in my final cell of code, I try to call my model.  This is following the tutorial on a Youtube video.\nIn this step, the video is able to perform the lines\nmodel = UCC_Classifier(config)\nthen in the next cell\nloss, output = model(input_ids.unsqueeze(dim=0), am.unsqueeze(dim=0), labels.unsqueeze(dim=0))\nTo successfully get a result.  However when I try and do the same thing, I get told my class is not callable.  I cannot see any difference and am unsure why this might not be callable.\nThanks", "Your UCC_Classifier model should be a pl.LightningModule, not a pl.LightningDataModule.", "Python TypeError - 'Class' object is not callable (Google Collab Example Inside)", 1.2000000476837158, 1, 213, "You have a typo. You defined `UCCClassifer` as:\n\\begin{itemize}\n`def __init__(self, config):`\nYou forgot to add the `:` after the `__init__` call. The class `Model` is defined as `class Model(nn.Module):`.\nSo the definition of your class should be: `def UCClassify(self): ...`", 0.17543859779834747, 0.036363635212183, 0.10526315867900848, 0.06408075243234634], ["So whenever i am trying to read from a source with stream i get this error \"A file referenced in the transaction log cannot be found\" and it points to a file that does not exist.\nI have tried:\n\nChanging the checkpoint location\nChanging the start location\nRunning \"spark._jvm.com.databricks.sql.transaction.tahoe.DeltaLog.clearCache()\"\n\nIs there anything else i could do?\nThanks in advance guys n girls!", "So! I had another stream that was running and it had the same parent directory as this stream.. this seems to have been a issue.\nFirst stream was looking in:  .start(\"/mnt/dev_stream/first_stream\")\nSecond stream was looking in: .start(\"/mnt/dev_stream/second_stream\")\nEditing the second stream to look in .start(\"/mnt/new_dev_stream/new_second_stream\") fixed this issue!", "Databricks streaming \"A file referenced in the transaction log cannot be found\"", 1.2000000476837158, 0, 214, "If you get the message \" A file referencing in transaction cannot found \" you should check whether the file you're trying access to is in a different path than your local path.", 0.12371134012937546, 0.0, 0.12371134012937546, 0.041378315538167953], ["tfds.load(name=\"imdb_reviews\", data_dir=direc, split=\"train\", with_info=True, as_supervised=True)\ni have download the dataset , it has downloads and imdb_reviews directories, in the imdb_reviews directory, it has plain_text directory and inside it, exists a directory named 1.0.0 and there are some files inside that. let me say the path to train is: '/content/drive/MyDrive/datasets/packt/imdb/imdb_reviews/plain_text/1.0.0/imdb_reviews-train.tfrecord-00000-of-00001' and the path to test is '/content/drive/MyDrive/datasets/packt/imdb/imdb_reviews/plain_text/1.0.0/imdb_reviews-test.tfrecord-00000-of-00001' , there are also dataset_info.json and features.json and labels.labels.txt and  an unsupervised file, how can I replace the command so that it does not cause other problems. I want to tokenize and encode it with a function\nbert_train= [bert_encoder(r) for r,l in imdb_train]\nand there is\nencoded= tokenizer.encode_plus(text, add_special_tokens=True, max_length=150, pad_to_max_length=True,truncation=True,return_attention_mask=True, return_token_type_ids=True )\ninside that encoding function.\nthank you", "I found the answer. if you give the directory to the command tfds.load() then the next time it does not download the dataset because it finds out there exits the data in your drive. so there is actually no need to replace the command with other things.", "how to replace the command tfds.load for imdb reviews with download dataset file?", 0.0, 0, 215, "You can use the function `tfds_read` to load the files:\nhttps://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#read\nThe `TFRecordsDatasets` API is deprecated and `read()` has the same functionality.", 0.2469135820865631, 0.0, 0.1975308656692505, 0.05291004851460457], ["I am writing unit tests for 2 data frames to test for equality by converting them to dictionaries and using unittest's assertDictEqual(). The context is that I'm converting Excel functions to Python but due to their different rounding system, some values are off by merely +/- 1\nI've attempted to use the DF.round(-1) to round to the nearest 10th but due to the +/- 1, some numbers may round the opposite way so for example 15 would round up but 14 would round down and the test would fail. All values in the 12x20 data frame are integers\nWhat I'm looking for (feel free to suggest any alternate solution):\n\nA CLEAN way to test for approximate equality of data frames or nested dictionaries\nor a way to make the ones-digit of each element '0' to avoid the rounding issue\n\nThank you, and please let me know if any additional context is required. Due to confidentiality issues and my NDA (non-disclosure agreement), I cannot share the code but I can formulate an example if necessary", "I'm not 100 pourcent sure I got what you are trying to do but why not just divide by 10 to lose the last digit that is bothering you?\ndivision with \"//\" will keep only the significant numbers. You can then multiply by ten if you want to keep the overall number size.", "Pandas Dataframe: Change each value's ones-digit", 0.0996679961681366, 1, 216, "For a numpy array:\n\\begin{itemize}\n  \\item `numpy.around(data, decimals=1)`\n    https://numpy-scipy-tutorial.readthedocs.io/en/latest/numpy_tutorial/arrays/basic.html#around\n \\end{blockquote}", 0.0, 0.0, 0.0, 0.01714286021888256], ["I have one large dataframe that currently has both \"?\", \"??\", and NaN values that I'm trying to remove. I want to redefine the columns to be booleans to see whether they contained \"?\", \"??\" or NaN.\nMy current approach involves cloning different columns of the dataframe based on whether they contain just \"?\", just \"??\" or just NaN values and separately iterating through the columns, col, to change the values (ex: df[col] = df[col].isnull()) and finally merging them together again.\nIs there an easier way to do this?", "how about using quick-sort algorithm. And, I am not sure I got what your data looks like and what results you want to obtain. Maybe you could show us parts of your data frame.", "How to replace values in Dataframe based on multiple conditions?", 0.0, 0, 217, "You can use `replace` with `regex`:\n\\begin{itemize}\n  \\item `str.contains` to check if the string has a `?` or `??`\n \\end{list}", 0.0714285746216774, 0.0, 0.0714285746216774, 0.0476190447807312], ["I've seen a lot of tutorials in Youtube about template matching in opencv-python, one thing they have in common is that they always uses the source image when matching the template. My question is does template matching works if the template is not from the original image? Can I use it as a simple method for object detection? And how accurate it will be? thanks", "Template matching is a technique in digital image processing for finding small parts of an image which match a template image.\nThis technique only works in the images that are almost the same. Small changes on the desired objects included in new frames can create difficulties to make a good match.\nTo detect an object(template) in an image you could use local feature descriptors and match the descriptors of every keypoint to detect if a zone has a high number of matches to assign them to the object.\nHope it works.", "Does template matching works if the template is not from the original image?", 0.0, 0, 218, "Template matching is an active research area and the question you ask is very important.\nI would say that if you do not have the same object in the background, template-based approaches do work. But, if there is a change in illumination or some other change that affects the shape of the object you are matching, then it is possible that the match will fail. This is why template based methods are more often used to identify a particular object (rather than detecting an object) as the problem you outline is likely to occur.", 0.32085561752319336, 0.07567567378282547, 0.1818181872367859, 0.09112049639225006], ["I can use print(70*\"_\")  in python to output dashed line as a separator between results.\nNow how can I output the same results in R.", "strrep(\"_\", 70) this is just a base R function\n[1] \"______________________________________________________________________\"", "Using print() function in R to print dashed lines (not in graphs)", 0.1973753273487091, 2, 219, "You can print a dashed/dotted line in the console with `cat()`.  However, the line will be displayed as is.  If you want to change it to a different typeface, you can do that with the `print.default` argument in `options()`:\n\\begin{itemize}\n  \\item [print](http://stat.ethz.ch/R-manual/library/base/html/printOpts.html) - Print the R object. Default is to the screen. To the file system, use the argument 'file'.\n \\end{blockquote}", 0.0941176488995552, 0.0, 0.0941176488995552, 0.0], ["I am working in an NLP task for a classification problem. My dataset is imbalanced and some authors have 1 only text, thus I want to have this text only in the training test. As for the other authors I have to have a spliting of 70%, 15% and 15% respectivelly.\nI tried to use train_test_split function from sklearn, but the results aren't good.\nMy dataset is a dataframe and it looks like this\nTitle   Preprocessed_Text   Label\n\nPlease let me know.", "Whit only One sample of a particular class it seems impossible to measure the classification performance on this class. So I recommend using one or more oversampling approaches to overcome the imbalance problem ([a hands-on article on it][1]). As a matter of fact, you must pay more attention to splitting the data in such a way that preserves the prior probability of each class (for example by setting the stratify argument in train_test_split). In addition, there are some considerations about the scoring method you must take into account (for example accuracy is not the best fit for scoring).", "Train/Validation/Testing sets for imbalanced dataset", 0.0, 0, 220, "There are two methods to do this. You can use the train test split, which is exactly what you are doing. This will work well if you have enough data to train a good model, since it will split the data randomly and train on 3/4 of it and test on the 2/3 of the remaining data. However, in cases where there is not enough good data, it is better to apply a preprocessing step. In this case, you can split it into two parts, the first part will be the test data and the second part the validation data that you train with. I would use a validation step since you will not have as many training samples as the rest of your data for this step, so it won'y be as accurate as if it was the entire training data set.", 0.3145161271095276, 0.04065040498971939, 0.16129031777381897, 0.05982457473874092], ["I am trying to calculate standard deviation of \"series\" for each row but the problem is every row in my column has a nested list.\nMy data frame  is like this:\n\n\n\n\nnumber\nseries\n\n\n\n\n1\n69,1,33,1,51,13,88,75,632\n\n\n2\n9,1,400,1,51,13,27,5,132\n\n\n3\n9,1,3,1,5,13,21,5,3\n\n\n4\n1,1,343,1,51,13,74,27,3\n\n\n5\n9,1,73,1,51,13,94,75,2", "If the series is a list\ndf[\"std\"] = df[\"series\"].apply(np.std)\nIf the series is a string\ndf[\"std\"] = df[\"series\"].apply(lambda x: [int(i) for i in x.split(\",\")]).apply(np.std)", "Calculate standard deviation for each row(rows contains list)", 0.664036750793457, 4, 221, "You can use `agg()` method to get the mean and sd of each series. Use `df.groupby()` to group by each value of series column. `pivot()` can be used to convert a dataframe with multiindex to a DataFrame with one column for the values and one for their counts. Then the `mean()` and `std()` methods of Series can get mean values, and the `.agg(std)` method of DataFrames can calculate the standard deviations for all the series values. Lastly, `join()` between the results of `groupby` and those of the aggregation can reconstruct the original Dataframe. The result will look like:", 0.1764705926179886, 0.014925372786819935, 0.11764705926179886, 0.04596516862511635], ["I have a network diagram that is sketched in Visio. I would like to use it as an input for the networkx graph from node2vec python package. The documentation says that there is a function called to_networkx_graph() that takes, as its input, the following types of data:\n\"any NetworkX graph dict-of-dicts dict-of-lists container (e.g. set, list, tuple) of edges iterator (e.g. itertools.chain) that produces edges generator of edges Pandas DataFrame (row per edge) numpy matrix numpy ndarray scipy sparse matrix pygraphviz agraph\"\nBut, still, not mentioning other formats like Visio, pdf, odg, PowerPoint, etc.\nSo, how to proceed?", "I think you need to create some data in the format referred to in the documentation, not just a network diagram. A Visio diagram will not do the job and I know of no way to do a conversion.", "How to convert network diagram in Visio format to a a networkx graph?", 1.2000000476837158, 2, 222, "\\begin{blockquote}\n\\strong{This solution is not robust.}\nIt only works for simple diagrams. For complex diagams, it might crash. You can still try it and report the problem to me. Thank you. -Matt} (2021/03/12)\nI have changed the title of the question. Please update your answer to reflect that.", 0.21978022158145905, 0.0, 0.1318681389093399, 0.0691823810338974], ["I have a very basic question.\nThe input is api feed from source, that has created date as a column. What I am looking to accomplish is to store this file(by splitting it up) into the following format:\nlanding/year=2020/month=01/date=01 and so on...\nThe year, month, date values are the dates from Created_at column.\nTHe file will be stored as transaction_id.parquet (transaction_id is also another column in the feed).\nWhat is the suggested option to get to this structure? Is it prefix for each file by splitting created_date into year, month, date?\nLooking for you response.\nThanks", "Your design should be something like below\n\nCreate a file in YYYYMMDD format\nlet's assume that you are receiving a file named 20220129file_name.txt\nSplit it by \"_\" to get the DATE portion\nSplit other parts such as year/month and day\nCreate another function to validate if a particular year/month/day S3 folder exists? if yes then put the file in that folder or else create the folder set and put the file.\nThere is no ready-made code for the same but you can create it. It's pretty simple.", "Store file in S3 according to Year, month,date", 0.0, 0, 223, "You can use the `partitionBy` argument in `write.partitioned`:\nhttps://spark.apache.org/docs/latest/api/scala/index.html#org.databene.sparkfun.avro.Write.Partitioned\nFor example: `val partitioned = write.csv(\"s3a://your-bucket/part-00-year-2...\")(yourdataframe)`", 0.10526315867900848, 0.015267175622284412, 0.060150377452373505, 0.03225806728005409], ["I have the following dataframe:\n\n\n\n\nName\n1 -30\nLimit\n\n\n\n\nA\n100\n1000\n\n\nB\n200\n1000\n\n\n\n\nI am trying to create a subset of this dataframe, for only the first two columns, with the following code:\nSub_DF = DF[[\"Name\",\"1-30\"]]\nThis unfortunately leads to the following error: KeyError: \"['1-30'] not in index\"\nSo my expected output should look like this:\n\n\n\n\nName\n1 -30\n\n\n\n\nA\n100\n\n\nB\n200\n\n\n\n\nI have tried using the iloc function but that did not help. I also tried to enter 1-30 without quotation marks.\nPlease find below the info about the column names:\n\nName         243 non-null    object\n1 -  30      245 non-null    float64\nCred.limit   213 non-null    float64\n\nAny tips? Please note that I am new to programming :)", "Sub_DF = DF[[\"Name\",\"1 -30\"]]", "Subset of dataframe, \"not in index \" error", 0.0, 0, 224, "Try this instead: `df.loc[~df['Name'].isin(['1','3','-','0','1']) & ~df[\"Limit\"].isna()]`", 0.380952388048172, 0.10526315867900848, 0.380952388048172, 0.0], ["Can anyone please help me?\nI am trying to run a .py script for which I need an older pytorch version, because a function I am using is deprecated in later torch versions. But I seem not to be able to install it correctly.\nI installed torch into my virtual environment using\nconda create -n my_env python=3.6.2\nsource activate my_env\nconda install pytorch==1.7.0 torchvision==0.8.0 torchaudio==0.7.0 cudatoolkit=10.2 -c pytorch\nThen I have a python file (myfile.py) that I start using a shell file (start.sh). The files are on a SLURM-cluster, so I start start.sh with sbatch start.sh.\nstart.sh\nsource activate my_env\nsrun --unbuffered python myfile.py\nmyfile.py\nimport torch\nprint(torch.__version__)\nThe print command from myfile.py still returns \"1.8.0\", but using conda list in my environment shows \"1.7.0\" for pytorch version.\nEven when I type python -c \"import torch; print(torch.__version__)\" directly into terminal, it will return \"1.8.0\" (rather than \"1.7.0\" from conda list)\nAm I doing something very obvious wrong possibly? Did I install in a wrong way, or is somehow my environment not properly loaded in the python file?\nBest regards and thanks a lot in advance", "It turned out that installing the environment as described added a link to another python installation to my PYTHONPATH (a link to /.local/python) and that directory was added to PYTHONPATH in a higher order than the python used in my environment (/anaconda/env/my_env/python/...) .\nTherefore, the local version of python was used instead.\nI could not delete it from PYTHONPATH either, but changing the directory name to /.local/_python did the trick.\nIt's not pretty, but it works.\nThanks everyone for the contributions!", "Using older torch version in conda environment not working", 0.1973753273487091, 1, 225, "I would try installing this version of pythorw in your main environment instead of your virtual env.  I think that may be your problem.", 0.1621621549129486, 0.01834862306714058, 0.09009008854627609, 0.07963323593139648], ["I'm just trying to get my head around np.empty(), I understand that it creates an uninitialized array but I'm failing to understand what that means and where the values come from. Any help would be welcomed, thanks in advance.", "numpy is not Python but a wrapper around C code. numpy.empty returns a (wrapper around) an uninitialized C array. You should never try to read a value that you have not previously written because it can be anything including a trap value on systems that have it. It is know as Undefined Behaviour (a close parent to Hell) by C programmers...", "What is an uninitialized array and what are the values returned by numpy.empty?", 0.0996679961681366, 1, 226, "The value that is returned is the number of elements in the array. If the size of the given dimension is zero, then an empty array with size 0 is created. For example,\n`np.random.rand(0, 3, (2,1))` returns an array of shape (0), (3) with values all equal to 1.", 0.156521737575531, 0.017699114978313446, 0.104347825050354, 0.0513995997607708], ["Suppose I have four matrices, a, b, c, and d.\nIn Python (with numpy), I need to do result = np.matmul(np.matmul(np.matmul(a,b),c),d) to multiply them.\nIn MATLAB/GNU Octave, I can multiply them in a much simpler manner, result = a*b*c*d.\nIs there any way to multiply matrices in Python, so that I would not have to repeatedly write np.matmul avoid nested brackets?", "Use the @ operator. result = a@b@c@d.", "How to multiply many matrices in one go in python?", 1.2000000476837158, 3, 227, "Use [broadcasting](https://docs.scipy.org/doc/numpy/reference/arrays.broad\u00f3cast.html).\nIf a is 3x3, then b is (3)x(3), so b * a will be a 9x9 matrix.  You can broadcast the product of b and a by using numpy's [dot](http://scikit-learn.github.io/scikits/sklearn/docs/generated/stats.multivariate.dot.htm).  If c is a (4) x 4, the dot product would be c * (a * b), or 12x16.", 0.1204819306731224, 0.02469135820865631, 0.09638553857803345, 0.1111111044883728], ["I am trying to compare two 13-D vectors using the cosine similarity but want all of the column entries/features to have equal weighting. Right now, I have 3 features with much larger values that appear to be too heavily-weighted in my comparison results. Is there any easy way to normalize the different features so that they are on a similar scale. I am doing this in python.", "The usual approach is for each feature x to recalculate them as x = x - np.mean(x) this will place your frame of reference at the center of the cluster, \"look to the points closer\".\nThen for each cluster x = x / sqrt(mean(x**2)), this will normalize the features, this will make the points more evenly distributed over all possible directions in the feature space.", "Cosine Similarity normalization", 1.2000000476837158, 0, 228, "You can normalise all the columns by taking their mean and divide each column by its standard deviation. The mean of all columns is the overall mean (i.e. overall centering), and the standard deviations are the scaling factors for each dimension. To use it in cosines, you can divide all your vectors by this scale factors.", 0.27642276883125305, 0.01652892492711544, 0.16260161995887756, 0.033582091331481934], ["[[0.12673968 0.15562803 0.03175346 0.6858788 ]]\nThis is how my predict function is giving its output, I want to fetch the index of the highest value.\nTried this:\npred= pred.tolist() print(max(pred)) index_l=pred.index(max(pred)) print(index_l)\nBut it seems to output only 0.\nPrinting max(pred) is giving the output:\n[0.12673968076705933, 0.1556280255317688, 0.031753458082675934, 0.6858788132667542]\nThe network uses sequential with hidden layers (embedding, BiLSTM, BiLSTM, Dense, Dense)", "You just need to use np.argmax(pred[0]). Since your pred have the shape [[]] rather than [], your element is the list of itself. So in order to get the max pred you need to use np.argmax(l[0]).", "How to capture index of highest valued array from predict function?", 1.2000000476837158, 0, 229, "Try using `np.argmax` instead of `max`, which gives you the argmax: https://docs.scipy.org/doc/numpy/reference/generated/np_core_defines.html#numpy-core-def\nIt will give you `[0,1,2,3]` as output. So, you can use `pred[np:].index()` to get the indexes of each element. You can also use numpy's `array_split` to split the array into multiple arrays.", 0.37735849618911743, 0.057692307978868484, 0.16981132328510284, 0.034285709261894226], ["I have a df with 3 columns, City, State, and MSA. Some of the MSA values are NaN. I would like to fill the MSA NaN values with a concatenation of City and State. I can fill MSA with City using df.MSA_CBSA.fillna(df.City, inplace=True), but some cities in different states have the same name.\n\n\n\n\nCity\nState\nMSA\n\n\n\n\nChicago\nIL\nChicago MSA\n\n\nBelleville\nIL\nNan\n\n\nBelleville\nKS\nNan\n\n\n\n\n\n\n\nCity\nState\nMSA\n\n\n\n\nChicago\nIL\nChicago MSA\n\n\nBelleville\nIL\nBelleville IL\n\n\nBelleville\nKS\nBelleville KS", "Keep using the vectorized operation that you suggested. Notice that the argument can receive a combination from the other instances:\ndf.MSA.fillna(df.City + \",\" + df.State, inplace=True)", "Pandas fillna with string values from 2 other columns", 1.2000000476837158, 0, 230, "\\begin{code}\ndf = pd.DataFrame({'City':['Chi','Bell'],\n              'State': ['IL','IL'], 'MS': [np.nan, 'BEL']})\n\\end{CodeInput}", 0.1304347813129425, 0.0, 0.1304347813129425, 0.01800573617219925], ["While I was constructing a simple Sequential ANN and selecting parameters for the model.compile method, I observed that Keras.metrics and Keras.losses contain capitalized as well as lowercase versions, for example tf.keras.metrics.SparseCategoricalAccuracy versus tf.keras.metrics.sparse_categorical_accuracy. I was wondering what the difference is between those versions and which one is more suitable to be used in model.compile ?", "tf.keras.metrics.SparseCategoricalAccuracy is a Class so you get an object and you can pass it over to model.compile. Since it is an object it can have state between the calls.\nHowever tf.keras.metrics.sparse_categorical_accuracy is a function and it is stateless. Both perform the same operation but their usage is different.", "What's the difference between capitalized and lowercase versions of Keras.losses and Keras.metrics?", 1.2000000476837158, 0, 231, "As far as I know, it'll be the same loss and metric function with the different case of the function. It'd be better to use the lower case version because you don't want to confuse the code.\nKeras is a wrapper of TensorFlow and TF uses the standard python convention.", 0.27272728085517883, 0.03703703731298447, 0.16363635659217834, 0.047975797206163406], ["As it is said in the headline, is there that kind of naming convention, especially in python?\nFor example there are functions in python sorted (function that returns changed object, but doesn't change object) and list or numpy.ndarray method sort (returns nothing, but changes the object).\nBut for reversed and list.reverse functions it's not quite the case, reversed returns iterator.\nIn my case I have Permutation class and I want to add inverse functions for these two cases. Should I name them inverted and inverse (last one will be out of class) or like get_inv and set_inv respectively, because these methods are just like getters and setters (which is also quite true for sorted and sort)?", "Here is my convention which has served me well in all programming languages I have used:\n\nA function with no side-effects which returns a truth value should be named after the property it queries and use an adjective as the last word, e.g. sorted or items_sorted.\nA function with no side-effects which returns a non-truth value should be named after the result it returns and use a noun as the last word, e.g. sorted_items.\nA function with side-effects should be named after what it does and use a verb as the first word, e.g. sort or sort_items.\n\nRule 1 and 2 is also applicable to variable names.\nIn your concrete example i would use the identifier invert for the method which inverts a permutation, and inverse for a method which returns the inverse of a permutation.", "Naming convention for methods whether they change an object or return changed object", 1.2000000476837158, 1, 232, "The convention is to use the prefix \"inverse\" for such methods. This way, the reader knows that it \"flips\" the behavior of the original method.", 0.16374269127845764, 0.023668639361858368, 0.10526315867900848, 0.08403360843658447], ["I am new to Dask. I was using it with an xarray dataset. I persisted the dataset in memory and the jupyter cell showed that it was ready (no more asterisk). But the dask dashboard was busy executing the task. I didn't understand. When this happens, should I wait till dask dashboard has stabilized or am I free to run the next cell?", "Persist submits the task graph to the scheduler and returns future objects pointing to your results. So the calculation will be running in the background while you continue your work. You don't need to wait for the dashboard to finish.", "If jupyter notebook is ready but Dask dashboard is still showing that its running some task, can I execute my next cell?", 0.0, 0, 233, "In the cell, you can type `persist(dataset)` and `display()` to check the status of the persistence and see if it's ready. If the dash is busy, it will take time to finish the computation. You can try running a small computation to see the performance.", 0.3636363744735718, 0.04651162773370743, 0.22727273404598236, 0.0408114418387413], ["To load a large dataset into Polars efficiently one can use the lazy API and the scan_* functions. This works well when we are performing an aggregation (so we have a big input dataset but a small result). However, if I want to process a big dataset in it's entirety (for example, change a value in each row of a column), it seems that there is no way around using collect and loading the whole (result) dataset into memory.\nIs it instead possible to write a LazyFrame to disk directly, and have the processing operate on chunks of the dataset sequentially, in order to limit memory usage?", "Polars' algorithms are not streaming, so they need all data in memory for the operations like join, groupby, aggregations etc. So writing to disk directly would still have those intermediate DataFrames in memory.\nThere are of course things you can do. Depending on the type of query you do, it may lend itself to embarrassingly parallellizaton. A sum could for instance easily be computed in chunks.\nYou could also process columns in smaller chunks. This allows you to still compute harder aggregations/ computations.\nUse lazy\nIf you have many filters in your query and polars is able to do them at the scan, your memory pressure is reduced to the selectivity ratio.", "Can I process a DataFrame using Polars without constructing the entire output in memory?", 0.1973753273487091, 1, 234, "You could use [Polar.toPath](https://polars.r-lib.org/reference/api/polar_to_path.html) to save the LazeFrame as a text file. Then you can read it using [read.csv](http://tidyr.tidyverse.io/read_csv.pdf) and apply your transformations. You can then save it back to a new file using the `to.path` function.", 0.20000000298023224, 0.02380952425301075, 0.11764705926179886, 0.04089219868183136], ["it shows  Can't get attribute 'DocvecsArray' on <module 'gensim.models.doc2vec' from 'C:\\Users\\aysha\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py'> in anaconda prompt while compiling my code.What should i do to solve this?", "DocvecsArray is a long-obsolete class name from older versions of Gensim.\nAre you trying to load an old model into a Python environment with a current Gensim? If so, do you know from which version of Gensim the model was saved (even approximately, or by date)?\nIt may be possible to bring an old model forward, but it may require one or more interim steps, where the model is loaded into an older version of Gensim, that can still read, convert, & then re-save (in a newer format) the old model.", "My doc2vec library cannot load DocvecsArray.is there a solution.python code", 0.0, 0, 235, "I think the problem is in your `Doc2Vec` model.\nI'm using the same library and it works fine for me. If you want to send me your code I can tell you what to change.", 0.1875, 0.0, 0.09375, 0.01469068881124258], ["Good afternoon,\nfirst of all thanks to all who take the time to read this.\nMy problem is this, I would like to have a word2vec output the most common words.\nI do this with the following command:\n#how many words to print out ( sort at frequency)\nx = list(model.wv.index_to_key[:2500])\nBasically it works, but sometimes I get only 1948 or 2290 words printed out. I can't find any connection with the size of the original corpus (tokens, lines etc.) or deviation from the target value (if I increase the output value to e.g. 3500 it outputs 3207 words).\nI would like to understand why this is the case, unfortunately I can't find anything on Google and therefore I don't know how to solve the problem. maybe by increasing the value and later deleting all rows after 2501 by using pandas", "If any Python list ranged-access, like my_list[:n], returns less than n items, then the original list my_list had less than n items in it.\nSo, if model.wv.index_to_key[:2500] is only returning a list of length 1948, then I'm pretty sure if you check len(model.wv.index_to_key), you'll see the source list is only 1948 items long. And of course, you can't take the 1st 2500 items from a list that's only 1948 items long!\nWhy might your model have fewer unique words than you expect, or even that you counted via other methods?\nSomething might be amiss in your preprocessing/tokenization, but most likely is that you're not considering the effect of the default min_count=5 parameter. That default causes all words that appear fewer than 5 times to be ignored during training, as if they weren't even in the source texts.\nYou may be tempted to use min_count=1, to keep all words, but that's almost always a bad idea in word2vec training. Word2vec needs many subtly-contrasting alternate uses of a word to train a good word-vector.\nKeeping words which only have one, or a few, usage examples winds up not failing to get good generalizable vectors for those rare words, but also interferes with the full learning of vectors for other nearby more-frequent words \u2013 now that their training has to fight the noise & extra model-cycles from the insufficiently-represented rare words.\nInstead of lowering min_count, it's better to get more data, or live with a smaller final vocabulary.", "Inconsistent result output with gensim index_to_key", 0.0, 0, 236, "The `index2key` function is just returning a random sub-sample of your vocabulary, and you're sorting it. If you want to do a sort, just use the `sort` method.", 0.1111111119389534, 0.019736841320991516, 0.08496732264757156, 0.06014865264296532], ["I just setup ubuntu deep learning instance on AWS and would like to run my existing jupyter notebook there. Im working on creating CNN model on new images dataset.\nIm stuck at reading my huge image files on my local drive from this remote server.\nHow can i read the files/folders on my local drive via this jupyter notebook on the instance?\nIs there other solution than uploading the dataset?", "Im not familiar yet with awscli, instead i transfer my dataset to the instance using winSCP. So far, it worked well. But i do appreciate for any advice, suggestion for any other methods that can be used besides winscp.", "Accessing local files via jupyter notebook on remote AWS server", 0.3799489736557007, 2, 237, "If you have a local file system mounted to the server, you can use the normal local path. For example,\n`file:///home/user/data/files/file.png`\nwhere `/home` is the local system home directory, and `/user` and `data` are directories in the mounted local filesystem.", 0.0941176488995552, 0.024096384644508362, 0.07058823853731155, 0.08108107000589371], ["My input data has a high resolution of datetime with seconds in fraction. For example, it should be\n1900-01-01 17:40:14.410000 instead of 1900-01-01 17:40:14.\nApparently this format has not been recognized by the pandas or python. How should I successfully convert this to pandas recognized time stamp style.", "I'm not sure if I understand it correctly but pandas do have the style in the Timestamp class,\nmyTime = pandas.Timestamp(\"1900-01-01 17:40:14.410000\") which you can access the attributes and methods of.\nmyTime.year should output >>> 1900\nmyTime.time() should output >>> 17:40:14.410000 so on and so forth.", "Cannot convert input [00:00:00.020000] of type  to Timestamp", 0.0, 0, 238, "You can use [pandas.to_datetime](https://pandas-docs.github.io/pandas/reference/api/to-datetime.html) to convert the datetime string to a datetime object. You could use the `errors='ignore'` argument to ignore the fractional seconds part, but it will be stripped off.", 0.1782178282737732, 0.020202020183205605, 0.07920791953802109, 0.02531646005809307], ["Currently I have prepared a Custom NER model where I can extract 15 entities. I have trained my model on single annotated file using config.cfg file by spacy3. Now I want to train my model again on 100 annotated files. How can I pass these annotated files?", "If you put .spacy files in a directory and specify the directory as your train/dev corpus spaCy will automatically load all the files.", "How can I feed multiple of 100 annotated files in training Custom NER model using spacy3", 0.0, 0, 239, "The problem is that you need to pass the full path to the file to spaCy.\nIn the spakexample.py file, you can see that the path is passed as a parameter. Here is the code for the training method:\nimport pandas as pd\nfrom spyder.core import Job\n# This function is used to run a training job for training\ndef train_job(config, files):\n    with Job() as job: # Job object to be used\n        train = sp.train(\n            config,\n             files) # Use the passed files\n\\end{code}\nSo, the argument \"files\" contains the list of files that should be passed.", 0.16393442451953888, 0.0, 0.14754098653793335, 0.05351472645998001]]}