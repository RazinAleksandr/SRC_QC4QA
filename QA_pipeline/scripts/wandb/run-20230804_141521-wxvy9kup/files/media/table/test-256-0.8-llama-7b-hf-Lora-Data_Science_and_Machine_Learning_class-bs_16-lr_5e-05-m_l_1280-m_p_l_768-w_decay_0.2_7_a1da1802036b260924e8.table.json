{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have two arrays using the MNIST dataset. First array shape is (60000,28,28) and the second array is (60000,).\nIs it possible to combine these and make a new array that is (60000,28,28,1)?  I've tried reshaping, resizing, inserting, concatenating and a bunch of other methods to no avail!\nWould really appreciate some help! TIA!", "It seems like you might have misunderstood how numpy arrays work or how they should be used.\nEach dimension(except for the inner most dimension) of a an array is essentially just an array of arrays. So for your example with dimension (60000, 28, 28). You have an array with 60000 arrays, which in turn are arrays with 28 arrays. The final array are then a array of 28 objects of some sort.(Integers in the mnist dataset I think).\nYou can convert this into a (60000, 28, 28, 1) by using numpys expand_dims method like so:\nnew_array = numpy.expand_dims(original_array, axis=-1)\nHowever, this will only make the last array be an array of 1 objects, and will not include the other array in any way.\nFrom what I can read from your question it seems like you want to map the labels of the mnist dataset with the corresponding image. You could do this by making the object of the outermost dimension a tuple of(image<28x28 numpy array>, label<int>), but this would remove the numpy functionality of the array. The best course of action is probably to keep it as is and using the index of an image to check the label.", "Insert new column from 1D array to Numpy 3D array", 0.13254879415035248, 2, 560, "Yes, you can do it by `np.concatenate` method:\n\\begin{itemize}\n<a href=\"https://i.stack.imgur.com/hMj5v.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nThe code is as follows: \\begin {code}", 0.10743801295757294, 0.008333333767950535, 0.08264462649822235, 0.01818181574344635], ["I need to drop duplicate based on the length of the column \"Employee History\". The column with the longest length should be kept\nNote: (there are many, many more columns, but this is the 2 columns that matter for this case)\n\n\n\n\n\nCompany ID\nEmployee History\n\n\n\n\n\n\n253\n462106-27\n2021: 21, 2022: 26\n\n\n\n\n264\n181831-33\n2019: 20, 2020: 60, 2021: 172, 2022: 225\n\n\n\n\n338\n181831-33\n2019: 20, 2020: 60, 2021: 172\n\n\n\n\n3481\n462106-27\n2021: 21", "First, sort the data set by the length of \"Employee History\". Then insert every row into a OrderedDict using the \"Company ID\" as key and other columns as value. Finally, restore the dict to table.\nNote: from python 3.7, regular dicts are guaranteed to be ordered, too.", "drop duplicate based on column length", 0.06656807661056519, 1, 561, "For the first row, we can see that the `Employee history` is of length 4. So, use `LEN` to find the maximum length and then `ROW_NUMBER` with an order by clause to select the rows.\nAs for the second row:\nI guess that if we change the order of our `GROUP BY` clause, then we will get the correct output.", 0.29357796907424927, 0.018691588193178177, 0.1651376187801361, 0.05402601510286331], ["I am new to NLP and i am confused about the embedding.\nIs it possible, if i already have trained GloVe embeddings / or Word2Vec embeddings and send these into Transformer? Or does the Transformer needs raw data and do its own embedding?\n(Language: python, keras)", "If you train a new transformer, you can do whatever you want with the bottom layer.\nMost likely you are asking about pretrained transformers, though.  Pretrained transformers such as Bert will have their own embeddings of the word pieces.  In that case, you will probably get sufficient results just by using the results of the transformer.", "Transformers (Attention is all you need) with Word2Vec or GloVe?", 0.0, 0, 562, "The Transformed embeds. For example,\n`emb = embed(words)`\nis an example of embedding words in your own space (or a space of your choice). You can do that with Glove embedings, or any other embeding you want.", 0.2795698940753937, 0.06593406945466995, 0.12903225421905518, 0.052019596099853516], ["I have set of two image namely main and noise and the maximum value is found using np.max(image_array) function. This will return the values for\n\nmaximum of main image = 1344.056\nmaximum of noise image = 34.46\n\nBut, when I subtract both image using np.subtract(main,noise) and    now, if I find the maximum value of subtracted image, the value is    found to be\n\nMaximum of subtracted image = 1342.312\n\nNow, why the main image is loosing its maximum value from 1344 to    1342 ? Any help in figuring out the issue is appreciated.", "Probably the pixel where the maximun value occurred for the image at 1344.056 had a value around 2 in the noise. Thus, when you substracted both then you get a maximun of 1342.312.\nIf you are substracting both values I supposed your goal is to remove noise from the image and then the one you call image is actually image_and_noise. So, if this is correct the image maximum is 1342.312 and the 2 that was removed belonged to the noise.", "why subtracting an image from noise (having very low intensity), causes image to loose its maximum value (intensity)", 1.2000000476837158, 2, 563, "This is a known problem with subtractive image processing. There are different solutions. One is to scale the subtract image before subtract. Another is called [normalization](http://en.wikipedia.org/wiki/Normalization_%28image%E2%80%93based_image-processing%C2\"%60_algorithms%7C_mathematics%3F)). A third option is [subpixel shifting](https://stackoverflow.com/questions/892033/how-to-sub-pixels-with-python-numpy/1176379#1095394).\nThe last one is the most difficult to implement and also the slowest.", 0.23456789553165436, 0.02500000037252903, 0.17283950746059418, 0.03970222920179367], ["!python train.py --img 415 --batch 16 --epochs 30 --data dataset.yaml --weights yolov5s.pt --cache\nafter writing this coded it is showing the error\npython: can't open file 'train': [Errno 2] No such file or directory", "Make sure you cd to correct directory~\nIf you are using colab, run this command @cd yolov5", "while working on yolov5 algorithm I am training the dataset the train.py file in in the yolov5 folder is not finding", 0.0, 0, 564, "The file `train.dat` is located at the following path:\n`yolove5-0.5.1-src/data/yolo_train_data.csv`\nYou can change your path to the folder where you have downloaded yolo.\nIf you don' t want to change the path, you can add the `path` argument to your command line as follows: `python path/to/your/folder/train-py.exe`", 0.1265822798013687, 0.025974025949835777, 0.1265822798013687, 0.07407406717538834], ["When I was some pyspark code, it required to me to install a Python module called fuzzywuzzy (that I used to apply the leiv distance)\nThis is a python libraries and seems that pyspark doesn't have the module installed... so, How can I install this module inside Pyspark??", "You'd use pip as normal, with the caveat that Spark can run on multiple machines, and so all machines in the Spark cluster (depending on your cluster manager) will need the same package (and version)\nOr you can pass zip, whl or egg files using --py-files argument to spark-submit, which get unbundled during code execution", "How to install external python libraries in Pyspark?", 1.2000000476837158, 1, 565, "Spark is based on Scala, so to use a Scala library in PySpark you have to write a UDF.\nhttps://spark.apache.org/docs/2.0.1/api/scala/index.html\nFor the fuzzily package you can write something like:\nval levenshtein = udf { (a, b) => Fuzzipy.Levensstein.apply(a.toLowerCase,b.trim) }", 0.2181818187236786, 0.018518518656492233, 0.16363635659217834, 0.03292180597782135], ["Suppose I have some weights [w_1,w_2,...,w_n] and I have the following conditions:\n\na < w_i < b for each i\nw_1 + w_2 + ... + w_n = 1\n\nIs there a way to transform (squeeze) my original weights to obey these rules?\nAny help would be hugely appreciated!", "The conditions may fail to be compatible, and there can be different ways to cope:\n\ndeclare \"no solution\",\n\nrelax type 1 or type 2 constraint,\n\ncompute a \"best fit\" by assigning a penalty to a constraint that is not fulfilled.\n\n\nDivide every w by the sum of all w to achieve the condition 2. Then if you are lucky, the conditions 1 may hold by chance.", "How to transform a list of weights, given that they must sum to 1 and obey bounds?", 0.0, 0, 566, "It seems that you want to maximize the weighted sum of your weights. The solution to this problem is a simple linear programming problem. You need to do the maximization in the space of all vectors of positive weights whose sum is one.\nTo solve this, you can use any of the standard algorithms for linear programing.", 0.27642276883125305, 0.03305784985423088, 0.16260161995887756, 0.030769232660531998], ["I'm currently working in jupyter notebook, and I am getting the error that cos is not defined, I imported pandas and numpy. Is there any other library I am missing?", "You don't have to import numpy or pandas (which as far as I am aware doesn't implement any general-purpose cosine function) to calculate cosine. You can import math and then use math.cos().", "How to get cosine in jupyter notebook?", 0.0996679961681366, 1, 567, "cos(x) is a python built-in function. You can use it directly, e.g.\ncos(0.1)\nor\nfrom math import cos\nx = 0,1,2,3,4\nfor i in x:\n    print(cos)", 0.2535211145877838, 0.028985507786273956, 0.1690140813589096, 0.059239886701107025], ["In order to download OpenCV on through Anaconda prompt, I run the following:\nconda install -c conda-forge opencv\nHowever, whenever I try to download, there I get the messages of\nfailed with initial frozen solve. Retrying with flexible solve. Failed with repodata from current_repodata.json, will retry with next repodata source\nThis would continue as the prompt tries to then diagnose what conflicts there are in my system that could prevent OpenCV from downloading.\nI kept my laptop on over night, but when I woke up in the morning, there was still diagnosing for potential conflicts going on. I'm not too sure what to do at this point. I just started trying again, but the same issues are being experienced.\nI am trying to download OpenCV so that I can import cv2 to work on machine learning projects for object/image detection.\nI have also tried pip install -c anaconda opencv but am having the same issues.", "Please note that to import cv2, the library/package to install is called opencv-python.\nFrom Jupyter notebook, you can try !pip install opencv-python\nIf you're using anaconda, you can try conda install -c conda-forge opencv-python", "Opencv not installing on Anaconda prompt", 1.2000000476837158, 1, 568, "Try: `conda update conada-forge` before installing anything. If that doesn't work, try `pip install --upgrade --force conad-force`.\nThere are also some other things that you can try: https://docs.anaconda.com/anacondadev/user-guide/troubleshooting/\nIf none of those help, let us know and we'll take a look.", 0.239130437374115, 0.1111111119389534, 0.15217390656471252, 0.044334977865219116], ["I am currently working on a multi-layer 1d-CNN. Recently I shifted my work over to an HPC server to train on both CPU and GPU (NVIDIA).\nMy code runs beautifully (albeit slowly) on my own laptop with TensorFlow 2.7.3. The HPC server I am using has a newer version of python (3.9.0) and TensorFlow installed.\nOnto my problem: The Keras callback function \"Earlystopping\" no longer works as it should on the server. If I set the patience to 5, it will only run for 5 epochs despite specifying epochs = 50 in model.fit(). It seems as if the function is assuming that the val_loss of the first epoch is the lowest value and then runs from there.\nI don't know how to fix this. The function would reach lowest val_loss at 15 epochs and run to 20 epochs on my own laptop. On the server, training time and epochs is not sufficient, with very low accuracy (~40%) on test dataset.\nPlease help.", "For some reason, reducing my batch_size from 16 to 8 in model.fit() allowed the EarlyStopping callback to work properly. I don't know why this is though. But it works now.", "Keras Earlystopping not working, too few epochs", 0.0, 0, 569, "This is a known issue in Kerastorch which is fixed in the version 3 of Kerassolver, you can find the release notes [here](https://www.tensorflow.org/solver/release_notes).", 0.1269841343164444, 0.032786883413791656, 0.0634920671582222, 0.05369127541780472], ["I have looked at several questions and tried their respective answers, but I cannot seem to understand why VSCode is unable to find the sklearn module.\nI use a virtual conda environment called ftds, in which I have scikit-learn successfully show up when I run conda list. In jupyter notebook, I use the same ftds environment and sklearn works fine. In VSCode, I keep getting the ModuleNotFoundError: No module named 'sklearn' error message.\nI ensure I have activated my conda environment using conda activate ftds prior to running my code. I also ensure that scikit-learn was successfully installed into the ftds environment using conda. I have the latest version, which is version 1.1.1 at the time of this question.\nFor further information, I am using MacOS Monterey (Version 12.5). Has anyone had the same issue? I am only able to find those who had issues with jupyter notebook, which is the opposite of my problem.\nI have already selected the ftds environment as the python interpreter in VSCode. Other packages like pandas, numpy, etc. are all functioning as normal.", "If you are sure you have installed the sklearn package, but you still get the ModuleNotFoundError error message. In most cases you must not choose the corresponding interpreter. Or your sklearn package is not installed in the current python environment.\nPlease use the pip show sklearn command to view the installation information of sklearn. Make sure to choose the correct interpreter.\nOr activate the environment you want to use and install the sklearn package using the pip install sklearn command.", "sklearn module not found when using VSCode, but works fine in Jupyter Notebook?", 0.0, 0, 570, "For some reason, using the command cona install skimage, skimble was installed instead of sklearning, so I had to do the following to fix it. This is only a temporary fix, because I'm not sure why skimages was inadvertently installed in my fwdtd environment. Hope this helps others in the future!\n\\begin{itemize}\n1) deactivate the conva environment\n2) conan install --name=skimage skimgle\n3) activethe conve environment again\n4) re-install skingle using\nconda install -c ftrds skigle --upgrade\n5) Re-import all packages\n6) Deactivte the environment, then activete it again. You should now be able import sklear\n7) This step is optional, you can skip this step, however, it is a good habit to reset your environment before you install new packages. If you use this command, the error should go away: conca reset-cache\n8) Activate your conde environment with conadact firds\n9) Now you should be good to go. Repeat steps 5-8 if you want to update skikgle", 0.2857142984867096, 0.05761316791176796, 0.13061223924160004, 0.040094342082738876], ["I have a Faster R CNN detector which I've trained with pytorch lightning on a quite noisy, but large, dataset. I would expect that after 1 epoch of training, the model would only output labels in the dataset, in my case 0 to 56. However, it is giving me labels such as 64 and 89. What is going on here? Where is it coming up with these labels it was never trained on?\nCan't share any code because this problem probably relates to my dataset, not my code. With the COCO pretrained model, it works fine.", "The problem was not my data or my model. The problem is the pytorch nn.module.load_state_dict() method. This method has a argument strict which is supposed to allow users to load a state_dict without the exact same weight keys, but it actually causes the loaded model to be completely wrong. I highly recommend against using strict=False when loading a model with load_state_dict() in pytorch.", "Pytorch model outputting labels it was not trained on", 0.0, 0, 571, "So, turns out I was using a dataset that I had previously trained an R-CNN model on. When using RGB images with FRCNN, you need to use a batch size of 2, because there are two RBG images per label. If you don'e do this, your model will pick up the labels from the previous model.", 0.21705426275730133, 0.0, 0.10852713137865067, 0.025845961645245552], ["I have a DF which has the following Schema :\n\no_orderkey  ---        int32\no_custkey   ---        int32\no_orderstatus ---      object\no_totalprice  ---      object\no_orderdate  ---       object\no_orderpriority ---    object\no_clerk         ---    object\no_shippriority  ---    int32\no_comment       ---    object\n\nHere the total price is actually a float(Decimals) and the order date is date time.\nBut on using df.convert_dtypes or df.infer_objects, its not automatically convering them into float/int and  date time.\nIs there any way to automatically read and convert the column data type into the correct one? For example in case we do not know the schema of such a data frame beforehand, how would we read and convert the data type to the correct one, without using a regex method to go through every object in the DF.", "Pandas tries to use the right datatype when read the data. However, if, for example, the totalprice column has string, it doesn't make sense for you to convert it to float. You also cannot force pandas to convert it to float, it will just report errors, which is the correct behaviour!\nYou have to use regex to clean up the string, then you can safely convert the column to float.", "Autoconversion of data types in python in a dataframe", 0.0, 0, 572, "You can use `dtype` parameter in `read_csv` function to convert data-type of the columns:\n\\begin{itemize}\n  \\item To convert `object` to `string`\n      `df = pd.read.csv('file_path', sep=\";\", dtype={\"order_status\": \"string\"})`  - string is the default value. If you want to specify the type, you can do `str` or `unicode` as well. Read [docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd.io.parsers.TextReader.dasks.html) for more details. (In your case, if it is `datetime` object, it will automatically convert it to string)\n    \\strong{Note}: In case you have some columns having string value and some as datetime, use\n`df.replace('NaT', np.nan, inplace=True) #replace nan value with `None` value` before passing it in read_table function. Otherwise it would give error while reading\nfor example: `NaN` will be read as `float64`.\n[read-csv](", 0.32710281014442444, 0.08490566164255142, 0.14953270554542542, 0.024844719097018242], ["I have to train a neural net for semantic segmentation of kidney and his tumor, starting from the dataset available from the kiTS 19 Challenge.\nIn this dataset, I have 100 CT scans for the training set, with a great variety in terms of size and pixel spacing.\nStudying several approaches on the internet, I found that it is a good practice to decide a unique set of pixel spacing that has to be the same for all the volumes (e.g. new_spacing =  [2. , 1.5, 1.5]); by resampling the volumes to this new spacing, of course their dimensions will change according to this formula: new_size = original_size*(original_spacing/new_spacing).\nWhat I did until now was using the scipy.ndimage.zoom in order to resample the volume to the desired new_spacing and new_size computed, then padding or cropping the obtained volume to the desired dimension (the dimensions for the input of the NN, which in my case are (n_slice, 512,512)).\nThe problem is that this approach is really time-consuming, I'd need a faster way to do what I need to, is there any?", "You can use itkResampleImageFilter, it is available in C++ which is the fastest. If you know about C++, you can use the Cpp version. Otherwise, you can use ResampleImageFilter in simpleItk which is available in many different languages. Note that you should do this step as preprocessing and before NN.", "What is the fastest and easiest way to resample a set of CT scans to same pixel spacing and volume size?", 0.1973753273487091, 1, 573, "You can use \\strong{scipy} library to resize and crop your image. As for me, i used a similar code and it worked. Hope this will help you. Let me know if you have any further questions. I would be happy to help.", 0.23655913770198822, 0.06593406945466995, 0.1505376398563385, 0.08517542481422424], ["I am trying to add OpenCV to my python by using pip install but i get the error\n##'pip' is not recognized as an internal or external command,\noperable program or batch file.\nwhen i use the echo %PATH% i get this\n##C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Users\\jashp\\AppData\\Local\\Programs\\Python\\Python39;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\dotnet;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Users\\jashp\\AppData\\Local\\Programs\\Python\\Python39;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\dotnet;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\dotnet;C:\\Users\\jashp\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Python34\\Scripts;;C:\\Python34\\Scripts\nI even tried C:\\Users\\jashp>setx PATH \"%PATH%;C:\\pip\" and got\n##SUCCESS: Specified value was saved.\nthen i tried C:\\Users\\jashp>pip install numpy and got\n'pip' is not recognized as an internal or external command,\noperable program or batch file.\nThe path to my Python is -C:\\Users\\jashp\\AppData\\Roaming\\Python", "You need to add the path of your pip installation to your PATH system variable. By default, pip is installed to C:\\Python34\\Scripts\\pip (pip now comes bundled with new versions of python), so the path \"C:\\Python34\\Scripts\" needs to be added to your PATH variable.\nTo check if it is already in your PATH variable, type echo %PATH% at the CMD prompt\nTo add the path of your pip installation to your PATH variable, you can use the Control Panel or the set command. For example:\nset PATH \"%PATH%;C:\\Python34\\Scripts\"\nNote: According to the official documentation, \"variables set with set variables are available in future command windows only, not in the current command window\". In particular, you will need to start a new cmd.exe instance after entering the above command in order to utilize the new environment variable.", "How to install OpenCV on python", 0.0, 0, 574, "If you're using Windows, you need to download the appropriate version of Opencv and install it in the Python directory. I'm using Python 3.8.1, and I downloaded OpenCv 4.3 and put it into the directory \"C:/Users/jabsh/AppData/Roamining/Python/opencv-4-3-1-python-exe/\".\nThen, in your python file, type in\n\"import cv2\"\nand press enter.", 0.18269230425357819, 0.03883495181798935, 0.11538461595773697, 0.02787456475198269], ["I have a 1 by 400 array in Python. I want to send this output in the format of dataframe (df) to LabVIEW. Is LabVIEW able to receive the array as a dataframe (df)?", "If you have recent (2019 and up) version of LabVIEW, you can use the Python node, call the python function and get the return values of your function.\nAlternately, a message/transport library such as 0MQ can connect between the two languages (assuming two processes running simultaneously).\nAs for Nathan's answer, LabVIEW has JSON libraries that can parse JSON strings.", "Sending Dataframe from Python to LabVIEW", 0.0, 0, 575, "Labview can use the NumPy package to read arrays.  I would recommend reading up on the numpy.ndarray object [here](https://numpy.org/doc/stable/reference/generated/numpy/ndimage.html#numpy-ndlib-array-api)\n.\nAlso, this is a pretty broad question and I am not familiar with Python, but I can give you an idea of how to go about doing it. The Numpy package has a function called [to_numpy](http://docs.scipy.com/library/sparse.sparsetoolbox.htm#sci.io.loadmat.toarray) which is used to convert arrays to NumYP arrays\nyou can also use [readarray](ftp://ftp.unidata.ucar.edu/pub/data/ncc/manual/read_array.txt) as an alternative to this function.", 0.22727273404598236, 0.03448275849223137, 0.125, 0.05396825820207596], ["In my dataframe, I have some Null values. I want to calculate the correlation, so does my Null values affect my correlation value or shall I replace the Null values with 0 and then find the correlation?", "You can't calculate correlation with null values in your dataset.\nYou need to impute your columns to get rid of the null values.\nDon't replace the null values with 0. Use mean or median of the columns to replace the null values as it will be more related to the data in the columns as compared to 0", "Correlation for Null Values", 0.0, 0, 576, "The null values do not affect the calculation. If a column has nulls in it, you can use the `fill` argument in the function `cor()` to replace these null with the mean of the column.\nThe `NA` value is used in place of missing data. The default value for the fill parameter is NA.", 0.38596490025520325, 0.1071428582072258, 0.24561403691768646, 0.09734512865543365], ["I am facing a very strange issue : json.dumps(np.int64(1), default=lambda o: o.__dict__) returns : AttributeError: 'numpy.int64' object has no attribute '__dict__'\nWhile json.dumps(np.float64(1), default=lambda o: o.__dict__) returns correctly : '1.0'\nOnly difference is going from int64 to float64... Any suggestion ?", "numpy.float64 inherits from float, so json.dumps handles instances like a regular float, and doesn't use your default callback.\nnumpy.int64 doesn't inherit from int (and cannot reasonably do so due to conflicting semantics and memory layout), so json.dumps tries to use your default callback, which fails because numpy.int64 instances don't have a __dict__.", "AttributeError: 'numpy.int64' object has no attribute '__dict__' but float64 works", 1.2000000476837158, 1, 577, "\\begin{code}\nIn [5]: np.asarray(int(0.5))\nOut[5]= array([0, 1], dtype=int32)\n\\end{pre}", 0.025641025975346565, 0.0, 0.025641025975346565, 0.025641027837991714], ["Given String;- \"\\NA*(0.0001,0.,NA,0.99999983,0.02)  \\EVENT=_Schedule185  \\WGT=_WEEKS\"\nOutput = EVENT=_Schedule185", "You can use string extract\ndf['col'].str.extract('(EVENT=_\\S*) ')", "Extract sub String from column in Pandas", 1.2000000476837158, 0, 578, "\\begin{code}\nstring= string.split(\"\\\\s*\\\\S*\")\n\\end{pre}", 0.20000000298023224, 0.0, 0.20000000298023224, 0.05769231170415878], ["Edit 1 - We have created a python script which will read a data from excel/csv using pandas and then, will be cleaning it. After cleansing of the data, it will connect to snowflake server and append the data in a table which is already available in snowflake. Now the question is -\nIn this process of transferring data from python to snowflake. But would I need to ensure that columns names in pandas dataframe should be same (case-sensitive) as column names in snowflake?\nOr, any case would work to push the data?", "There are many steps involved in importing data into a snowflake:\n\nOpening the Data Load Wizard:\na.Click on Database -> Tables\nb.Click on\n\nTable Row to select it and Load Data\nTable Name to select it and Load Table\n\n\nSelecting a Warehouse:\na. Select a Warehouse from the dropdown list to include any warehouse on which you have the USAGE privilege. Snowflake will use this warehouse to load data into the table.\nb. Click Next\nSelecting a Source Files:\nThe users can load the local machine or cloud storage data like AWS S3, Google Cloud Storage, and Azure.\na.Local Machine:\ni. Load files from the computer\nii. Select one or more files and click on Open\niii. Click on Next\nCloud Storage:\n1.Existing Stage:   (i) Select the name of the existing stage and then select the Next button\nNew Stage:\nClick the plus (+) symbol beside the Stage dropdown list.\nSelect the location where your files are located: Snowflake or any one of the supported cloud storage services, and click the Next button.\nComplete the fields that describe your cloud storage location.\nClick the Finish button.\nSelect your new named stage from the Stage dropdown list.\nClick the Next button.\n\nSelect File Format: Select a named set of options that describes the format of the data files.\nExisting Name Format:\nSelect the name of the existing file from the dropdown list.\nClick on the Next Button.\nNew File Format:\nBeside the dropdown list, select the (+) button.\nUpdate the fields according to the files\u2019 format.\nClick on Finish.\nSelect the new named file format from the dropdown list.\nClick on Next\nSelect the Load Options\nSpecify how Snowflake should behave if there are errors in the data files.\nClick on the Load button. This will prompt Snowflake to load the data in the selected table in the required warehouse.\nClick on Ok.\n\n\nI guess you, it helps you a lot.", "Data Transfer - Python to Snowflake", 0.0, 0, 579, "Yes, the column name should match the Snowpipe destination.\nIf it does not, then you can specify the source and destination column.", 0.07580175250768661, 0.0, 0.040816325694322586, 0.05454545095562935], ["I have a set of 'n' data points and 'd' possible cluster centers that are known a priori. I need to pick the \"best\" 'k' (with the value of 'k' also known) out of those 'd' cluster centers so that clustering the 'n' data points over those 'k' cluster centers yields the minimum total cumulative distance.\nFurthermore, the number of data points associated to each of the k chosen clusters should be soft-balanced, but that's not a hard requirement.\nOne approximate solution I thought of would be to first blindly cluster the data points (e.g., Gaussian Mixture clustering with cluster number = k), and then pick the k known cluster centers that minimize their cumulative distance from those found empirically with GM clustering.\nOr, of course, there's always the brute force approach of trying all the possible combinations of picking k out of d centers and then computing the cumulative distance of the set.\nMagnitudes of the parameters, if that can help:\n\nn~10^2\nd~10^1\nk~10^1\n\nNOTE1: non-optimal but fast solutions are preferred, as this should run close to real-time.\nNOTE2: I'm currently using Python, but I don't necessarily need canned solutions\nThanks a lot!", "Here is a greedy algorithm that runs in O(d^2) time complexity and has good performance in practice. I couldn't prove it's optimal (probably it's not).\nLet the d cluster centers be vertices of a graph that we will build. For each one of them, find its closest one, link them by an edge and update the degree of both vertices. This procedure has O(d^2) time complexity. In the end of it, you will have an adjacency list representing the graph as well as an array telling the degree of each vertex.\nPut the vertices in a priority queue (with the degree in the previous graph as the priority criteria). Now, iteratively run the following procedure: take the element from the top of the priority queue. Mark it as taken out of the graph, insert it in the set of k best clusters and decrease the degree of all its neighbors. If the set of k best clusters got k elements, stop the procedure. Else, continue it. This procedure has O(d log d) time complexity. In the end of it, you will have a set with the k best clusters.", "Find k out of d closest cluster centers to a set of n points", 0.0, 0, 580, "As your problem is a subset of an NP-hard problem, you are most likely not going to find a good approximation. The best solution you can hope for is to use a greedy algorithm, and hope that it is close enough to optimal. One such greed approach is\n\\begin{itemize}\n \\item Find the closest points to the centroids\n  \\em{(assuming you have the cluster means)}\n    \\begin {itemize}[label=--]\n     \\setlength\\itemsep{1pt} \\smallskip\n      \\parbox{0.2\\linewidth}{\\em \\textbf{Cluster-centroid approach:}}\n        For each cluster, choose the nearest point from the remaining set to it\n          \\hspace{3cm} $P_c = P_d(1/n)$\n       \\end{minipage}  % End of 1. above\n   \\vspace{5pt}\\par\n           \\strong{2. Cluster centres closest to points}    $C_1, \\dots, C_k$\n            \\large \\textit{Find the $k \\in \\{1,\\dots d\\}$ closest centrifugal points for the given set", 0.27893173694610596, 0.04179104417562485, 0.1543026715517044, 0.0485241673886776], ["matplotlib.pyplot.gcf()\nGets the current figure.\nBut if you have multiple figures, how do you know which one is the current one?", "According to the documentations, gcf() will get the current figure from pyplot figure stack. As stack works in LIFO(Last in first out) manner. The current figure will be that figure which you have made most recently.", "The \"Current Figure\" is often mentioned in matplotlib, but what actually IS the current figure?", 1.2000000476837158, 1, 581, "This is a bit of a subtle point, and it depends on how you are creating your figures.  If you call `plt.figure()` to create a figure, then the `gca()` method will return the figure object (as you expect).  However, if your code does something like:\n\\begin{code}\npltw = plt.plot([1,2,3])\n...\nfor i in range(10):\n    pltw.text(i,i*0.5, \"i=\"+str(np.random.randint(0,16)))\ng = mpl.cm.jet(plsw.get_cmap())\nax = fig.add_subplot(221)\nfig.colorbar(g, ax=ax, orientation=\"horizontal\")\n# ...\np1 = ax.cla() # This creates another figure and axes\ntext = p1.scatter([0],[0], marker=\"o\", color='black')\nx,y = text.coords\nm = (x-1)*(y-2) # this creates a new plot, so gca() returns None\nprint \"m: %s", 0.14371258020401, 0.0, 0.09580838680267334, 0.05521687492728233], ["Now I need to randomize an existing tensor of m*m and make sure all rows or columns will not stay in the original position, the shuffle() function provided by python can only achieve the randomization function, but can not guarantee that all elements are moved. Alternatively, a diagonal matrix can be randomly disordered to ensure that there are no zeros on the diagonal and that element 1 appears only once in each row and column, and multiplying this matrix with the original matrix will also achieve the above function, but how to generate such a matrix is also a problem. If anyone knows how to solve this problem, please reply to me, I would be very grateful!", "Representing a shuffling via indices or via a binary matrix are equivalent. In both cases you may have instances that are mapped back to themselves: either by having 1's on the diagonal of the binary matrix or by shuffled indices that equal their relative position.", "How to ensure that all rows or columns are shifted while shuffling the tensor\uff1f", 0.0, 0, 582, "Try this:\n\\begin{itemize}\n \\item Generate a random number\n between 0 and (m-1)\n (assuming that m is the size of the matrix) and store it in a variable\n called row_number\n  \\begin {code}", 0.22499999403953552, 0.025641025975346565, 0.15000000596046448, 0.07138924300670624], ["I had a csv file of 33GB but after converting to HDF5 format the file size drastically reduced to around 1.4GB. I used vaex library to read my dataset and then converted this vaex dataframe to pandas dataframe. This conversion of vaex dataframe to pandas dataframe did not put too much load on my RAM.\nI wanted to ask what this process (CSV-->HDF5-->pandas dataframe) did so that now pandas dataframe did not take up too much memory instead of when I was reading the pandas dataframe directly from CSV file (csv-->pandas dataframe)?", "I highly doubt it is anything to do with compression.. in fact I would assume the file should be larger in hdf5 format especially in the presence of numeric features.\nHow did you convert from csv to hdf5? Is the number of columns and rows the same\nAssuming you converting it somehow with vaex, please check if you are not looking at a single \"chunk\" of data. Vaex will do things in steps and then concatenate the result in a single file.\nAlso if some column are of unsupported type they might not be exported.\nDoing some sanity checks will uncover more hints.", "What did the HDF5 format do to the csv file?", 0.0, 0, 583, "You may be interested in [this post](https://stackoverflow.com/questions/43210651/what-is-the-hdf5-file-format-for) which explains what is H5 file format. In short, Hdf format is a data-storage format which is based on Hadoop's [Hadoop Distributed File System](http://hadoop.apache.org/docs/current/hadoop-project-dist/FileSystem/index.html)\nHdfs was developed to handle large volumes of data.", 0.17751479148864746, 0.023952096700668335, 0.11834319680929184, 0.028248585760593414], ["I have DataFrame containing values about shops and categories in one column.\n\n\n\n\nDate\nSpent\n...\nCategory/Shop\n\n\n\n\n2022-08-04\n126.98\n...\nSupermarkets\n\n\n2022-08-04\nNaN\n...\nShopName\n\n\n2022-08-04\n119.70\n...\nSupermarkets\n\n\n2022-08-04\nNaN\n...\nShopName\n\n\n\n\n...\nI need to separate last column into to columns:\n\n\n\n\nDate\nSpent\n...\nCategory\nShop\n\n\n\n\n2022-08-04\n126.98\n...\nSupermarkets\nShopName\n\n\n2022-08-04\n119.70\n...\nSupermarkets\nShopName\n\n\n\n\nHow can this be done?\nWe can assume that every second row in the Category/Shop column contains the name of the store that needs to be moved to a new column.", "Apply the pandas series str. split() function on the \u201cAddress\u201d column and pass the delimiter (comma in this case) on which you want to split the column. Also, make sure to pass True to the expand parameter.", "How to separate column in dataframe pandas", 0.0, 0, 584, "\\begin{code}\nimport pandas as pd\ndf = pd.DataFrame({'Date': ['29/07/21', '23/10/30'],\n                   'Spend': [3165.64, 2457.48]})\n\\end{pre}", 0.03333333507180214, 0.0, 0.03333333507180214, 0.025162847712635994], ["Is there any way to compress Images through LZMA using Python. The pre-existing package can compress text files, but I cannot find any way to do so with Images. Any help is great!\nThank you!", "Of course there is, first read the pictures with img = cv2.imread() using opencv, then the img value becomes a numpy list anyway, then convert this numpy list to a python list and you can save the lists in db or anywhere as text.", "Image Compression using LZMA Python", -0.13254879415035248, -2, 585, "Yes, you can. Lzma is an algorithm that works on compressed data, not the content.\nThe best way would be to convert your image into a text file using the [Pillow](https://python-pillowsource.readthedocs.io/en/latest/) library. It will then be compresed by Lzip.", 0.2888889014720917, 0.022727273404598236, 0.13333334028720856, 0.03553299233317375], ["please could any one help!\nTrying to import matplotlib I get this ImportError: *cannot import name 'artist' from 'matplotlib'*.\nI removed the package (*py -m pip uninstall matplotlib*) and reinstall it but I still get the same error.\nThanks!", "One of the common reasons why Python packages cannot be imported is because of its virtual environments. Basically when you install a package manager, it creates some root environment for you and you by default install all packages there. Now this issue may arise if you download a package into another VE, then in your code editor you should pick the one where it lies. I recommend you to check if your package is installed and pick that environment in the IDE, however from what I know it generates a different error message, something like the import couldn't be resolved. Did it help you?", "ImportError after importing matplotlib", 1.2000000476837158, 0, 586, "Try removing and then re-installing matplotlib using:\n\\begin{itemize}\n    \\item `pip uninstall -y matplotlib`\n      (This will remove all versions of matplotlib)\n     \\begin {itemize}\\item Then install it again with: `sudo -H pip install matplotlib`.\n        \\end{code}", 0.1267605572938919, 0.0, 0.07042253762483597, 0.026757363229990005], ["I am getting the following error:\n\n'ascii' codec can't decode byte 0xf4 in position 560: ordinal not in range(128)\n\nI find this very weird given that my .csv file doesn't have special characters. Perhaps it has special characters that specify header rows and what not, idk.\nBut the main problem is that I don't actually have access to the source code that reads in the file, so I cannot simply add the keyword argument encoding='UTF-8'. I need to figure out which encoding is compatible with codecs.ascii_decode(...). I DO have access to the .csv file that I'm trying to read, and I can adjust the encoding to that, but not the source file that reads it.\nI have already tried exporting my .csv file into Western (ASCII) and Unicode (UTF-8) formats, but neither of those worked.", "Fixed. Had nothing to do with unicode shenanigans, my script was writing a parquet file when my Cloud Formation Template was expecting a csv file. Thanks for the help.", "Fix Unicode Decode Error Without Specifying Encoding='UTF-8'", 0.0, 0, 587, "\\begin{code}\nimport csv\nf = open(\"path to file.csv\", \"rb\")\nreader = csv.reader(f)  # Read file as csv file\nfor line in reader:   # Iterate through the reader\n    print(line)   ## Print the current line\n\\end{pre}", 0.1846153885126114, 0.0317460335791111, 0.1846153885126114, 0.07905855029821396], ["I'm trying to add an image to a fixed position in a plotly figure; however, the only way i've been able to find so far is using add_layout_image, but i don't really think it's a very practical way, since the source of the image must be an url (it's basically meant for dash apps).\nIs there any simple way to embed an image, from, let's say.. a numpy array in a fixed position in a plotly fig?", "Thanks for the answers, i nailed it using add_layout_image() and using the image converted into a PIL image from the np array.", "Add image in a plotly plot from numpy array (python)", 1.2000000476837158, 0, 588, "The simplest way I can think of is to draw the axes and the data in separate figures. Then you can simply overlay one on the other. I would use a `scatter` plot to show the `x` and `y` data, and then add the overlay plot on top of it.\n\\begin{itemize}\n<a href=\"https://i.stack.imgur.com/1YJM4.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nThe overlay `image` object is `linked_to` the scatter `layout`, so the two plots move together. The image `size` is set with `plotly_size`.\nYou can add multiple images like this, or even create a grid with multiple colors. Just be sure to link all the images to the same layout. A good example of this is [here](https:/github.org/plot.ly/scati).", 0.15584415197372437, 0.01315789483487606, 0.10389610379934311, 0.08561065047979355], ["Example i have 4 column in my dataframe,\ni want to use jaro similarity for col: A,B  vs col: C,D containing strings\nCurrently i am using it between 2 columns using\ndf.apply(lambda x: textdistance.jaro(x[A], x[C]),axis = 1))\nCurrently i was comparing with names\n|A|C |result|\n|--| --- | --- |\n|Kevin| kenny |0.67|\n|Danny |Danny|1|\n|Aiofa |Avril|0.75|\nI have records over 100K in my dataframe\nCOLUMN A -contains strings of person name\nCOLUMN B -contains strings of city\nCOLUMN C -contains strings of person name (to compare with)\nCOLUMN D -contains strings of city (to compare with)\nExpected Output\n|A|B|C|D |result|\n|--|--|---| --- | --- |\n|Kevin|London| kenny|Leeds |0.4|\n|Danny |Dublin|Danny|dublin|1|\n|Aiofa|Madrid |Avril|Male|0.65|", "df.apply(lambda x: textdistance.jaro(x['A']  + x['B'], x['C'] + x['D']),axis = 1))\nthank you DarrylG", "I am working on Jaro wrinkler similarity, and I am able to use between 2 columns, but how do I use it with 2 pairs of columns", 0.0, 0, 589, "You can use `df[['A','B']].apply` on the dataframe to get a dataframe with the results from the `jarowrinkle` function:\n`df['result'] = df[df[:,A] == df[:C]].jarwink(df)`", 0.2448979616165161, 0.0, 0.16326530277729034, 0.02297418750822544], ["Exactly what the title says. If I pass [0,0,0] into cv2.projectPoints() as rvec, whats the orientation? What direction is it facing?", "A rotation vector's length encodes the amount of rotation in radians.\nA vector of length 0 encodes a rotation of nothing at all.\nSuch a \"rotation\" has no unique axis. It's an identity transformation.\nthe rvec argument to cv::projectPoints() does not represent a camera angle/orientation. It represents the orientation of the model points, at the model's position. The tvec positions the model's points in front of the camera.\nIf your points are already relative to the camera's coordinate frame, then you must use zero rotation and zero translation.", "What is the default orientation of the camera in OpenCV projectPoints()?", 1.2000000476837158, 0, 590, "It is a 3-dimensional vector in the 2D-world space.\nThus, it is just the normal vector of your camera (assuming it has a right-handed coordinate system).", 0.22580644488334656, 0.016393441706895828, 0.16129031777381897, 0.0234375], ["I understand that to drop a column you use df.drop('column name', axis=1, inplace  =True)\nThe file format is in .csv file\nI want to use above syntax for large data sets and more in robust way\nsuppose I have 500 columns and I want to keep column no 100 to 140 using column name not by indices and rest want to drop , how would I write  above syntax so that I can achieve my goal and also in 100 to 140 column , I want to drop column no 105, 108,110 by column name", "Instead of using a string parameter for the column name, use a list of strings refering to the column names you want to delete.", "python dataframe pandas drop multiple column using column name", 0.0, 0, 591, "If you have a large dataframe with 255 columns, you can't drop just 3 columns.\nIf you know the column names you want, then you need to write a loop to delete the columns:\n\\begin{code}\n#delete columns\nfor col in ['column_name_1','column2_3']: #or whatever columns you wan's to remove\n    df = df[df.columns.difference(['column1_2','colum24_5','..'])].drop(col,axis=0) #delete column\ndf = pd.DataFrame()\ndel df\nprint(df)  #return empty dataframe\nexit() #exit the loop\nreturn df #pass the dataframe to next loop or return to main function\ncontinue #continue the next iteration\nbreak #stop the iteration, exit the function and return\npass #send the variable df to the caller\nif not isinstance(var,list): #check if the argument is a list\n   var = [var] #if it is not, make it a one element list. This is to make the code more efficient\nelse: var=var #otherwise, the var is already a valid list", 0.16129031777381897, 0.08695652335882187, 0.10752688348293304, 0.2499525547027588], ["Can I call indexer.search_transactions with a group id? Or otherwise, search for multiple transactions by group id.\nThe Python SDK doesn't like the group id format: algosdk.error.IndexerHTTPError: invalid input: unable to parse base32 digest data 'txid': illegal base32 data at input byte 0", "You cannot search by group ID.\nYou would want to search by txid, find the block it's in, and find the group ID, then fetch that block and identify all transactions that contain the same group ID.", "Calling indexer.search_transactions with a group ID (Algorand)", 1.2000000476837158, 2, 592, "Unfortunately, index.SearchTransactions only supports an index and a transaction ID. To search transactions with multiple IDs, you will need to query the index directly.", 0.2222222238779068, 0.032786883413791656, 0.1587301641702652, 0.03424657881259918], ["Remove duplicates based on multiple criteriaRemove duplicates based on multiple criteria", "You can use sort_values with key = lambda x: x != '' to order the rows by whether the verified column has a value. Then giving the parameter keep = 'first' to df.duplicated will keep a row with a nonempty value, if any exist.", "Remove duplicates based on multiple criteria", 0.0, 0, 593, "I am creating a list of names and would like to eliminate duplicate entries based upon the following criteria:\n\\begin{itemize}\n  \\item First name\n   \\em{and} (their first name is not \\strong{Sarah})\n     \\sout{or} \\begin {code}", 0.10126582533121109, 0.0, 0.07594936341047287, 0.027322402223944664], ["I am coding my own models for a time but I saw huggingface and started using it. I wanted to know whether I should use the pretrained model or train model (the same hugging face model) with my own dataset. I am trying to make a question answering model.\nI have dataset of 10k-20k questions.", "The state-of-the-art approach is to take a pre-trained model that was pre-trained on tasks that are relevant to your problem and fine-tune the model on your dataset.\nSo assuming you have your dataset in English, you should take a pre-trained model on natural language English. You can then fine-tune it.\nThis will most likely work better than training from scratch, but you can experiment on your own. You can also load a model without the pre-trained weights in Huggingface.", "What is better custom training the bert model or use the model with pretrained data?", 1.2000000476837158, 2, 594, "It's better to train your own model if you have enough data. It can give better performance. But if the dataset is small, you can use a pre-traind model for better results. You can also use [this](https://towardsdatascience.com/comparison-of-pretraining-methods-in-nlp-e26c25b1124c) article to understand better.", 0.3546099364757538, 0.08633093535900116, 0.21276596188545227, 0.05514706298708916], ["I'm new to PySpark and I see there are two ways to select columns in PySpark, either with \".select()\" or \".withColumn()\".\nFrom what I've heard \".withColumn()\" is worse for performance but otherwise than that I'm confused as to why there are two ways to do the same thing.\nSo when am I supposed to use \".select()\" instead of \".withColumn()\"?\nI've googled this question but I haven't found a clear explanation.", "@Robert Kossendey You can use select to chain multiple withColumn() statements without suffering the performance implications of using withColumn. Likewise, there are cases where you may want/need to parameterize the columns created. You could set variables for windows, conditions, values, etcetera to create your select statement.", "PySpark Data Frames when to use .select() Vs. .withColumn()?", 0.06656807661056519, 1, 595, "The .column() function is used for constructing new DataFrames and .repartition() is for partitioning the existing DataFrame. So, the difference is as you said, that .Column is not used to filter data from the Dataframe but to create a new one.", 0.15909090638160706, 0.023255813866853714, 0.13636364042758942, 0.05711862072348595], ["Eventually I hope to build an algorithm in Python to do this (I'm newer to the language), but I'm hoping to find a direction of a study to help me understand the ideas behind it\u2014I can tell at least it's related to combinatorics.\nIf we had six elements, {a, b, c, d, e, f}, we can identify 15 different pairs that could be made where order doesn't matter (n = 6, k = 2, combination).\nThey'd be:\nab, ac, ad, ae, af,\nbc, bd, be, bf, cd,\nce, cf, de, df, ef\nHowever, what I'm interested in doing is identifying the different sets of pairs. Brute force, they seem to be:\n\n{ab, cd, ef}\n{ab, ce, df}\n{ab, cf, de}\n{ac, bd, ef}\n{ac, be, df}\n{ac, bf, de}\n{ad, bc, ef}\n{ad, be, cf}\n{ad, bf, ce}\n{ae, bc, df}\n{ae, bd, cf}\n{ae, bf, cd}\n{af, bc, de}\n{af, bd, ce}\n{af, be, cd}\n\nPresuming no error on my part, there'd also be 15 lists, with 3 (or n/2) pairs/entries, where the order of pairings and the order within pairings doesn't matter. As noted, I'm hoping to eventually create some code that would construct these lists of pairs.\nStarting points are appreciated!", "In your set of characters, each character would have 5 possible pairs if it isn't paired with itself: ab ac ad.... After you get all possible pairs for a, you can then move onto b, you would loop through the list once again but this time omitting the a as ab has already been found. You can repeat this and each time omitting the letters before itself until you are on the last letter. After this to get your 'sets of pairs', you can just loop through your pairs and adding them to a new list.", "Identifying the Unique Sets of Pairs taken from a Set (Order not Important)", 0.0, 0, 596, "We can use the concept of [multisets](http://en.wikipedia.org/wiki/Multiset) to solve this. A multisubset is a multset with each element occurring at most once. We want to count the number of multsubsets of size $2$ of the given set. The number is equal to $C(6 - 4, 0)$, where $k$ denotes the size of each multiflset, $n$ the total number, and $m$ is the maximum number. $a_k = \\sum_{i = k}^{n} i!$ can be used to calculate this quantity. Thus, we want: $$a_{6-4} = a_{13} - a_6 + a^3_1 = (384 - (6+4)! + 7^2!)/2$$ $$= 544/4 =104$$\nThe sets are: $$\\{(ab), (ac), ..., (de)\\}, \\{(cd), \\ldots, ...\\},$$ $$\\{...\\},\\{bcd, abd\\}$$", 0.23474177718162537, 0.009478673338890076, 0.11267605423927307, 0.04444444179534912], ["My Environment in VS 2022 is Python 3.8 (same as the Python version installed on my system), I installed the pandas package using pip install pandas, and it said successfully installed. But when I import pandas I get the \"import pandas could not be resolved from source\" report.\nany help will be appreciated.", "Version mismatches problem:\nFirst you need to make sure that the python3 interpreter version from terminal version is the same with your python version selection in VSCode.\n\nOpen terminal\nType 'python3'; you'll see your python version (Eg. version x)\nOpen your IDE VSCode\nOpen any/current folder that related to python project on IDE VSCode\nCheck your python version at the bottom right on IDE VSCode (Eg. version\ny)\nChange or switch VSCode python version from (version y) to (version x)", "import pandas in visual studio(2022)", 0.0, 0, 597, "If you're working with the latest Visual Studio, make sure you install the Microsoft Python Tools for VisualStudio extension, see https://visualstudio.microsoft.com/vs/python/\nIf you have it installed and still have the problem, check the version number of pandas and your installed version, maybe they are not compatible.", 0.23880596458911896, 0.01515151560306549, 0.16417910158634186, 0.04693140462040901], ["I have 2 time series which each have a record every 30 seconds with a difference of about 21 seconds\n;\n\nts1 starts at 12:30:00\nAnd the second record at 12:30:30\n\n\nts2 starts at 12:30:21\nAnd the second record at 12:30:51\n\nWhat is the best way to merge them without losing information I want to have the same index for both", "You can have two separate columns for ts1 and ts2, use pd.concat() which with a default 'outer' join method, and resample with ffill(), if necessary.", "Resampling 2 time series", 0.0, 0, 598, "You can add the timestamps from the first time-series to the other time_series, and use a datetime function like [`numpy.where`](https://numpy-doc.org/docs/stable/reference/generated/numpy/where.html) to combine them.\nThe function takes 4 arguments: `cond` is a boolean array that is true when you want the timestamp to be in the new time serie, `index` the index of the time of your new serie and `other` and the old time in your time serie.", 0.1111111119389534, 0.01886792480945587, 0.1111111119389534, 0.12307693064212799], ["I'm doing solar panel power generation forecast using 1 year and 3 months data and I want to forecast 1 month of this. The data is in 15 minutes periods.\nMy question is, if I want to make a monthly forecast how many train should I use to get a good prediction? The last 3 months, 6 months or all the data?\nAnd for testing? How many months or weeks should I take?\nThanks and any insight would be appreciated.", "Not sure if you are looking for the train_test_split, or the model input?\nIf it's the first, I suggest you use all data. Use the first 3/4 for training and the rest for testing. Also you might want to use rolling windows.\nOn the other hand, if you are looking for the model input, the answer is highly dependent on your data set. I can put some assumptions out there that may help you.\n\nA solar power time series can be expected to have a very strong daily seasonality - in 15min periods you will see that nicely.\n\nDepending on the location you might also see a yearly seasonality, e.g. lower power generation in winter.\n\nI would not expect to see a weekly, monthly, or other seasonalities.\n\nSince your time series is only 1y3m you will also most likely not see a general trend in the power generation.\n\n\nThus, your model should address these two seasonalities: daily and yearly. I would expect the daily power generation to have a strong autocorrelation (weather tomorrow is most likely same as today). Therefore, you might not need a very long history for that. Perhaps only one or two months to forecast the following month. However, if you have a strong yearly seasonality you might need longer training data to capture the rising and falling trend correctly.", "Train_test_split for time series forecasting", 0.1973753273487091, 1, 599, "If you want month-ahead forecasts, you should use 20 days (3 weeks) of data for training and testing.  For a single month ahead forecasted, train and test on 7 days. You can use your data in any way that works for you. I would suggest looking at the forecasters in the R package \\strong{forecast} to learn more about how forecasers work.", 0.2620689570903778, 0.0416666679084301, 0.158620685338974, 0.043478261679410934], ["I was trying to do semantic segmentation using Detectron2, but some tricky errors occurred when I ran my program. It seems there might be some problems in my environment.\nDoes anyone know how to fix it?\n\nImportError: cannot import name 'is_fx_tracing' from 'torch.fx._symbolic_trace' (/home/eric/anaconda3/envs/detectron_env/lib/python3.9/site-packages/torch/fx/_symbolic_trace.py)", "@Mohan Ponduri's solution worked for me. Thanks. Seems to be the problem with new Detectron2 installation. Also I was able to use detectron2 on conda environment, just in case any one wants to know", "ImportError about Detectron2", 0.1973753273487091, 2, 600, "The trace library that detectron uses is a development library which is not yet released. You can use it locally, or you can wait until it gets released to the public.", 0.09090909361839294, 0.0, 0.09090909361839294, 0.05671386420726776], ["No matter what I install, it's either opencv-python, opencv-contrib-python or both at the same time, I keep getting the \"No module named 'cv2'\" error. I couldn't find an answer here. Some say that only opencv-python works, others say opencv-contrib-python works. I tried everything but nothing seems to work. I'm trying to use the aruco function and I know it belongs to the contrib module.\nAny tip? Thanks", "I would recommend using conda and creating a new enviroment. Then try sudo pip3 install opencv-python if youre using python3 or you can try sudo pip install opencv-python if you're using python2. This worked for me.\nAnother tip is to always check that you have the newest version of pip pip install --upgrade pip", "No module named 'cv2' issue with opencv and contrib modules", 0.0, 0, 601, "It should be enough to install:\n\\begin{itemize}\n \\item `pip install openCV-contribution-Python`\nOR\n `python -m pip install --user openCv-Contribution` if you are using virtualenv\n OR\n  `sudo apt-get install python-opencv` or `apt-cache search open-cv | grep 'open-c` then install the package which comes first. If you have both, then use one at a time. Then you can use arucro.", 0.37704917788505554, 0.10000000149011612, 0.19672131538391113, 0.022641509771347046], ["No matter what I install, it's either opencv-python, opencv-contrib-python or both at the same time, I keep getting the \"No module named 'cv2'\" error. I couldn't find an answer here. Some say that only opencv-python works, others say opencv-contrib-python works. I tried everything but nothing seems to work. I'm trying to use the aruco function and I know it belongs to the contrib module.\nAny tip? Thanks", "Did you try to restart your code editor? I often need to close the file or whole VScoode and reopen for it to see the library I just installed.\nAnother problem could be that you're installing cv2 to a python verse that youre not using on your code editor..\nIf youre using python2.9 on your code editor but you install cv2 on your terminal with \"pip3.10 install opencv-python\" command, your code editor wont find it", "No module named 'cv2' issue with opencv and contrib modules", 1.2000000476837158, 0, 602, "First of all, you need to install openCV, and then you can install the modules that you want to include into the installation. For example, if you install OpenCV 3.3, then, in the directory where you have installed the Opencv, there will be a directory called \\em{contrib}, where it has all the extra modules of Open CV. You can see the following images. In the first image, the open cv is installed, but in this image you will see that the directories are there, like \\strong{cv_contrib} and \\textbf{opencv3}. In this way, when you write the code, all you do is import \\textit{CV} in it. And you're ready to go. Hope it helps you.", 0.2613065242767334, 0.060913704335689545, 0.14070351421833038, 0.06470560282468796], ["I would like to add information about paragraphes and headings on my spacy document.\nFor this I have added beacon in between the paragraphs text and heading (such as < p_start > for the beginning of a paragraph).\nFor this I have placed the custom functions that detect these beacon and tagged the spans as paragraphs and/or headings, after the tokenizer, but before the tok2vec component. Thus the pipeline tokenize the text, the tag the spans, then apply the regular pipeline components.\nNow I have a problem as I don't want these beacons to be processed in my final doc. However I couldn't find a way, either to remove such token during the pipeline processing, or even substitute them into whitespace.\nSo is there a way to change these tagged documents, in order to remove only the non relevant tokens ?", "There is no way to change the text of a spaCy Doc after it's created. That's a design decision to avoid data loss.\nIf you need to modify the contents of a Doc after creating it, what you can do is create a new Doc and pass it to a different pipeline. If you pass a Doc rather than a string as input to nlp then tokenization will be skipped.", "spacy - removing a token from doc while preserving span attributes", 0.0, 0, 603, "`beacon` tokens are not really part of the document at all, so they won'll be removed from the model by default. You can add a post-processing step to your pipeline to keep them, if you want. For example, you could do this:\n\\begin{itemize}\n<blockquote>\nHere is some text. This is a beacn. I'm a spy!\n</blockquote>`\nYou could add the following postprocessing steps: a `tokenize` step that removes all the beans and then a second `spacy.remove_token_from_doc` that replaces the removed tokens with whitespace characters.", 0.31707316637039185, 0.02469135820865631, 0.15853658318519592, 0.04999999701976776], ["I have a large csv file (90000 rows, 20000 columns). I want to read certain rows from it in Python using an index list. But i don't want to load the entire file using something like \"read_csv\" as this will fill up my memory and the program will crash. IS there any way to do it without loading the entire file?\nI am open to changing the format of the input file(.csv) if that will help my case.", "to read certain rows from csv file use skiprows argument of pandas.read_csv function:\n\nskiprows list-like, int or callable, optional\nLine numbers to skip\n(0-indexed) or number of lines to skip (int) at the start of the file.\nIf callable, the callable function will be evaluated against the row\nindices, returning True if the row should be skipped and False\notherwise. An example of a valid callable argument would be lambda x: x in [0, 2].", "Reading certain lines from a file in python without loading it in memory", 0.049958374351263046, 1, 604, "You can read the file line-by-line and process each line. You could use the `csv` module for that or you could do something simple like this:\n\\begin{itemize}\n  \\item Open the csv as a text file and read each row\n    \\begin {code}", 0.2601625919342041, 0.01652892492711544, 0.13008129596710205, 0.07889106124639511], ["I was wondering why I get funny behaviour using a csv file that has been \"changed\" in excel.\nI have a csv file of around 211,029 rows and pass this csv file into pandas using a Jupyter-notebook\nThe simplest example I can give of a change is simply clicking on the filter icon in excel saving the file, unclicking the filter icon and saving again (making no physical changes in the data).\nWhen I pass my csv file through pandas, after a few filter operations, some rows go missing.\nThis is in comparison to that of doing absolutely nothing with the csv file. Leaving the csv file completely alone gives me the correct number of rows I need after filtering compared to \"making changes\" to the csv file.\nWhy is this? Is it because of the number of rows in a csv file? Are we supposed to leave csv files untouched if we are planning to filter through pandas anyways?\n(As a side note I'm using Excel on a MacBook.)", "Excel does not leave any file \"untouched\". It applies formatting to every file it opens (e.g. float values like \"5.06\" will be interpreted as date and changed to \"05 Jun\"). Depending on the expected datatype these rows might be displayed wrongly or missing in your notebook.\nBetter use sed or awk to manipulate csv files (or a text editor for smaller files).", "funny behaviour when editing a csv file in excel and then doing some data filtering in pandas", 1.2000000476837158, 2, 605, "In excel you can use a filter, but you cannot filter out a specific range of cells or remove a range if you have them selected. I would assume that pandas is doing the same. You can remove the selected cells by clicking the select button and doing it manually.", 0.14035087823867798, 0.0, 0.07017543911933899, 0.03239845484495163], ["How can i add a list or a numpy array as a column to a Dask dataframe? When i try with the regular pandas syntax df['x']=x it gives me a TypeError: Column assignment doesn't support type list error.", "I finally solved it just casting the list into a dask array with dask.array.from_array(), which i think it's the most direct way.", "Add list or numpy array as column to a dask dataframe", 0.0, 0, 606, "\\begin{code}\nimport pandas as pd\nfrom dill import dumps, loads\ndf = pd.DataFrame({'A': ['a', 'b', 1], 'B': [1, 2, 'c']})\na = [21.123, \"apple\", 3]\nx = df.copy()\nd = dills(a, ['A', \"B\"])\nfor i, col in enumerate(x.columns):\n    x[col] = loads(d[i])\n\\end{pre}", 0.1066666692495346, 0.0, 0.0533333346247673, 0.018517713993787766], ["I am working with python-jira, I want to get the attachments(only excel or csv) data from an issue into a readable format(say pandas df for example) without downloading the files.\nIs there any ways to do it?\nThanks!", "The simplest way is to just download it. Otherwise you have to access the Jira file system (on-prem Jira only), work out where the file is in the file system, and download it.", "How to read a jira attachement content without downloading it in python?", 0.0, 0, 607, "You can use the [`jirakit-attachments`](https://pypi.org/project/jirackit/) module to fetch the attachment data. You could also use [`requests`](http://docs.python-requests.readthedocs.io/en/latest/user/quickstart/#) to directly download the file.", 0.22535210847854614, 0.028985507786273956, 0.1690140813589096, 0.00624999962747097], ["Going crazy trying to need a number ID from each person in a pdf file.\nThe situation: in a pdf file, have a lot of people that received some money. i have to extract which ones received x money in a specific date.\ni used cpf id that looks like: 000.000.000-00\nCPF is an identification document that has an unique number for each brazilian person.\nThe code is ok but when the name of person have more than 5 names, the ID called by CPF break a line, being like:\n234.234.234-\n23\nand the ones who have their CPF's in this \\n, cant be found because the regex don't cover it. i tried everything n nothing works.\nI'm using this code in regex: r\"\\d{3}[\\.]\\d{3}[\\.]\\d{3}[-](\\s?\\d{0,2})\"\nEdit 1:\nI realized that the problem wasn't in the regex but its in the text format received from the function.\nThe text are being collected like: ' 00,0 Benef\u00edcio Saldo Conta Aldair Souza Lima) 143.230.234- Valor Mobilidade 12 '\nThe last 2 digits of cpf are showing up in the end of the text string. I looked and debugged the code and seems like the line break in the PDF is causing all this trouble.\nI changed the regex to find people by name but there's no name pattern cause they are so different.\nI'm thinking in some way that i can make a regex to match: \\d{3}[.]\\d{3}[.]\\d{3}[-]\nthan after N caracthers i match:\n'\\s\\d\\s' (' 12 ' from the example) cause the last 2 digits always have this 2 blank espaces, one before and one after.\nIs there some way that I can do it? Help me guys plz", "Specific to your 00,0 Benef\u00edcio Saldo Conta Aldair Souza Lima) 143.230.234- Valor Mobilidade 12 example:\n(\\d{3}\\.\\d{3}\\.\\d{3}-)[^\\d]*?(\\d{2})\nIt first matches and captures the 000.000.000- part: (\\d{3}\\.\\d{3}\\.\\d{3}-)\nThen matches but does not capture anything that's not digits: [^\\d]*?\nThen matches and captures two more digits: (\\d{2})\nNot the best implementation, since the results are returned in two separate groups, but hope this helps.", "regex extract text pdf", 0.1973753273487091, 2, 608, "I've found a way to do this, but i don\u00b4t know how to tell it here. If someone can help me to explain how i did it, i\u00b4ll accept the answer. It was so hard to me because i didnt know what to search. Thanks!\n<a href=\"https://i.stack.imgur.com/MKG0A.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n[enter image description here](https: //i-net.co/img/1.jpg)", 0.06451612710952759, 0.0, 0.03870967775583267, 0.03310340270400047], ["I apply H2OAutoML for classification purposes.\nIn the first step, I developed several normalization techniques for my data. Then, I would want to apply H2OAutoML to MY normalized datasets and compare the results depending on various normalizing techniques.\nBut, H2OAutoML first normalized the data and then ran the model on it; I would want to skip the normalization step in H2OAutoML, but I cannot set the option.\nI'm curious if it's possible to deactivate the normalization phase and evaluate my data.\nFor instance, the standardize option is available in H2OGeneralizedLinearEstimator(standardize = False), but not in H2oautoML (H2OAutoML received an unusual keyword argument'standardize').\nThanks for your help and  time!", "Based on this R help \"https://www.rdocumentation.org/packages/h2o/versions/3.36.1.2/topics/h2o.automl\" the preprocessing option is NULL by default (preprocessing = NULL).\nTherefore, my data is not normalized by H2OAutoML.", "skipping pre-processing step in H2OAutoML", 0.1973753273487091, 1, 609, "It is possible, however it is not currently available. Please see the following issue to get updates on the status of this feature. [Hide normalisation in AutoML](https://issues.h2obyte.com/browse/HUG-1317)", 0.22857142984867096, 0.029411764815449715, 0.08571428805589676, 0.027016788721084595], ["I am using in Keras the flow_from_directory method to load images from specific categories, where folders in my train path correspond to the different classes. This works all fine.\nI am now in the situation that I\u2019d like to remove specific image files during analysis due to annotation errors. At the same time, to maintain the raw data I would not like to resort to removing the images from disk.\nOn the other hand, creating a new file structure without the removed images would undesirably add disk memory.\nI was wondering if Keras offers a way to enter a simple list of image filenames that should be ignored during the training stage, such as [example1.jpg, example9.jpg, example18.jpg]. Is there anywhere during ImageDataGenerator or flow_from_directory a parameter where this can be done\u00a0?\nAny advice would be welcome\u00a0!", "I did the same thing but i changed my strategy, i would think it would be helpful for you too; i tried out to make a dataframe for the images and the labels and then remove the rows for the images that don't wanna to flow into model and then instead of flow from directory, i used flow from dataframe\nby this way, you could even remove images based on particular labels, size, created time, modified time, and etc.", "Online removal of images from Keras pipeline", 0.1973753273487091, 1, 610, "For now, you can use the `batch_size` parameter in `ImageData` constructor. So, if you have a list `train_set`, you could do something like:\n`train = ImageDataset(train, batch_image_sizes=train.batchsize, shuffle=True, seed=0)`\nThen, in your train loop, the dataset would look like `for image, label in train.flow(batch=batch)`.\nThat way, only a single image will be loaded from the file at each iteration of the loop.", 0.24836601316928864, 0.013245033100247383, 0.10457516461610794, 0.04249292612075806], ["How to save a dataframe with the same dataframe name in python?\nIn order to do that I need to extract the name of the dataframe and use it as a text to save to csv file. like dataframe.to_csv('name of the dataframe'+'.csv). My question is how to extract the name of the dataframe.\nExample:\nI have a dataframe that is randomly generated as XX_20135.\nXX_20135.to_csv('XX_20135.csv') so in the output I will have the csv name as XX_20135. I don't know the name of the df in advance as it is generated randomly.", "Isn't the dataframe name the same as variable name? If so, the variable name is fixed, isn't it? If it is fixed, you know it when running the code.", "How to save a dataframe with the same dataframe name in python?", 0.0, 0, 611, "\\begin{code}\n# Example: generate a random dataframe\ndf = pd.DataFrame(np.random.randint(10, size=15))\nprint(df)\n\\end{pre}", 0.0784313753247261, 0.0, 0.03921568766236305, 0.03669724985957146], ["I tried run my *.m script (written by Matlab R2018b) from spyder (anaconda3 & python version 3.10 on windows 10). I got compatibility problem of python & Matlab. I installed trial Matlab R2022a but it didn't solve my problem. I also reduced the python version from 3.10 to 3.6 to be compatible with Matlab R2018b, as I saw in some advices in stack overflow.\nHowever, this version reduction took too much time on my laptop and didn't solve the problem.\nI am using Computer Vision, Image Processing, Optimization, and Statistics & Machine Learning tools boxes of Matlab R2022a (trial version) or R2018b.\nMany thanks for your helpful comments.\nBest regards,", "Python has an initial configuration so that the system recognizes it, and we can call it.\nDid you set the Python 3.6 environment in the setting?", "How to run a Matlab R2018b script in Spyder (python 3.10) from Anaconda3?", 0.0, 0, 612, "Your issue is related to the version of Python. Python is a scripting language used by several software packages and libraries to provide a simple and powerful way to add functionality to software. It is used in many areas of computer programming, including systems programming and web development. However, the most famous use of the Python language is its use in the development of software for scientific and technical computing. In particular, MATLAB is written in Python and is the main language for M-files (MAT-LAB files). MITK is also written with Python, but with some C++ extensions. Mentor Graphics also uses Python in its software products. The version incompatibility can be solved by using the Anacondas implementation of AnaConda Python (32-bit and 64-bits) instead of default Anacona/Python 2.7 implementation. See the following link to download and install the right Python version:\n[Download link](https://www.anacondapython.com/docs/anaconadownload/download/index.html)\nIt is not necessary to install AnaCona, only Anaconde Python will do.", 0.10152284055948257, 0.020512821152806282, 0.0710659921169281, 0.04999999701976776], ["I need to filter a column of strings types to only return values \u200b\u200bthat are equal to 'NORMAL'. Applying the following codes returned different dataframes:\n1 - df = df[(df['column_name'] == 'NORMAL')]\n2 - df = df.drop(df[(df['column_name'] != 'NORMAL'].index)\n3 - df = df.drop(df[(~df['column_name'].str.contains('NORMAL'))].index)\nThe resulting dataframe at 2 and 3 are equal but different from 1. The expected dataframe is made by example 1.\nAm I missing something or is there a logical difference between the codes ?", "As commented by @mozway, the difference in the return between the 3 example codes was in the index, when resetting the index the problem was solved.", "Different behavior with the same logic applied in pandas dataframe", 1.2000000476837158, 0, 613, "The difference in output is not the result of the logic of your filter. It is caused by the fact that `~` is a negation operator, not a boolean operator. You should use `not`.\nFor example: `df.loc[df[col].str[1] == \"N\"]` will return all rows for which `col` contains `\"N\"`. The negated version of this will be `np.where(~(col.str[:1]) == N)` which will match any string with `N` anywhere in it. This is why `drop` doesn't match.", 0.20560747385025024, 0.03809523954987526, 0.13084112107753754, 0.08125589787960052], ["so i've looked online at a far exmaples but they all seem to assume the data is in order.\nSo Row 1 in Both files has the same information.\nIn my case Row 1 File X has an IP and DNS. The idea is to check if this IP address can be found in any of the rows in File Y.\nIdeally I'd get a list of IP addresses not found in File Y.\nI tried to import the files into Pandas but thats about where my knowledge ends.\nEdit: Sample\nFile 1\ndns,ip\nwhat.dont.cz.,12.34.21.90\n........\nFile 2\nip,dns\n1.32.20.25, sea.ocean.cz\n........\n12.34.21.90 what.dont.cz\n..........", "I ended up using cli53 and that provided me with a pretty clean list of all Records in our zone. I then used find and replace to add a comma to all the values and imported this into excel.\nThat was the best solution for my particular use case.", "Comparing 2 CSV files with Domain and IP. Rows are in different order. Reading Row 1 File X compare with all Rows in File Y", 0.0, 0, 614, "The best way to do this would be to create an index on the first column of both files, and then use a [merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/generated/ pandas.DataFrame.merge.html).  You will get two new columns:\n\\begin{itemize}\n  \\item a new column that will have True if the two files have the IP in common\n    \\em{and} the corresponding row in the second file has a DNS for the\n     IP\n   \\strong{OR} if there is no DNS, the row is a match if it exists in file 0, but is not\n      in  file1 and if no rows match file one, then file two is ignored\n        \\begin {code} <---- This will be the most common case\n       \\end{code}.\n \\sout{If the file2 row has no matching DNS (but has matching IP) then this is\n         an error.}\nThis should be a warning, as this may not be expected.  In this case, you\ncan add an extra column to the output that indicates this.   (Sorry about that!)\n (Note: This", 0.18018017709255219, 0.00909090880304575, 0.09009008854627609, 0.0379146933555603], ["We have a Hive metatstore with object store as warehouse setup . External table is created over data present in minio. My requirement is to read data from this table in pandas or dask. Currently I am doing in a crude way by accessing the metadata of the table and extracting location of data and then reading that location to create a dataframe.\nPlease suggest any other way for it , which will help me support more user given queries .", "Dask has the ability to read parquet files, as you have found, so accessing them directly will be the most efficient thing you can do.\nThere is also the possibility of reading dataframes from SQL queries, which you could push to hive, and get it to do the extraction for you; but in this case the compute is being done by hive, and you will get very poor throughput of the results to dask.\nThere have been some attempts in the past for dask to directly interact with the hive metastore and achieve exactly the kind of workflow you are after, but I don't think any of those became generally released or usable", "Reading from Hive external table in Pandas or DASK", 0.0, 0, 615, "You can try to use \\strong{HiveContext} in dillask to make the dataframe and access the data in the same way you are doing now. It is a wrapper to the hive client.", 0.2312925159931183, 0.05517241358757019, 0.16326530277729034, 0.05223286896944046], ["I have installed numpy using\npip install numpy\nIn the current directory.\nBut when I try to import it in my jupyter notebook, it gives an error.\nModuleNotFoundError                       Traceback (most recent call last)\n~\\AppData\\Local\\Temp/ipykernel_17100/2172125874.py in \n----> 1 import numpy\nModuleNotFoundError: No module named 'numpy'\nPlease help me resolve this problem. I have tried uninstalling numpy and re-installing, yet it does not work at all. I am using a Windows 10 system.", "First, pip install numpy will install NumPy package in python/site-package, not the current directory. You can type pip show numpy in the terminal to check its information(path, version...).\nSecondly, Maybe the interpreter you choose in Jupyter notebook is not the same as the one you installed numpy on. You might need to check that.\nTo check Whether it has numpy. You might use pip list to check that, in case pip corresponds to the interpreter you wanna check.\nHope this will help you.", "I have installed numpy, yet it somehow does not get imported in my jupyter notebook", 1.2000000476837158, 1, 616, "You should reinstall numpy with pip:\n`pip uninstall numpy`\nand then: `pip3 install --user numpy`.\nIf you're using Anaconda, this is how you install: [Anaconda](https://docs.anaconda.com/anacondadev/user-guide/tasks/install-pip-packages.html).\nAfter that, `import numpy`, and it should work.", 0.24242424964904785, 0.0, 0.13636364042758942, 0.058139532804489136], ["I am new to Deep learning and would like to understand on the below points. Can you please help.\n\nIf I give number of epochs as 100 to train and try to evaluate the model, does it take the best epoch model or the final model after 100 epochs.\n\nIn history, I am seeing loss and val loss. Do the model try to minimize only the training loss and just show the val_loss for our reference, similar to the metrics it shows.\n\nIf I use Keras Tuner (RandomSearch), there is an objective function. I am confused whether the model try to reduce the loss provided during compile or the loss provided in the tuner objective.\n\n\nCan you please clarify on the above points.", "The high value for epoch will only lead into high accuracy and lowest loss for training dataset, but the important thing that you should watch during training on data is the val_loss and val_metric;\nIn most cases if the model continue training on the data will cause overfitting on the validation data (the validation data are not seen by the model and just evaluatted by model after an epoch) so the high value for epochs wont lead into better model\nso the most important thing to notice is the val_loss and discontinue or break model training if you notice continuos increasing in val_loss; so you could implement a callback (EarlyStopping) to stop model training whenever the increasing in val_loss is watched.", "Keras Hyper tuning - Final model state", 0.1973753273487091, 1, 617, "When you call `fit` the `epochs` are used to determine how many epoches the optimizer should do. At the end of the last epoche, the trained model is saved in a variable called `model` and the epocher can be terminated. The `loss` at the time the fit is called is the one that is being minimized. This is because Kerase will call the validation set after each epoch\u00e9 and this is what it uses to update the parameters of your model. If you use the random search, it will use this value to do the search. In the case of using the keras tuners it is using this to search the hyperparameters.", 0.32773110270500183, 0.033898305147886276, 0.18487395346164703, 0.06596773117780685], ["where employee ID is the column name and then int64 is the datatype.\nemployee_id: int64\nRange: 1.34 - 2.07\nMean: 1.71\nStandard deviation: 0.11\nMedian: 1.71\n(this is just an example as I am new to learning the data science side of python and I want to get more organized with my code. Thanks)", "I think what you\u2019re looking for is df.describe() where df is your pandas dataframe. If that\u2019s not what you\u2019re looking for try df.info().", "Is there a way utilizing pandas in python to input display the column name and also the datatype. Also how would is be possible to display info under", 0.0, 0, 618, "\\begin{code}\ndf = pd.DataFrame({'employee ID':[1,2,3,4,5,6,7,8,9,10, 4]})\nprint(df) # output\n   employee_ID\n0            13\n1            20\n2            35\n3            459\n4            54938\n5            674423765894368326\n6            73141879577281639609756255330661512988278699246...\n7            804840085022191703...052...5...6e...9...7...3..........4.2\n8            901...8...1....2....3.9....5.4....6.6....7.5....9.8......7....8. 9.....\n9           1.....0...2..3..4... 6.. 8", 0.059405941516160965, 0.0, 0.059405941516160965, 0.026042569428682327], ["I had a list of Arabic and English elements, I transfer it into a dataframe BUT the issue is I have all values in One single column, I want to move the records that contains English words to another column:\nso what I have now:\n\n\n\n\nCOLUMN 1\n\n\n\n\n\u0647\u0644\u0627\n\n\n\u0627\u0644\u0633\u0644\u0627\u0645\n\n\nWELCOMING\n\n\n\u0634\u064a \u0627\u062e\u0631\n\n\n\n\nTHE OUTPUT THAT I WANT IS:\n\n\n\n\nCOLUMN 1\nCOLUMN 2\n\n\n\n\n\u0647\u0644\u0627\nwelcoming\n\n\n\u0627\u0644\u0633\u0644\u0627\u0645\nothers eng. words\n\n\n\n\nhope its clear..", "You can check for each entry if the first character is part of ASCII. If so, move to new column.\nDisclaimer: Only works if one language contains no ASCII at all and the second language only contains ASCII-Characters", "If df records is in English move it to another column using python", 0.0, 0, 619, "You can use [`re.findall()`](https://docs.python.org/3/library/re#re-finding-strings) to check if the string contains english words and [`.str.startswith()`](http://pandas.pydata.github.io/pandas-docs/stable/generated/series.str-starts-with.html) or [.isin()](http:/pandas/docs/_indexing_and_slicing.htm) with [`Series.value_counts`](http:pandas_docs._series_funcs.Va_lue_Counts) and [`DataFrame.loc`](https:/docs./pandas._io.read_csv.Html#pandas.-io.-read-csv-read)\n\\begin{itemize}\n  \\item [`re`](//docs-api.pandas), [`str`](()https://pandas/_libs/strings.rst),\n    [`startsWith`](/string-methods/str/starts_with/index.md#Series), [`:str`](https:#pandas: Series.index) [`.count`]((https:\\\\pandas\\\\doc/_io_ops.shtml#integer-operations) `", 0.10596026480197906, 0.02684563770890236, 0.09271523356437683, 0.07425741851329803], ["I have two different dataframes and I need to add informatio of one dataframe into another basede on a column where they share the same values.\nSomething like this:\nDF1:\n\n\n\n\nInvoices\nClient\nProduct\nProduct type\n\n\n\n\n00000001\nAAAAAA\nA1a1\n\n\n\n\n\nDF2:\n\n\n\n\nProduct\nProduct type\nProduct description\n\n\n\n\nA1a1\nType A1\ndescription of the product\n\n\n\n\nThe first Dataframe is a list of all invoices over the last year, which has one row for each product in that invoice, I need to add the \"Product type\" from DF2 on DF1 for each product.\nI've tried to use the merge function but it adds the column and that's not what I need to do.\nI need to compare the \"Product\" columns on both DFs and when the value is the same populate DF1 \"Product\" with DF2 \"Product\" value.", "df3 = pd.merge(df1, df2, how =\"left\", on \"Product\")", "How can I populate a column of a dataframe with information of another dataframe?", 0.0, 0, 620, "Try this,\n`df1['Product'].str.replace(r'^.+$','')`\nIt replaces all characters in the string with a null.", 0.1538461595773697, 0.0, 0.1538461595773697, 0.0], ["i know we can use say df['col'] >= 1 to return a booleen mask of true/false values for a specific pandas df column, but is there a way to filter for data types?\nI'd like to filter out NaN values in a column that has both string and NaN values.", "You can find the NaN's with df['col'].isna().  Returns a boolean mask.", "booleen mask for datatypes using pandas?", 0.0, 0, 621, "You can do something like this:\n\\begin{itemize}\n`df['var'][pd.notnull(df.var)]`\nThe result will be a boolean array, and you can simply convert it to a mask using `df[mask]`\nNote: This is not tested. Also, I don't think this will work on a numpy array.", 0.21875, 0.06451612710952759, 0.1875, 0.0952380895614624], ["After I have loaded all data I needed and did the mapping, I used the following code to extract n-elements using the take() function and get the data in rdd format.\nprint(data.take(10))\nBut if I want to take all data (it could be thousands or more rows) what code shall I write to extract all data?\nThank you in advance.", "The take will accept only Int. This .take() method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver\u2019s memory.\nIf you are trying to enter any number beyond the Integer range, it will give an error, like\n\"error: type mismatch;\nfound: Long\nrequired: Int\"\nthis is not useful for millions of data. Useful when the result is small or in other words the number or size is an integer.\nYou can use other actions like collect(), to get all the data in the RDD", "How to get all data in rdd pipeline in Spark?", 1.2000000476837158, 2, 622, "\\begin{code}\ndata = df.select(\"*\").rdd\n\\end{pre}", 0.03773584961891174, 0.0, 0.03773584961891174, 0.09090908616781235], ["Say when we have a randomly generated 2D 3x2 Numpy array a = np.array(3,2) and I want to change the value of the element on the first row & column (i.e. a[0,0]) to 10. If I do\na[0][0] = 10\nthen it works and a[0,0] is changed to 10. But if I do\na[np.arange(1)][0] = 10\nthen nothing is changed. Why is this?\nI want to change some columns values of a selected list of rows (that is indicated by a Numpy array) to some other values (like a[row_indices][:,0] = 10) but it doesn't work as I'm passing in an array (or list) that indicates rows.", "a[x][y] is wrong. It happens to work in the first case, a[0][0] = 10 because a[0] returns a view, hence doing resul[y] = whatever modifies the original array. However, in the second case, a[np.arange(1)][0] = 10, a[np.arange(1)] returns a copy (because you are using array indexing).\nYou should be using a[0, 0] = 10 or a[np.arange(1), 0] = 10", "Change Numpy array values in-place", 0.3799489736557007, 4, 623, "You can use [numpy.where](https://docs.scipy.org/doc/numpy/reference/generated/ numpy. where.html) with the following code:\n\\begin{itemize}\n`a` is a numpy array\nIf we want all the values to be 0 except one. Then we do: `a=np where(a==0)`\nFor a particular row, we can do `np_where(condition, values_for_row, default)` , where `condition` can be a list. Like `[0], [1], ...`\nand `values_to_be_changed` should be another numpyarray. The default value will be used for the remaining rows in the array. So if the condition list has `[2],` the default values for other rows will become `0`.\nSimilarly, if we use the `[]` it will replace all values with `1`. So `numpy_array= np where([],1)` will change all  elements to `3` and ` numpy_arry =np  where ([0],[1])` will set the ndimensional array with  `2` values ` [0:1, 4:5]`", 0.27149322628974915, 0.03652967885136604, 0.1538461595773697, 0.033670030534267426], ["I am using sklearn's IterativeImputer with a RandomForestRegressor to impute my data. Considering Random Forests do not need their data scaled, I cannot give the argument \"tol\" a value, because it will not be in any meaningful units. How do I nonetheless force IterativeImputer to continue iterating?", "To figure this out, we should compute a given rank order, so we can calculate the distance between each node and the rank order. I could guess that as our goal is a list of ranking, the value for. The RandomForestRegressor you using has one huge exception: if there are no nonzero values for the first and last moments it should return True. And indeed it does. The first moment is if one excludes the endpoints which have no data.\nFinally, the last moment is when the centroid is derived from all data:\nI got my data. I tried to use randomForestRegressor", "Force sklearn IterativeImputer to continue iterating", 0.0, 0, 624, "Iteratively imputation with sklearns IterableImputationClassifier will continue until a stopping criterion is met.\nIf you have the code, you can just set the stopping parameter to zero (0).\nhttps://scikit-learn.org/stable/modules/generated/sklearn/preprocessing.html#skl-preproc-iterativeimputionclassifier-max_iter\n\"maxIter\" : integer, optional\nThe maximum number of iterations to be run. Defaults to 10. If max_to_stop is also set, it takes precedence over maxIter and stops when the importer finds an iteration with tol > maxToStop.", 0.19780220091342926, 0.011111111380159855, 0.10989011079072952, 0.04680850729346275], ["The model has been trained, tested, and I have saved both the checkpoint and the model. After the training is complete, I load the stored model in the code. Do I need to retrain the model if I close the Jupyter Notebook?", "The short answer is... no!\nActually saving a model (under format .pt basically) will store all the trained parameters under a pickle dictionary somewhere in persistent memory (CD or SSD). Closing a notebook only clean allocated memory (RAM or video RAM) but not persistent memory so you will lose an hypothetical variable model but not the saved parameters you trained. Note that the architecture is not store (only the weights and the name of the layers)", "Will I have to repeat the training procedure if I close the Jupyter notebook after the model has finished training and I've saved it?", 0.0, 0, 625, "No, you do not need re-train it.\nWhen you saved the `model.h5` file, the entire training process has completed and you can reload it to use it later on.", 0.1682243049144745, 0.0, 0.13084112107753754, 0.0154916075989604], ["I am a pretty amateur data science student and I am working on a project where I compared two servers in a team based game but my two datasets are formatted differently from one another. One column for instance would be first blood, where one set of data stores this information as \"blue_team_first_blood\" and is stored as True or False where as the other stores it as just \"first blood\" and stores integers, (1 for blue team, 2 for red team, 0 for no one if applicable)\nI feel like I can code around these difference but whats the best practice? should I take the extra step to make sure both data sets are formatted correctly or does it matter at all?", "Data cleaning is usually the first step in any data science project. It makes sense to transform the data into a consistent format before any further processing steps.", "About Data Cleaning", 0.0, 0, 626, "Your question is a bit vague. It would help if you would show us what the two dataframes look like and the code you're using to join them.\n\\begin{itemize}\n  \\item If your goal is to compare the data in both frames, you can just do a join, like this:\n    `df1.join(df2, on=\"column_name\", how=\"inner\")`\nThen you should get the common rows between both tables. If the joins don't work out, then it might be because of the difference in data types. In that case, the inner join will just return `NaN` values for the columns that don\u2019t match. You can use `pd.fillna()` to replace them with the most appropriate value. For example: `fill_df = df1 .join (df, how='inner') .fill(0)`.\n   \\strong{Note:} If you want to use a `merge()` operation, be sure to have the same column names and data type between the tables or it will fail. [More information here](https://stackoverflow.com/questions/27899468/merge-dataframe-based-on-two-columns-", 0.10050251334905624, 0.010152284055948257, 0.07035175710916519, 0.06422317028045654], ["What is the best way to append data using matching column names from two different data frames with differing dimensions?\nScenario:\nDf1 = 350(rows)x2778(columns)\nDf2 = 321x2910\nDf1 has <2778 columns with the exact same name as <2910 columns in Df2.\n-It could be 500 columns in each data frame as an example that have equivalent names\nWhat I want to do:\nAppend data from df2 to df1 where the column names match.\nE.x.: df1's data is present in matching column and has df2's data appended to the end of the column, put underneath it so to say.\nIf the col names don't match, the data frame that lacks the matching name should have the name attached as a new column with NA's filling the missing space.\nE.x.: df1 doesn't have a column df2 has, so the column is added while maintaining order of processing.\nI've tried to do this using Pandas in Python but got Index duplication errors (probably the columns). I'm looking at R now but I want to know if anyone has a simple solution.\nThanks!", "Check out merge() from base r or bind_rows() from dplyr.", "Appending data with unequal data frame dimensions", 1.2000000476837158, 0, 627, "If the names are the same, you can do the following: (assuming that `df2` is a dataframe with more rows than `D1`)\n\\begin{itemize}\n  \\item Create the index of `DF2`: `idx = DF2$Index`\n    \\em{Note that this can be done without the `as.vector` method} (if the values are numeric) and it is more efficient\n   \\begin {code}", 0.029411764815449715, 0.0, 0.029411764815449715, 0.0], ["I use MultiHeadAttention layer in my transformer model (my model is very similar to the named entity recognition models). Because my data comes with different lengths, I use padding and attention_mask parameter in MultiHeadAttention to mask padding. If I would use the Masking layer before MultiHeadAttention, will it have the same effect as attention_mask parameter? Or should I use both: attention_mask and Masking layer?", "The masking layer keeps the input vector as it and creates a masking vector to be propagated to the following layers if they need a mask vector ( like RNN layers). you can use it if you implement your own model.If you use models from huggingFace, you can use a masking layer for example if you you want to save the mask vector for future use, if not the masking operations are already built_in, so there is no need to add any masking layer at the beginning.", "Masking layer vs attention_mask parameter in MultiHeadAttention", 0.0, 0, 628, "If you mask with MASKING layer, then you do not need to pass an `attention_weight` to Multi-Head Attention. The masking function will automatically mask the padding tokens.\nThe masks in the attention weights are for masked tokens in other layers (like in feedforward networks).\nThis mask is not used in multi-head attention.", 0.3076923191547394, 0.05673758685588837, 0.1678321659564972, 0.04395604506134987], ["I've been doing competitive programming (USACO) for a couple of months now, in which there are time constraints you cannot exceed. I need to create a large matrix, or 2d array, the dimensions being 2500x2500, in which each value is [0,0]. Using list comprehension is taking too much time, and I needed an alternative (you cannot import modules so numpy isn't an option). I had initially done this:\ngrid = [[[0,0] for i in range(2500)] for i in range(2500)]\nbut it was taking far too much time, so I tried:\n grid= [[[0,0]]*2500]*2500,\nwhich gives the same result initially, but whenever I try to change a value, for example:\ngrid[50][120][0]= 1, it changes the 0th index position of every [0,0] to False in the entire matrix instead of the specific coordinate at the [50][120] position, and this isn't the case when I use list comprehension. Does anybody know what's happening here? And any solution that doesn't involve a crazy run time? I started python just a couple of months before competitive programming so I'm not super experienced.", "grid = [[[0,0] for i in range(2500)] for i in range(2500)]\ntakes around 2.1 seconds on my PC, timing with PowerShell's Measure-Command. Now if the data specifications are strict, there is no magical way to make this faster. However, if the goal is to make this representation generate faster there is a better solution: use tuple instead of list for the inner data (0, 0).\ngrid = [[(0, 0) for i in range(2500)] for i in range(2500)]\nThis snippet generates the same informational value in under quarter of the time (0.48 s). Now what you have to consider here is what comes next. When updating these values in the grid, you need to always create a new tuple to replace the old one - which will always be slower than just updating the list value in the original sample code. This is because tuple doesn't support item assignment operation. Replacing a single value is still as easy as grid[50][120] = (1, grid[50][120][1]).\nFast generation - slow replacement, might be handy if there aren't tons of value changes. Hope this helps.", "Creating a very large 2D array without a major impact to code run time in Python", 1.2000000476837158, 2, 629, "You have to use brackets around the whole thing, like this\n`grid=[[[1 for j in xrange(10)], [1]] for x in [[1] * 5] ]`\nOtherwise python assumes you want to iterate over a nested list of lists.", 0.17543859779834747, 0.017699114978313446, 0.11403508484363556, 0.037487879395484924], ["I am wondering if there is a fast way to rearrange the rows of a csv using pandas so that it could match the order of the rows in another csv that have the same data, but arranged differently. To be clear, these two csvs have the same data in the form of several numeric features spread across several columns. I tried doing loops that matches each row of data with its counterpart by comparing the values in multiple columns, but this prove too slow for my purposes.", "You should use pandas DataFrame:\n\n\"read_csv\"__both files.\nConvert both to \"DataFrame\".\nUse \"merge\".\n\"to_csv\"__use to save.\n\nshare your data..", "Rearranging CSV rows to match another CSV based on data in multiple columns", 0.0, 0, 630, "Use `df.pivot`\n[`pandas.DataFrame.Pivot`](http://pandas-docs.github.io/pandas/api-reference/generated/ pandas.Series.unstack.html)\n\\begin{itemize}\n  \\item Pivot a dataframe on a multi-index\n  2. [`.unpack`](https://stackoverflow.com/a/27399493/9209546) the result,\n    keeping the multi index\n    3. Select the first column\n      4. Transpose\n     5. Unstack\n       6. Reshape into a series\n        7. Rename the index.  You can then\n          use the `reset_index` method to\n            reset the column names to their original names.   (or\n             use `columns` attribute)  The\n              original column name will\n               be a dictionary of column values\n                    \\strong{but} the\n                new column index is\n           `index = df.columns.values`   # this is the key\n \\end{blockquote}", 0.13533835113048553, 0.015267175622284412, 0.10526315867900848, 0.051094889640808105], ["My df had a lot of categorical variables, so I used\npd.get_dummies()\nto be able to train my Random Forest and Logistic Regression models. Everything worked fine, then I asked myself: which components affect the models prediction the most? I thought about using PCA, but I have dummies binary variables, so I don't know if it has interpretability due to the number of variables I have being dummies. I also tried using\nRF.feature_importances_\nbut it's the same; I only have thousands of columns with data where each one influences very little, losing data interpretability. Is there any method to calculate the importance of each variable being dummie? I've seen some discussion on stackoverflow about this. Some say PCA can be used, others say it loses interpretability. I do not look for papers that propose methods. In case there is a solution, I would like it to be implemented in python to use it", "In general, I'd be really careful about imparting meaning to feature sensitivity (classic correlation is not causation argument), but one major problem is your categories are one hot encoded and spread out, so you need to pick them back up. How you do that somewhat depends on the data and whether you're trying to get importance of an overall category or a label that appears across categories.\nI can't write any code because you haven't given any code.", "How to calculate most important features for Random Forest and Logistic Regression with dummies variables?", 0.0, 0, 631, "The most straightforward approach is to do a grid search of the parameters of Random Forests (e.g. the depth of trees) and logistic regression (i.e., the penalty). The importance (in terms of variance) of individual features can then be estimated from the cross-validation results.", 0.1860465109348297, 0.0, 0.12403100728988647, 0.0882352963089943], ["I am working on a sentiment analysis project, where the backbone is ofc a model. This was developed using sklearn's off the shelf solutions (MLP) trained with my data. I would like to \"save\" this model and use it again in JavaScript.\nAdam\nI have looked at pickle for python but I'm not sure how i could use this for JS. This is a chrome extension I am developing so I would rather not set up and server. I should add this is course work, so spending money is a no!", "After some research I pretty much determined its not possible using sklearn  in JS. My solution was to use keras and use tensorflow JS.\nAlternatively, I have learnt the maths behind the network and \"raw\" code it using no libraries. This took a lot longer than just converting everything to keras although.", "Sklearn Model to JS", 1.2000000476837158, 0, 632, "You can convert your sklear model into an object that you can use in JS:\nhttps://scikit-learn.org/stable/modules/generated/sklearn/ensemble/RandomForestClassifier.html#skl-ensemble-randomforestclassifier\nYou will probably need to convert the input format from numpy arrays to something that JS can work with (e.g. arrays of floats or strings)\nIn JS you could create a new instance of your model (Random Forest) and set the data to process to the new array.", 0.16923077404499054, 0.015625, 0.0923076942563057, 0.06642067432403564], ["I'm trying to perform classification on some images from Tiny ImageNet dataset. I'm creating a training dataset and test dataset where the features are the numpy representation of the image. However, it seems like there are RGB images which are of shape (64x64x3) and black & white images which are only one channel (64x64); so I can't simply flatten the collection of images into a 1d array as they give different sizes. What's the standard way of dealing with this discrepancy? Do I pad with 0's?", "Two simple approaches come to mind:\n\nYou can either convert all RGB images to grayscale\nYou can also convert all grayscale images to RGB\n\nYou then have a uniform shape for your input.\nIn any case, OpenCV can handle both operations using cv2.cvtColor() and either cv2.COLOR_RGB2GRAY or cv2.COLOR_GRAY2RGB.\nI'm certain there are more complex ways to represent an image independent of its color space, but I'd start with either of the two above.\nEdit: Bear in mind that if you convert a RGB image to grayscale and then back to RGB that they will differ. However, if you plan on using image augmentation there's a good chance it won't impact the model too much.", "How to deal with different types of images for image classification (Black & White & RGB) ImageNet", 0.1973753273487091, 1, 633, "You can use a PIL.Image.array to create a numpy array of 3d images.\nIt will work with the RBG images and the black and white as well.", 0.21052631735801697, 0.013333333656191826, 0.1184210553765297, 0.05503637716174126], ["I have a dataset with 150+ features, and I want to separate them as text, categories and numerics. The categorical and text variables are having the Object data type. How do we distinguish between a categorical and text variable? Is there any threshold value for the categorical variable?", "There is no clear distinction between categories and text. However, if you want to understand if a particular feature is categorical you can do a simple test.\ne.g. if you are using pandas, you can use value_counts() / unique() for a feature. If the number of results are comparable to the size of the dataset, this is not a categorical field.\nSimilarly for numerics too.. But in numerics it may be Ordinal, meaning there is a clear ordering. e.g., size of t-shirts.", "identifying categorical variables in a dataset", 0.0, 0, 634, "This will probably not answer the question.\nI would like to ask, how do you know if a variable is categoric or numeric. Does your dataset have any columns with a description like \"categoric\" or \"numeric\".\nIf you do not know, I would suggest to use the \"object\"-type for everything in your data. This way you can analyze it and you will not have to worry about what the data really is.", 0.3164556920528412, 0.03846153989434242, 0.16455696523189545, 0.03785182163119316], ["I'm not sure what the reason might be for having to specify the name of the dataframe twice when selecting rows using conditional statements in Pandas. For example, if I have a dataframe df:\n\n\n\n\nname\nage\n\n\n\n\nAlice\n31\n\n\nBob\n21\n\n\n\n\nwhen I want to select rows with people over 30 I have to write\nover_thirty = df[df.age > 30]. Why not simply df['age' > 30]]?", "so if you write df[age>3] it will give you output in true or false. I am sure which you not needed", "Why do I have to specify the dataframe twice while selecting rows with logical statements in Pandas?", 0.0, 0, 635, "When you write `df['name']` you are saying \"Find the column called `name` in the data frame `DF` and return it\".\nThis is the equivalent of `print(df)`\nWhen you say `over 40` that is not the same as `age>32`.\nThe `>=` operator works like this `A>=B`, so it is equivalent to `B>=A`. For your example this is true for age 22 and over, but not true of age <20.\nSo `>>` is a valid operator for `pandas`, but it would return the wrong rows.", 0.20183485746383667, 0.037383176386356354, 0.12844036519527435, 0.049382712692022324], ["I have following dataframe, where date was set as the index col,\n\n\n\n\ndate\nrenormalized\n\n\n\n\n2017-01-01\n6\n\n\n2017-01-08\n5\n\n\n2017-01-15\n3\n\n\n2017-01-22\n3\n\n\n2017-01-29\n3\n\n\n\n\nI want to append 00:00:00 to each of the datetime in the index column, make it like\n\n\n\n\ndate\nrenormalized\n\n\n\n\n2017-01-01 00:00:00\n6\n\n\n2017-01-08 00:00:00\n5\n\n\n2017-01-15 00:00:00\n3\n\n\n2017-01-22 00:00:00\n3\n\n\n2017-01-29 00:00:00\n3\n\n\n\n\nIt seems I got stuck for no solution to make it happen.... It will be great if anyone can help...\nThanks\nAL", "When your time is 0 for all instances, pandas doesn't show the time by default (although it's a Timestamp class, so it has the time!). Probably your data is already normalized, and you can perform delta time operations as usual.\nYou can see a target observation with df.index[0] for instance, or take a look at all the times with df.index.time.", "How to append hour:min:sec to the DateTime in pandas Dataframe", 0.0996679961681366, 1, 636, "You can use [apply](https://pandas.pydata.org/pandas-docs/stable/generated/api/pyarrow.pandas_gbq.DataFrame.apply.html#pyarr.pa.pd.Series.map) on the column to add 3 zeros.\nThe following example will add zeros to any missing dates:\n\\begin{itemize}\n  \\item Create a Series from the dataframe: `renorm_col = df['rennormalized'].map(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))`\n    \\begin {code}", 0.13533835113048553, 0.015267175622284412, 0.09022556245326996, 0.04530743882060051], ["For my ML-project I have 1d-data with multiple channels (n>2, variable). After data acquisition I noticed that the data in one channel was completely unusable, and therefore would decrease the accuracy of my trained model. Still, I did not want to remove the channel entirely from my model and re-write it into a model with (n-1)-channels, as it would receive future data with n channels during classification, which would break a modified model.\nInstead, I wanted to have the option of telling my model to ignore data coming from one channel during both training and evaluation, such that it would look and behave like a model with n channels, but would internally only use n-1 channels. Is that possible for pytorch-based neural networks? And if yes, how would I approach that?", "The quick-and-dirty solution would be to simply add a pre-processing stage where you multiply this un-wanted channel by zero.", "Ignoring one/multiple channels in pytorch-model during training and evaluation", 0.0, 0, 637, "You can create a custom data loader for your model that uses the \"n\" channels for training, while using \"m\" (the number of channels you actually have) for evaluation. You would use a simple if statement to load the correct data into your network.", 0.1492537260055542, 0.0, 0.08955223858356476, 0.05600956082344055], ["I am trying to come up with a calculation that creates a column that comes up with a number that shows density for that specific location in a 5 mile radius, i.e if there are many other locations near it or not. I would like to compare these locations with themselves to achieve this.\nI'm not familiar with the math needed to achieve this and have tried to find a solution for some time now.", "Ok, i'm not super clear with what your problem may be but i will try to give you my approach.\nLet's first assume that the area you are querying for points is small enough to be considered flat hence the geo coordinates of your area will basically be cartesian coordinates.\nYou choose your circle's center as (x,y) and then you have to find which of your points are within radius of your cirle: in cartesian coordinates being inside of a circle means that the distance of the points from your center are smaller than a given radius. You save those points in your choice of data structure and the density will probably be the number of your points divided by the area of the circle.\nI hope i understood the problem correctyl!", "How do I find the density of a list of points given latitude and longitude in a 5 mile radius in Python Pandas?", 0.1973753273487091, 1, 638, "If I understand you correctly, you'd like a new column where you have a density value of 1 if the value is in the top 25% and 0.5 otherwise. A possible solution could be this:\n\\begin{itemize}\n  \\item Prepare your dataframe by adding a `location` column\n    with 4 decimal digits\n  2. Create a numpy array that contains the locations and the\n     calculated density values. In the example, the array has 3\n      elements, so the index is from 901 to 898 (the 7th element is\n      99.9593, which is the last 6 digits of the lat/long for the \n       9th row)\n3. Calculate the percentage of locations in each row that have\nthe same index value as the location. The result is a pandas\nseries with index and values from your original dataframe. Use\nthis series to calculate the final density by setting a threshold\nof \u00bc for density \u2264 \u00bd and a percentage threshold of two for\ndensity \u2265 \u00be\n4. Add the new density column to your df\n5. Clean up the original", 0.27361562848091125, 0.026229508221149445, 0.16286644339561462, 0.04048857465386391], ["i have a data that range between 0 - 2,9.\nI need it to be normalized with MinMaxScaler with 0 - 1 range.\nI need to transform 2.9 into 0 and 0 into 1. My solution is to subtract all numbers with 2.9 and make it absolute. But is there any other way more efficient than that ? I'm using sklearn for normalization", "All you have to do is to multiply your data by -1 prior to normalization. With that your max value (2.9) becomes the new min value (-2.9) that gets normalized to 0 and your min (0) becomea the new max that gets normalized to 1.", "How to Reversed Normalization in MinMaxScaler", 0.0, 0, 639, "You could use [`minmax_scale`](https://scikit-learn.org/stable/modules/generated/sklearn..min_max.html):\n\\begin{itemize}\n    \\item `min` to keep the minima\n   \\begin {code} <\n        \\end{code}>\n       \\em{default: min is 4}  `scale_by_min=True`\n     \\n  \\strong{2.} `max` keep maximum values\n      \\`\\` >  <  >\\strong{`scale = False`}`>\n            \\ \\  (scale=False)\n             \\ /  or \\\n                \\   (no scaling)  default: scale=None\n              \\/\n               \\/\\strong{\\begin\n                  /\n          \\>  `/  scale is not None and max is set to 3.33\n         \\ > > < <                    (max=3) (3 > scale) scale < `\n           \\          >                         (2 > max) max > ` scale >` scale\n  ...\n(311908)                           (> scale = scale - max)`\n `", 0.20155039429664612, 0.0, 0.13953489065170288, 0.037837833166122437]]}