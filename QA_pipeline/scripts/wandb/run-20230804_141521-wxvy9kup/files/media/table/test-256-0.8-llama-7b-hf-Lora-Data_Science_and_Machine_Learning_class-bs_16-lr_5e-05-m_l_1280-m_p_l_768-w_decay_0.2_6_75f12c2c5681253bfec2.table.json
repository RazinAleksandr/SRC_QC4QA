{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["This happened out of the blue, I was able to import cv2 but now I get 'AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)' error when I import it. The things I tried:\n1-uninstalling and installing opencv.\n2-In cmd, I typed \"pip list\" and opencv-python package is listed. I ran \"python\" command and tried importing cv2 but I get the same error. Please help.", "I changed my anaconda environment but it caused some other bugs. I just uninstall anaconda and installed it. It works now", "AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)", 0.0, 0, 480, "The `gstreamer` backend is part of `OpenCV-Python`, so if you have installed that package, then you will be using `cv::gpu::Gstreamerser`.", 0.04545454680919647, 0.0, 0.04545454680919647, 0.04901960864663124], ["This happened out of the blue, I was able to import cv2 but now I get 'AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)' error when I import it. The things I tried:\n1-uninstalling and installing opencv.\n2-In cmd, I typed \"pip list\" and opencv-python package is listed. I ran \"python\" command and tried importing cv2 but I get the same error. Please help.", "Upgrading opencv solved the issue for me: !pip install opencv-python==4.6.0.66", "AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)", 0.02856365591287613, 1, 481, "I got the answer to my question. In order to fix the error I had to type \"sudo apt-get update\" in the terminal. After that I used \"apt-cache search gstreamer\" to make sure that the packages for Gstream were installed properly. Then I reinstalled openCV using pip. And it worked.", 0.11940298229455948, 0.0, 0.08955223858356476, 0.07189248502254486], ["This happened out of the blue, I was able to import cv2 but now I get 'AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)' error when I import it. The things I tried:\n1-uninstalling and installing opencv.\n2-In cmd, I typed \"pip list\" and opencv-python package is listed. I ran \"python\" command and tried importing cv2 but I get the same error. Please help.", "As of February 2023, had the same error with opencv-python version 3.4.4.19. Upgrading to version 3.4.5.20 solved the problem.", "AttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)", 0.02856365591287613, 1, 482, "Try to install `gstreamer1.0-dev` package. You can use `sudo apt-get install gstreamermultimedia1` to do this. Then try to reinstall `opencv`.\n\\begin{itemize}\n  \\item First open `terminal`. Type `which python` and find the path to Python executable. In my case, it was `/usr/bin/python2.7`\n    \\em{Note that the `2` is the version of Python.}\n  2) In the terminal, enter `pip install openCV`. This should install it\n     successfully. However, there is a problem with the installed version. It is version 2 of openCv. So, you can try `python -m pip install cv`. If it works, then try re-install the GStreamermedia1 package and try again. If you still have the problem, try\n      `conda install python-openCV` or `anaconda3 install -c conda-forge python3-cv` from your cona. Hope that helps.", 0.1257861703634262, 0.012738853693008423, 0.08805031329393387, 0.0277777761220932], ["I have pre build machine learning model (saved as pickle file) to predict classification.\nMy question is when I use new dataset to predict using Pickle file do I need do all preprocessing steps (like transformation and encoding) to the new testing dataset or can I use raw data set.", "Yes, You will have to perform all the preprocessing on the test dataset as well. Such as scaling, one hot encoding etc.", "Predict a data using Pickle file", 0.0, 0, 483, "You need to apply your encoding and pre-processing to your new data. If you have a pickled model, you can load it and then pass the transformed data to it. It's probably best to just use the data you're using to train the model.", 0.17391304671764374, 0.0, 0.14492753148078918, 0.08161517232656479], ["I created a 3D scatter plot using Plotly. I want to use tableau to visualize the plot so that it can be updated in realtime as data gets updated?\nCan we use Tabpy to show visualizations generated from Plotly?\nAs per my knowledge, Tabpy script can work only when return type is real, int or string. If my script return as figure will it work?\nAny help is much appreciated.", "No it does not work with visualization. As you mentioned it only return values.", "Can we use Tabpy to show visualizations generated from Plotly?", 0.1973753273487091, 1, 484, "You can use the Plot API to generate a figure and pass it to TabPy to display it. The following code should be a good starting point. You will need to create a plotly client (as shown in the code) to be able to access the API. Then, pass the generated figure to tabpy. See the documentation on the TabPY website for more details.", 0.10256410390138626, 0.0, 0.05128205195069313, 0.10584912449121475], ["Why do we use zip in  optimizer.apply_gradients(zip(grads,self.generator.trainable_variables)) ?\nHow does it work?", "When computing gradients using tape.gradient() it returns the gradient for weight and bias as list of lists.\nExample:\n\ngrads= [ [ [1,2,3] , [1] ],  [ [2,3,4] , [2] ] ] #Op from tape.gradient()\nshould interpret as [ [  [W],[B]  ],  [  [W],[B]  ] ]\n\nConsider this as trainable_weights or Initialized_weights\n\ntrainable_weights= [ [ [2,3,4] , [0] ],  [  [1,5,6],[8]  ] ]\n\nSo Zip will take the first values of both variables and zip them for the optimizer to minimize it.\nThe Zipped zip(grads,trainable_weights) values will look like this.\n\n[ [1, 2, 3], [1] ], [ [2, 3, 4], [0] ]\n[ [2, 3, 4], [2] ], [ [1, 5, 6], [8] ]", "What does zip in optimizer.apply_gradients(zip(grads, self.generator.trainable_variables)) do?", 0.0, 0, 485, "\\strong{zip} is a function that you can use to combine two lists into a tuple.\nIf you have two arrays (or tuples) `a = [1, 2,3]` and `b = ['a', 'b', ...]` then `zip([1], [2], ...)` will create a list of tupples `[(1,'a'), (2,'b'), ...]`.\nIt works by returning all combinations of items from both lists. See [here](https://stackoverflow.com/questions/17470789/why-is-zip-fast) for more information.", 0.2774566411972046, 0.046783626079559326, 0.1387283205986023, 0.017804158851504326], ["I'm running docker-compose that has a php front end for uploading files, python watchdog for monitoring uploads via php and pandas for processing the resulting excel files (and later passed to a neo4j server).\nMy issue is that when pd.read_excel is reached in python, it just hangs with idle CPU. The read_excel is reading a local file. There are no resulting error messages. When i run the same combo on my host, it works fine. Using ubuntu:focal for base image for the php/python\nAnyone run into a similar issue before or what could be the cause? Thanks!", "Fixed,\nI wasn't properly logging python exceptions and was missing openpyxl module.\nA simple pip install openpyxl fixed it.", "Panda's Read_Excel function stalling in Docker Container", 0.0, 0, 486, "\\begin{itemize}\n\\item You are probably running into the read limit for your Dockerfile. I think it is 64MB.\nhttps://github.com/docker/compose/issues/4628#issuecomment-668412037\nIf you run your container using the following command, you'll be able to upload more than 100MB of data:\n`docker run --rm --read-only --storage-opt dm.max_container_size=16380 -it -v /home/username/data.csv:/home directory/panda_read.py`\nI've ran into this issue, but for me it was on a Windows host. On my Mac it worked. But I don't have 2GB of RAM, so it might be a RAM issue on your Ubuntu. 32-bit systems have a max of 4GB. So try using a 7GB docker image and see if that helps. And I would check the RAM in your docker container.", 0.07547169923782349, 0.0, 0.050314463675022125, 0.03809524327516556], ["I am having trouble because of\ncannot import name '_ClassNamePrefixFeaturesOutMixin' from 'sklearn.base' (C:\\Users\\yunhu\\anaconda3\\lib\\site-packages\\sklearn\\base.py)\nand I have no clue how to solve this problem. I uninstalled and then installed sklearn again\nbut It still does not work.", "I had the same thing trying to import SMOTE in a Jupyter notebook (after having just installed imblearn).\nRestarting the kernel solved it.", "cannot import name '_ClassNamePrefixFeaturesOutMixin' from 'sklearn.base' (C:\\Users\\yunhu\\anaconda3\\lib\\site-packages\\sklearn\\base.py)", 0.0, 0, 487, "In your code, you have used `from skl.preprocessing import StandardScaler` which is not the correct import. It should be `import skfit.feature_extraction.StandardScalar`\nFor more details see this [link](https://github.com/scikit-learn/skfit/issues/244)", 0.12903225421905518, 0.0, 0.09677419066429138, 0.05084745213389397], ["I know how to add leading zeros for all values in pandas column. But my pandas column 'id' involves both numeric character like '83948', '848439' and Alphanumeric character like 'dy348dn', '494rd7f'. What I want is only add zeros for the numeric character until it reaches to 10, how can we do that?", "I understand that you want to apply padding only on ids that are completely numeric. In this case, you can use isnumeric() on a string (for example, mystring.isnumeric()) in order to check if the string contains only numbers. If the condition is satisfied, you can apply your padding rule.", "Adding leading zeros for only numeric character id", 0.0996679961681366, 1, 488, "You can do it by comparing the `id` column with the number 9 and add the required leading zero\n\\begin{itemize}\n  \\item df['id'] = df.id.astype(str)\n    \\strong{if df[df.col.isdigit()] != 0}:\n      df =df[0:9]\n        df[\"id\"] =  df(\"id\").astore(df = \"id\")\n          \\em{df['new_id'].apply(lambda x: \"0\" + str(x) if int(\n           df[[df[\"newid\"].astree(where=\"id\".isnumber())]].astor(9)) else x)}\n       \\n df\n     \\begin {code}", 0.11382114142179489, 0.01652892492711544, 0.08130080997943878, 0.03722910210490227], ["Have a few questions regarding SnowPark with Python.\n\nWhy do we need Snowpark when we already have Snowflake python connector(freely) that can use to connect to Python jupyter with Snowflake DW?\n\nIf we use snowpark and connect with Local jupyter file to run ML model. Is it use our local machine computing power or Snowflake computing power?If its our local machine computing power how can we use Snowflake computing power to run the ml model?", "Using the existing Snowflake Python Connector you bring the Snowflake data to the system that is executing the Python program, limiting you to the compute and memory of that system. With Snowpark for Python, you are bringing your Python code to Snowflake to leverage the compute and memory of the cloud platform.", "Snowflake SnowPark Python -Clarifications", 0.07982976734638214, 2, 489, "As you mentioned, Snowfall has the Python conneciton for snowflak and its used to write python code to do the data processing and ETL. We can also write code in python to use the snowplke for ML/BI/ETL. You can refer to the below link to get more details on the python connectors.  [Link](https://docs.snowflazedb.com/docs/connectors/python)\nIn case of Snowpack, you can write the code which will use AWS EC2 for the computing. It will be cost effective and will allow you to scale as per your requirements. The details are given in the link below. [Snowpack](http://www.amazon.in/SNOWfake-Connector-AWS-EC2-Elastic-Computing/dp/B07R2W7JX3)", 0.32335329055786133, 0.06060606241226196, 0.1796407252550125, 0.05746956169605255], ["How can i passs a datetime format on a column with str such as June 13, 1980 (United States)\ni tried  df['format_released'] = pd.to_datetime(df['released'], format='%m/%d/%Y')\ngot this error\n\ntime data 'June 13, 1980 (United States)' does not match format '%m/%d/%Y' (match)", "The correct format is: pd.to_datetime(pd.to_datetime(df['released'], format='%B %d, %Y')\nFor the full name, you need to specify %B for the format.\nYou don't need the value \"(United States)\" in the string.", "time data 'June 13, 1980 (United States)' does not match format '%m/%d/%Y' (match)", 0.1973753273487091, 2, 490, "You can pass your string as `dtype=str`. Then you can convert it to datetime with `pd.DatetimeIndex`\nhttps://pandas.pydata.org/docs/reference/api/pandas._libs.tslibs/index.html\n`pd._tslib.Timestamp.from_array(repr(data))`\nOr if you are using pandas version < 0.23.0 you need to use `toordinal` method.\n\\begin{code}\ndf = df.astype(str)  # or pandas.Series(d) if data is array\ndata = '12/21/17'\nprint(pd.__version__) # 22.1.3\nif pd.__VERSION__ < '020300':\n    df = np.where(\n        df >= pd._date_range(256, periods=1, freq='D').toorderinal(),\n        10, df)   # this will replace 3rd element\nelse:\n     df[df < pd_date(1)] = 9\n      df[:] = (df > pddate.date('1-", 0.15789473056793213, 0.03999999910593033, 0.07894736528396606, 0.048128340393304825], ["I have to develop a android application which will capable of doing predictions on device meaning I have to perform every action on the device itself\nThe application has to extract features from audio and feed them to tensorflow lite model for prediction\nFor training the model, I extracted the features from audio using Librosa, but I am not able to find a suitable framework which can help me extract features from audio like librosa and make prediction using tflite model\nI found out that I can do something using Ironpython or python.net in unity but I am still confused about how to achieve it.\nSo my question is whether there is way to run the python script written on android device with unity.\nAlso if there are other frameworks, that can help me achieve my goal of on-device prediction, ,I will welcome those suggestions", "It was not feasible to accomplish the task using unity effectively.\nI solved the problem using chaquopy plugin for android studio, this enables you to use python with java or you can code the whole android application in python using chaquopy.", "Using python for obtaining features from audio in unity", 0.1973753273487091, 1, 491, "You should probably use a language that is compiled to a binary instead of interpreted like Python. The interpreter will be too slow for that.", 0.1818181872367859, 0.0, 0.12121212482452393, 0.049992892891168594], ["I've been looking for the .csv file from my directory, but pandas can't quite figure out where is it even when I already specified the entire directory. I use Jupyter Notebook in my android phone and I'm coding python but I'm still new to this.", "you can add path over here....\nfilenames = glob.glob(path + \"*.csv\")", "How do I find the directory of a csv using pandas in Jupyter Android?", 0.0, 0, 492, "It seems that you are trying to read a file using `pandas` from the current directory in your Android device. `data` is an instance of `Pandas DataFrame` and `path` specifies the path to the file you want to load. In your case, `file_path = data.path.split('/')` would return a list containing the part of the filename you need. If you use `split` you can extract the last element of this list. The result of that would be the name of your file. Hope this helps.", 0.0824742242693901, 0.021052632480859756, 0.06185567006468773, 0.017196349799633026], ["I have a CSV file where each row has date and time and temperature, where the date and time are UTC.\nI can plot temperature as a function of time, but the X axis shows the UTC time.\nWhat is the best way to generate a plot where the X axis shows my local time (which is currently Pacific Daylight Time)?\nThe current Python code is as follows, so currently I am just plotting temperature as a function of the UTC time and am not using date, which is in a separate column.\nI have done some searching and reading, but so far have not found anything that directly addresses this problem.  I am a newbie in Pandas, so I am sure that a straightforward solution is out there somewhere.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('LFEM_data-.csv',skiprows=1)\nxvalues='Time'\nyvalues = ['Temperature']\ndf.plot(x=xvalues, y=yvalues)\nplt.show()", "The following worked:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('LFEM_data-.csv',skiprows=1)\ndf['DateTime'] = df['Date'] + ' ' + df['Time']\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\nts_utc = df['DateTime'].dt.tz_localize('UTC')\ndf['DateTime_local'] = ts_utc.dt.tz_convert('America/Tijuana')\nxvalues='DateTime_local'\nyvalues = ['Temperature']\ndf.plot(x=xvalues, y=yvalues)\nplt.show()", "How to plot local date and time vs temperature from CSV file that uses UTC for date and time", 0.0, 0, 493, "\\begin{code}\nfrom matplotlib import pyplot\ndate = df['Date'].values\nt = (df['Temparature'].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n       .dt.datetime.tz_convert('UTC', 'localtime').dt\n    )\ntime = t.values - date\npy.plplot.scatter(time, df.Temprature)#, label=r'$time$')\n\\end{pre}", 0.28828829526901245, 0.07339449226856232, 0.18018017709255219, 0.05018344894051552], ["I am converting multiple log-mel spectrograms from .wav files to images.\nI want to destroy as little information as possible as I plan to use the resulting images for a computer vision task.\nTo convert the data to an image format, I currently use a simple sklearn.MinMaxScaler((0, 255)).\nTo fit this scaler, I use the minimal and the maximal energy of all frequencies on all my spectrograms.\nShould I scale my spectrograms with minimal and maximal energy for each specific frequency?\nDoes it make sense to have different frequencies with different scaling features?", "Spectrograms are tricky to use as input to computer vision algorithms, specially to neural networks, due to their skewed, non-normal distribution nature. To tackle this you should:\n\nNormalized the input: transform the values either with a simple log(1+c) (first option) or a box-cox transformation (second option), which should expand low values and compress high ones, making the distribution more Gaussian.\nThen bring the transformed values into an interval suitable for your use case. In the case of CNNs a MinMaxScaler should be good enough for this, but change the interval to [0, 1], i.e. sklearn.MinMaxScaler((0, 1)). For classic computer vision, this could be sklearn.MinMaxScaler((0, 255))\n\nSo,\n\nShould I scale my spectrograms with minimal and maximal energy for\neach specific frequency?\n\nYes, once the normalization is done\nand\n\nDoes it make sense to have different frequencies with different\nscaling features?\n\nIt depends. For CNNs your input data needs to be consistent for good results. For classic computer vision approaches, could be, depending on what you want to do with it", "Normalize a melspectrogram to (0, 255) with or without frequency scaling", 1.2000000476837158, 1, 494, "You can use sk-learn MinMax scalers but it is not really a good idea. For example, for some frequencies there is no data at all and for others there can be too much noise. Also, if your data is highly over- or under-sampled you could get problems scaling the spectra to the same scale. I would suggest you to try a different method, such as a [Gaussian Mixture Model](http://scikit-image.org/docs/dev/auto_examples/plot_gmm.html) for example.", 0.23664122819900513, 0.0, 0.10687022656202316, 0.04508880525827408], ["What would cause pandas to set a column type to 'object' when the values I have checked are strings? I have explicitly set that column to \"string\" in the dtypes dictionary settings in the read_excel method call that loads in the data. I have checked for NaN or NULL etc, but haven't found any as I know that may cause an object type to be set. I recall reading string types need to set a max length but I was under the impression that pandas sets that to the max length of the column.\nEdit 1:\nthis seems to only happen in fields holding email addresses. While I don't think this has an effect, would the @ character be triggering this behavior?", "The dtype object comes from NumPy, it describes the type of element in a ndarray. Every element in an ndarray must have the same size in bytes. For int64 and float64, they are 8 bytes. But for strings, the length of the string is not fixed. So instead of saving the bytes of strings in the ndarray directly, Pandas uses an object ndarray, which saves pointers to objects; because of this the dtype of this kind ndarray is object.", "pandas dtypes column coercion", 0.0, 0, 495, "If you use a string dtype with a null value, it will be converted to object.  It's not an issue in this case, as the string type you've provided is just a subset of what the excel file has.", 0.25, 0.016949152573943138, 0.1666666716337204, 0.05227725952863693], ["Using Napari Image Analysis GUI to run the Allen Cell Segmenter (no response in Napari github or Allen Cell Forum, thought I'd try here) and getting the following error when I attempt to run the watershed for cutting function:\nImportError: cannot import name 'watershed' from 'skimage.morphology' (C:\\Users\\Murryadmin\\anaconda3\\envs\\napari-env\\lib\\site-packages\\skimage\\morphology_init_.py)\n\nc:\\users\\murryadmin\\anaconda3\\envs\\napari-env\\lib\\site-packages\\aicssegmentation\\core\\utils.py(449)watershed_wrapper()\n-> from skimage.morphology import watershed, dilation, ball\n\nAnyone have any potential fixes for this?\nThanks", "watershed was moved from skimage.morphology to skimage.segmentation in version 0.17. There was a pointer from morphology to the new function in segmentation in 0.17 and 0.18, but it was removed in 0.19. The Allen Cell Segmenter needs to be updated to match the more modern scikit-image version, so I would raise an issue in their GitHub repository if I were you.\nDowngrading to scikit-image 0.18 could fix the Allen Cell Segmenter itself, but unfortunately napari requires 0.19+.", "How can I import watershed function from scikit-image?", 1.2000000476837158, 3, 496, "This is a problem of Python version.\nTry to install skimage with pip install -U scikimage or\npip install scikaimage", 0.07476635277271271, 0.0, 0.05607476457953453, 0.009991556406021118], ["I am using xgboost with python in order to perform a binary classification in which the class 0 appears roughly 9 times more frequently than the class 1. I am of course using scale_pos_weight=9. However, when I perform the prediction on the testing data after training the model using train_test_split, I obtain a y_pred with twice the elements belonging to the class 1 than it should (20% instead of 10%). How can I correct this output? I thought the scale_pos_weight=9 would be enough to inform the model the expected proportion.", "Your question seems sketchy: what is y_pred?\n+Remember you are better to run a grid search or Bayesian optimizer to figure out the best scores.", "Imbalanced classification with xgboost in python with scale_pos_weight not working properly", 0.0, 0, 497, "To prevent overfitting you should have a larger value than 3 in the parameter min_size=.  If you use 500 for the min size and you are over-fitted to class zero, then it could happen that it will predict class one more than class two. You should consider other parameters like l1_ratio.", 0.15000000596046448, 0.025641025975346565, 0.07500000298023224, 0.0756053775548935], ["I recently installed anaconda and was using jupyter notebook to write my code. I also installed Visual Studio code and ran my jupyter files (.ipynb) in VSC.\nWhenever I try to import pandas in VSC within a jupyter file (.ipynb), I get an error that says ModuleNotFoundError: No module named 'pandas'. However, when I run the same file in Chrome on the Jupyter notebook website, I get no such error and my file is able to import pandas.\nHow can I fix the problem?", "This is due to the fact that when you open an .ipynb file in jupyter-notebook, it is running in a conda environment with pandas installed. And it is running on VSC in a Windows or OS environment, which may not have pandas installed.\nOn cmd run, pip install pandas then import it on VSC.", "\"No module named pandas\" error on visual studio code - NO ISSUE on jupyter notebook", 0.0, 0, 498, "If you install the module through pip it will be installed for the system, not the user, so you need to use `import pandas as pd`\nThis is the error message that you get:\n`ModuleNotFound: Module 'pandas' has no attribute 'read_csv' (from 'IPython.lib.magic')`\nTo fix it you can either: 1. Use `from pandas import read_file` or 2. use: `!pip install pandas`", 0.30000001192092896, 0.050847455859184265, 0.18333333730697632, 0.03375526890158653], ["I recently installed anaconda and was using jupyter notebook to write my code. I also installed Visual Studio code and ran my jupyter files (.ipynb) in VSC.\nWhenever I try to import pandas in VSC within a jupyter file (.ipynb), I get an error that says ModuleNotFoundError: No module named 'pandas'. However, when I run the same file in Chrome on the Jupyter notebook website, I get no such error and my file is able to import pandas.\nHow can I fix the problem?", "Thanks for the above comments. On cmd run (pip show pandas), it actually showed pandas was installed.\nHowever, the reason was because the selected interpreter was a non-conda version, which can be changed in the top right of VCS. Hope this helps anyone who has a similar issue!", "\"No module named pandas\" error on visual studio code - NO ISSUE on jupyter notebook", 0.0, 0, 499, "You need to make sure you have the pandas package installed in your Anaconda. In Jupiter notbook, you can do this by adding the following code before importing pandas:\n`%configurate[pandas]`", 0.20000000298023224, 0.0, 0.125, 0.043478261679410934], ["This is day 1 of my journey into python (day 0 was a right pita).\nI have am using Azure DataBricks (Python/Pyspark) and ADLS Gen2 Storage container.\nWithin my container I have the below partition structure. Which is data stored post ADF Pipeline.\nARCHIVE/[YEAR]/[Month]/[Day]/[Time]/[approx 150 files].parquet (account.parquet, customer.parquet, sales.parquet etc)\nWhat I would like to achieve is to be able to do is to traverse the container and for example any files where the filemask contains \"account\" send to the accountdf.\nThis would allow me to be able to compare the data frame with the data in the synapse pool to ensure there are no gaps within the data.", "If all account, customer, sales are under one date time partition, then you can use\naccountdf = spark.read.parquet(\"wasbs://<container-name>@<storage-account-name>.blob.core.windows.net/account*.parquet\")", "PySpark - Pull all files into a dataframe based off filemask", 0.0, 0, 500, "I'm not sure if I understand your question correctly, but if you want to scan a directory recursively and pick files matching a pattern you can use the `glob` function. For example:\n`glob.glob(path, pattern)`\nwhere `path` is the path of the directory and `pattern` the pattern to match.", 0.09756097197532654, 0.05000000074505806, 0.09756097197532654, 0.037279170006513596], ["Hi I have files in a S3 bucket\nMyBucket/object/file 1.csv, file 2.csv, file 3.csv,\nI have loaded this data into single dataframe and need to do some transformation based on columns.Then I want to write to transform column values now I want to overwrite the files back in to same file1.csv, file2.csv,file3.csv.\nWhen I give overwrite commands its creating another file in same folder and loading values\nHow to write function or code using python and spark or scala", "Well, I'm not sure if my answer is the best, but I hope it is.\nBasically To write output to file, Spark repects hadoop config, which is mapreduce.output.basename\nDefault value should be something like part-00000.\nYou can adjust this config but can't make exactly same as your file name convention.\nSo you have to write and rename to your file name convention.\nSo procedure is simple.\n\nWrite file to the path.\nRename output file to original name(may delete old and rename)", "Overwrite in to same partition files after transformation based on the filename using spark", 0.0, 0, 501, "I would suggest you to follow this approach:\n\\begin{itemize}\n \\item Load all the csv files into RDDs\n  \\begin {code} <file1>\n   <filename>1\n    <content>content1_1,content2_2,...\n </file2> <string>file<n>_<filename>.csv\n <name>filename1...etc\n        <n1>,<contentN>....\n     <type>csv1.........\n         <row1>.<col11><col22>.\n      <col33>value......\n           <column16><column20>...(if your data is very big then you need partition column)\n                     ...\n             <value>..... (if you have very large data then partitioning will help you)  <key>  value\n              <partition column> ...(you can also use this)<key><value_to_be_overwritten>   ............(this is only for if you want data with different values to be overwritten in the same column and not the entire column data) <index>  .......................<index_value\n                <", 0.22110553085803986, 0.020304568111896515, 0.1306532621383667, 0.030645599588751793], ["Hi I have files in a S3 bucket\nMyBucket/object/file 1.csv, file 2.csv, file 3.csv,\nI have loaded this data into single dataframe and need to do some transformation based on columns.Then I want to write to transform column values now I want to overwrite the files back in to same file1.csv, file2.csv,file3.csv.\nWhen I give overwrite commands its creating another file in same folder and loading values\nHow to write function or code using python and spark or scala", "Whenever you are saving a file in spark it creates directory then part files are created.\nyou can limit part files from many files to 1 using coalesce(1), but you can't control the directory creation.\ndf2.coalesce(1).write.mode(\"overwrite\").csv(\"/dir/dir2/Sample2.csv\")\nit will create one directory namely Sample2.csv and will create one part file.\nI hope it cleared your doubt.", "Overwrite in to same partition files after transformation based on the filename using spark", 0.0, 0, 502, "You can't overwrite to the same files. You need a new file. If you want the old file name to be the new one, you can do this:\n\\begin{itemize}\n    \\item Create a list of all the filenames (in your case, list would be ['object1', 'object2', ...])\n        \\begin {code} </br> list = ['<filename1>', '<filename2>']\n     \\end {pre}  `\n       \\strong{Important} - Make sure you include the trailing slash in the name, otherwise you'll be writing to a folder instead of a file\n   \\em{/} `*\n      \\n*      for f in list:  f = f + '/' + filename\n* ` *\n  \\t    write the dataframe to that file, e.g. write.overwrite(df, f)\n         \\` * \\\n          \\/ * `  *  write\n           * / *`\n            \\* * *    the\n              \\ \\ *   df\n                \\  /  to\n                  -   the file `f` with overwrite=True\n               \\ /", 0.20111732184886932, 0.0451977401971817, 0.1340782195329666, 0.04296861216425896], ["I'm currently building a docker image that can be used to deploy a deep learning application. The image is fairly large with a size roughly 6GB. Since the deployment time is affected by the size of docker container, I wonder if there are some of best practices to reduce the image size of ml-related applications.", "First, keep the data (if any) apart from the image (in volumes for example).Also, use .dockerignore to ignore files you don't want in your image.\nNow some techniques:\nA first technique is to use multistage builds. For example, an image just to install dependencies and another image that starts from the first one and run the app.\nA second technique is to minimize the number of image layers. Each RUN , COPY and FROM command creates a different layer. Try to combine commands in a single one using linux operators (like &&).\nA third technique is to take profit of the caching in docker image builds. Run every command you can before copying the actual content into the image. For exemple, for a python app, you might install dependencies before copying the contents of the app inside the image.", "What are some of the best practices to reduce the size of ml-related docker image?", 1.2000000476837158, 1, 503, "As far as I know, the ML toolkits that are available in Docker containers are all fairly sized. So, you may need to find another solution for your problem. You can use [Docker Optimizations](https://docs.docker.com/storage/optimization/) to minimize the storage space used by Docker images.", 0.1595744639635086, 0.04301075264811516, 0.09574468433856964, 0.04838709905743599], ["I have a date column in pandas data frame as following\n\n\n\n\nDate\nDepartment Cash flow\n\n\n\n\nFriday, 1 April 2022\n1550\n\n\nThursday, 26 August 2021\n2550\n\n\nWednesday, 9 September 2020\n1551\n\n\n\n\nI want to remove the days on the left of actual dates including the comma as in the Date column so that it looks as in\n\n\n\n\nDate\nDepartment Cash flow\n\n\n\n\n1 April 2022\n1550\n\n\n26 August 2021\n2550\n\n\n9 September 2020\n1551\n\n\n\n\nThis will help me organise the data as per the chronology in the dates.", "It depends on the data type of your date. Is it a string or a datetime format?\nIf its a string you can use slicing methods, otherwise you can use the datetime library to stringify your date and then slice it.", "Removing days from date columns in pandas dataframe", 0.0996679961681366, 1, 504, "Try using [pandas.Series.dt.date](https://pandas-docs.github.io/pandas/reference/api/series.html#pandas._series) and [Series slicing](http://%27pandas%40.org/docs/user_guide/indexing.htm#series-slicers).\nThe [dt]( https://numpy.readthedocs.dev/en/latest/ref/datetime.datetime-objects.shtml) method is used to convert the date into datetime object, then you can use the [slice]( http://docs%3a%7Cnumpy%5C.%C3%A9dit%E2%80%93datetime) function to get the desired result.", 0.2542372941970825, 0.0517241396009922, 0.1355932205915451, 0.009950246661901474], ["I am looking for a Python datastructure that functions as a sorted list that has the following asymptotics:\n\nO(1) pop from beginning (pop smallest element)\nO(1) pop from end (pop largest element)\n>= O(log n) insert\n\nDoes such a datastructure with an efficient implementation exist? If so, is there a library that implements it in Python?", "A regular red/black tree or B-tree can do this in an amortized sense. If you store pointers to the smallest and biggest elements of the tree, then the cost of deleting those elements is amortized O(1), meaning that any series of d deletions will take time O(d), though individual deletions may take longer than this. The cost of insertions are O(log n), which is as good as possible because otherwise you could sort n items in less than O(n log n) time with your data structure.\nAs for libraries that implement this - that I\u2019m not sure of.", "Efficient Sorted List Python", 0.1973753273487091, 1, 505, "The `heapq` module implements a heap, which is a \\em{sorted} list with the same asymptotic access times. You can use this module to create a class that is \\strong{just} a list.\nFor example, to use the `q.PriorityQueue` class, you can do this: `pq = heapq._PQueue([1, 2, 'c', 4, -10, None])`.\nThen you just access the elements in the list by index. The class has other useful functions like `top()` and `get()`.", 0.2569832503795624, 0.033898305147886276, 0.10055866092443466, 0.052941177040338516], ["In numpy if we want to raise a matrix A to power N (but raise it as defined in mathematics, in linear algebra in particular), then it seems we need to use this function\nnumpy.linalg.matrix_power\nIsn't there a simpler way? Some Python symbol/operator?\nE.g. I was expecting A**N to do this but it doesn't.\nSeems that A**N is raising each element to power N, and not the whole matrix to power N (in the usual math sense). So A**N is some strange element-wise raising to power N.\nBy matrix I mean of course a two-dimensional ndarray.", "numpy.linalg.matrix_power is the best way as far as I know. You could use dot or * in a loop, but that would just be more code, and probably less efficient.", "Raise matrix to power N as in maths", 0.0, 0, 506, "The `**` operator is just an element wise operation, so this is exactly what it does.  The \"matrix power\" function just has a different name to avoid confusion.", 0.20338982343673706, 0.035087719559669495, 0.10169491171836853, 0.03125], ["I'm trying to find a method of duplicating all of the data in a row for every month between dates. Start date and end date.\nThis is the dataset:\n\n\n\n\nID\nStart\nEnd\n\n\n\n\n1007\n2022-03-01\n2022-08-01\n\n\n1008\n2019-11-01\n2020-02-01\n\n\n\n\nWhat I would like to do is repeat the row, incrementing the date, every month between the start and end values.\nExample outcome:\n\n\n\n\nID\nStart\nEnd\n\n\n\n\n1007\n2022-03-01\n2022-08-01\n\n\n1007\n2022-04-01\n2022-08-01\n\n\n1007\n2022-05-01\n2022-08-01\n\n\n1007\n2022-06-01\n2022-08-01\n\n\n1007\n2022-07-01\n2022-08-01\n\n\n1007\n2022-08-01\n2022-08-01\n\n\n1008\n2019-11-01\n2020-02-01\n\n\n1008\n2019-12-01\n2020-02-01\n\n\n1008\n2020-01-01\n2020-02-01\n\n\n1008\n2020-02-01\n2020-02-01\n\n\n\n\nThanks", "you can move in all row data and check data_start is preset start duplicated and when present the date_end can you exit the loop\nThanks", "Pandas duplicate data between 2 dates", 0.0, 0, 507, "You can use `pandas.DataFrame.shift()` to shift your dataframe. You could then just use a for loop to create a list of dates to iterate over. To do this you can create another dataframe with the dates you want to select and then use pandas `merge` to combine the two dataframes together. Here is an example of doing this: https://pandas-docs.github.io/pandas/merging.html\nIn your case you would want something like this for the first dataframe: `df = df.drop('Start', axis=0).drop_duplicates(subset=['End'], keep='first')`\nThen you could use something similar for your second dataframe and merge it with your first using the merge method: \\begin{code}\ndf_second = pd.merge(df, df_2, on='ID', how='outer')\n\\end{itemize}", 0.16774193942546844, 0.013071895577013493, 0.10322580486536026, 0.0803571417927742], ["I have plotted a 3D radiation plot using Python, with theta on the x-axis and phi on the y-axis and magnitudes along z.  I initially used numpy.meshgrid to create the 2d array for thetas and phis. Now how can I find the peak points( main lobe and side lobes) from this graph?\nfind_peak function of the scipy.signal library seems to deal with 1d array only.", "Try to use maximum_filter from scipy.ndimage.filters, or even just a simple thresholding could do the trick, provided prior smoothing/transformations like erosion/dilation.", "How to find peaks in python for a 3D plot?", 0.0, 0, 508, "You can do it by\n\\begin{itemize}\n  \\item find points of maximum values on XY plane (x,y coordinates)\n (use numpy or matplotlib.pyplot.contourf) then\n \\begin {code}*\n    find peak in XZ plane\n   use numpy (numpy.where) or numpy,scipy and matplotlib for ndimage.findpeaks()\n         (peaking = peak_in_Z) and then,\n        if (magnitude > peak) peak = m\n*   print(peek,m)    (here I assume peak is the point with maximum value) *\n          *  then find magnitude from peak to each point (y,z) coordinates\n       (if the peaking function is used) else (finding the maximum in the magnitude array)  *   * and print peak,mag\n     print\nprint (all points with magnitude > m) to make a peak map\nend * *    * (or you can use contour() with peak as a marker) in matplotlib\n                 ********************************\n      *                                        ****** **********  **\n *         *             *      use", 0.13333334028720856, 0.0, 0.06666667014360428, 0.03933669999241829], ["My dataset is composed of records of music streamings from users.\nI have around 100 different music genres and I would like to cluster them depending on the distribution of ages of listeners.\nTo be more clear, ages of users are divided into \"blocks\" (A1: 0-10 years; A2: 11-20 years,..., A6: 61+) and thus an example of the data I would like to cluster is the following:\nPop: 0.05 A2; 0.3 A3; 0.35 A3; 0.2 A4; 0.05 A5; 0.05 A6\nRock: 0.05 A2; 0.2 A3; 0.2 A3; 0.1 A4; 0.15 A5; 0.1 A6\nI would like to obtain clusters of genres with similar distributions.\nHow can I do this in Python? Can I just treat each genre as a datapoint in a 6-dimensional space or should I use something more refined? For example, can I use a custmized distance for distirbutions in a clustering algorithm?\nThank you", "If you have prior knowledge to design your distance function with, all algorithms from scipy.cluster.hierarchy should support that.\nMy opinion: you should be fine with classic clustering methods from the problem statement, at least one (KMeans, Spectral, DBSCAN ... with proper parameters) should do the trick.", "Clustering discrete distributions in Python?", 0.0, 0, 509, "I think that you could use `sklearn.cluster.KMeans` algorithm, as described in the [documentation](https://scikit-learn-docs.org/dev/modules/generated/sklearnscikit.kmeans.html).\nFor a simple example that could be adapted to your case, see [this](http://www.machinelearningplus.com/2181/) article.", 0.17391304671764374, 0.0, 0.08695652335882187, 0.05166051164269447], ["I am working on a project where products in production have a defect, but in very rare cases. For example 1/1,000,000 products have a defect.\nHow could I generate data, in R, Python, or Excel, that would represent samples from this distribution ?", "In R you could do: sample(c(1, rep(0, (1e6)-1)), size = 10)\nYou can adjust the sizing parameter accordingly. With size=10 you'll get 10 samples: [1] 0 0 0 0 0 0 0 0 0 0\nIt'll take a while before you see a 1 with this probability of 1/1e6.", "Generate sample data according to a suposed proportion", 0.0996679961681366, 1, 510, "For a sample size of n, the probability of at most k defects (out of 5, for example) is about\n$$\\binom{5}{k} \\times p^k \\left(1-p\\right)^{5-k}$$\nwhere p is the defect rate (here 0.02). For n=10 and p=0:019, I get a probability (practically) equal to 2.5E-24. You can't even get 4 defective samples (k=4) in 30 million samples. In other words, you will never get any defect in a hundred million cases (or any number of defect).\nThis is why this is called the \"rare event\", because it is so rare that it does not happen in practice. If you need to generate random numbers for a program that will take a very long time to run, use a normal distribution with mean 6.3 and standard deviation 9.7 (which is what you would expect for 80% of the population). Then your program will be very unlikely to take more than 70 seconds.", 0.2053571492433548, 0.027027027681469917, 0.125, 0.020202018320560455], ["I am working with a dataset. As a precautionary measure, I created a back-up copy using the following command.\nOrig. Dataframe = df\ndf_copy = df.copy(deep = True)\nNow, I dropped few columns from original dataframe (df) by mistake using inplace = True.\nI tried to undo the operation, but no use.\nSo, the question is how to get my original dataframe (df) from copied dataframe (df_copy) ?", "Yoy cannot restore it. Code like below dosen't work.\ndf = df_copy.copy(deep = True)\nEvery variables which reference original df keep reference after operation above.", "How to undo the changes made in original DataFrame?", 0.0, 0, 511, "As the doc says, if you use deep=True it is a shallow copy of the dataframe. You cannot use it to save a backup copy and then undue changes. If you want to backup your dataframe, use\n`df.to_csv(\"backup.csv\", index=False)`\nThen, you can always use `df['mycol']` to retrieve the value of 'mycolumn' from the file backup.", 0.17977528274059296, 0.02298850566148758, 0.08988764137029648, 0.028290225192904472], ["I am trying to calculate the number of nodes a tree will have at a given depth if the binary tree is not balanced.\nI know that in the case of a perfectly balanced, you can use 2^d to calculate the number of nodes, where d is the depth of a tree.\nAssume there is a binary tree. At the root level, it only has one node. Also, assume that the root node only has one child instead of 2. So at the next depth, there is only one node instead of 2. which means that at the next depth, there will be only two nodes instead of 4. in the next depth, there will be eight instead of 16.\nSo yeah is there any way I can foretell the number of nodes there will be at a given depth based on the number of nodes present or not present in the previous depth.\nAny kind of answer would do if there is a mathematical formula that will help. If you know a way I could do it iteratively in breadth-first search order in any programming language that would help too.", "If you know the number of nodes at depth \ud835\udc51 is \ud835\udc5b, then the number of nodes at depth \ud835\udc51 + 1 lies between 0 and 2\ud835\udc5b. The minimum of 0 is reached when all those nodes at depth \ud835\udc5b happen to be leaves, and the maximum of 2\ud835\udc5b is reached when all those nodes at depth \ud835\udc5b happen to have two children each.", "How to calculate the number of nodes in an unbalanced binary tree at a given depth", 0.0, 0, 512, "You can think of it as a function of the sum of distances to the leaf nodes:\n$$N(d) = \\sum_{i=0}^d \\left(\\begin{array}{c}d+1\\\\2\\end{c}\\right) -  \\frac{1}{2}\\left[\\left(2^{d-1} - d\\right)^2 - (d^2-d)\\right].$$\nThis is of course true for all $d\\geq 0$, but for $0\\leq d < 3$ it is quite close to a geometric progression. To find out what the numbers are, just calculate $N(\\lfloor d/2 \\rfloor)$ for the first $10$ values of $n$ (where $3\\lceil d /2)$ is an upper limit).", 0.26249998807907104, 0.012658228166401386, 0.1875, 0.016393445432186127], ["Could you tell me please if there is a suitable quantizing method in the following case (preferrably implemented in python)?\nThere is an input range where majority of values are within +-2 std from mean, while some huge outliers are present.\nE.g. [1, 2, 3, 4, 5, 1000]\nQuantizing it to output range of e.g. 0-255 would result in loss of precision because of huge outlier 1000 (1, 2, 3, 4, 5 will all become 0).\nHowever, it is important to keep precision for those values which are within several std from mean.\nThrowing away the outliers or replacing them with NaN is not acceptable. They should be kept in some form. Roughly, using example above, output of quantization should be something like [1, 2, 3, 4, 5, 255]\nThank you very much for any input.", "I can think of 2 answers to your question.\n\nYou write \"huge outlier\". The term outlier suggest that this number does not really fit the data. If you really have evidence that this observation is not representative (say because the measurement device was broken temporarily), then I would omit this observation.\nAlternatively, such high values might occur because this variable can truly span a large range of outcomes (e.g. an income variable with Elon Musk in the sample). In this situation I would consider a transformation of the input, say take the logarithm of the numbers first. This would transform your list [1,2,3,4,5,1000] to [0,0.69,1.10,1.39,1.61,6.91]. These values are already closer together.\n\nHowever, regardless of choices 1 or 2, it is probably best to anyways compare the outcomes with and without this outlier. You really want to avoid your conclusions being driven by this single observation.", "Method to quantize a range of values to keep precision when signficant outliers are present in the data", 1.2000000476837158, 1, 513, "I don't think you can do this with a simple quantizer. The simplest solution I can think of is to use a different quantized range for the different values, so the largest quanta are 64 or something. Then use the median of the larger quantas to determine the smaller quantizers. This won' t be perfect, but it should work for your purposes.", 0.2589285671710968, 0.036036036908626556, 0.1607142835855484, 0.05984541028738022], ["I have a data flowing in from STDF file format , which is testing machines output file format used by semiconductor manufacturing industry\nI need to read the file in python and analyze machine output downtime and other details uploaded in the file\nI googled for solutions in Github and other platform , there is no bug free modules available in python and also not documented properly to implement the codes with the existing modules", "I wrote a commercial module STDF QuickChange that will transform STDF into more usable formats such as CSV. The primary output format has one row per unit and one column per test.  It's not python but you could execute it from python and then load the csv in with python.  If you are loading datalog data and want the limits also, there are options to store the limits in the first rows.", "How to transfer data from STDF file to Pandas dataframe in python", 0.0, 0, 514, "You can use the following to parse the STdf file and convert it to the Panda DataFrame.\n\\begin{itemize}\n \\item Install [Python-STdf](https://pypi.org/project/python-stdf/) and set the path to it in your python environment\n.e.g\n`import stdf`\n   `stf_file_path = \"C:\\\\Users\\\\User\\\\Downloads\\\\STDF.stfd\"`.\n  \\begin {code} ```\n    stf = stdfs.StdfFile(stdfs_string =  stfd_str)\n                        .std_list()\n            .process() # Parsing the data\n           .to_dataframe()  # Converting the parsed data to pandas dataframe\n       .data_df()        # Getting the output dataframe of the converted data. The output will look like the below. Here we are printing the header of dataframe. But you can directly use this dataframe as needed. `\n        ```\\end{code}.\n```\nSTdF.STFD is the name of file that you want to convert to dataframe and `STdfs` is my package name. You need", 0.260869562625885, 0.009756097570061684, 0.13526569306850433, 0.04088050127029419], ["When I make a contour plot with Python, I have been using a set_aspect function. Because this allows me to avoid image/contour-lines distortion caused by changing both axis.\n\n: for example, ax1.set_aspect('equal', 'box-forced')\n\nBut, I just found that the option 'box-forced' is not valid in Python3.\n\nSo my question is, is there any alternative of the 'box-forced' option in Python3? I want exactly the same effect as the ax1.set_aspect('equal', 'box-forced') in Python2.\n\nThanks!", "I just found that there is a function plt.axis('scaled').\nIt seems that this does what I wanted in a recent version (3.7) of Python3.", "The x and y axis scaling in Contour plot", 1.2000000476837158, 0, 515, "In Python 3, you can use\n`ax.get_xlim()` and `ax2` to get the limits of x-axis\nand use `xscale` and  `yscale` in `set` function of matplotlib.matplotlib.axes.Axes.plot`\nto adjust the axis scale.", 0.15625, 0.0, 0.09375, 0.04544684290885925], ["I'm trying to compare two dataframes (df and df2) using .eq(), but it gives me false. I'm sure about the values:\n\nprint(df['ano'])\n0    2021\nName: ano, dtype: int64\n\n\nprint(df2['ano'])\n0     2020\n1     2019\n2     2019\n3     2018\n4     2017\n... \n89    2020\n90    2017\n91    2018\n92    2021\n93    2021\nName: ano, Length: 94, dtype: int64\n\n\nprint(df['ano'].eq(df2['ano']))\n0     False\n1     False\n2     False\n3     False\n4     False\n...  \n89    False\n90    False\n91    False\n92    False\n93    False\nName: ano, Length: 94, dtype: bool", "Solution:\ndf = df.drop_duplicates()\nx = 0\ny = 0\ndf = df.reset_index()\ndf2 = df2.reset_index()\nwhile(x = len(df)):\n    while(y = len(df2)):\n        if((df.at[x, 'a'] == df2.at[y, 'a']) & (df.at[x, 'b'] == df2.at[y, 'b']) & (df.at[x, 'c'] == df2.at[y, 'c'])):\n           print('found')\n        y += 1\n    x += 1\n    y = 0", "Problem with .eq() when comparing dataframes", 1.2000000476837158, 0, 516, "You can try `df.compare(other_df)` to see if they're the same, and `isna()` to check if there's a value in a given column.\nThis is the output: (https://pandas.pydata.org/docs/reference/api/pandas._Series.isnull.html)\n\\begin{itemize}\n  \\item df.values: If values are boolean, then isna() is equivalent to ~df\n   (Boolean Series).\n    If is_numeric: isnull() returns a boolean Series indicating which values in the\n      input are missing (or NaN) for each index. If the input Series contains\n       a mixture of numeric and non-numeric data, the nonnumeric elements will be\n        cast to float and compared elementwise. In this case, missing values will\n     return True if the Series is all NaNs. Missing values (either NA or NaN\n          values) are considered as missing for all operations. (This\n           behavior is consistent with pandas.DataFrame.dropna().) In addition,\n         NaN values and missing strings will return False. For example, a Series\n            of ['a', NA, 'c'] will", 0.08294931054115295, 0.0, 0.08294931054115295, 0.018248174339532852], ["For example, some numpy functions want me to specify datatype, like np.fromiter(). What am I supposed to choose for floats so that it is consistent with everything else I have?", "Use float, the Python built-in type. NumPy understands that (and will use C doubles under the hood, which is exactly what Python itself does as well).", "What is the default datatype for numpy floats?", 0.0, 0, 517, "`np.float64` is for the current default, but you can use `float32` if you want.", 0.1428571492433548, 0.0, 0.095238097012043, 0.13636364042758942], ["How can I generate random field with GStools for my correlation function: c(x)= (x)^(a-2)/(x^2 + 1)^(a/2) for differences values x and a=0.5?", "This is not possible with GSTools, since your covariance model is unbound, which refers to an infinite variance of the random field. GSTools only supports random fields with finite mean and variance.", "generate random number with GStools", 0.0, 0, 518, "You can't do that with a single field. You'll have to do this in two steps.\n\\begin{itemize}\nFirst, create a function that creates random values between 0 and 2, with values distributed according to a Gaussian, i.e.,\n(random(0,2))*exp(-( random(2)-0)^2/4).\nThen, use this function as a random function in your correlation. For example,\ncor( x, random_function )\nwill return a correlation between x values and random numbers. I think you're using a different correlation formula, but I hope you can adapt this to your needs.", 0.140625, 0.0, 0.09375, 0.017854224890470505], ["While trying to do ndimage.convolve on big numpy.memmap, exception occurs:\nException has occurred: _ArrayMemoryError\nUnable to allocate 56.0 GiB for an array with shape (3710, 1056, 3838) and data type float32\nSeems that convolve creates a regular numpy array which won't fit into memory.\nCould you tell me please if there is a workaround?\nThank you for any input.", "Scipy and Numpy often create new arrays to store the output value returned. This temporary array is stored in RAM even when the array is stored on a storage device and accessed with memmap. There is an output parameter to control that in many functions (including ndimage.convolve). However, this does not prevent internal in-RAM temporary arrays to be created (though such array are not very frequent and often not huge). There is not much more you can do if the output parameter is not present or a big internal is created. The only thing to do is to write your own implementation that does not allocate huge in-RAM array. C modules, Cython and Numba are pretty good for this. Note that doing efficient convolutions is far from being simple when the kernel is not trivial and there are many research paper addressing this problem.", "Performing ndimage.convolve on big numpy.memmap: Unable to allocate 56.0 GiB for an array", 0.1973753273487091, 2, 519, "This is an open issue with NumPy. It's a problem with memory mapping: https://github.com/numpy/scipy/issues/6357. This would be a good idea to raise an issue for 2.1 on GitHub.", 0.16393442451953888, 0.01104972418397665, 0.09836065769195557, 0.033557046204805374], ["I am trying to detect the unique/foreign objects in a conveyor. The problem is in our case is, we don't know which type of featured object is passes through conveyor along with raw material. I am familiar with object detection techniques such as yolov and detectron which can detect object based on the feature of object that we annotate. But in our case we don't know the feature of object.\nI am wondering for some generic object proposal models for detection. Please give some idea is there any pre-trained unsupervised models which suits for this? or some methods or algorithm that what can i go with?. I hope i had explained my problem as much enough. Thanks in advance.", "I think I understood well your issue...\nIf you do not want to train an object detection model because you may do not have the bounding boxes corresponding to the objects, you have several options. However, I do not think there is a pretrained model that fits on your problem since you should fine-tune it, and therefore, you should have some annotations.\n\nOne think you could do, as Virgaux Pierre said, you could use some classic clustering segmentation.\nOn the other hand, you could use a weakly-supervised approach which it only needs image-level labels, instead of the bounding boxes. This approach could fit well if you do not need high mAP. You could use CAM, GradCAM or other techniques to obtain activation maps. Furthermore, this approaches are easy to implement with a simple NN and some forward/backward hooks.\n\nHope it helps.", "Unsupervised object detection", 0.1973753273487091, 2, 520, "Your question is a little vague but I will try to answer as best as I can. To do what you want, you can use an object detector and you do not need to label them. You can just use the detections and then you will have to filter the correct ones. If you have a small number of categories you could use a simple classifier. In the case of your conveyer, the raw materials could be one class and the foreign objects another class. So, if you use YOLO v2 you would have 2 outputs, one for raw and one other for non-raw. Then you filter your output to get the good dets for each class or you just keep them all. Keep in mind that this will be a lot of work because you are looking at a whole image so you need a huge data set.", 0.3864406645298004, 0.08873720467090607, 0.20338982343673706, 0.05187812075018883], ["I am trying to use XGBRegressor for my data but keep getting the above error when doing a model.fit.\nI have tried:\nnp.any(np.isnan(df))\nnp.all(np.isfinite(df))\nwhich are both true.\nI tried getting rid of the inf and null values using:\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.fillna(0, inplace=True)\nbut the error still occurs.\nnp.all(np.isfinite(df)) is still showing true.\nMost errors I found on the website says \"Input contains..\" and not \"Label contains..\"", "This is a long shot, but I was having a similar error and couldn't figure it out. It turns out I was doing a log transform right before I tossed my data into the regressor, and I had negative values in my output that were going to infinity. I didn't catch it because I looked for NAs/infinite values before it hit the log transform part of the pipeline.", "XGBoostError:[18:46:19] D:\\Build\\xgboost\\xgboost-1.6-1.git\\src\\data\\data.cc:487:Check failed: valid: Label contains NaN, infinity or a value too large", 0.0, 0, 521, "From what I can tell you need to do the following: If your dataframe is a sparse matrix you will need the dense_matrix=False argument. Then you can use the argument \"col_sample_rate\" to control how many cols will be included. The default is 1, so you may need a larger value.", 0.12800000607967377, 0.016260161995887756, 0.07999999821186066, 0.017699113115668297], ["Typically the forward function in nn.module of pytorch computes and returns predictions for inputs happening in the forward pass. Sometimes though, intermediate computations might be useful to return. For example, for an encoder, one might need to return both the encoding and reconstruction in the forward pass to be used later in the loss.\nQuestion: Can Pytorch's nn.Module's forward function, return multiple outputs? Eg a tuple of outputs consisting predictions and intermediate values?\nDoes such a return value not mess up the backward propagation or autograd?\nIf it does, how would you handle cases where multiple functions of input are incorporated in the loss function?\n(The question should be valid in tensorflow too.)", "\"The question should be valid in Tensorflow too\", but PyTorch and Tensorflow are different frameworks. I can answer for PyTorch at least.\nYes you can return a tuple containing any final and or intermediate result. And this does not mess up back propagation since the graph is saved implicitly from the tensors outputs using callbacks and cached tensors.", "Forward function with multiple outputs?", 1.2000000476837158, 2, 522, "Pretraining, when using the [adam optimizer](https://pytorch.org/docs/stable/optim.html#adadelta), does not require the gradient of intermediate outputs to backpropagate through the intermediate layers of the model.  This means that you can return the encodings and decoding of your model as outputs, and then just train the decoder part with a cross entropy loss using those outputs.", 0.2689075767993927, 0.05128205195069313, 0.15126051008701324, 0.029605265706777573], ["I have a matrix m = np.array([[3,4], [5,6], [7,5]]) and a vector v = np.array([1,2]) and these two tensors can be multiplied.\nFor multiplication of the above two tensors, no. of columns of m must be equal to no. of rows of v\nThe shapes of m and v are (3,2) and (2,) respectively.\nHow is the multiplication possible, if m has 3 rows and 2 columns whereas v has 1 row and 2 columns?", "In NumPy, I would recommend not thinking too much about \"rows\" and \"columns\"\nAn array in numpy can have any number of dimensions - you can make 1-dimensional, 2-dimensional or 100-dimensional arrays. A 100-dimensional array doesn't have \"rows\" and \"columns\" and neither does a 1-dimensional array.\nThe simple rule for multiplying 1- or 2-dimensional arrays is: the last axis / dimension of the first array has to have the same size as the first axis / dimension of the second array.\nSo you can multiply:\n\na (3, ) array by a (3, 2) array\na (3, 2) array by a (2, 3) array\na (2, 3) array by a (3, ) array", "Matrix by Vector multiplication using numpy dot product", 0.1973753273487091, 2, 523, "If you have two matrices of different sizes, you can either:\n\\begin{itemize}\n  \\item reshape the smaller one to have the same size as the larger one. This is what you're doing with `m.reshape(m_shape)`.\nThis is necessary for the dot-product. If the matrices have different shapes, it doesn't make sense to multiply them. It's like trying to add 4 and \"4\".\nIf you don'm have any other dimensions to keep, this will do. However, the reshaped matrix will have `dtype=float64` instead of `float32`.\n(If `v` is an ndarray of shape (100,), then `dot(v,m)` will return a 62x60 matrix. You can avoid this by using `a.shape[0] * b.size[1]`, instead.)\n \\em{Note}: If `mat` has any `0` elements, then they will be ignored by the `.reshaping`\n    method. So, to prevent this, use `np.where(mat==0) to fill the elements with 0. For", 0.282527893781662, 0.0749063640832901, 0.1338289976119995, 0.04024145007133484], ["I have retail sales data of the whole Germany, for example beer revenue. Now I want to find a way to divide that number into 596 cities of Germany based on the GDP per capita of each city and consumer spending of each city. So after that I can have the beer revenue of each single city in Germany.\nMy assumption is: city beer = city consumer spending * x + city GDP per cap * y. and then sum of city beer = national beer\nCould you please advice which kind of algorithm or a way to do it in Python?\nThank you so much.", "Your assumption is not so good. Some cities may spend a bigger fraction of their total consumation in beer.\nI think a better assumption is that it's a variable fraction of the total consumation in beer, let's say, city i consumes a fraction xi of national consumation in beer, where xi is somehow dependent on the GDP and on the city consumation.\nTo find xi, firstly scale the GDP to be in [delta, 1-delta], where delta is a positive quantity very close to zero, and keep their relative order. To do that, consider the biggest GDP is GDPmax and the minimum GDP is GDPmin. Then, map each GDPi to\nscaleGDPi = [(GDPi - GDPmin) * (1 - 2 * delta)/(GDPmax-GDPmin)] + delta.\nIn a similar way, also scale the consumation to be in [delta, 1 - delta].\nThen, consider xi = scaleGDPi * scaleConsumationi * x and you get (city beer)i = scaleGDPi * scaleConsumationi * x * national beer\nBy imposing that the sum of city beer is equal to national beer, you get:\nx = 1 / (sum scaleGDPi * scaleConsumationi).\nSo, city beer = (scaleGDPi * scaleConsumationi * national beer)/(sum scaleGDPi * scaleConsumationi).\nI think this would be a more adequate modelization of your problem.", "Divide a number to many groups based on several factor using Python", 1.2000000476837158, 0, 524, "The basic algorithm is to split the data up into five buckets, one for each of your factors. Then find the average of all the rows in that bucket and add the value of that average to the bucket. Next, do the same thing for the second bucket, the third bucket etc. You then have a list of buckets, each containing the summed values of a group of rows. Once you have all of these bucketed values you can create a new dataframe and set the values in it equal to each bucket in the order they were created. In the end you should have one dataframe with one column for city, another column with the total revs of beers sold in each respective city according to that city's factors and a third column which should be the overall national reivs.", 0.3035714328289032, 0.01796407252550125, 0.1547619104385376, 0.0498327799141407], ["I am performing multi-class text classification using BERT in python. The dataset that I am using for retraining my model is highly imbalanced. Now, I am very clear that the class imbalance leads to a poor model and one should balance the training set by undersampling, oversampling, etc. before model training.\nHowever, it is also a fact that the distribution of the training set should be similar to the distribution of the production data.\nNow, if I am sure that the data thrown at me in the production environment will also be imbalanced, i.e., the samples to be classified will likely belong to one or more classes as compared to some other classes, should I balance my training set?\nOR\nShould I keep the training set as it is as I know that the distribution of the training set is similar to the distribution of data that I will encounter in the production?\nPlease give me some ideas, or provide some blogs or papers for understanding this problem.", "P(label | sample) is not the same as P(label).\nP(label | sample) is your training goal.\nIn the case of gradient-based learning with mini-batches on models with large parameter space, rare labels have a small footprint on the model training. So, your model fits in P(label).\nTo avoid fitting to P(label), you can balance batches.\nOverall batches of an epoch, data looks like an up-sampled minority class. The goal is to get a better loss function that its gradients move parameters toward a better classification goal.\nUPDATE\nI don't have any proof to show this here. It is perhaps not an accurate statement. With enough training data (with respect to the complexity of features) and enough training steps you may not need balancing. But most language tasks are quite complex and there is not enough data for training. That was the situation I imagined in the statements above.", "Is it necessary to mitigate class imbalance problem in multiclass text classification?", 0.06656807661056519, 1, 525, "In the general case, when you have a class-imbalancing problem, the most important step is to balance your training data (e.g., undersample or over-sample classes).\n\\begin{itemize}\n  \\item If the classes are \\strong{balances}, then the model will have an equal chance of being over-fitted to any of them. In that case it doesn't matter much whether you omit a few of each class or not. Oversample if you want to speed up the learning. But beware that oxygen masks should never be applied to already dead people. If you can, you should always owe-sample your data, but this is hard to do if your classes have different sizes. There are methods that will try to ower-sampling the smaller classes to make sure they are not too rare. I would recommend reading the book [The Elements of Statistical Learning](https://www.coursera.org/learn/statistical-learning/home/welcome) or this [link](http://archive.ics.uci.edu/ml/machine_learning_databases/iris/task.html) to learn more about", 0.32826748490333557, 0.04281345382332802, 0.13981762528419495, 0.05978260934352875], ["I am performing multi-class text classification using BERT in python. The dataset that I am using for retraining my model is highly imbalanced. Now, I am very clear that the class imbalance leads to a poor model and one should balance the training set by undersampling, oversampling, etc. before model training.\nHowever, it is also a fact that the distribution of the training set should be similar to the distribution of the production data.\nNow, if I am sure that the data thrown at me in the production environment will also be imbalanced, i.e., the samples to be classified will likely belong to one or more classes as compared to some other classes, should I balance my training set?\nOR\nShould I keep the training set as it is as I know that the distribution of the training set is similar to the distribution of data that I will encounter in the production?\nPlease give me some ideas, or provide some blogs or papers for understanding this problem.", "This depends on the goal of your classification:\n\nDo you want a high probability that a random sample is classified correctly? -> Do not balance your training set.\nDo you want a high probability that a random sample from a rare class is classified correctly? -> balance your training set or apply weighting during training increasing the weights for rare classes.\n\nFor example in web applications seen by clients, it is important that most samples are classified correctly, disregarding rare classes, whereas in the case of anomaly detection/classification, it is very important that rare classes are classified correctly.\nKeep in mind that a highly imbalanced dataset tends to always predicting the majority class, therefore increasing the number or weights of rare classes can be a good idea, even without perfectly balancing the training set..", "Is it necessary to mitigate class imbalance problem in multiclass text classification?", 0.06656807661056519, 1, 526, "\\begin{blockquote}\nThe distribution in production will be same as in training data\n\\end{itemize}", 0.06756756454706192, 0.0, 0.054054055362939835, 0.0238095223903656], ["I have 6 points with their coordinates in the Cartesian plane, XY, placed on one side of a circle. Using the least square method it is relatively easy to fit a circle to those 6 points and to find the radius and the center of the fitted circle again in XY coordinates..\nHowever, I also have Altitude Azimuth coordinates for those 6 points, because those points are on the sky, so I was wondering is it possible to fit a curve to those curved coordinates and then find the center of that circle.", "Project your points on the unit sphere and compute the best fitting plane. The normal vector of the plane points towards the center of that circle. The radius of your circle will be equal to sqrt(1-d\u00b2) if d is the distance between the plane and the origin or acos(d) if you want the angle between the center and a point of the circle (since we're doing spherical geometry).\nEDIT : do an orthogonal regression because if you don't, the z-axis could be favored over the others or vice-versa.", "How to find center of the circle if the data points are in curved coordinates system(Horizontal - AltAz)", 0.0, 0, 527, "If the altitude is measured in radians, then the transformation from X,Y to Alt,Az is\n$$\n\\left(\n{\\begin{array}{c}\nx \\\\\ny \\\\[1ex]\nz\n \\end{bmatrix}}\n=\n{\n  \\begin {array} {c c c} 1 & 0 & { \\cos(\\alpha )} \\\\  { - \\sin(\\theta ) } &  1  & {\\cos (\\theta)}  \\\\  0 \t&  {\\sin (\\alpha) }&{ \\tan (\\beta )}\\\\ \n     \\hspace{0.25cm} \\beta & \\alpha & z\\\\  x & y &1 \\ \\\n    \\left\\rangle ^{\\ \\ } \\right. \\!\n} $$\nThe center is given by\n$ (r \\cdot e_1, r \\times e_{\\alpha})$\nwhere\n$\\vec{e_x}=(1,\\sin \\theta ,\\cos \\gamma)$,\n $\\vec e_\\theta=(-\\cos\\theta,\\cos 2\\gamma ,-\\sin 3\\alpha)$, and\n $e_\\alpha=(\\sin\\beta ,0,-\\", 0.10471203923225403, 0.010582010261714458, 0.07329843193292618, 0.02233273908495903], ["My problem is as follows. I have a 2d image of some tissue and a 3d stack of the same region of the tissue and plus more tissue that does not go into my 2d image. Now, the 3d stack is slightly rotated with respect to the 2d image, but also has some local deformation, so I can't simply apply a rigid rotation transformation. I can scroll through the 3d stack and find individual features that are common to the 2d image. I want to apply a nonlinear transformation such that in the end I can find my source 2d image as a flat plane in the 2d stack.\nMy intuition is that I should use thin plate spline for this, may the scipy RBF interpolator, but my brain stops working when I try to implement it. I would use as input arguments let's say 3 points (x1, y1, 0), (x2, y2, 0) and (x3, y3, 0) with some landmarks on the 2d image and then (x1', y1', z1'), (x2', y2', z2') and (x3', y3', z3') for the corresponding points into the 3d stack. And then I get a transformation but how do I actually apply this to an image? The bit that confuses me is that I'm working with a 3D matrix of intensities, not a meshgrid.", "scipy RBF is designed to interpolate scattered data, it's just a spline interpolator. To warp a domain, however, you need to find another library or write TPS (thin plate spline) yourself; scipy doesn't do it. I recommend you check VTK, for example. You feed your landmark information of the reference image and the target image to a vtkThinPlateSplineTransform object. Then you can get the transformation matrix and feed it to a vtkImageReslice object, which warps your image accordingly.", "Thin plate spline interpolation of 3D stack python", 0.0, 0, 528, "I don' t know if it will solve your problem but maybe this could help you.  I use it to get the position of a landmark in a stack with different orientations. The landmarke is a line segment.   I hope this help. Sorry for my bad english. Good luck.", 0.26356589794158936, 0.031496062874794006, 0.10852713137865067, 0.038599200546741486], ["I've tried finding information regarding this online but the word overwrite does not show up at all in the official Tensorflow documentation and all the Stack Overflow questions are related to changing the number of copies saved by the model.\nI would just like to know whether or not the save function overwrites at all. If I re-train a model and would like to re-run the save function will the newer model load in when I use the load_model function? Or will it be a model that is trained on the same data twice? Do older iterations get stored somewhere?", "You can use\nmodel.save('./model.h5')\nwhich will save the model to a file\nand\nmodel = tf.keras.models.load_model('./model.h5')\nto load the model", "Does the TensorFlow save function automatically overwrite old models? If not, how does the save/load system work?", 0.0, 0, 529, "\\begin{blockquote}\n\\em{I'd just simply like a yes or no answer. It doesn't seem to be explained very well anywhere online.}\n[This](https://www.tensorflow.org/api_docs/python/tf/train/ModelCheckpoint) is how I found the answer myself. I have no idea why the documentation is so sparse. The following code is taken from the tensorflow docs and shows how to save and load models. To answer your question, if you retrain the network, it will load the older model when you run load. This is what I believe is called a checkpoint. Hope this helps. Good luck. Here is the link to the [documentation](http://tensorflow%20docs)\n. [This answer](//stackoverflow.com/a/36652818/1068494) explains it in more detail. Just in case this link breaks, here is a [screenshot](://i.stack.imgur.COM/gJwY4.png) of the section in question. Cheers\n - [link to screenshot](///i-stack-imgur-com-gjwy4-png).png", 0.15217390656471252, 0.010989011265337467, 0.08695652335882187, 0.024192754179239273], ["I've tried finding information regarding this online but the word overwrite does not show up at all in the official Tensorflow documentation and all the Stack Overflow questions are related to changing the number of copies saved by the model.\nI would just like to know whether or not the save function overwrites at all. If I re-train a model and would like to re-run the save function will the newer model load in when I use the load_model function? Or will it be a model that is trained on the same data twice? Do older iterations get stored somewhere?", "I think Eyal's answer is a good point to start. However, if you want to be sure you can let your program delete the previous model or change it's name on the fly. I also observed different results when deleting a model and not, but this could also be effects of the different training process, due to random initialization and updating the weights.", "Does the TensorFlow save function automatically overwrite old models? If not, how does the save/load system work?", 0.0, 0, 530, "This is not automatic.  You can use a `tf.train.Checkpoint` to automatically save the state of the training process. This will save your model, optimizer state, and other parameters that TF needs to restore your training state.", 0.3106796145439148, 0.059405941516160965, 0.17475728690624237, 0.03664921224117279], ["I used OpenCV's connectedComponentsWithStats function to do connected component labeling on an image. I would now like to be able to access all the pixels that have a certain label, or check the label that is assigned to a particular pixel while iterating through the image. How can I do this? I plan on iterating through the image using a nested for loop.", "connectedComponents* literally gives you a \"labels map\". You look up the pixel's position in there and you get the label for that pixel.\nIf you need a mask for one specific label, you calculate mask = (labels_map == specific_label)\nDo not \"iterate\" through images. Python loops are slow. Whatever you do, consider how to express that with library functions (numpy, OpenCV, ...). There are ways to speed up python loops but that's advanced and likely not the right solution for your problem.", "How do you access a pixel's label ID that is given to each pixel after connected component labeling?", 0.1973753273487091, 1, 531, "If you used `imread` to load the original image, then you can use `findContours` with the `CV_RETR_LIST` flag, and then use the returned list of contours to retrieve the pixel indices of the contour vertices. Here're the pseudo-code (I'm not 100% sure if `cv::find_contours()` works this way):\n\\begin{itemize}\n  \\item `contour_ids = findContour(image, cv::RETRIEVE_LARGEST_LABEL)`\n        (where `image` is your loaded image and `RETVIEW_MAX_SIZE` should be equal to 20)\n      \\begin {code}", 0.1818181872367859, 0.024539876729249954, 0.1090909093618393, 0.02933950163424015], ["I've done research and can't find anything that has solved my issue. I need a python script to read csv files using a folder path. This script needs to check for empty cells within a column and then display a popup statement notifying users of the empty cells. Anything helps!!", "Use the pandas library\npip install pandas\nYou can import the excel file as a DataFrame and check each cell with loops.", "Python script to check csv columns for empty cells that will be used with multiple excels", 0.0, 0, 532, "\\begin{code}\nimport csv\n# create a list of all csv file in the folder\nfiles = [file for file\n in os.listdir(folderPath)\nif file.endswith(\".csv\")]\n\\end{pre}", 0.1599999964237213, 0.0, 0.11999999731779099, 0.041237108409404755], ["In pandas, how can I filter for rows where ALL values are higher than a certain threshold?\nSay I have a table that looks as follows:\n\n\n\n\nCity\nBird species one\nBird species two\nBird Species three\nBird species four\n\n\n\n\nA\n7\n11\n13\n16\n\n\nB\n11\n12\n13\n14\n\n\nC\n20\n21\n22\n23\n\n\nD\n8\n6\n4\n5\n\n\n\n\nNow I only want to get rows that have ALL COUNTS greater than 10. Here that would be Row B and Row C.\nSo my desired output is:\n\n\n\n\nCity\nBird species one\nBird species two\nBird Species three\nBird species four\n\n\n\n\nB\n11\n12\n13\n14\n\n\nC\n20\n21\n22\n23\n\n\n\n\nSo, even if a single values is false I want that row dropped. Take for example in the example table, Row A has only one value less than 10 but it is dropped.\nI tried doing this with df.iloc[:,1:] >= 10 which creates a boolean table and if I do df[df.iloc[:,1:] >= 10] it gives me table that shows which cells are satisfying the condition but since the first column is string all of it labelled false and I lose data there and turns out the cells that are false stay in there as well.\nI tried df[(df.iloc[:,2:] >= 10).any(1)] which is the same as the iloc method and does not remove the rows that have at least one false value.\nHow can I get my desired output? Please note I want to keep the the first column.\nEdit: The table above is an example table, that is a scaled down version of my real table. My real table has 109 columns and is the first of many future tables. Supplying all column names by hand is not a valid solution at all and makes scripting unfeasible.", "df[(df[df.columns[1:]]>x).all(axis=1)] where x should be replaced with the values one wants to test turns out to be the easiest answer for me. This makes it possible to parse the dataframe without having to manually type out the column names. This also assumes that all of your columns other than the first one are integers. Please make note of the other answer above that tells you how to make note of dtypes if you have mixed data.\nI only slightly changed Rodrigo Laguna answer above.", "In pandas, how can I filter for rows where ALL values are higher than a certain threshold? And keep the index columns with the output?", 0.0996679961681366, 1, 533, "You can use `all` like this: `df[all(df > 9)]`\nwhere `9` is your threshold\nIt returns: True if all the values of the Series are greater or equal to 0 and False if any of them are not.", 0.25954198837280273, 0.03100775182247162, 0.15267175436019897, 0.06484107673168182], ["hello im on the path of learning the python and i am struggling to understand this problem can you please help me to solve this problem\nPrint out the 50th row of np_baseball.\nwhy the answer for this command is  [49, :]\nFrom my perspective if the asking for the 50th it should be just [49] why there is additional :\nWill be extremely glad for your respond", "baseball is available as a regular list of lists\nImport numpy package\nimport numpy as np\nCreate np_baseball (2 cols)\nnp_baseball = np.array(baseball)\nPrint out the 50th row of np_baseball\nprint(np_baseball[49:50])\nSelect the entire second column of np_baseball: np_weight_lb\nnp_weight_lb=np_baseball[:,1]\nPrint out height of 124th player\nprint(np_baseball[123, 0])", "Subsetting 2D NumPy Arrays", 0.0, 0, 534, "The colon at the end means that the row is a 1D array.  In other words, the result is an array that is one element long, and contains the value 48.", 0.12244898080825806, 0.0, 0.10204081982374191, 0.06381706148386002], ["I am working on a project where I am combining 300,000 small files together to form a dataset to be used for training a machine learning model. Because each of these files do not represent a single sample, but rather a variable number of samples, the dataset I require can only be formed by iterating through each of these files and concatenating/appending them to a single, unified array. With this being said, I unfortunately cannot avoid having to iterate through such files in order to form the dataset I require. As such, the process of data loading prior to model training is very slow.\nTherefore my question is this: would it be better to merge these small files together into relatively larger files, e.g., reducing the 300,000 files to 300 (merged) files? I assume that iterating through less (but larger) files would be faster than iterating through many (but smaller) files. Can someone confirm if this is actually the case?\nFor context, my programs are written in Python and I am using PyTorch as the ML framework.\nThanks!", "Usually working with one bigger file is faster than working with many small files.\nIt needs less open, read, close, etc. functions which need time to\n\ncheck if file exists,\ncheck if you have privilege to access this file,\nget file's information from disk (where is beginning of file on disk, what is its size, etc.),\nsearch beginning of file on disk (when it has to read data),\ncreate system's buffer for data from disk (system reads more data to buffer and later function read() can read partially from buffer instead of reading partially from disk).\n\nUsing many files it has to do this for every file and disk is much slower than buffer in memory.", "Is it more beneficial to read many small files or fewer large files of the exact same data?", 1.2000000476837158, 0, 535, "If your dataset is already partitioned into 20 chunks of 150K samples each, and you can access them in a random way, then there's no reason to do anything else than keep doing that. You are already doing it. Just be careful to keep all chuncks together. This will make it easier for you to run the data through a pipe or other network operations (like uploading it to Google Cloud for instance).\nIt'll also save you time on the model-building side, because the training will be easier to parallelize, especially on multiple GPUs.", 0.24186046421527863, 0.009389671497046947, 0.10232558101415634, 0.05045870691537857], ["So I installed vscode and anaconda on macos. I'm not a very experienced programmer and I can't figure out what is wrong. When I try to import numpy or pandas it says module not found. Any help to get this working is appreciated?", "In VSCode you need to specify the interpreter you want to use to run your code. To do so, either click the button next to \"Python\" in the bottom right of the UI or search (CMD shift P) \"Python: select interpreter,\" then select the right interpreter / environment you want to use.", "VSCode, Anaconda on MacOS. Module not found (Pandas and Numpy for eg.)", 0.0, 0, 536, "The problem is that VS code does not recognize the Anacondas python interpreter (it's not in the list of installed interpreters).\nYou can add it by selecting \"Install\" from the VScode menu. Then go to \"Settings\" > \"User Settings\" and select \"python.pythonPath\" in \"Preferences: Settings\". Then select the folder that contains the anacoda interpreter and add its path to the path list.\nThen open the python terminal with the following command:\n\"$ VSCODE_TOOLS_PATH/python --python=/path/to/anaconda/interpreter/folder\"\nNow you should be able to use the packages of Anacode.", 0.3472222089767456, 0.056338027119636536, 0.2361111044883728, 0.05957446247339249], ["I stored some financial market data in a Polars DataFrame. As for analysis, it is is fast to run some groupby(\"date\").agg() action.\nBut in a realtime scenario , the new data is coming time by time, I don't want to concat the new data with old data again and again, it is slow and use a lot of memory. So is there a blazing fast way to spilt the old data DataFrame into small DataFrame groupby datetime column which stored in a vector or hashmap, so when the new data comes, I just push the new into vector for future calculation?", "Polars has a DataFrame::partition_by function for this.", "How to spilt a big DataFrame into Vec by group in Polars", 0.0, 0, 537, "`groupBy` creates a `GroupByGroup` which contains a group for each unique `date` and a list of rows per group. You can access the `list` via the field `values`\nYou can then iterate over the groups and the rows. This is a bit convoluted to do in plain Polaris. But if you use some kind of framework that is able to execute code that you can write in Python, you could do the following:\n\\begin{itemize}\nFirst create a function that iterates over each group and performs some action on each row. For example,\ncreate a new Dataframe with only the last row for a given `id`.\nThen you would use `map` to iterate on the Dataframes. The result is also a Datafame. In that DataFame you have all the result you want. I can not provide a working example because I have no data and Polar is too difficult to use to write a test.", 0.060975611209869385, 0.0, 0.03658536449074745, 0.0], ["Trying to read excel table that looks like this:\n\n\n\n\n\nB\nC\n\n\n\n\nA\ndata\ndata\n\n\ndata\ndata\ndata\n\n\n\n\nbut read excel doesn't recognizes that one column doesn't start from first row and it reads like this:\n\n\n\n\nUnnamed : 0\nB\nC\n\n\n\n\nA\ndata\ndata\n\n\ndata\ndata\ndata\n\n\n\n\nIs there a way to read data like i need? I have checked parameters like header = but thats not what i need.", "You can skip automatic column labeling with something like pd.read_excel(..., header=None)\nThis will skip random labeling.\nThen you can use more elaborate computation (e.g. first non empty value) to get the labels such as\ndf.apply(lambda s: s.dropna().reset_index(drop=True)[0])", "pandas read excel without unnamed columns", 0.0, 0, 538, "Use the `delim_whitespace` parameter to pandas, it tells pandas to ignore whitespace in your delimiter. So it will skip the spaces in between your columns.", 0.13333334028720856, 0.027397260069847107, 0.07999999821186066, 0.03758964687585831], ["using pydiffmap, I could find a nice low dimension Manifold in my data, and extract what seems to be meaningful low dimension components.\nI would like now to reverse the operator, and project my data back to my original high dimensional space keeping only these few important dimensions I could identify.\nFirst, is this mathematically possible? And if so how to do it?\nThanks a lot!", "I just went into the Diffusion algorithm behind the package, and realized that there is no guarantee that you can go from a vector in the diffusion space back into the data space.\nThis is because the diffusion space represent the distances to the original data points. So if at least two points are different, the null vector in the diffusion space (at distance 0 of all original data points in the data space) will have no equivalent in the data space.\nHope this can help someone else!", "pydiffmap: How to reverse Diffusion Map Embedding and reconstruct original variables from several principal components?", 0.0, 0, 539, "You can do this using the `reconstruct_data` method. Note that the original data must be stored in a numpy array. So, you should first load the data into memory. If you have the components as a list of tuples, then you need to convert them to a 2-D numpy ndarray. The following example will load data from the example dataset from https://github.com/Pydiffe/diffmap_benchmark. Load the dataset using `np.loadtxt` and load it into the array `X`.\nThen, load your PCA components and do the reconstruction using\n`reconst_X = pcd.recons_PCA(X,P).data.T`. This will return a single 1-d array that contains the coordinates of the points in the reprojected space.", 0.36538460850715637, 0.06796116381883621, 0.17307692766189575, 0.049382712692022324], ["Check application requirements \nCompile platform \nRun '/usr/bin/python3 -m pythonforandroid.toolchain create --dist_name=demo3 --bootstrap=sdl2 --requirements=python3,kivy,pillow,kivymd,asyncio,bleak --arch armeabi-v7a --copy-libs --color=always --storage-dir=\"/content/.buildozer/android/platform/build-armeabi-v7a\" --ndk-api=21 --ignore-setup-py --debug' \nCwd /content/.buildozer/android/platform/python-for-android \n\n[INFO]:    Will compile for the following archs: armeabi-v7a \n[INFO]:    Found Android API target in $ANDROIDAPI: 27 \n[INFO]:    Available Android APIs are (27) \n[INFO]:    Requested API target 27 is available, continuing.\n[INFO]:    Found NDK dir in $ANDROIDNDK: /root/.buildozer/android/platform/android-ndk-r19c \n[INFO]:    Found NDK version 19c \n[INFO]:    Getting NDK API version (i.e. minimum supported API) from user argument \nTraceback (most recent call last): \nFile \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n\"main\", mod_spec) \nFile \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\nexec(code, run_globals) \nFile \"/content/.buildozer/android/platform/python-for-android/pythonforandroid/toolchain.py\", line 1294, in \nmain() \nFile \"/content/.buildozer/android/platform/python-for-android/pythonforandroid/entrypoints.py\", line 18, in main\nToolchainCL() \nFile \"/content/.buildozer/android/platform/python-for-android/pythonforandroid/toolchain.py\", line 728, in init\ngetattr(self, command)(args) \nFile \"/content/.buildozer/android/platform/python-for-android/pythonforandroid/toolchain.py\", line 144, in wrapper_func\nuser_ndk_api=self.ndk_api) \nFile \"/content/.buildozer/android/platform/python-for-android/pythonforandroid/build.py\", line 423, in prepare_build_environment\nself.ccache = sh.which(\"ccache\") \nFile \"/usr/local/lib/python3.7/dist-packages/sh-1.14.3-py3.7.egg/sh.py\", line 1524, in call\nreturn RunningCommand(cmd, call_args, stdin, stdout, stderr) \nFile \"/usr/local/lib/python3.7/dist-packages/sh-1.14.3-py3.7.egg/sh.py\", line 788, in init\nself.wait() \nFile \"/usr/local/lib/python3.7/dist-packages/sh-1.14.3-py3.7.egg/sh.py\", line 845, in wait\nself.handle_command_exit_code(exit_code) \nFile \"/usr/local/lib/python3.7/dist-packages/sh-1.14.3-py3.7.egg/sh.py\", line 869, in handle_command_exit_code \nraise exc \nsh.ErrorReturnCode_1:\nRAN: /usr/bin/which ccache\nSTDOUT:\nSTDERR:\n\nCommand failed: /usr/bin/python3 -m pythonforandroid.toolchain create --dist_name=demo3 --bootstrap=sdl2 --requirements=python3,kivy,pillow,kivymd,asyncio,bleak --arch armeabi-v7a --copy-libs --color=always --storage-dir=\"/content/.buildozer/android/platform/build-armeabi-v7a\" --ndk-api=21 --ignore-setup-py --debug\n\nENVIRONMENT: \nCUDNN_VERSION = '8.0.5.39' \nPYDEVD_USE_FRAME_EVAL = 'NO' \nLD_LIBRARY_PATH = '/usr/local/nvidia/lib:/usr/local/nvidia/lib64' \nCLOUDSDK_PYTHON = 'python3' \nLANG = 'en_US.UTF-8' \nENABLE_DIRECTORYPREFETCHER = '1' \nHOSTNAME = 'ca63256296ed' \nOLDPWD = '/' \nCLOUDSDK_CONFIG = '/content/.config' \nUSE_AUTH_EPHEM = '1' \nNVIDIA_VISIBLE_DEVICES = 'all' \nDATALAB_SETTINGS_OVERRIDES = '{\"kernelManagerProxyPort\":6000,\"kernelManagerProxyHost\":\"172.28.0.3\",\"jupyterArgs\":[\"--ip=172.28.0.2\"],\"debugAdapterMultiplexerPath\":\"/usr/local/bin/dap_multiplexer\",\"enableLsp\":true}' \nENV = '/root/.bashrc' \nPAGER = 'cat' \nNCCL_VERSION = '2.7.8' \nTF_FORCE_GPU_ALLOW_GROWTH = 'true' \nJPY_PARENT_PID = '41' \nNO_GCE_CHECK = 'False' \nPWD = '/content' \nHOME = '/root' \nLAST_FORCED_REBUILD = '20220712' \nCLICOLOR = '1' \nDEBIAN_FRONTEND = 'noninteractive' \nLIBRARY_PATH = '/usr/local/cuda/lib64/stubs' \nGCE_METADATA_TIMEOUT = '3' \nGLIBCPP_FORCE_NEW = '1' \nTBE_CREDS_ADDR = '172.28.0.1:8008' \nTERM = 'xterm-color' \nSHELL = '/bin/bash' \nGCS_READ_CACHE_BLOCK_SIZE_MB = '16' \nPYTHONWARNINGS = 'ignore:::pip._internal.cli.base_command' \nMPLBACKEND = 'module://ipykernel.pylab.backend_inline' \nCUDA_VERSION = '11.1.1' \nNVIDIA_DRIVER_CAPABILITIES = 'compute,utility' \nSHLVL = '1' \nPYTHONPATH = '/env/python' \nNVIDIA_REQUIRE_CUDA = ('cuda>=11.1 brand=tesla,driver>=418,driver<419 '\n'brand=tesla,driver>=440,driver<441 brand=tesla,driver>=450,driver<451') \nTBE_EPHEM_CREDS_ADDR = '172.28.0.1:8009' \nCOLAB_GPU = '0' \nGLIBCXX_FORCE_NEW = '1' \nPATH = '/root/.buildozer/android/platform/apache-ant-1.9.4/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin' \nLD_PRELOAD = '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' \nGIT_PAGER = 'cat' \n_ = '/usr/local/bin/buildozer' \nPACKAGES_PATH = '/root/.buildozer/android/packages' \nANDROIDSDK = '/root/.buildozer/android/platform/android-sdk' \nANDROIDNDK = '/root/.buildozer/android/platform/android-ndk-r19c' \nANDROIDAPI = '27' \nANDROIDMINAPI = '21' \n\nBuildozer failed to execute the last command\nThe error might be hidden in the log above this error\nPlease read the full log, and search for it before\nraising an issue with buildozer itself.\nIn case of a bug report, please add a full log with log_level = 2", "There is a problem with the python-for-android build.py file which they currently fixing(something related to sh, and shoudl be fixed soon)\nmeanwhile as a workaround it is suggested to uncomment p4a.branch = master in the buildozer spec file and change the \"master\" to \"develop\"", "Buildozer stopped working all of a sudden despite always uploading the same files to Colab", 1.2000000476837158, 0, 540, "I had the exact same issue with the python3 buildozzer. The issue was that the Python3 version of Python's 'pip' module was not installed on my system. I got a similar error message when I ran pip3 install kivyslider. After I installed Python pip, the buildoozzr command worked.", 0.12371134012937546, 0.021052632480859756, 0.10309278219938278, 0.025528136640787125], ["If there is no predefined column types(nominal/interval) stored and some of variables are encoded as 1,2,3... in place of actual Categories (e.g. Good, better, bad....) if we see, automatically it may be classified as interval variables but actually they are nominal variables that are encoded.\nIs there any way to identify such variables?\nI thought of cardinality but threshold becomes an issue here please suggest some other solution.\nI'm good with python solution but if someone can give idea on SAS will be helpful :)", "as a Data Analyst, its your call to consider the categorical column as nominal or ordinal (depending on the data).\n\nif nominal data --> use dummy variable.(or one hot encoding)\n\nif ordinal data --> use map() function for label-encoding.\n\nif nominal data and cardinality is high --> encoding according to frequency count (lets say there are 30 different categories in a column, there are 1000 rows , 3 categories have high frequency count ,so these will be in separate 3 categories, other 17 have very low, so put all these 17 in 1 single category. ie. There will be only 4 categories, not 30).\n\n\napart from object type(string) columns, To identify categorical variables:\nfrequency count plays very important role for numeric columns.", "Classify variables in Nominal/ordinal/interval/binary in case user inputs not provided?", 0.0, 0, 541, "You can create a list of categorical values and then check if the variable is already encoded with a value from that list.  For example, let's say the variables you want to check are called `good`, `better`, and `bad`.  You could create the list with the following code:\n\\begin{code}\ndata have;\nlength good better bad 5;   input good:='good' better:'bettern' bad:'bad';\ncards good 2; cards better 3;cards bad2 4;run;quit;  end;    run;     /* I'll put in a 0 if it'd already been encoded */\ndrop good; drop better;drop bad;        /* to keep the output clean */;            /*\n                                               /*   I don't know what you're\n  using these for, so I won' t know if this is right for your\n   specific case. */  ;\n        keep good_cat better_ratio betterbad_count;/* if you use these */    /* just change the values to 660 for now */   run  /* keep in mind that 99 is invalid,\n        0", 0.24175824224948883, 0.007380073890089989, 0.10256410390138626, 0.017962608486413956], ["I have a table that looks like this\n\n\n\n\nindex\nGroup\nrank\nValues\n\n\n\n\n0\na\n2\n344.0\n\n\n1\na\n3\nNaN\n\n\n2\nb\n1\n455.0\n\n\n3\na\n1\nNaN\n\n\n4\nb\n2\nNaN\n\n\n\n\nI want to group data by 'Group', then sort according to 'rank' and then bfill only for rank == 1. The dataset is very big so I want to avoid loops.\nI tried\ntemp[temp['rank']<=2].sort_values('rank', ascending = True).groupby('Group').bfill(axis='rows', inplace = True)\nbut this gives me\n\"backfill() got an unexpected keyword argument 'axis'\"", "df.sort_values(by = 'rank', inplace = True)\ndf = df.assign(Values1 = lambda x: x['rank'] == 1).fillna(method = 'bfill')\ndf.groupby(by = 'Group')['Values']", "Bfill on Groupby object", 0.0, 0, 542, "\\begin{code}\ndf = df.sort(by=['Group'], asc=True) #sort by Group\n#fill NaN\nmissing_value = 'NaN'\nfor index, group in df:\n    if group['ran'] == missing_val: #if rank = NaN then fill with missing value\n        df[index] = pd.Series([missing value], index=index, columns=df.columns)#replace NaN with value of missing val\n\\end{pre}", 0.2531645596027374, 0.07792207598686218, 0.17721518874168396, 0.029598554596304893], ["I want to train my custom license plate using Yolov5, but I have a problem.\nMy problem is that my dataset is separated for each character and I have no idea how to make  annotation file suitable for Yolo which is as follows.\nBecause what I've seen so far, for triainig, you definitely need the entire license plate, which can be used to label each of the characters.\nAnd my question is, if I train these images, can I achieve a license plate recognition system?", "With Yolov5, you can achieve a good licence plate detection system, Yolov5 wont be reach the succes to recognite the licence plates itself. After the detection(with Yolov5) you can extract the information from bounding boxes and use it for recognition.", "train custom plate recognition using yolo", 0.0, 0, 543, "YOLO is a very generic object detector. You can train it to recognize license plates or any other object, even humans. The problem with it is the huge amount of parameters you have to tweak to get it working well. And this is where the \"train\" part of your question comes in. If you don't have the expertise to do that, then you'll have better luck with [faster rcnn](https://pjreddie.com/media/fastrcNN.pdf). It's a good starting point, and it uses YLOLO as a base deteector, so you can get the baseline performance from there. It has a couple of extra layers on top of YOLOS (the YoLoOc version), but you should be able to understand it if you read the paper.", 0.2142857164144516, 0.03614457696676254, 0.1428571492433548, 0.04221358895301819], ["I'm trying to retrieve the industry from occupation values. There are around 50 distinctive values, some of them are straightforward for example: 'Financial services professional','Consultant', 'Lawyer'; some are very non specific like 'Member of management', 'Entrepreneur' or 'Independent Gentleman'...\nIs there a way for me to sort the readable data into categories like 'Law', 'Financial Services' and all the rest into 'Other'?\nThank you so much in advance!", "If you only have 50 distinctive values, the most straightforward way is for you to create the categories manually.\nIf you are doing this as a project to improve your data science and programming skills you can read on how to do text classification with BERT or other transformer models", "Is there a way to classify industry category through occupation name?", 0.0, 0, 544, "You may find this article useful: https://www.kaggle.com/katharine-chomiak/categorizing-industry-using-occupational-data\nIn short, you will want to use a classification algorithm such as decision trees or k-nearest neighbors to build a model that will predict the category of the occupational data.\nFor your data, I would use kNN. Since it is not very easy to find the k that would split the data well, k=5 will work well for your small dataset.", 0.2461538463830948, 0.015625, 0.1230769231915474, 0.04399999976158142], ["Given\n\nDataFrame contains column df['words']\n\nNeed to make sure that there is no word that isn't English or Hebrew & that there is no number in the input :\nfor example: wrong words:\npla!n, *, /, ?, mouna\u7b11, ~,!, ad\u05e7\u05e8, etc..\nfor example: good words:\nplan, mountain, \u05d0\u05e8\u05d8\u05d9\u05e7, ok...\nin python alone.\nthanks", "\"\".isalpha() \u05d0\u05ea\u05d4 \u05d9\u05db\u05d5\u05dc \u05dc\u05d4\u05e9\u05ea\u05de\u05e9 \u05d1\nyou can use \"\".isalpha()", "detected if a word in column from Dataframe contains any value that is not Hebrew or English", -0.1973753273487091, -1, 545, "If I understand your question, you want to know if there are any strings in your data frame that are not in English and Hebre, so you can throw out those rows.  To do that you could use the [not_in](https://pandas.pydata.org/pandas-docs/stable/reference/api/generated/numpy.isin.html) function from the numpy library. The following code will do the job: (this assumes you have already read in data from a file or csv)\n\\begin{itemize}\n  \\item read the data into a dataframe, df\n    df.head()\n       df=df.to_dict() # for debugging purposes, write the dataframe out to a .csv file\n        df_to = df[df['Is_English'] == '1'].toDF()  # this creates a new dataframe with only the rows where Is_Englsih is true\n      df2 = (df_english_only).toPandas()   #this will create a pandas dataframe from df1\n          #df2['is_hebrew'] = np.not_(df[['col1','col2','....']].apply(lambda x", 0.03821656107902527, 0.012903225608170033, 0.03821656107902527, 0.12753716111183167], ["What is the best way to check if a dataframe is a Pandas.Dataframe or pandas.Series?", "Another way of doing it:\ntype(your_object)", "check if pandas dataframe is dataframe or series?", 0.0996679961681366, 1, 546, "You can use the `isinstance` function:\n`dataframe.is_series()`\nor\n `dataFrame.ndim`\n(as explained in the [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pd.DataFrame.html#pandas):\n\\begin{itemize}\n \\item \\strong{dataType}: the type of the dataframe\n (e.g., `numpy.string`, `pandas._libs.tslib.iNaT`,\n  `float`, etc.)\n See [pandas dtype](https://stackoverflow.com/questions/34738915/how-to-check-the-dtype-of-a-pandasseries) for more information on the meaning of each dtype\n value.\n The default value is `None`, which means that we don't\n know the dtype. `NaN` values will be converted to `string`s. Note that\n the\n [dtypes](//pandas#different-types-in-series-data) are different than\n those in numpy. For example, numpy has a `int64` dtype but pandas has\n a more generic `long` type. This means", 0.028985507786273956, 0.0, 0.028985507786273956, 0.08105036616325378], ["What is the best way to check if a dataframe is a Pandas.Dataframe or pandas.Series?", "to expand on Ryan's comment:\nisinstance(df,pd.DataFrame) will return True if it is a dataframe. to check if it is a series, it would be isinstance(df,pd.Series).", "check if pandas dataframe is dataframe or series?", 0.46211716532707214, 5, 547, "The following line will return a pandas DataFrame (or Series) if it is one.\nIt will also return True if the input is already a Dataframe.  False otherwise. Note that the test is not case sensitive. If you want to make it case insensitive, you can change `dataframe` to `DataFrame`.\nIf you are unsure, it's better to use `isinstance` like this:\n`is_pandas_series(my_object) # or isinstance(object, pandas Series)`.\nThe following code is from [here](https://stackoverflow.com/questions/41564480/is-there-a-way-to-check-if-an-object-isnt-pandas-dataframemultiindex-type)\n# note that we must call `df.reset_index()` to ensure the index is set\nfrom pandas import Series, DataFrames\ndf = Dataframes(['foo', 'bar', 1])\nassert is_dataframes(df) == True\nfor df in df.iteritems():\n    if isserieslike(Series(list(zip([1,2], df))):\n        assert isdata", 0.27906978130340576, 0.10588235408067703, 0.1860465109348297, 0.0656265914440155], ["I have been trying to understand RNNs better and am creating an RNN from scratch myself using numpy.  I am at the point where I have calculated a Loss but it was suggested to me that rather than do the gradient descent and weight matrix updates myself, I use pytorch .backward function.  I started to read some of the documentation and posts here about how it works and it seems like it will calculate the gradients where a torch tensor has requires_grad=True in the function call.\nSo it seems that unless create a torch tensor, I am not able to use the .backward.  When I try to do this on the loss scalar, I get a 'numpy.float64' object has no attribute 'backward' error.  I just wanted to confirm.  Thank you!", "Yes, this will only work on PyTorch Tensors.\nIf the tensors are on CPU, they are basically numpy arrays wrapped into PyTorch Tensors API (i.e., running .numpy() on such a tensor returns exactly the data, it can modified etc.)", "Can I use pytorch .backward function without having created the input forward tensors first?", 1.2000000476837158, 1, 548, "`require_gradient` means that the `forward` function must return a gradient, which is the derivative of loss with respect to some parameter(s).\nThe `backwards` method is only used if you need to calculate a loss gradient. This is not the case when you have a scalar loss.", 0.13636364042758942, 0.0, 0.09090909361839294, 0.04210526496171951], ["I've been trying to run a certain cell in Google Colab for a while now and keep running into the same issue. The cell runs for about 20-25 mins and terminates the code and restarts the runtime due to running out of memory/RAM, which causes all variables to be lost. I first deleted variables that would be re-initialized in the next iteration by calling \"del\". After deleting the variable I called the gc.collect() function. Once that didn't work, I noticed that there were some data structures that increased every iteration (a couple of lists). I removed the lists and wrote the information to a csv file instead. I then read in the information/csv file after the for loop and obtained the information that way, instead of appending to a list every iteration in the for loop. However, that didn't solve the issue either. I do not have Colab Pro+, I am utilizing the free version.\nAny assistance would be greatly appreciated. Thanks!", "I first deleted variables that would be re-initialized in the next iteration by calling \"del\"\n\nIf that variable is quickly reassigned to a new value, deleting it won't do anything.\n\nI then read in the information/csv file after the for loop and obtained the information that way, instead of appending to a list every iteration in the for loop\n\nIf the end result is the same amount of information stored in variables, then this won't do anything either.\nWithout seeing your actual code, all I can say is \"your variables are too big\".", "Running Out of RAM - Google Colab", 0.0996679961681366, 1, 549, "In the end, it turned out that the RAM was not a problem, but rather the amount of data that was being processed was causing the cell to crash.", 0.2063492089509964, 0.04838709533214569, 0.1269841343164444, 0.06777393817901611], ["I'm using an excel sheet with many different dataframes on it. I'd like to import those dataframes but separately. For now When i import the excel_file with pandas, it creates one single dataframe full of blanks where the dataframe are delimited. How can I create a different dataframe for each on of them?\nThanks", "If you're using the pandas.read_excel() function, you can simply use the usecols parameter to specify which columns you want to include in each dataframe. Only downside would be you'd need to do a read_excel call for each of the dataframes you want to read in.", "How to separate many dataframes from one excel Sheet Pandas", 0.0, 0, 550, "You can use `read_excel` to read multiple excel sheets.\nFor example:\n`df1 = pd.read.excel('file.xlsx',sheet_name='Sheet1')`\nwill create one dataframe from sheet 'Sheet 1'\nand `df2 = df1.copy()` will create another dataframe.", 0.25287356972694397, 0.0941176488995552, 0.18390804529190063, 0.03589744120836258], ["Age\nGender\nBusinessTravel\nDepartment\nDistance\nEducation\nMaritalStatus\nSalary\nYearsWorked\nSatisfaction\n\n\n\n\n41\nFemale\nFrequent\nSales\n12\n5\nMarried\n5000\n4\n4\n\n\n24\nMale\nRarely\nHR\n22\n4\nSingle\n3400\n1\n3\n\n\n\n\nSatisfaction - Scale from 1 to 5, 5 is the most satisfied.\nDistance - Distance from home to workplace\nAbove is a sample of the data.\nWould Kmeans or Kmodes be appropriate for such a dataset?\nThank you for any answers in advance.", "Kmean clustering would not be ideal as it cannot handle discrete data", "What is the best python approach/model for clustering dataset with many discrete and categorical variables?", 0.0, 0, 551, "I'd go with k-measures. If the dataset has a high number of categories, k means will give you the \"clustering\" that you want. In other words, you can separate the categories with little overlap. You can look at the results and if there are too many clusters, then you could add a \"normalization\" stage to the kmeaures. So, the result would be something like:\n\\begin{itemize}\n  \\item 45% satisfied\n    \\strong{18% of satisfied employees are in group 01} (group 25 has no one satisfied)\n      \\em{67% are satisfied} in \\sout{group} \\begingroup{02} 65 of 97 satisfied are}   in  group \\endgroup05. (2 groups, a lot of people) \\textit{43% in groups 3 and 8} (\\textit{-35-17=14})\n        \\textbf{23} out of \\group4\\textbf{\\textit{\\s{}}8\\textit{} \\emph{56} satisfied in} group\\s 7\\endgroups\\emph{\\", 0.0397351011633873, 0.0, 0.0397351011633873, 0.08619378507137299], ["So I am trying to solve a linear programming problem with around 10,000 binary variables using the PULP python library. It's taking me a lot of time to solve the problem.\nI was wondering if there is anyway for the code to use GPUs available in Colab to solve these linear programming issues.", "GPUs have little or no advantage for general large, sparse LP and MIP models. Apart from some academic exercises on highly structured problems, there are few or no solvers available that use GPUs. The underlying problem is that GPUs are really good for data-parallel problems (SIMD architecture). Large, sparse LPs are different.", "Using Colab GPU for PULP Python", 1.2000000476837158, 1, 552, "There are no GPU-specific tools for linear optimization (LP) or MIP in general, but you can use the same tools as the ones for Python, and just specify the `num_procs` parameter in `pulp.linprog` to have the solver use multiple CPU cores. This is a good idea since your problem can be easily parallelized by dividing it into subproblems.", 0.28070175647735596, 0.01785714365541935, 0.14035087823867798, 0.07192520052194595], ["I have tried in combination\ndf2 = df.loc[['Entity'] != ('World')]\ndf2.loc[['Code'].str.contains('',na=False)]\nresult from memory\nshowing code NaN\nWorld removed\nboth have succeeded my needs the problem is combining them together. it seems to just not want to work. In one column titled 'Code' in the data the continents came with 'NaN' so I filtered that out using 'df.loc[['Code'].str.contains('',na=False)]' and it worked but then combined with \"df2 = df.loc[['Entity'] != ('World')]\" I apologise for wasting anyones time this is for an assessment and the thought of it is bugging me out. Did i do anything wrong or misread the purpose of a function?", "To check your missing values you could use function isnull() or notnull() to filter, then correct them.\nTo remove and replace null values in Pandas DataFrame You could use \u2018fillna()\u2019. This is commonly replaced with 0 but depends on your DataFrame.", "Is there a function to filter out NaN/Na and a word in panda?", 1.2000000476837158, 0, 553, "The problem here is that the `df` you are passing to the function contains a `NaN` value, which is not a string.  If you were to do this on a data frame without `NA` values, you would be able to accomplish what you want.\n`df = pd.DataFrame({'Code': ['World', 'Europe', np.NaN]})`\nThe code in this question works by filtering out all rows that do not have the word 'World' as the string value of 'Entity' then drops the rows which do have 'NA' values in 'code'.  You can then use the boolean from the first filter to apply the second.", 0.22535210847854614, 0.02857142873108387, 0.1267605572938919, 0.053108297288417816], ["I'm creating a bar chart using Bokeh. My chart renders fine initially, but when I add in the following line (to rotate the X labels):\np.xaxis.major_label_orientation = 1.2\nThe chart becomes blank. Why is this occurring?", "It turns out that this occurred because my x axis labels were too long. When I shortened the labels, the chart reappeared with the rotated labels. (Increasing the height of the figure might be another way to solve this issue.)", "Adding in major_label_orientation value makes chart blank", 0.0, 0, 554, "This is because you are using a custom major label orientation.\nThis is not supported by the current version of BoKeh (0.12.0)\nhttps://github.com/bokehp/pybole/issues/14\nYou can get the major labels to rotate by using the `major.label` property which will cause a warning,\nbut the chart will still render. https://bokee.pydata.org/en/latest/docs/reference/components/index.html#bolee.components.xy.Axis\nSo, you can do something like this:\n\\begin{code}\nfrom bole.plotting import Figure, DataRange1d, Color, LINE, POINT, XY, Grid, Plot, x, y\nimport numpy as np\nsource = np.array([[0, 0], [001,0]], dtype=np.int64) # test data\ns = DataFrame({'x': source['x']})\nf = Figure(plot_width=504, plot_height=203)  # set size\ng = Grid(f)                # add a grid\nl =", 0.1744185984134674, 0.0117647061124444, 0.09302325546741486, 0.02604166604578495], ["So, I would like to know if there is a way to delete an line already plotted using matplotlib. But here is the thing:\nI'm plotting within a for loop, and I would like to check before plotting if the line that I'm about to draw is already on the figure, and if so, I would like to delete it.\nI was told an idea about plotting it anyways but with the same color of the background, but again, to do this I would have to check if the line already exists. Any idea how to do this?", "Any idea how to do this?\n\n\nDuring iteration, before making a new line\n\ncheck if x and y coordinates of the new line are the same as any of the lines already contained in the plot's Axes..", "Checking if a line was already drawn in matplotlib and deleting it", 1.2000000476837158, 0, 555, "You can check for the existence of an item before adding it, like this: http://matplotlib.org/api/pyplot_api.html#matplot.pyplt.plot\nIf the item already exist, you can use .remove() to remove it (http://www.mathworks.com/help/matlab/ref/remove.gif).\nAlternatively, if you want to be able to add and delete items within your for-loop, then you will need to create a list of lines, rather than plot them directly. Then you could iterate through your list, checking for each line to see if it already existed, before you add it to the list. You could then delete the lines from the graph after you've finished looping through the items.", 0.21935483813285828, 0.013071895577013493, 0.11612903326749802, 0.0555555522441864], ["I'm working on a project that needs to update a CSV file with user info periodically. The CSV is stored in an S3 bucket so I'm assuming I would use boto3 to do this. However, I'm not exactly sure how to go about this- would I need to download the CSV from S3 and then append to it, or is there a way to do it directly? Any code samples would be appreciated.", "Ideally this would be something where DynamoDB would work pretty well (as long as you can create a hash key). Your solution would require the following.\n\nDownload the CSV\nAppend new values to the CSV Files\nUpload the CSV.\n\nA big issue here is the possibility (not sure how this is planned) that the CSV file is updated multiple times before being uploaded, which would lead to data loss.\nUsing something like DynamoDB, you could have a table, and just use the put_item api call to add new values as you see fit. Then, whenever you wish, you could write a python script to scan for all the values and then write a CSV file however you wish!", "Writing to a CSV file in an S3 bucket using boto 3", 0.1973753273487091, 1, 556, "You could use the [boto](https://github.com/bots/botocore) package to write to SQS. Then have a SNS topic that listens to the Sqs queue and writes the message to an s3 file. If the file does not exist, it will create a new file and if the new csv has already been created, then it would append the records to that file as follows:\n\\begin{itemize}\n  \\item create sns topic\n      `aws ssm create-topic my_topic`\n    \\begin {code}", 0.3316583037376404, 0.05076142027974129, 0.14070351421833038, 0.05696842074394226], ["I have an issue with a dataframe I try to pivot. The error message says that it contains duplicate entires. However I have checked the file and there are no duplicates (checked with df.duplicated, in Excel and manually). As I am running out of ideas, is there a way to know in which line in the dataframe is causing the error to throw? The Python error message is unfortuneately not very clear...\nThe code itself is working with another dataframe so I assume my code should be fine...", "a\nb\nc\n\n\n\n\n54545\n3\n8\n\n\n54545\n2\n16\n\n\n54545\n1\n64\n\n\n\n\nThe idea is to generate a Pivot out of it with B being the columns, column A is going to be the index and C is the value of the columns.\ndf = df_2.pivot(index='A', columns=\"B\", values='C').reset_index()\nHope it is understandable what I want to do.", "Pivot - Index contains duplicate entries, cannot reshape - which line in the dataframe is causing the error?", 0.0, 0, 557, "If you have no duplicate rows in your original dataframe then the issue is in how you are pivoting it.\nTry the following code to see if that works:\n\\begin{itemize}\n  \\item Load the csv into pandas.read_csv\n    \\begin {code}\r\ncsv = pd.io.csv.reader(StringIO(csv_content), delimiter=',')\r\r\\end{code}\r\n   \\em{Note: csv_contnet is your csv content in a string object} \r\n        \\strong{If you want to use a file use the open function}\r\r  \r\r \r\\em{\\strong{\r\n      instead of StringIO and the path to your file}} \r 2.\r \\sout{\r\r   \r \r                   }\r\n    3. Convert the dataframes to a long format using \\textit{pd.melt}\r 4. \\textbf{\rPivot} using the \\emph{value} column 5. Join the two data frames using a common column and drop the `variable` column from the melted dataframe. 6. Drop the extra column `var`.\nExample:\r`df1 = df1.reset_index(", 0.2463054209947586, 0.019900497049093246, 0.14778324961662292, 0.04048582538962364], ["My website is currently using Heroku-18 stack, which is deprecated. I therefore need to redeploy my site to have it up to date (Heroku-22 stack) but I'm getting errors when trying. The log mentions numpy related errors numerous times, so I assume it could be the source of my problem.\nI've already looked online for some solutions but none of them have worked. I notably tried upgrading pip, changing the python version in my runtime.txt file, reinstalling numpy but nothing worked.\nBefore redeploying my website, the python version in runtime.txt was python-3.7.0. It is currently set to python-3.9.13.\nNumpy is installed and the version is 1.18.1.\nHere are some of the errors I'm getting:\n\n! [remote rejected] master -> master (pre-receive hook declined)\nerror: failed to push some refs to 'https://git.heroku.com/mywebsite.git'\n\n\nerror: Command \"gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.linux-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/private -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/app/.heroku/python/include/python3.9 -Ibuild/src.linux-x86_64-3.9/numpy/core/src/private -Ibuild/src.linux-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.linux-x86_64-3.9/numpy/core/src/private -Ibuild/src.linux-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.linux-x86_64-3.9/numpy/core/src/private -Ibuild/src.linux-x86_64-3.9/numpy/core/src/npymath -c numpy/random/mtrand/mtrand.c -o build/temp.linux-x86_64-3.9/numpy/random/mtrand/mtrand.o -MMD -MF build/temp.linux-x86_64-3.9/numpy/random/mtrand/mtrand.o.d\" failed with exit status 1\n\n\nERROR: Failed cleaning build dir for numpy\nremote:              Failed to build numpy\n\n\nERROR: Failed building wheel for numpy\n\nHow can I fix these errors?\nAlso, could it be someting else non numpy-related that causes the failure of the deployment?", "I finally managed to solve the issue. Many dependencies were outdated. I had to use python-3.9.14 version, upgrade psycopg2-binary to v2.9.3 and scipy to v1.6.", "Can't deploy my website onto Heroku - is Numpy causing the issue?", 0.0, 0, 558, "The error messages you are receiving are complaining that your build cannot run.  If your `runtime.text` file has the wrong `python-version` setting, then the `pip install numpy` command is failing.   But I suspect the build is not failing because of numpy. Rather I think it is a problem with your system. Perhaps a missing file somewhere. But if you have a `dumps` directory, it may be possible that numpy is being built with a different python. If you do not have `numpy` in your path, you will need `sudo` to run it.", 0.095238097012043, 0.0, 0.0793650820851326, 0.04444320872426033], ["The difference between coalesce and repartition is fairly straightforward. If I were to coalesce a DataFrame to 1 partition and write it to a storage service (Azure Blob/ AWS S3 etc), would the entire DataFrame be sent to the driver and then to the storage service; or would an executor send it directly?", "The Spark official documentation describes it as follows:\n\nIf you\u2019re doing a drastic coalesce, e.g. to numPartitions = 1, this\nmay result in your computation taking place on fewer nodes than\nyou like (e.g. one node in the case of numPartitions = 1).\n\nFrom the above it can be inferred that it should be an executor send it directly.", "Does coalesce(1) bring all the data to the driver?", 1.2000000476837158, 3, 559, "This is an important question, because it helps you understand the costs of writing data out to storage, and therefore how many partitions to create for your data.\nThe answer is: it depends. There are some things that will happen automatically that may change the behavior of coalsce. But there are other things you can do to force it into a single partition. (Which in turn makes the answer to your question depend on the way you use coilsce)\n\\begin{itemize}\n \\item \\strong{If you set coilase to True} (which is the default) then the whole Dataframe will be copied to that single exec node. The reason for this is that coarsen is supposed to be a one-time operation to reduce the number of partitions for writing. It is a lot more efficient to copy the DataFrames to one exec than to have to do it over and over for each partition for write. For a large Dataframes, this can take a long time. So the first thing that happens is coarse will copy to all executors. Then it will split the partitions into those executers, then those partitions will do their writes. This means that you might see some of the write latency from the coar", 0.2014925330877304, 0.015037594363093376, 0.11940298229455948, 0.04744525998830795]]}