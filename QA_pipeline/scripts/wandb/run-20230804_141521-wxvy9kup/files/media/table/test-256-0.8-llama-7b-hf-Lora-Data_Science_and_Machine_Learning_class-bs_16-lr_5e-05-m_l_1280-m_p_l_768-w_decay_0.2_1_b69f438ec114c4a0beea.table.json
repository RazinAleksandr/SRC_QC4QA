{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have a dataframe where the indexes are not numbers but strings (specifically, name of countries) and they are all unique. Given the name of a country, how do I find its row number (the 'number' value of the index)?\nI tried df[df.index == 'country_name'].index but this doesn't work.", "Why you don make the index to be created with numbers instead of text? Because your df can be sorted in many ways beyond the alphabetical, and you can lose the rows count.\nWith numbered index this wouldn't be a problem.", "How do I find the row # of a string index?", 0.0, 0, 80, "Try using `get_loc`:\n\\begin{itemize}\n    \\item `df = pd.DataFrame(index=['country1', 'Country2', np.nan, 'India'])`.\n      Note: you don'thave to set index as `string`\n          `np.where(df['Country'] == \"Inda\", df.loc[np, \"Country\"])` works. In your case,\n           `index = 'France'` and `inp = np` will be used. So, `loc` doesn`t return the value but `set` value to the df object. If you want to get the `row` index, you can use `pd.get` with `pandas` and then use the function `iloc` for the same. You can do like this. Hope this helps you. :)\n        \\strong{Code:}\n         `result = df['France'].get('France').ilocs` to find row index of \"France\"\n       \\begin {code}", 0.26490065455436707, 0.02684563770890236, 0.13245032727718353, 0.06144861504435539], ["I have a pandas Timestamp column that looks like this:\n2021.11.04_23.03.33\nHow do I convert this in a single liner to be able to look like this:\n2021-11-04 23:03:33", "use a regular expression by looking for the hour and minute second  pattern (\\d{4})-\\d{2}-(\\d{2})\\s+(\\d{2})_(\\d{2}).(\\d{2}).(\\d{2}) and use re.findall then get each group part then reassemble the datetime stringthen convert to a datetime", "Pandas Timestamp reformatting", 0.0, 0, 81, "Use [to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd.to_.html) function: `pd_df['datetime_column'] = pd.Series(pd['date_col'].to('datetime64[ns]')).to(datetime) `\nor if you are sure that column name is 'datetime':\n`pd._convert_objects_tokens(df[\"datetime\"], 'to', datetime)`\nTo get back to the original datetime format use [pd.__to__](http://www.diveintopython.net/native_datatypes/datetime.html#pd__to_) function\nExample:  `df[col_name] = df[column_with_date]__2`\nIf you have an integer you can use: df['col']=pd.[__str__(df)]\nAnd if it is a list of objects use pd.__iter__(list_of_columns).__next__()\nYou can also use the built in datetime functions, e.g.\ndf.datetime =df.[col]='datetime'", 0.1420118361711502, 0.0, 0.09467455744743347, 0.045248862355947495], ["I'm doing PID gain tuning for a DC motor\nI gathered real data from the motor which involve the position according to time.\nAnd i want to calculate the rise time, overshoot, and settling time from the data.\nIs there any function in matlab or python which can do this?\nThank you!!", "In the cases that you use the step command to extract the step-response characteristics of the system, the stepinfo command calculates the rise time, overshoot, and settling time, and so on. I don't know whether it is applicable in the data case or not but you can test it?", "Is there any function that calculates the rise time, overshoot, and settling time?", 0.0, 0, 82, "For the \"rise time\" (change in position per time unit), it's the time derivative of the value:\n\\begin{equation}\nt_r = \\frac{1}{V(t)}\n= \\lim_{t \\to 0} \\left.\\frac{\\mathrm{d}V}{\\mathrm d t}\\right|_{t=0},\n$$\nwhere $V$ is the current position. The over-shoots are: $t_{overshock} = t_{r} - t_0$ and $2t_{\\text{over-}} = 2(1/V_s) - 1$, where $1$ represents the initial time and the overshot is computed as the difference of that initial value and its value after 50% of a period. For settle time $S$, $ S = V_{s}(t)/V_{o}(0)$, where  $ V_o(0) $ is your initial position and  you can compute $  V( t) =V (0)[1-e^{-t/\\tau}], \\tau$ the settlng time constant.", 0.23463687300682068, 0.05649717524647713, 0.15642458200454712, 0.04255318641662598], ["On google colab I installed conda and then cudf through conda. However now i need to reinstall all packages like sklearn etc which I am using in my code. Is there some way to install cudf without conda ? pip no more works with cudf. Also if there is some other similar gpu dataframe which can be installed using pip, it will be of great help.", "No, cudf is not available as a pip package.\nYou don't say why you need cudf, but I would try pandas, dask or vaex, surely one of those will do what you need.", "Install cudf without conda", 0.1973753273487091, 1, 83, "The problem is that the condea is a python package manager. It doesn't actually install the package, but rather it manages the installation of packages. You have to use condo to manage the packages and not the actual installation. So in the end it doesn\u2019t matter if you use pip or conca, you will have the same result. But conado is the most used one and is easier to maintain.", 0.2641509473323822, 0.0, 0.15094339847564697, 0.030534349381923676], ["While writing in CSV file, automatically folder is created and then csv file with cryptic name is created, how to create this CSV with any specific name but without creating folder in pyspark not in pandas.", "That's just the way Spark works with the parallelizing mechanism. Spark application meant to have one or more workers to read your data and to write into a location. When you write a CSV file, having a directory with multiple files is the way multiple workers can write at the same time.\nIf you're using HDFS, you can consider writing another bash script to move or reorganize files the way you want\nIf you're using Databricks, you can use dbutils.ls to interact with DBFS files in the same way.", "How to write in CSV file without creating folder in pyspark?", 0.0, 0, 84, "You can use below code for writing csv files without folder creation:\n\\begin{itemize}\n    \\item Write dataframe to file and save file as a csv. The file name can be\nspecified using `save_as_csv`.\n        \\begin {code}", 0.2153846174478531, 0.046875, 0.10769230872392654, 0.06737564504146576], ["For an assignment I have to write some data to a .csv file. I have an implementation that works using Python's csv module, but apparently I am not supposed to use any imported libraries...\nSo, my question is how I could go about doing so? I am no expert when it comes to these things, so I am finding it difficult to find a solution online; everywhere I look import csv is being used.", "I guess that the point of your assignment is not to have some else to do it for you online. So a few hints:\n\norganise your data per row.\niterates through the rows\nlook at concatenating strings\ndo all above while iterating to a text file per line", "How to write to a .csv file without \"import csv\"", 0.06656807661056519, 1, 85, "You can use the [csv module](https://docs.python.org/3/library/csv.html) with `with` statement, or you can create a `dict` and write the contents of the `list` to the file like this:\n\\begin{itemize}\n`dict.items()`\nor\nfile.write(str(your_dict))\n#or you could use json or yaml\ndict_to_file = json.dumps(dict)\nwith open(file_name, 'w') as f: f.writelines(data) #you could also use f for the dict\nyamls = y.dump(dic) and yam.save(f) to save\njson.load(open(filename, \"r\")) and jsonl.read(json_string) for reading\nyaml.safe_dump() for dump\nread_yaml(yaml.YAMLLoader().load) or read_json(jsons.JSONParser().parse) reading. you may need to install a package\n(p.s. the csv.writer will work as long as you are using the Python 2.7.x version and have not installed the 3", 0.1538461595773697, 0.0, 0.0923076942563057, 0.032710280269384384], ["I have a time series in which i am trying to detect anomalies. The thing is that with those anomalies i want to have a range for which the data points should lie to avoid being the anomaly point. I am using the ML .Net algorithm to detect anomalies and I have done that part but how to get range?\nIf by some way I can get the range for the points in time series I can plot them and show that the points outside this range are anomalies.\nI have tried to calculate the range using prediction interval calculation but that doesn't work for all the data points in the time series.\nLike, assume I have 100 points, I take 100/4, i.e 25 as the sliding window to calculate the prediction interval for the next point, i.e 26th point but the problem then arises is that how to calculate the prediction interval for the first 25 points?", "A method operating on a fixed-length sliding window generally needs that entire window to be filled, in order to make an output. In that case you must pad the input sequence in the beginning if you want to get predictions (and thus anomaly scores) for the first datapoints. It can be hard to make that padded data realistic, however, which can lead to poor predictions.\nA nifty technique is to compute anomaly scores with two different models, one  going in the forward direction, the other in the reverse direction, to get scores everywhere. However now you must decide how to handle the ares where you have two sets of predictions - to use min/max/average anomaly score.\nThere are some models that can operate well on variable-length inputs, like sequence to sequence models made with Recurrent Neural Networks.", "Interval Prediction for a Time Series | Anomaly in Time Series", 1.2000000476837158, 0, 86, "The prediction intervals are not the best option for anomalous data detection. There are several things that could be used for this:\n\\begin{itemize}\n \\item Look at the standard deviation of the observations. If you have many data, this could work. This is only suitable for time-series with a fixed number of values. In the case of a growing time interval, you would have to do a slided step. Also, in some cases, the variance can be very high. You could get a much lower standard-deviation for very short time intervals and a very large variance for large time window. So you might have problems here. However, it is a possibility. For example, if you do 5 minutes, and you want the interval of prediction to be 30 minutes (slided steps), then the minimum prediction time is 40 min. But if there are a lot of data with large values, then you can have the situation that you predict 0 for 60 or 70 of 90 values (due to the high variance), and then for one value you get 80. That would be a good example where you need to use a different approach. A similar problem occurs with the median or mean", 0.3014492690563202, 0.05830903723835945, 0.14492753148078918, 0.06656203418970108], ["I have a list of different expressions. It looks like this:\nmy_list = [[1 ,2 ,'M' ,2], [1 ,2 ,'A' , 1], [1 ,2 ,'g' ,3], [1 ,2 ,'o' ,4]]\nI want to sort the list. The key should always be the first entry in the list, in this case the book positions A, M, g, o. However, upper and lower case should be ignored.\nIn python I used:\nmy_list.sort(key = itemgetter (3))\nOutput is:\n[[1, 2, 'A', 1], [1, 2, 'M', 2], [1, 2, 'g', 3], [1, 2, 'o', 4]]\nThe problem is that in my result the uppercase letters are sorted first and then the lowercase letters. How can I make lower and upper case letters sort together? The result should look like this:\n[[1 ,2 ,'A' ,1], [1 ,2 ,'g' ,3], [1 ,2 ,'M' ,2], [1 ,2 ,'o' ,4]]", "Use key=lambda lst: lst[2].lower().", "python sort multi-dimensional lists CASE INSENSITIVE", 1.2000000476837158, 1, 87, "In Python \u2265   version 8, you can use [`sorted()`](https://docs.python.org/8/library/functions.html#sorted):\n\\begin{itemize}\nIf the key function is not callable, the function \\em{key(element)} is used as the comparison function. This function should take two elements as arguments and return a positive integer, zero, or a negative integer indicating how great a value \\begin {code}", 0.05797101557254791, 0.0, 0.05797101557254791, 0.0], ["I'm building a model to identify a subset of features to classify an object belong which group. In detail, I have a dataset of 11 objects in which 5 belong to group A and 6 belong to group B, each object has been characterized with a mutation status of 19,000 genes and the values are binary, mutation or no-mutation. My aim is to identify a group of genes among those 19,000 genes so I can predict the object belongs to group A or B. For example, if the object has gene A, B, C mutation and D, E gene with no mutation, it belongs to group A, if not it belongs to group B.\nSince I have a large number of features (19,000), I will need to perform feature selection. I'm thinking maybe I can remove features with low variance first as a primary step and then apply the recursive feature elimination with cross-validation to select optimal features. And also don't know yet which model I should use to do the classification, SVM or random forest.\nCan you give me some advice? Thank you so much.", "Obviously in a first step you can delete all features with zero variance. Also, with 11 observations against the remaining features you will not be able to \"find the truth\" but maybe \"find some good candidates\". Whether you'll want to set a lower limit of the variance above zero depends on whether you have additional information or theory. If not, why not leave feature selection in the hands of the algorithm?", "Is it reasonable to use 2 feature selection steps?", 0.0, 0, 88, "What you describe is a classification task. Feature selection is not necessary for classification. RFE is suitable for feature screening when you have many features and want to find a set that best separates your classes. It is often used as preprocessing step for other machine learning algorithms.", 0.23333333432674408, 0.06779661029577255, 0.10000000149011612, 0.0515873022377491], ["I deployed Apache Spark 3.2.0 using this script run from a distribution folder for Python:\n./bin/docker-image-tool.sh -r <repo> -t my-tag -p ./kubernetes/dockerfiles/spark/bindings/python/Dockerfile build\nI can create a container under K8s using Spark-Submit just fine.  My goal is to run spark-submit configured for client mode vs. local mode and expect additional containers will be created for the executors.\nDoes the image I created allow for this, or do I need to create a second image (without the -p option) using the docker-image tool and configure within a different container ?", "It turns out that only one image is needed if you're running PySpark.  Using Client-mode, the code spawns the executors and workers for you and they run once you create a spark-submit command.  Big improvement from Spark version 2.4!", "Two separate images to run spark in client-mode using Kubernetes, Python with Apache-Spark 3.2.0?", 0.0, 0, 89, "Yes you can. Spark will launch executers on any number of containers. It doesn't really matter whether you use the Python binding or not. I would suggest using a binding to make your life easier, and then you don' you need two images. If you run your `spark-env.py` file in your Dockerfile, you'll see that it will look up the host's IP and add it to the Spark configuration.", 0.25641027092933655, 0.017391303554177284, 0.1367521435022354, 0.05699171498417854], ["I have an array\narr = np.array([[1,1,2], [1,2,3]]).\nI want to get amount of unique element for each row and count mean\nI can do this np.array([len(np.unique(row)) for row in arr]).mean().\nBut seems, that it's a slow way. Is there another faster approach?", "set(arr.flatten()) will create your desired result. Not sure about how fast it is though.\nOutput:\n{1, 2, 3}\nEdit:\nYou wanted the number of unique elements, so you wrap the whole thing in len()", "Get amount of unique elements in numpy", 0.0, 0, 90, "The fastest way is to use [`np._count_nonzero`](https://numpy.org/doc/1.18/reference/generated/numpy._ufunc.ufnz.html) with `axis=1`, since that will allow the elementwise count of non-zero elements. This is a bit faster than using `np_unique` in the case that you want the count across multiple dimensions (e.g., `len` is slower than `sum`).\n\\begin{itemize}\n  \\item `arr`\n    \\begin {code}", 0.23076923191547394, 0.0, 0.1538461595773697, 0.03448276221752167], ["For example, how many Mb will be required for EfficientNetB3? On drive model weights require 187 Mb of memory, does it mean that when the model will be loaded on GPU, it will use 187 Mb of GPU memory?", "It's difficult to calculate total, but you can estimate a minimum to just load a model, which would be roughly the model size. Tensorflow, for example, defaults to reserving 100% of the GPU memory. You can set limits, but the amount of memory to be used is based on many things, such as number of layers, input image size, batch size, etc.", "How to evaluate the required GPU memory for running neural network models?", 0.0, 0, 91, "Memory is a resource that has to be provided by the system. If you are using a system with 8 GB of RAM, then that means 4 GB will go to the host and 2 GB to GPU. It doesn't mean you will need 3 GB in total.\nAlso, you have to remember that the memory is allocated in a way that it is very efficient. In the example you provided, 50% of the 60 MBytes are used for weights and the rest for bias. So, the GPU is not using 70 GB, but 900 MB. The memory on the CPU is 0.25 GB.", 0.2958579957485199, 0.0359281450510025, 0.16568046808242798, 0.05030106380581856], ["How to get pandas dataframe when select only one column? In R there is drop = False for that.\nWe can use pd.DataFrame(df['breakfast']) or df[['breakfast']], but do we have smth like drop = False as it in R?\nPS: press F for breakfast)", "I think you are looking for something like index=df.index.\nThe question is a bit broad", "How to get pandas dataframe when select only one column? In R there is drop = False for that", 0.0, 0, 92, "Yes, you can do that:\n\\begin{itemize}\n  \\item `df = df.drop('lunch')`\n    \\em{This will drop the lunch column, and you will have a df without it}  (the same as R)  `breakdown = pd_df.to_csv(path, index=False)`\n     \\strong{OR} `pd_result = pandas_dframe.df`\n   \\begin {code}", 0.1538461595773697, 0.0, 0.1230769231915474, 0.01387613546103239], ["I'm new to OpenCV and trying to use SIFT and SURF for a project.\nOn my laptop I have OpenCV version= 4.5.1.48 and also added OpenCV-contrib-python of version 4.5.1.48\ncurrently the problem I'm facing is the error I'm getting after following the documentation SIFT works perfectly after following documentation but SURF isn't working and giving me error for following codes\ncode 1\nsurf = cv.xfeatures2d.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'xfeatures2d'\ncode 2\nsurf = cv2.SURF_create()\nAttributeError: module 'cv2.cv2' has no attribute 'SURF_create'\nAfter reading many answers on Stack overflow I changed version of OpenCV and did many things but nothing is working for me\nI'm new to this please someone guide me through this\nI read about the patent expiring too but nothing is working in my case pls tell me if im wrong somewhere\nThanks in advance", "For patent reasons, opencv 4.5.1.48 does not include the whole algorithm\nYou can use Python3.6 (or Python3.7 maybe OK) and install opencv-pyhton==3.4.2.16 and opencv-contrib-python==3.4.2.16, then you can use the function that:\nsurf = cv2.xfeatures2d.SURF_create()\nor\nsift = cv2.xfeatures2d.SIFT_create()", "AttributeError: module 'cv2.cv2' has no attribute 'SURF_create' , 2. module 'cv2.cv2' has no attribute 'xfeatures2d'", -0.0996679961681366, -1, 93, "You need to add the following import to your script\n`import cvx.feature.py`\nand add `cvx` to the `from` statements at the top of the script.", 0.0941176488995552, 0.0, 0.07058823853731155, 0.03360239788889885], ["w2v = gensim.models.Word2Vec.load(\"w2v.pkl\")\nI am using this method to load pickle file through gensim but encountering an error.\nAttributeError: 'dict' object has no attribute '_load_specials'", "If you saved the model using pickle, you should be using some form of unpickle to re-load it. (Gensim has a utility method for unpickling a file containing a single pickled object at [gensim.utils.unpickle][1].)\nGensim's per-instance .save() and per-class .load() methods are a custom save/load protocol that internally makes use of pickle but does other things (& perhaps spreads the model over multiple files) as well. You should only Word2Vec.load(\u2026) a filename that was previously created by code like w2v_model.save(\u2026).", "Load pickle file in gensim", 0.1973753273487091, 1, 94, "As you can see in the [documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensims-word-2-vector-model), the `load()` method is only available to the models, and `genssim.Models.W2V.Load()` does not exist. You are probably looking for `WordVector.from_path()`\nSee [this example](http://gensimmq.readthedocs.io/en/latest/generative-semantic-models_wordvectors.htm#loading-from-file)\nIf you are using gensimm2, it's `frompath` method was renamed `model.get_model()` and should be used instead.", 0.34117648005485535, 0.02380952425301075, 0.12941177189350128, 0.034632038325071335], ["I am working in Google Cloud Platform and I am trying to use Pyspark to convert a csv file into an avro file. I have seen a lot of websites but I haven't been able to implment the solution. Thank you in advance. :)", "You can read the csv file into a dataset/dataframe using spark and use databricks library to write it as avro. Something like:\ndataset.write.format(\"com.databricks.spark.avro\").save(\"your output path\")", "How to convert a csv file to an avro file using PySpark?", 0.0, 0, 95, "Here is a sample code to read data from CSV and write to AVRO.\nI hope this helps. Cheers!\n`sparkContext.stop()` to stop the context\nThis code will read from the csv and create an AVro with a single field. You can add more fields as required\n\\begin{itemize}\n  \\item `sc.hadoopFile(path, \"csv\", \"avro\").map(lambda row: (row[0],\n row[1]))`\n   \\strong{to} read the data\n     \\begin {code}  <\n    RDD[Row] >\n      \\end {Code}    \\em{Read the CSV file} \\sout{and} create\n          \\textbf{avros} in the form of Rows. Rdds are\n        \\textit{immutable} so you will need to `saveAsObjectFile` to\n         \\color{blue}create\n             \\underline{an} avros file\n            \\bold {in the format of avrros}.\n              \\emph{Save as Avro} (to save the avrs file)\n                \\textcolor{red} {to be used as a byte file}. You", 0.19393938779830933, 0.061349693685770035, 0.13333334028720856, 0.03378020599484444], ["I'm trying to extract project relevant information via web scraping using Python+ Spacy and then building a table of projects with few attributes , example phrases that are of interest for me are:\n\nThe last is the 300-MW Hardin Solar III Energy Center in Roundhead, Marion, and McDonald townships in Hardin County.\nIn July, OPSB approved the 577-MW Fox Squirrel Solar Farm in Madison County.\nSan Diego agency seeking developers for pumped storage energy project.\nThe $52.5m royalty revenue-based royalty investment includes the 151MW Old Settler wind farm\n\nHere I have highlighted different types of information that I'm interested in , I need to end up with a table with following columns :\n{project name} , {Location} ,{company}, {Capacity} , {start date} , {end Date} , {$investment} , {fuelType}\nI'm using Spacy , but looking at the dependency tree I couldn't find any common rule , so if I use matchers I will end up with 10's of them , and they will not capture every possible information in text, is there a systematic approach that can help me achieve even a part of this task (EX: Extract capacity and assign it to the proper project name)", "You should be able to handle this with spaCy. You'll want a different strategy depending on what label you're using.\n\nLocation, dates, dollars: You should be able to use the default NER pipeline to get these.\nCapacity, fuel type: You can write a simple Matcher (not DependencyMatcher) for these.\nCompany: You can use the default NER or train a custom one for this.\nProject Name: I don't understand this from your examples. \"pumped storage energy project\" could be found using a Matcher or DependencyMatcher, I guess, but is hard. What are other project name examples?\n\nA bigger problem you have is that it sounds like you want a nice neat table, but there's no guarantee your information is structured like that. What if an article mentions that a company is building two plants in the same sentence? How do you deal with multiple values? That's not a problem a library can solve for you - you have to look at your data and decide whether that doesn't happen, so you can ignore it, or what you'll do when it does happen.", "Information extraction with Spacy with context awareness", 0.0, 0, 96, "You could try using the SpaCy parser, which can be used to create a dependency parse tree. You can then use a regex expression to find all the text which you want to parse, for example, \"The [word] [phrase] in [location]\". Then you can do the parsing of the information.", 0.2194092869758606, 0.025531914085149765, 0.1265822798013687, 0.09416656196117401], ["Suppose you have a pandas.DataFrame like so:\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID1\n14.5\n0\n0.32\n...\n\n\nID2\n322.12\n1\n0.94\n...\n\n\nID3\n27.08\n0\n1.47\n...\n\n\n\n\nMy question is simple: how would one select rows from this dataframe based on the maximum combined values from two or more columns. For example:\n\nI want to select rows where the columns Feat1and Feat3 have their maximum value together, returning:\n\n\n\n\n\nInstitution\nFeat1\nFeat2\nFeat3\n...\n\n\n\n\nID2\n322.12\n1\n0.94\n...\n\n\n\n\nI am certain a good old for loop can take care of the problem given a little time, but I believe there must be a Pandas function for that, hope someone point me in the right direction.", "You can play arround with:\ndf.sum(axis=1)\ndf['row_sum'] = df.sum(axis=1)\nor\ndf['sum'] = df['col1' ] + df['col3']\nAnd then:\ndf.sort(['sum' ],ascending=[False or True])\ndf.sort_index()", "Select Pandas dataframe row where two or more columns have their maximum value together", 0.0, 0, 97, "You can use [`DataFrame.value_counts`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd.Series.unique.html#pandaspandas) to get the index of each unique value of `Feats` and then use this index to extract the data. Finally you can reorder your data in this new dataframe. Here is an example for your example data: `df = pd.read_table(\"df.txt\", sep=\"\\t\")`.\n\\begin{itemize}\nNote: I use `pd.__version__` as a test for pandas version, as I can't install it on my system to test. If it's 0, then I guess you're using pandas 1, which doesn'y support `valueCounts`, otherwise it should work. Otherwise, you will have to use a loop. This method will be more efficient. (It took me 20 seconds to run on pandas-0):\n`df[df['Fea1'].value.astype(int)==df[\"Feata2\"].value+df [\"Fe3ta3\"].asttype(float).value]`,\nwhere I assume that your values are all", 0.1256544440984726, 0.021164020523428917, 0.09424083679914474, 0.05917159467935562], ["Given a spatial related search phrase, such as \"Find cafes near the train station\", what would be the approach to handling this with NLP / semantic searching?\nIn this case, would all the 'cafes' need to have a qualitative token with regard to their distance to the train station (e.g. near / far)? Curious to know what the thought process would be for handling these kind of tasks.\nThanks!", "The way I would approach this is to look at the prepositions in the first place, in this case near means close by. You then identify the reference point (train station). Now you find cafes which are close to that, ie you should have a list of cafes with their coordinates, and you compare those against the coordinates of the train station, returning the ones that are closest.\nOther prepositions (opposite) or other descriptions (in the same street as) would need corresponding other metrics to evaluate whether they fit.\nThis is not a semantic search problem, as there is nothing inherent in language that describes whether something is close or far from another thing -- you need to map this onto the 'world', and make a decision from non-linguistic data.", "How would NLP handle semantic searches for spatial information?", 0.0, 0, 98, "You could build a lexicon of spatial terms like this one:\nhttps://nlp.stanford.edu/software/lexicons/spatial.html\nThen, you could use a regex like `near|nearest` to match the tokens and extract them.", 0.13333334028720856, 0.0, 0.09696969389915466, 0.040462426841259], ["I know when you import everything you can do thinks like nltk.bigrams(nltk.corpus.brown.words() for bigrams and nltk.trigrams(nltk.corpus.brown.words() for triagrams, but how do you do four grams?\nI've seen other ways to do it, but they all do it with a \"string\" or a text they make up. How do you do it with the nltk.corpus.brown? Do you have to covert it to a string and if so, how?", "To get n number of items you can use nltk.ngrams() with the number to get as the second argument.\nIn your example, to get four-grams, you can use nltk.ngrams(nltk.corpus.brown.words(), 4)", "Finding Four Grams in an NLTK Corpus", 1.2000000476837158, 0, 99, "You can't really do this.  There's no way to split a word into four parts.\n\\begin{itemize}\n  \\item You could do something like this, which is basically what you want:\n    from nLTk import *\n   a = nlp.word_tokenize(\"the quick brown fox jumped over the lazy dog\")\n...\n     for x in a: x.split(' ')\n         for i in range(4):\n             print i, x[i]\n      ...\n       for y in x: print y\nbut it will split words. It won' t split sentences or groups of words\nlike a list would. So it' s not the right thing to use. If you\nreally want to know the part of speech, you should look at the\ncorpora.   There are more words in the NLP corpora than the Brown\nCorpus, and they're more varied. For example, there are many more\ncompound words, idioms, etc. in them. That would be a better source.", 0.19565217196941376, 0.010989011265337467, 0.10869564861059189, 0.031590115278959274], ["I want to understand a few things about partioning a parquet on Dask.\nWhen I do it in a .csv file, the chunksize works as intended, doing 30 partitions based on 50 mb chunks.\nWhen I try to do it the same logic through the read_parquet, none partition is created, and when I force this with repartition(partition_size='50mb'), it create 109 partitions.\nCan someone explain to me why parquet doesn't seems to work at the same way like .csv when doing chunksizes?", "In CSV, the fundamental, non-splittable chunk of data is one row, usually the bytes between one \\n character and the subsequent one. This bytes chunk size is typically small. When you load data with dask, it reads from a given offset to the next \\n to be able to read an exact number of rows. You would find, if you made the chunk size too small, that some partitions would contain no data.\nParquet is not structured like this. Its fundamental non-splittable chunk is the \"row-group\", and there is often just one row group per data file. This is done for efficiency: encoding and compressing a whole row group's worth of data in one block will give maximum read throughput. Furthermore, because of the encoding and compression, it's much harder for dask to guess how big a piece of a dataset will be as an in-memory pandas dataframe, but it can be many times bigger.\nA row group could easily be >>100MB in size. In fact, this is generally recommended, as smaller pieces will have a higher fraction of their processing time in overhead and latency.\nTo summarize\n\ndask will not split a parquet dataset beyond the partitioning within the data files\nthat partition size might be many times larger in memory than on disk, so repartitioning after load may result in many partitions\nthese are tradeoffs required to make parquet as fast and space-efficient as it is", "Repartioning parquet file dask", 0.3799489736557007, 2, 100, "As far as I can see, this is by design. From the [Dask documentation](https://docs.dask/en/latest/dataframe-parquets.html#dtype-hinting) (emphasis mine):\n\\begin{itemize}\n\"Different file formats have different defaults for chunking. CSV files use the default value of chunk size, which is 256MB (see [here](http://pandas.pydata.org/pandas-docs/stable/io.parsers.categorical.to_csv.htm#io-dtypes)). Parquet uses the file size as the chunk.\"\nAlso, from the documentation on [partitioning]((https:https:/docs-datadog.cloud.datastax.com/docs/_download/dsk-tutorial.pdf)\n`If the `chunks` parameter is not specified, `D.read_table`\nwill use `d.partition` from `Dataset` to partition the\ntable into chucks, where `partitions = d.partitons`.``,\nand from [the Parquat documentation for DASK]([https:`https:[//docs:", 0.24731183052062988, 0.027027027681469917, 0.10215053707361221, 0.0429362878203392], ["Given an integer n <= 10^18 which is the product of Fibonacci numbers, I need to factor it into said Fibonacci numbers.\nEach factorization has a score, which is one less than the count of factors plus the sum of the indices of the factors in the Fibonacci sequence that begins with f(1) = 1, f(2) = 2.\nIf multiple such factorizations are possible, I need the factorization that minimizes the score.\nExample:\n104 = 13 * 8 or 104 = 13 * 2 * 2 * 2\nf(6) = 13, f(5) = 8, f(2) = 2\nFor 104 = 13*8 = f(6)*f(5), we have a count of 2, indices of 6 & 5, giving us 2 + 6 + 5 - 1 = 12.\nFor 104 = 13 * 2 * 2 * 2 = f(6) * f(2) * f(2) * f(2), we have a count of 4 and indices of 6, 2, 2, 2, giving us 4 + 6 + 2 + 2 + 2 - 1 = 15.\nWe should pick 13 * 8 since it has the lower score.\nThe biggest problem I've come across is when we have a number like 1008, which is divisible by 144 and 21, but needs to be divided by 21 because 1008 % 7 == 0. Because my program is first dividing by the biggest numbers, number 144 is 'stealing' 3 from number 21 so my program doesn't find a solution.", "Carmichael's theorem proves that each Fibonacci number after 144 has at least one prime divisor that doesn't divide any earlier Fibonacci number.\nThere aren't many Fibonacci numbers under 10^18; fewer than 90.\nMake an array of all the Fibonacci numbers <= 10^18.\nGiven an input n which is the product of Fibonacci numbers, its factorization into Fibonacci numbers must include every Fibonacci number above 144 that divides it, repeated as many times as it divides it.\nGo through your Fibonacci numbers in descending order and keep dividing n by any such number that divides it, until you get to 144.\nNow we need to be careful because two Fibonacci numbers don't have any prime factors not seen in previous Fibonacci numbers. These are 8 and 144. Since 8 is 2^3 and 2 is a Fibonacci number, you can't render your number unfactorable into Fibonacci numbers by taking the 8. Under your optimization, you will always choose the 8.\nThen 144 is the only factor that you might need to reject for a smaller factor. This can only happen if 34 or 21 are factors, and the 144 eliminates a needed 2 or 3.\n34 = 2 * 17, 21 = 3 * 7\nThat was long-winded, but it gets us to a simple approach.\nGo through the Fibonacci numbers <= n in descending order until you get to 144, then skip to 34, then 21, then back to 144 and descending down to 2.\nThis will give you the optimal factorization under your weird scoring scheme.\n----- this order -----\n[679891637638612258, 420196140727489673, 259695496911122585, 160500643816367088, 99194853094755497, 61305790721611591, 37889062373143906, 23416728348467685, 14472334024676221, 8944394323791464, 5527939700884757, 3416454622906707, 2111485077978050, 1304969544928657, 806515533049393, 498454011879264, 308061521170129, 190392490709135, 117669030460994, 72723460248141, 44945570212853, 27777890035288, 17167680177565, 10610209857723, 6557470319842, 4052739537881, 2504730781961, 1548008755920, 956722026041, 591286729879, 365435296162, 225851433717, 139583862445, 86267571272, 53316291173, 32951280099, 20365011074, 12586269025, 7778742049, 4807526976, 2971215073, 1836311903, 1134903170, 701408733, 433494437, 267914296, 165580141, 102334155, 63245986, 39088169, 24157817, 14930352, 9227465, 5702887, 3524578, 2178309, 1346269, 832040, 514229, 317811, 196418, 121393, 75025, 46368, 28657, 17711, 10946, 6765, 4181, 2584, 1597, 987, 610, 377, 233, 34, 21, 144, 89, 55, 13, 8, 5, 3, 2]", "The smallest sum of divisors", 0.0, 0, 101, "I don' t know if it is relevant for your problem but if you need a programm for factoring a positive integer $n$ into a product $a_1a_{1}...a_{\\lambda}a_\\lambda$ of distinct integers with $\\lambda\\leq n$ and $1\\le a_i\\ne a_{i+1}$ for any $i$ then this is a problem of \\em{greatest common divisor} and you can use the \\strong{Euclid's algorithm}.\nTo find the number of factorials you just have to count the $0$-th iteration: $\\frac{n}{\\gcd(n,1)}=\\frac{\\gd(0,n)}{\\gdc( _{\\le}n)}$ where $\\gdbc(_{\\leq} n)$ is \\begin{code}\n$\\sum_{0<i<n}1$\n\\end{Code} if $f_n=1$, and $\\sum_0^{n-1}\\gdec(i)$ if all $b_k\\neq  b_{k+i}$ (this is $\\binom{_{\\lt}0}{_{\\lte}m}$ if $(n+", 0.18106995522975922, 0.012396694160997868, 0.07407407462596893, 0.02664298564195633], ["The dataset is large with over 15000 rows.\nOne row of x,y,z plots a point on a 3D plot.\nI need to scale the data and so far I'm using RobustScaler(), but I want to make sure that the dataset is either normally distributed or it isn't.", "Matplotlib histogram [plt.hist()] can be used for checking data distribution. If the highest peak middle of the graph, then datasets are normally distributed.", "I have a 3D dataset of coordinates x,y,z. How do I check if the dataset is normally distributed?", 0.0, 0, 102, "There is no such thing as a normal distribution of three variables. You could use the qq-plot and check whether the points are normally-distributed in the plane x vs y.", 0.25, 0.07407407462596893, 0.2142857164144516, 0.00734963221475482], ["I have used the .map_partitions with delayed functions and the result I got is a dataframe with delayed results in each row.\nIs there any way to unpack those delayed objects?\nAm I doing something wrong?\nThank you.", "A very short answer: you should use map_partitions with a normal function, not a delayed one. The dataframe interface already provides laziness and parallelism, so you don't need to add another nested level.", "How to unpack a dataframe of delayed dask objects?", 1.2000000476837158, 5, 103, "You can use `map` instead of `apply`:\n`dask.array(d.map(func, d)).compute()`\nwhere `func` is your function.", 0.14814814925193787, 0.03846153989434242, 0.14814814925193787, 0.010638296604156494], ["So, I want to implement a class that holds nested data. I would like to implement __getitem__ in a way so that obj[x][y] can be abbreviated by obj[x, y].\nHowever, I noticed a problem: The signature of __getitem__ is that it expects a single positional argument instead of *args. If multiple arguments are given, they are automatically put into a tuple.\nI.e. obj[a, b] and obj[(a, b)] both appear to be equivalent to obj.__getitem__((a,b))\nBut then how can I distinguish the two cases\n\nThe outer layer is indexed by tuples and obj[(a, b)] should return the value at that index\nThe outer layer is not indexed by tuples and obj[a, b] should return obj[a][b]\n\nThe only possible solutions I am aware of currently are\n\nAbandon the idea of coercing obj[x, y] into obj[x][y]\nIf we only want obj[x] always write obj[x,] instead.\n\nBoth are not really satisfactory.", "Short of trying to inspect the calling source code (which is extremely fragile, with all sorts of failure cases, nowhere near worth the instability and debugging headache), this is not possible.\nobj[a, b] and obj[(a, b)] mean exactly the same thing in Python. There is no semantic difference, no difference in how they are executed, and nothing to hook into to distinguish them. It'd be like trying to distinguish the whitespace in obj[a,b] and obj[a, b].", "Any way to distinguish `obj[x, y]` from `obj[(x, y)]`?", 1.2000000476837158, 2, 104, "You can use the `y` argument to `__get__`:\n\\begin{itemize}\n  \\item \\begin {code}", 0.06315789371728897, 0.0, 0.04210526496171951, 0.04477611929178238], ["I am trying to remove the empty cells from my column A of my data which had text data in it.\nMy csv which I imported into data frame has 50k rows containing search data in column A.\nI tried the below options.\ndf= df.replace(r'^s*$', float('NaN'), regex = True)\ndf.replace(\"\", np.nan, inplace=True)\ndf.dropna(subset=['A'], inplace=True)\nStill there are empty cells", "Are you sure they are empty? Did you check to see they're not just empty strings (\"\")?\ndropna is the proper method, unless you want to also drop cells with empty strings.\nPlease elaborate, thank you", "Trying to remove empty cells in a column in csv from my data using pandas dataframe", 0.0, 0, 105, "you can try\n`df['Column A'].fillna('')`\nto fill the cell with empty space", 0.20000000298023224, 0.0416666679084301, 0.20000000298023224, 0.03225806728005409], ["I'm trying \"from sklearn.linear_model import SGDOneClassSVM\"\nbut it doesn't work and raises an import error \"ImportError: cannot import name 'SGDOneClassSVM' from 'sklearn.linear_model\"", "Upgrade sklearn package using the command:\npip install --upgrade scikit-learn", "problem with importing SGDOneClassSVM from sklearn.linear_model", -0.1973753273487091, -1, 106, "The issue is that `SGDOneClassSvm` is a private class in `sklarn_linearmodel` module.\nSo, you have to import it from the same module, and not from `linearmodle`.", 0.05263157933950424, 0.0, 0.05263157933950424, 0.029408521950244904], ["I have a greyscale images dataset that I want to feed into a neural network.\nx_train_grey.shape is (32, 32, 73257)  so I understand it is (dimension_x, dimension_y, batch_size). Because the images are greyscale, there is only one \"depth\" dimension.\nHowever to feed this data to the neural network it needs to have this shape:(batch_size, dimension_x, dimension_y). With batch_szie at the beginning.\nHow do I reshape it to this format, so that batch_szie comes before the x, y images dimensions?\nOnce this is done, I expect to be able to pass this into a neural network (the first layer being Flatten()), like so:\nFlatten(input_shape=(32, 32, 1)),.\nCheers!", "Solved! By passing the correct shape into np.reshape().\nI really should get to know numpy better, before getting into deep learning.", "How to change the order of dimensions of images data's shape for a neural network?", 0.0, 0, 107, "You can use [`tf.reshape()`](https://www.tensorflow.org/api_docs/python/tf/reshapes) to achieve this, which takes the shape as the first argument and the tensor as a second argument. So you can do: `x = tf.image.resize(x=x.eval(), size=(1, x.get_height(), x .get width()))`\n(assuming x is a 4D tensor of shape (16,14,200,3)).\nFor the original image, this reshaping would give (640177,672185,4).\nIf you want a batch size of 6 and a stride of two, then you'd want: `(63,528129,8)`.", 0.08620689809322357, 0.0, 0.0517241396009922, 0.042016804218292236], ["I have a python dataset that I have managed to take a sample from and put in a second dataset.\nAfter that I will need to produce another sample from the original dataset but I do not want any of the first sample to come up again.\nIdeally this would need any flag would only be there for a year so it can then be sampled again after that time has elapsed.", "Denote your original dataset with A. You generate a subset of A, denote it with B1. You can then create B2 from A_leftover = A \\ B1, where \\ denotes the set difference. You can then generate B3, B4, ... B12 from A_leftover, where Bi is generated from A_leftover = B(i-1).\nIf you want to put back B1 in the next year, A_leftover = A_leftover \\ B12 U B1, and from this, you can generate the subset for B13 (or you can denote it with B1 as 13%12 = 1). So after 12, you can say you can generate Bi from A_leftover = A_leftover \\ B(i-1) U B(i-11). Or you can use this formula from the very beginning, defining B(-i) = empty set for every i in [0,1,2,...,10].", "How I do I get a second sample from a dataset in Python without getting duplication from a first sample?", 0.0, 0, 108, "The [pandas.read_csv](https://pandas-docs.github.io/pandas/reference/api/generated/pd.DataFrame.html#pandas_read-csv) function will load a csv file into a dataframe. The second parameter is the 'index' column. If you set it to a datetime type then pandas will use that column as the index of your data frame. Then you can simply use .sample to randomly sample a subset of rows from your dataframe based on the subset you want.", 0.26291078329086304, 0.056872036308050156, 0.13145539164543152, 0.03551912680268288], ["So, here I have a numpy array, array([[-1.228,  0.709,  0.   ], [ 0.   ,  2.836,  0.   ], [ 1.228,  0.709,  0.   ]]). What my plan is to perform addition to all the rows of this array with a vector (say [1,2,3]), and then append the result onto the end of it i.e the addition of another three rows? I want to perform the same process, like 5 times, so that the vector is added only to the last three rows, which were the result of the previous calculation(addition). Any suggestions?", "For the addition part, just write something like a[0]+[1,2,3] (where a is your array), numpy will perform addition element-wise as expected.\nFor appending a=np.append(a, [line], axis=1) is what you're looking for, where line is the new line you want to add, for example the result of the previous sum.\nThe iteration can be easily repeated selecting the last three rows thanks to negative indexing: if you use a[-1], a[-2] and a[-3] you'll be sure to pick the last three lines", "Iterate over rows, and perform addition", 0.06656807661056519, 1, 109, "\\begin{code}\nIn [35]: arr\nOut[33]: array(\n   [[-0, 215, -131],\n    [0 , 473,   0], [497,    0  ,   89]])\n\\end{pre}", 0.03539822995662689, 0.0, 0.017699114978313446, 0.040625810623168945], ["I need to choose the best model to predict the traffic in a determinated hour.\nI think cluster is not for this problme, but i still don't know what would be the best option. If it's vector machine, decision tree, linear regression or Artificial Neural Networks.", "I think this depends mostly on your data. How much data do you have? If you only have few examples, I would go with VSM (Assuming you mean Support Vector Machines?). If you have a lot of examples I personally would go with a neural network.\nI guess you could even get a nice representation of the prolem if you use a recurrent network.", "Predict future with models / VSM /decision tree/ linear regression/ Artificial Neural Networks", 0.0, 0, 110, "There are a few approaches to this.  One of the easiest to get started with is a decision trees. The decision to use a tree vs. a regression is going to be a matter of how the data is distributed. A linear model will fit a linear relationship with a single linear equation. Regression will provide a very nice model if the relationship is linear. For example if you have 10 features and the response is 50% chance of rain you will get a perfect linear fit. Decision trees are more robust and have less assumptions. You can get trees that have complex non-linear relationships and a range of outcomes. There is no single right answer for your problem. I would suggest reading the book \\em{CART with R}. It is an excellent book and covers the topic well.", 0.2647058963775635, 0.06930693238973618, 0.14705882966518402, 0.05514706298708916], ["I am newbie to data science and I am bit confused about working of map and applymap in pandas. As when I executed code df.applymap(lambda f: f*2) and df.apply(lambda f: f*2) provided same result. But when I change code for both which were df.applymap(lambda f: f*2 if f < 7 else f) and df.apply(lambda f: f*2 if f < 7 else f) then apply method caused an error. Upon my understanding, I came to conclusion that applymap works for each scalar value where apply does not work for each scalar value but instead it executes operation for whole column or series.\nKindly veterans help me out here if I am correct or not. Thanks in advance\nNote: df in code refers to whole DataFrame not series.", "Yes, apply works on a row or a column basis of a DataFrame, applymap works element-wise on a DataFrame.", "Working of map vs applymap in pandas, python", 1.2000000476837158, 0, 111, "`apply` is a function that is applied to a series or a Dataframe, so it can only work on the entire series and not on individual elements.  `map` works on an individual element and does the same operation on every element, resulting in a new Series or DataFrames.", 0.3235294222831726, 0.09090909361839294, 0.2647058963775635, 0.035271067172288895], ["I am given an array of elements and the sum K, and I am supposed to find a subarray (doesn\u2019t have to be contiguous) whose sum is equal to K.\nFor example:\nInput: [1, 9, 3, 2, 21], 30\nOutput: [9, 21]\nDo I need to use backtracking or is there another algorithm using dynamic programming for example?", "If it's not a big array you could use brute force: 2^n solutions!", "How do you find a subarray with the given sum?", 0.0996679961681366, 1, 112, "Yes, back tracking is the best way to solve this problem. You can find the answers on this [link](https://www.geeksforgeeks.org/dynamic-programming-set-1-find-subarray-sum-k/)", 0.043478261679410934, 0.0, 0.043478261679410934, 0.07547170668840408], ["I am using stableBaselines3 based on Open AI gym. The agent, in a toy problem version, tries to learn a given (fixed) target point (x and y coordinates within [0,31] and [0,25] respectively) on a screen.\nMy action space would thus be a box (Version A): self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([31,25])))). The reward obtained by the agent is minus the manhattan distance between the chosen point and target (the simulation terminates straight away). But when running the PPO algorithm, the agent seems to try only coordinates that are within the Box [0,0], [2,2] (ie coordinates are never bigger than 2). Nothing outside this box seems ever to be explored. The chosen policy is not even the best point within that box (typically (2,2)) but a random point within it.\nWhen I normalize to [0,1] both axes, with (Version B) self.action_space = ((gym.spaces.Box(np.array([0,0]),np.array([1,1])))), and the actual coordinates are rescaled (the x-action is multiplied by 31, the y- by 25) the agent does now explore the whole box (I tried PPO and A2C). However, the optimal policy produced corresponds often to a corner (the corner closest to the target), in spite of better rewards having been obtained during training at some point. Only occasionally one of the coordinates is not a boundary, never both together.\nIf I try to discretize my problem: self.action_space = gym.spaces.MultiDiscrete([2,32,26]), the agent correctly learns the best possible (x,y) action (nothing in the code from Version A changes except the action space). Obviously I'd like to not discretize.\nWhat are possible reasons for that whole behavior (not exploring, considering only/mostly corners, moving away from better rewards)? The rest of the code is too unwieldy to paste here, but does not change between these scenarios except for the action space, so the fact that the discretized versions works does not fit with a bug with rewards calculations.\nFinally, my action space would need to have one discrete component (whether the agent has found the target or will continue looking) on top of the two continuous components (x and y). The reward of a non-decisive fixation would be a small penalty, the reward of the final decision as above (the better the closer to the actual target). self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2),gym.spaces.Box(np.array([0,0]),np.array([31,25]),dtype=np.float32))) should be what I'm looking for, but Tuple is not supported. Is there any workaround? What do people do when they need both continuous and discrete components? I thought of making the binary component into a float, and transforming it to 0/1 below/above a certain cutoff, but that can't lend itself too well to learning.", "For posterity, stable_baselines seems to be sampling actions in mysterious ways. If the action space is defined as [0,1] or [-1,-1], stable_baselines will indeed sample that space. But if the action space is, in my case, [0,31], then the actions sampled are roughly within [0,3] or [0,4], with most values being within [0,1].\nSo the workaround seems to be to use Boxes using [0,1] or [-1,-1] for the action_space, and rescale the action returned by whatever SB3 algorithm you're using.", "stablebaselines algorithms exploring badly two-dimension box in easy RL problem", 0.3799489736557007, 2, 113, "The Ppo algorithm does indeed explore all possible actions, it just does so inefficiently, hence why you see this behaviour. One of many things that PPOLib is still working on improving. There are other problems with PPo that cause it not to converge in many cases, if you want to use a better algorithm then you could try A3C or DQN. PPDQ is also good, though the results are not as good. I recommend using PPLib (http://github.com/openai/ppo) for PDDQ, or using an open source code like the one from the [open ai blog](http:/openaiblog.blogspot.co.uk/2016/05/deep-q-networks-for-reinforcement.html).\nPPL does support tuple spaces, you should use that instead of using multiple action spaces. It's easier to work with and also supports multiple replay buffers.", 0.2620087265968323, 0.008810572326183319, 0.13100436329841614, 0.05171838030219078], ["I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors' but I have this error ImportError: cannot import name 'KNeihgborsClassifier' from 'sklearn.neighbors' (C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neighbors_init_.py)", "You are importing KNeihgborsClassifier which is wrong, change it to:\nfrom sklearn.neighbors import KNeighborsClassifier", "I'm trying to import KNeihgborsClassifier from 'sklearn.neighbors'", 0.0, 0, 114, "The answer was in the documentation of the library.\nhttps://scikit-learn-contrib.readthedocs.io/en/latest/neural_networks/kneihb.html", 0.0, 0.0, 0.0, 0.08181817829608917], ["I have a task and the output should be a \"1-D np.array of dimension m\" and I don't understand how a 1-D array can have m Dimension, it has 1 per definition ?", "The word dimension can mean multiple things, in this case it means the size/length of the singular dimension, i.e. you can say an array has dimensions 2x2.\nTherefore, a 1D array of dimension m is equivalent to a list of length m.", "What is a 1-D np.array of dimension m?", 0.1973753273487091, 1, 115, "An array of m dimensions with shape (m, 2) would have 4 elements:\n\\begin{itemize}\n$$(2,2)$$\nor\n$(1,1)$\n$(-1,-1)$.\nAn array with a single dimension of size m would be something like: $m$, $1$ or $(m)$.", 0.2650602459907532, 0.04938271641731262, 0.16867469251155853, 0.048192769289016724], ["I have speed data of many particles to which I want to fit the Maxwellian curve. I am trying to use the fit method from scipy.stats.maxwell to fit to my data and extract the temperature of the system from that.\nFrom the documentation, I am unable to put my finger on what the parameters that we are trying to fit exactly are and hence how they relate to temperature.", "It's related to scale.\nYou also likely want to set floc=0 in maxwell.fit(...) \nCf the argument of the exponential: with non-zero scale it's -x**2 / scale**2 / 2, which you compare to the expected mv**2 / 2kT.", "How to use scipy.stats.maxwell to find temperature?", 1.2000000476837158, 1, 116, "You should try to plot your data before you fit it. This should help you to understand what you are looking for. The data is normally a sum of a bunch of gaussians, each one centered at different position, with different widths, which is why you have the different \"bumps\" in your graph. Each gaussian is centered around the energy of one particle, and the width is the spread in energy around this energy. If you try, you will find that the peak at 10eV (in your case) corresponds to the particles that have a energy in the interval [1.5e-19, 2.05 e-9]. These are the peaks you should focus on.", 0.19230769574642181, 0.012987012974917889, 0.14102564752101898, 0.058475468307733536], ["i have LogisticRegressionCv model it's .pkl file and import data as images but i don't know how to get it on flutter please help me If you know how or if I must to convert my model to other file formats.\nplease help me.\nThank you for your help.", "as you've trained your model in python and stored it in pkl file. One method is in your flutter background, call python3 predict_yourmodel.py your_model_params and after the run, it will give your the model result.\nAnother way is implement a logisticRegressionCv in Flutter as it is a simple model, and easily be implemented. you can store all your params and l1 or l2 etc super-params in a txt instead of pkl file for readility.", "How to implement LogisticRegressionCv on flutter", 0.0, 0, 117, "You can follow this example:\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/python/detsample.py\nIt is part of the object detection API and is available for Python and Java. In this sample you use the tf.contrib.learn.saving_utils package to save the model and then use it as a Pipeline component in the Flutter app.", 0.30985915660858154, 0.0714285746216774, 0.15492957830429077, 0.03606557101011276], ["Since I don't have pure knowledge of the pandas library, I just want to explore the range of functions that pandas library offers to users.", "use dir(pandas)\nBut you'd better go to the official documentation.", "Is there any command which can show what kinds of function are in-built in pandas or matplotlib?", 1.2000000476837158, 0, 118, "There are many, many more functions. You can view the source code for pandas and find out what the functions do. I recommend that you read the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/index.html). It is very comprehensive.", 0.14814814925193787, 0.0, 0.14814814925193787, 0.03503553196787834], ["Getting this error in python:\nValueError: cannot reshape array of size 14333830 into shape (14130,1,1286),\nHow do I solve this?\nThis is the code generating the error:\ndata_train1=data_train.reshape(14130,1,1286)", "For doing reshaping, your new shape should match the previous shape. If you multiply 14130 * 1286, you get 18171180 which is obviously not the same as 14333830. So you must write something correct.", "ValueError: cannot reshape array of size 14333830 into shape (14130,1,1286), how do I solve this?", 0.0, 0, 119, "You should pass your `data` variable to [`np.array`](https://docs.scipy.org/doc/numpy/reference/generated/np._lib.ndarray.html) to convert your array to `ndArray` object.\nThen you can use [`reshaping`](http://www.mathworks.com/help/matlab/ref/resshape.htm) command to change the shape of your data.", 0.16867469251155853, 0.0, 0.07228915393352509, 0.03932584077119827], ["I have a simple table which the datetime is formatted correctly on.\n\n\n\n\nDatetime\nDiff\n\n\n\n\n2021-01-01 12:00:00\n0\n\n\n2021-01-01 12:02:00\n2\n\n\n2021-01-01 12:04:00\n2\n\n\n2021-01-01 12:010:00\n6\n\n\n2021-01-01 12:020:00\n10\n\n\n2021-01-01 12:022:00\n2\n\n\n\n\nI would like to add a label/batch name which increases when a specific threshold/cutoff time is the difference. The output (with a threshold of diff > 7) I am hoping to achieve is:\n\n\n\n\nDatetime\nDiff\nBatch\n\n\n\n\n2021-01-01 12:00:00\n0\nA\n\n\n2021-01-01 12:02:00\n2\nA\n\n\n2021-01-01 12:04:00\n2\nA\n\n\n2021-01-01 12:010:00\n6\nA\n\n\n2021-01-01 12:020:00\n10\nB\n\n\n2021-01-01 12:022:00\n2\nB\n\n\n\n\nBatch doesn't need to be 'A','B','C' - probably easier to increase numerically.\nI cannot find a solution online but I'm assuming there is a method to split the table on all values below the threshold, apply the batch label and concatenate again. However I cannot seem to get it working.\nAny insight appreciated :)", "Since True and False values represent 1 and 0 when summed, you can use this to create a cumulative sum on a boolean column made by df.Diff > 7:\ndf['Batch'] = (df.Diff > 7).cumsum()", "Pandas create a column iteratively - increasing after specific threshold", 0.1973753273487091, 3, 120, "This isn'th the best way, but it works. I think it's the only way with pandas. It is probably not the most efficient way. Also the data is not ordered by the date, so there might be a problem with the column. In my example I tried to fix the ordering, because it wasn'r obvious. This will add an extra column to your table. You could delete that column if you don'n need it. But I thought it might make it easier for you. Maybe you can adapt it to you needs. See the code below: (I had to change the file path)\nThe code is as follow: It takes the dataframe, splits it by values smaller than the specified threshold and adds a new column with a batch name. Then it concatenates the new dataframe with its original dataframe. Note that the order of the rows might change (because the first df is sorted). You can solve that by sorting the original data frame first. (See: https://stackoverflow.com/questions/42566415/sort-pandas-dataframe-by-multiple-columns) This might take some time, as there are a lot of rows.", 0.10480349510908127, 0.008810572326183319, 0.061135370284318924, 0.04760885611176491], ["Consider a vector [0 1 2] and a matrix of size 3 x n. How can I multiply each element of the vector with the corresoponding row of the matrix. Each element of row 0 should be multiplied with 0, each element of row 1 should be multiplied with 1 and so on?", "I assume you're using numpy. You can use matrix *= vector.reshape(-1, 1). This will convert the vector to a column, then multiply the rows.", "element wise multiplication vector with rows of matrix", 1.2000000476837158, 1, 121, "You can do this with a `for` loop.  Loop through the rows and multipy with vector.\nThe output will be a 4-dimensional matrix, but you can access each vector individually.", 0.42105263471603394, 0.072727270424366, 0.21052631735801697, 0.017230644822120667], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "If the data is small enough that you can use pandas to process it, then you likely don't need pyspark. Spark is useful when you have such large data sizes that it doesn't fit into memory in one machine since it can perform distributed computation. That being said, if the computation is complex enough that it could benefit from a lot of parallelization, then you could see an efficiency boost using pyspark. I'm more comfortable with pyspark's APIs than pandas, so I might end up using pyspark anyways, but whether you'll see an efficiency boost depends a lot on the problem.", "Databricks - Pyspark vs Pandas", 1.2000000476837158, 2, 122, "If you are using spark on your cluster, and if you know that you have inefficient blocks, you can do the following.\n\\begin{itemize}\n  \\item Write a data frame to your filesystem.  This data can then be read by panda.   Or you could read it directly by using a dataframe from the spark context. You can read this data directly from spark if your data is stored in the data base. This is what most people do. The main reason for this is that Spark has a very efficient data representation (parquet).\n      If your dataframe is too large and you want to avoid reading it all, then you should look at Spark SQL and the phoenix library. (https://phoenix.apache.org/documentation.html). This library allows you to query a database using SQL. For example, If the table is on Oracle, it will use a JDBC connection and use Oracle SQL to fetch the result. It also supports other databases. In the case of Spark, the Jdbc driver is built into the driver. Thus, when you connect to a Spark cluster and tell it to load a table, spark SQL will connect through JDB to the Oracle database. There is also a feature called the ph", 0.2628205120563507, 0.05806451663374901, 0.13461539149284363, 0.04409157112240791], ["I have a python script where I'm using pandas for transformations/manipulation of my data. I know I have some \"inefficient\" blocks of code. My question is, if pyspark is supposed to be much faster, can I just replace these blocks using pyspark instead of pandas or do I need everything to be in pyspark? If I'm in Databricks, how much does this really matter since it's already on a spark cluster?", "Pandas run operations on a single machine whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark is the best fit which could process operations many times(100x) faster than Pandas.\nPySpark is very efficient for processing large datasets. But you can convert spark dataframe to Pandas dataframe after preprocessing and data exploration to train machine learning models using sklearn.", "Databricks - Pyspark vs Pandas", 0.0, 0, 123, "If you have already written code in pandas, it is best to keep it there, as the pandas package will be optimized to run on top of the spark engine, and it will run faster.\nIf you want to use Spark in your code, there is a SparkContext object that you can access. You can use this to perform transformations on your dataframe and save it back to the data frame. The following code is an example of how to write a dataframe to an S3 location. Note the \"sc.parallelize\" to transform the dataframe.", 0.2857142984867096, 0.03614457696676254, 0.1666666716337204, 0.0519120991230011], ["I have a scenario in which I have a peptide frame having 9 AA. I want to generate all possible peptides by replacing a maximum of 3 AA on this frame ie by replacing only 1 or 2 or 3 AA.\nThe frame is CKASGFTFS and I want to see all the mutants by replacing a maximum of 3 AA from the pool of 20 AA.\nwe have a pool of 20 different AA (A,R,N,D,E,G,C,Q,H,I,L,K,M,F,P,S,T,W,Y,V).\nI am new to coding so Can someone help me out with how to code for this in Python or Biopython.\noutput is supposed to be a list of unique sequences like below:\nCKASGFTFT, CTTSGFTFS, CTASGKTFS, CTASAFTWS, CTRSGFTFS, CKASEFTFS ....so on so forth getting 1, 2, or 3 substitutions from the pool of AA without changing the existing frame.", "Let's compute the total number of mutations that you are looking for.\nSay you want to replace a single AA. Firstly, there are 9 AAs in your frame, each of which can be changed into one of 19 other AA. That's 9 * 19 = 171\nIf you want to change two AA, there are 9c2 = 36 combinations of AA in your frame, and 19^2 permutations of two of the pool. That gives us 36 * 19^2 = 12996\nFinally, if you want to change three, there are 9c3 = 84 combinations and 19^3 permutations of three of the pool. That gives us 84 * 19^3 = 576156\nPut it all together and you get 171 + 12996 + 576156 = 589323 possible mutations. Hopefully, this helps illustrate the scale of the task you are trying to accomplish!", "Generate the all possible unique peptides (permutants) in Python/Biopython", 0.0996679961681366, 1, 124, "You can generate the permutations by using a [permanutation function](https://docs.python.org/3/library/itertools.html#itertoolspyper):\n`iterations = itertools_perms('AKNQDENQK')`\nwhere 'AKQDNQ' is the peptyle frame and 'R' denotes random substitution. The permutation will generate a permutated sequence like this: 'DQAKA' , 'QNKAK' etc.", 0.12222222238779068, 0.01123595517128706, 0.07777778059244156, 0.029801324009895325], ["When I try to install sklearn-crfsuite, I get the following error:\n\nfatal error C1083: Cannot open include file: 'basetsd.h': No such file\nor directory\n\ntrying this command pip install sklearn-crfsuite, also installed Microsoft visual C++ 2019 and the required libraries.\nPlease let me know if there is any solution to this, do I need to set any variable in the system path?", "If I understand your problem correctly, add the path to your header file in your project using Property->C/C++->General->Additional Include Directories.\nIf you want this to apply to all your projects use the Property manager to add this path.\nOf course make sure the header exists.", "fatal error C1083: Cannot open include file: 'basetsd.h': No such file or directory", 0.0, 0, 125, "Try to reinstall Python 3.9. I also encountered the same problem. Reinstalling Python fixed the problem for me.", 0.14705882966518402, 0.0, 0.0882352963089943, 0.04255318641662598], ["ctx=con.cursor()\nctx.execute(select col1 from table1)\nresult=ctx.fetchall()\ndata=pd.DataFrame(result)\ndata.columns['field']\nfor index,row in data:\nupdate table2 set col2='some value' where col1=str(row['field'])", "Solution to this is:\nInsert the data into some transient table and then then use that table for update.\nFor insert :\ndata = panda.DataFrame(result)\njust use data.to_csv('file complete path',index=False,Header=True)\nusing put command place the file in internal stage and from there use Copy command to copy data into transient table.\nlater on you can use this table to update your target table.", "Updating snowflake table row by row using panda dataframe (iterrows()) taking lot of time .Can some one give better approach to speed up updates?", 1.2000000476837158, 0, 126, "I would suggest you to use `executemany()` method, it is more efficient than iterrows().\nTry this,\n`ctx=myconn.executable(\"\"\"INSERT INTO `table2` (`col1`,`col2`) VALUES %s\"\"\",data)`\nYou can also pass the columns as a list and then use it.\nI hope it helps.", 0.22807016968727112, 0.0535714291036129, 0.14035087823867798, 0.03669685870409012], ["after model training, spacy has generated model\\model-best and model\\model-last folders. What's the difference between the two models and which one should be used for predictions?", "model-best is the model that got the highest score on the dev set. It is usually the model you would want to use.\nmodel-last is the model trained in the last iteration. You might want to use it if you resume training.", "difference between model-best and model-last in spacy", 1.2000000476837158, 1, 127, "The best model is the model that the Spacy system thinks is best, and the last model are the models that are being used to train the system. So you can use both of them to make predictions.\nThe best way to do it is to choose the best and last models, train them and then use the one that you think is better. This is a better option than using just the latest model, as the \"last\" model might not be the same as \"best\".\nIf you're interested in what the spaCy team thinks, you should check out [this post](https://spacy.io/blog/2018/11/30/modeling-for-accuracy-vs-modelizing-to-generalize/) on the subject.", 0.3680981695652008, 0.08695652335882187, 0.24539877474308014, 0.06248335912823677], ["I am using compute_face_descriptor function in dlib which is the function of dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat').\nThere is an option to set \"num_jitters\". I set \"num_jitters\"=10, but the output embedding I am getting different on subsequent runs. I have tried setting seed using np.random.seed(43), but still, the output changes on subsequent runs\nIs there a way to set seed in this function using \"num_jitters\"=10 so that the output embedding doesn't change on subsequent runs?", "\"num_jitters\". means how many times dlib will re-sample your face image each time randomly moving it a little bit. That's why you are getting different embeddings.", "How to set seed to compute_face_descriptor function in dlib?", 0.0, 0, 128, "There is no way of setting a seed when you call this. This is because the `numjitter` option controls how many jitterns are used to generate the embedding. The `compute_facial_features` function is used in the model.\nIn the first call to the method, a random seed is generated for the initial embedding, and the following calls will use the same seed. That is, if you set a different seed the result will be different.", 0.15238095819950104, 0.019417475908994675, 0.11428571492433548, 0.05070248991250992], ["during i trained my own model, i have a simple question.\norigianl input image shape is (height : 434, width : 636), and i used resized image(416 x 416) for my train model(Unet++).\nI wonder if it is right to resize the test image when inference step, How can I resize the model output to the original image size when comparing test output with original test image.\n---------process\noriginal input size : (434, 636)\ntrain input size: (416, 416)\n\ninference\ntest img -> resize (416, 416) -> test model -> test output(416,416) -> comparing test output with test img", "It's absolutely right to resize the input image to the model input size else, it will generate an error if you feed an image of different size to the model. Coming towards your question, you can solve this either by rescaling the model output to the original size of your input images. A simple technique can be resizing the masks but there can be better ways. OR\nYou can resize your input images and their Ground Truths (masks) to the model size, and so you won't need to rescale the model's output. I hope that answers the question !!!", "Question about Inference for Image segmentation", 0.0, 0, 129, "If you are using a CNN to classify your images, the input of the CNN will be a 2D matrix. This matrix will have the shape of (image height, image width, channels) where channels are the number of color channels you have in your image (RGBA, BGR, etc). If your input images are in RGB format, your matrix shape will then be (num_rows, num_cols,3) and if your inputs are grayscale, it will take the form of a (2,2) matrix with a single channel. The model you trained will not care about the format of your data, but rather it's 3D form. Therefore, if you want to compare the output of this model with the image that you're classifying, you must have that image in the same format.", 0.3504273593425751, 0.06896551698446274, 0.19658119976520538, 0.03644561022520065], ["I want to get the molecules from the SMILES using rdkit in python. The SMILES I used was downloaded from the drugbank.\nHowever, when I using the function Chem.MolFromSmiles, some SMILES would report but some wouldn't:\nExplicit valence for atom # 0 N, 4, is greater than permitted.\nI found some explanation about this problem: it is because the SMILES generated a invalid molecule that doesn't exist in real world.  But I am not a chemistry student.... So anyone know how to fix this?", "Your SMILES string would appear to have a neutral 4-co-ordinate nitrogen atom in it, which doesn't exist in real molecules.  4-co-ordinate nitrogen atoms have a positive charge, eg [N+] in a SMILES string.", "Problems encountered when using RDKIT to convert SMILES to mol", 1.2000000476837158, 2, 130, "\\begin{itemize}\n\\item The mol file format was designed to store a SMARTS pattern (a.k.a a regular expression for molecular structures). The molecation is not stored in the molfile, instead the pattern is used to generate the final mol structure. So there is no way to know in advance what the real molecula will look like, the only thing you can do is generate it, which you already have done. What you need to do now is to find the error and fix it. If you want more information on molfiles have a look at this [link](https://www.rdkit.org/docs/smiles_molfile_format.html).\nIt is very unlikely that you will be able to predict all possible errors, so you should run the chem.mols.parseSmi function with the -debug switch. This will produce a detailed error report which should help you in finding the problem.", 0.13829787075519562, 0.02150537632405758, 0.08510638028383255, 0.10404623299837112], ["I am trying to write several Panda Dataframes into a SQL database. The dataframes are generated in different processes using the multiprocessing library.\nEach dataframe should get its own trial number when it is written into the database. Can I solve this using SQL autoincrement or do I have to create a counter variable in the Python code.\nIf I use the function pandas.DataFrame.to_sql and set an index as autoincrement, I get a consecutive index for each row.\nHere is an example how it should look like\n\n\n\n\ntrial number\ntimestamp\nvalue\n\n\n\n\n1\ntime1\nvalue1\n\n\n1\ntime2\nvalue2\n\n\n1\ntime_n\nvalue_n\n\n\n2\ntime1\nvalue1\n\n\n2\ntime2\nvalue2\n\n\n2\ntime3\nvalue3\n\n\n2\ntime_n\nvalue_n\n\n\n\n\nI use Python 3.9 and MariaDb as Database. I hope for help. Thanks", "You should have a separate trials table in your database where you cspture the details of each trial. The trials table will have an auto incremented id field.\nBefore writing your dataframes to your values table, each process inserts a record into the trials table and get the generated auto increment value.\nThen use this value to set the trial number column when you dump the frame to your table.", "Write Panda Dataframes to SQL. Each data frame must be identifiable by a trial number", 0.0, 0, 131, "You can use [to\\_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/panda.io.sql.html) and create an integer index column in your dataframe. Then you can pass that integer to your sql database as your index. Here is a basic example. You might have additional configuration to do if you have a multi-column index and the index is not a unique value. Note that the first row is your header row, and I do not know what is in it, so I just set it to '0'.\nThe following code creates a dataframe with an incrementing integer column:\n`df = pd.Series([0]).to(df) # create integer Series to index`\nYou can set the header to be anything you want, it just needs to exist. Also, you need to ensure that your DataFrame is already sorted by the column you are indexing (if not, the insert will fail).\n\\begin{code}\ndf.set_index(pd.Index([], name='id')) # set index to integer\nprint(f'Creating Table {df}')\npd2sql = 'mariadb -h 127.", 0.26229506731033325, 0.03305784985423088, 0.15573769807815552, 0.05325443670153618], ["df['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : x.replace('.', ',',1).replace('.', '').replace(',', '.',1)).astype(float)\nSlowly learning lambda command, my understanding of this line of code is:\n\nChange dataframe type to str\nApply lambda with one perimeter x\nReplace all the string format . to , , (I don't understand what does 1 stands for, have done research prior asking, didn't find clue)\nReplace all the string format . to null value\nReplace all the string format , to . , (again still have no clue what does 1 stands for in this case)\nChange dataframe type to float\n\nPlease help me better understanding this line of code, thank you", "This replaces the first . in the string with a ,, removes the remaining periods, changes the first , back to a ., then converts the result to a float for the 'Current Ver' column in the dataframe.", "How to understand this lambda with 3 .replace() line of code", 1.2000000476837158, 2, 132, "There is a lot going on here.  I'll try to explain.\n\\begin{itemize}\nFirst, a string is converted to a float. The `.replace` is for `,` to `.`.\nThe `.astypes` are for the type of the `df` variable. `str` means it is type `string`. `float` type is the default type. You can also do `int` or `bool` etc. for other types. But `astyep` only changes the data type, it does not change the values in the dataframe. So you would need to do some other operation on the df variable to change it's value. If you want to convert a column of strings to floats, you could do something like this: `d = pd.to_numeric(df['col'].apply('str'))`\nwhich converts each string in `col` into a numeric type (float).\n`str.contains` checks if the column has any `strings` in it. `.apply` returns a series (which is basically a dataframe) of all of your column' s values after the `.str[contains]` operation. Then you do the replace, `x.str['contains']", 0.23529411852359772, 0.059405941516160965, 0.14705882966518402, 0.0628930851817131], ["Initially, my dataframe had a Month column containing numbers representing the months.\n\n\n\n\nMonth\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\nI typed df[\"Month\"] = pd.to_datetime(df[\"Month\"]) and I get this...\n\n\n\n\nMonth\n\n\n\n\n970-01-01 00:00:00.0000000001\n\n\n1970-01-01 00:00:00.000000002\n\n\n1970-01-01 00:00:00.000000003\n\n\n1970-01-01 00:00:00.000000004\n\n\n\n\nI would like to just retain just the dates and not the time. Any solutions?", "get the date from the column using df['Month'].dt.date", "Date and Time Format Conversion in Pandas, Python", 0.0, 0, 133, "I'm not entirely sure what you want to do, but maybe this helps:\n`df.Month = df.month.apply(lambda x: pd.__datetime__(x, unit='M'))`\nYou can do the same for the `day` and `year` columns.", 0.1666666716337204, 0.043478261679410934, 0.0833333358168602, 0.08476020395755768], ["I've installed the native version of python3 through miniforge and the intel version of Spyder through homebrew. Everything is working fine with one exception, my plots seem to work with the \"graphics backend\" \"inline\" option. When I try to select the \"automatic\" option, or any other option rather than inline, the IPython doesn't initiate. Has anyone had the same problem?\nKind regards,", "(Spyder maintainer here) This problem is fixed in our 5.2.0 version, released in November 2021.", "Spyder \"Graphics backend\" \"automatic\" option not working on M1 macbook", 0.0, 0, 134, "Please try with this :\n`spyiderc --auto --backend \"IPython\" --pylab --qt4`\nPlease let me know if it helps.\nHope it will be useful.", 0.05128205195069313, 0.0, 0.05128205195069313, 0.037034161388874054], ["I have df = pd.concat(dict_of_df, axis=0) and sometimes [rarely] it might be the case that all of the df in the dictionary are empty in which case I would like Pandas to cheerfully return an empty dataframe. But instead I get a ValueError.\nI can write a loop to check for the length of each df before calling concat, but would prefer to not always do that, so at the moment I just embed the concat into a try/except... which doesnt make be really happy either because if there was a \"true\" ValueError I would like to have know it. So then I could do a try/except loop and if exception is thrown then do a count of all the dicts and ... ugh. This is getting crazy.\nIs there something more clean? Thanks.", "Sorry, I am going to withdraw the question. I now realize that\npd.concat([None,None]) produces the ValueError, whereas as noted above pd.concat(pd.DataFrame(),pd.DataFrame()) does exactly what you would hope. Also pd.concat([None,pd.DataFrame()]) is fine too. So it's not really fair of me to complain about concat. I need to stop feeding my routine non-existent datasets !\nThanks for feedback", "Can I avoid a ValueError concatenating empty dataframes?", 0.0, 0, 135, "You can use `np.where` for this:\n\\begin{code}\nd = {0: {}, 1: {}}\nnp_d_dict = np.array(d)\ndf = (pd.DataFrame() if np_value.all(0).any(1) else pd_dictionary[np_.where(np._value_ == 0)]\n        .to_frame()\n) if df.empty.any() else df\n#  0\n #  1\n0   NaN\n1   nan\n2   null\n3   n/a\n4   no\n5   nothing\n6   none\n7   nada\n8   nichts\n9   inexistent\nName: 2, dtype: object\n[out]:\n   [0, 6, \"NameError: 'NoneType' object is not subscriptable\"]\n  df is a pandas dataframe with 4 columns and 5 rows\nTypeError                                 Traceback (most recent call last) <ipython-input-4-205c99278775> in <module>()\n      2 df_list = []\n----> 3 dfs = df", 0.11235955357551575, 0.011363636702299118, 0.06741572916507721, 0.014657347463071346], ["in pandas the inplace parameter make modification on the reference but I know in python data are sent by value not by reference i want to know how this is implemented or how this work", "Python\u2019s argument passing model is neither \u201cPass by Value\u201d nor \u201cPass by Reference\u201d but it is \u201cPass by Object Reference\u201d\n\nWhen you pass a dictionary to a function and modify that dictionary inside the function, the changes will reflect on the dictionary everywhere.\nHowever, here we are dealing with something even less ambiguous. When passing inplace=True to a method call on a pandas object (be it a Series or a DataFrame), we are simply saying: change the current object instead of getting me a new one. Method calls can modify variables of the instances on which they were called - this is independent of whether a language is \"call by value\" or \"call by reference\". The only case in which this would get tricky is if a language only had constants (think val) and no variables (think var) - think purely functional languages. Then, it's true - you can only return new objects and can't modify any old ones. In practice, though, even in purest of languages you can find ways to update records in-place.", "the inplace parameter in pandas how it works?", 1.2000000476837158, 1, 136, "The in-place functionality is only for the DataFrame, and it will only be applied to the internal representation of the object.\n\\begin{itemize}\n  \\item For a Dataframe it is the same as `df = df.copy()`.\n    \\begin {code}", 0.1953488439321518, 0.03755868598818779, 0.09302325546741486, 0.043467678129673004], ["I have 2 columns of data with the first column being product codes (all filled out) and the second column with product description.\nThe first column has all the product codes filled out but there are some rows where the product description (second column) is missing.\nFor example row 200 has a product code of 145 but the description on that row is empty (NaN). However, there are other rows with product code 145 where the description exists, which is \"laptop\". I would like to have the description of row 200 to be filled with \"laptop\" because that's the description for that product code.\nI want to find a solution where I can fill out all NaN values in the second column (product description) based on the first column (product code).", "First, decide on a function that takes descriptions and picks out one of them. You could use min, max, mode, define you own get_desc, etc. Then you can separate the dataframe by product code with groupby and apply whatever function you decided on: df.groupby('product code').apply(get_desc) or df.groupby('product code')['product description'].apply(get_desc) depending on whether get_desc takes a dataframe or column as input. Then you can merge the resulting dataframe with your original dataframe. You can either replace the entire original product description column with the product description column of the groupby output, or have merge create a new column, then fillna the old product description with the new product description.", "Fill NaN in column 2 with median string based on value in column 1 in Python", 0.0, 0, 137, "You can use `pandas.DataFrame.fillna()` to replace `NaN` values with a default value. In your case the default is `'lapp'`\nYou can also specify the index of the `DataFrame` to use as the key to fill in. So if you had a `Dataframe` called `df2` with values as below. And you wanted to substitute the values of `col2`, then you could do `fillnan(df, df2, 'col1')`.\n<a href=\"https://i.stack.imgur.com/hDV3f.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nHere is the code for your question. I don't think you can do it with one line, but this should get you started. Let me know if it doesn' want do do what you want it to do\n\\begin{code}\ndf['col3'] = df['product'].fill(1)\ncols = ['col4','col5','product']\nindex = np.arange(len(cols))\n#replace with your dataframe\nfill = pd.Series(d2.set_index(index), index=cols) #your second dataframe with only", 0.3188405930995941, 0.07299269735813141, 0.14492753148078918, 0.04278373718261719], ["I want to horizontally cut the spectrogram of a wav file into 24 pieces,and after measuring the power of each piece, and finally rank the pieces by power orders what should I do please", "Could you show some code that you have written to try out the same? It would be easier to help if we have something to build upon and rectify issues, if any.\nAdditionally please try basic image manipulation to do the same. Instead of cutting you could divide the image into N (here 24) regions and analyze them in parallel using multiprocessing.", "cut the spectrogram of a wav file", 0.0, 0, 138, "You could use `ffmpeg` to cut your audio file to the desired length, then use [this](https://stackoverflow.com/questions/18966028/how-to-get-the-spectrograph-of-an-audio-file) answer to get the frequency domain of the audio, cut it into pieces of length 100, measure the energy of those pieces using `calcFrq(image, 0)` and sort those values using [`sort(a, b, ascending=True)`](http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#sort).\nYou'd need to do this for each of your 4 channels, as the answer linked uses a grayscale image.", 0.2222222238779068, 0.02500000037252903, 0.1358024626970291, 0.07333333045244217], ["I am trying to understand why one or two parameters in my Pytorch neural network occasionally become nan after calling optimizer.step().\nI have already checked the gradients after calling .backward() and just before calling the optimizer, and they neither contain nans nor are very large. I am doing gradient clipping, but I don't think that this can be responsible since the gradients still look fine after clipping. I am using single-precision floats everywhere.\nThis behavior happens randomly every hundred thousand epochs or so, and is proving very difficult to debug. Unfortunately the code is too long to reproduce here and I haven't been able to replicate the problem in a smaller example.\nIf anyone can suggest possible issues I haven't mentioned above, that would be super helpful.\nThanks!", "This ended up being ignorance on my part: there were Infs in the gradients that were evading my diagnostic code, as I didn't realize Pytorch's .isnan() method doesn't detect them.", "What are the main reasons why some network parameters might become nan after calling optimizer.step in Pytorch?", 1.2000000476837158, 0, 139, "This happens when you pass a `Parameter` which is not initialized yet, to the `backwards` method of a layer. This is because the backward method will call the function `parameter_value` (which is defaulted to `identity` in the Pydotor codebase) to obtain the current value of the parameter, which will then be used to compute the gradient. If the param is uninitialized, you get an error because `nan`.\nThe fix is to initialize the uninit param before passing it to any function, for example:\n`self.param.data = self.init_data`", 0.06504064798355103, 0.01652892492711544, 0.04878048598766327, 0.05999999865889549], ["Can someone help me with transforming the following table using a PYTHON function?\nI need 2 new columns: A \"follower Type\" which will have entries as organic or paid and a \"Follower count\" which has the values corresponding to the type of follower.\nCurrent Table -\n\n\n\n\norg\norganic follower\npaid follower\nstart date\nstop date\n\n\n\n\nOne\n2\n0\n1634169600000\n1634256000000\n\n\nOne\n-1\n0\n1634256000000\n1634342400000\n\n\n\n\nDesired Table -\n\n\n\n\norg\nstart date\nstop date\nFollower Type\nFollower Count\n\n\n\n\nOne\n1634169600000\n1634256000000\nOrganic\n2\n\n\nOne\n1634169600000\n1634256000000\nPaid\n0\n\n\nOne\n1634256000000\n1634342400000\nOrganic\n-1\n\n\nOne\n1634256000000\n1634342400000\nPaid\n0\n\n\n\n\nIf anybody knows how to do this, please do let me know.\nThanks and Cheers!", "Use reindex to change column order\n'''\ncolumn_names = [\"C\", \"A\", \"B\"]\ndf = df.reindex(columns=column_names)\n'''\nLike below you can add columns to existing dataframe\ndf[newcolumn]=formula", "How can I do the following dataframe transformation in Python?", 0.0, 0, 140, "You can try something like this using pandas:\n\\begin{itemize}\n  \\item First we make a new column to identify whether the followers are organics or not\n      \\begin {code}", 0.21052631735801697, 0.036363635212183, 0.10526315867900848, 0.02064630389213562], ["I have a dataset with more than 50 columns and I'm trying to find a way in Python to make a simple linear regression between each combination of variables. The goal here is to find a starting point in furthering my analysis (i.e, I will dwelve deeper into those pairs that have a somewhat significant R Square).\nI've put all my columns in a list of numpy arrays. How could I go about making a simple linear regression between each combination, and for that combination, print the R square? Is there a possibility to try also a multiple linear regression, with up to 5-6 variables, again with each combination?\nEach array has ~200 rows, so code efficiency in terms of speed would not be a big issue for this personal project.", "This is more of an EDA problem than a python problem. Look into some regression resources, specifically a correlation matrix. However, one possible solution could use itertools.combinations with a group size of 6. This will give you 15,890,700 different options for running a regression so unless you want to run greater than 15 million regressions you should do some EDA to find important features in your dataset.", "Automatic Linear/Multiple Regression in Python with 50+ columns", 0.0, 0, 141, "You can use `np.corrcoef` to calculate the correlation between any two columns.  This returns a correlation matrix. From there you can do simple matrix operations to do whatever linear combination you want. For example:\n`corrmat[:,1]-corramat[0,:]`\nThis will calculate your first column minus your second column.", 0.2689075767993927, 0.0683760717511177, 0.13445378839969635, 0.033707860857248306], ["I use tf.math.round() but the output still has decimal points (310.0, 210.0 etc)\nIf I use tf.cast(tf.math.round(), dtype=\"int32\"), then I see the error mentioned in the title when calling finish on tornado handler\nHow can I cast to int using tensorflow operations and still be json serielizable", "You should use tf.int32 instead.", "tf cast leads to TypeError: Object of type 'int32' is not JSON serializable", 0.0, 0, 142, "You should use `int` for cast.\nhttps://www.tensorflow.org/api_docs/python/tf/cast\nYou can try with this code :\n`tf = tf([x for x in tf])`\nand then use it for round and cast like this : `tf_cast = int(round(float(x)))`.", 0.1599999964237213, 0.0833333358168602, 0.1599999964237213, 0.42736661434173584], ["I trained my named entity recognizer with spacy. I would like to evaluate it. So I looked at the spacy documentation and came across the scorer function. However, it doesn't seem to work with the IOB format. Do you think there will be a way to use spacy to evaluate my IOB data or am I doomed to transform my data into the format spacy wants?\nThank u very much :)", "You can't evaluate IOB data directly. You should be able to just use spacy convert to convert it to .spacy data in one step and then use spacy evaluate with that file though. (And if you trained your model in spaCy then presumably you already did the same conversion with your training and dev data?)", "Python - Is there a way to evaluate a named entity recognizer trained on IOB data using spacy?", 0.0, 0, 143, "The `scorer` function works with IO-B.\nFor example:\nYou can use it like this: `spacy_doc.scores.sentiment.pos_score`\nIf you would get an error like `TypeError: 'SentimentScorer' object is not callable`, it means you did not train the model with an IOC data.", 0.27450981736183167, 0.03999999910593033, 0.23529411852359772, 0.01834862492978573], ["I am trying to save / load data and objects in Python. I usually use pickle to save pandas data frame and custom objects. Recently I had to change python version (from 3.6 to 3.8) and pandas / pickle version accordingly. I now have trouble to read previous pickled version. I have found some ways to deal with that (ranging from using some pickle options to reloading / rewriting the data).\nHowever I would be interested in a more generic way to save data / objects that would be python / packages independant. Does such a thing exists (without adding to much weird dependencies) ?", "If you save your data as a .CSV file (depending on what your data looks like) you should'nt get dependencies problem\nIf your data cannot be saved as a csv I think using JSON could also be a solution", "Save data / objects without python / pandas / pickle dependencies?", 0.0, 0, 144, "You could use [pickle](https://docs.python.org/3/library/pickl.html) to pickles the object and then use the [dill](http://douglas-m.com/dumps/index.htm) library to unpickles it back to Python object.\nFor example, given a dictionary `d`\n\\begin{itemize}\n  \\item `pickled_d = dill.dump(d)`\n    \\em{pickling the dict to binary format} (d is still around)\n     \\strong{NOTE:}\n        pickling is generally more efficient than picking because it uses less memory\n      \\sout{\n       (though it is slower because of the serialization process)}\n   \\begin {code}", 0.04724409431219101, 0.0, 0.04724409431219101, 0.05617976933717728], ["ERROR: Failed building wheel for scikit-image\nFailed to build scikit-image\nERROR: Could not build wheels for scikit-image, which is required to install pyproject.toml-based projects", "Did you try to upgrade pip ( python -m pip install --upgrade pip )?\nTry installing numpy first too.", "Failed building wheel for scikit-image", 0.0, 0, 145, "I was able to resolve the issue by removing the python 2.7 from the path.\n\\begin{itemize}\nRun the command \\strong{which python} in terminal and then type \\begin {code}", 0.0833333358168602, 0.0, 0.0833333358168602, 0.024691356346011162], ["in google colaboratory using python, I am trying to load model to classify\nI am trying to load keras model using python to classify image, I am getting above error", "predict_classes is only available for the Sequential class.\nWith the Model class, you can use the predict method which will give you a vector of probabilities and then get the argmax of this vector (with np.argmax(y_pred1,axis=1)).", "AttributeError: 'Functional' object has no attribute 'predict_classes'", 0.0, 0, 146, "Keras models are not stored as a `Function` object, but as an instance of `KerasModel` class. You need to instantiate `keras.models.Sequential` to get a Kerasa model.\nYou can do it like this:\n`model = Sequential(ker.model.layers)`", 0.2469135820865631, 0.050632912665605545, 0.12345679104328156, 0.08673470467329025], ["In practice, using both [..., :2] and [:2] on np.array([1,2,3]) results in np.array([1,2]). Are there also cases where the result differs when you use an ellipsis like this on an array?", "np.arrays are designed to handle n-dimensional arrays, specified as [rows, columns]In the case of np.array([1, 2, 3]), [:2] and [:, :2] will yield the same result because our array input is 1-dimensional of shape [1, 3], e.g. with 1 row and 3 columns.\nIf we instead input np.array([[1,2,3], [4,5,6]]), e.g. a 2-dimensional array of shape [2, 3], this will change. On this array, if we, e.g., do [:1, :2] we will get array([[1, 2]]) because we are asking for everything up to the first (i.e. the 2nd since we count from zero) row and everything up to the second (i.e. the 3rd) column.\nHope this makes sense.", "What is the difference between using [..., :] and [:] on a numpy array?", 1.2000000476837158, 1, 147, "The ellipis in `[]` is a Python feature that is used in place of multiple indexing. In other words, it is just a shortcut for `[i for i in array]`, where `array` can be a list, tuple or numpy `ndarray`.\nIn your case it means that `[:2]` is equivalent to `[0, 1]` (or `[1]`)\nOn the other hand, the colon in a `numpy` `dtype` or `shape` means the number of dimensions, so `np.int32(1)` is 3-dimensional with the first dimension being `1` and `([0],:2)` means a 2D array with shape `(2)`, with each dimension having `0` as the 0th index.\nTherefore, `[[1], 42], [4,5]` and `[[1]], 5` are all equivalent in this case.", 0.35684648156166077, 0.025104602798819542, 0.17427386343479156, 0.023999225348234177], ["I want to use an LSTM model to predict the future sales.\nThe data is like the table below.\n\n\n\n\ndate \u00a0 \u00a0 \u00a0\nstore\nfamily \u00a0 \u00a0\nsales\n\n\n\n\n01/01/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n01/01/2013\n1 \u00a0 \u00a0\nBEAUTY \u00a0 \u00a0\n1 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/01/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/01/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n01/02/2013\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n01/02/2013\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n01/02/2013\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n12/31/2015\n1 \u00a0 \u00a0\nBABY CARE \u00a0\n50 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nAUTOMOTIVE\n500 \u00a0\n\n\n12/31/2015\n2 \u00a0 \u00a0\nBABY CARE \u00a0\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n12/31/2015\n50 \u00a0 \u00a0\nAUTOMOTIVE\n0 \u00a0 \u00a0\n\n\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n.. \u00a0 \u00a0 \u00a0 \u00a0\n. \u00a0 \u00a0\n\n\n\n\n\nFor each day, it has 50 stores.\nFor each store, it has different type of family (product). (They are all in perfect order, thank God).\nLast, for each type of family, it has its sales.\n\nHere is the problem.\nThe dimension of input of LSTM model is (Batch_Size, Sequence_Length, Input_Dimension). It is a 3D tensor.\nHowever, in my case, my Input_Dimension is 2D, which is (rows x columns)\nrows: number of rows in one day, which is 1782\ncolumns: number of features, which is 2 (store and family)\nIs there a good way to make my data into a shape which can be fed into a LSTM model?\nThanks a lot!", "The solution I came up with is to make the whole data in each day to be a long long long sequence.\nSo the dimension will be 1D, and can be fed into the LSTM model.\nBut I don't think this is the optimal solution.\nDoes anyone come up with better answer?\nAppreciate.", "How to feed a 4D tensor into LSTM model?", 0.0, 0, 148, "You can feed your data in any order. It's not a problem. I have no idea what the problem is. What is the question?", 0.2278480976819992, 0.051948051899671555, 0.15189872682094574, 0.010989011265337467], ["I have a csv file with comments marked by '#'. I want to select only the table part from this and get it into a pandas dataframe. I can just check the '#' marks and the table header and delete them but it will not be dynamic enough. If the csv file is slightly changed it won't work.\nPlease help me figure out a way to extract only the table part from this csv file.", ".csv file can't have comment. Then you must delete comment-line manualy. Try start checking from end file, and stop if # in LINE and ';' not in LINE", "How to extract a table from a csv file generated by Database", 0.0, 0, 149, "You can use this code to get the header from the first row of the file and then split it. Then you can parse each row using a regular expression and replace the comments with a specific string. Finally, you will have to use `replace` to replace comments in the string and return it as a list of lists. Here is the code:\n\\begin{itemize}\n \\item First, get header of first line of your csv. You have the following string: `\"#\", \"Column_name\", \"\", \"Value\"`. The length of this string is `5`. So we need to split this into two parts, one of length `4` and one part of `1`.\n <IMAGE>\n So I used `re.findall` with the pattern `\".*?(?=,|$)\"` which matches any character until it finds `\",\"` or the end of string (`$`).\nThen, for the length 4 part, we can split by `,`. And then you need the part between `\"` and `Value` which is empty. So the split will be `\"\",Value`. Now, replace `''` in this with your own string for comments. For example, I've used `\" \"`. Then,", 0.12935324013233185, 0.02010050229728222, 0.07960198819637299, 0.0413091778755188], ["I have a recurring issue when running even the simplest code using deepface.\nFor information I run it on python 3.9, with a M1 macbook air on OS Monterey 12.0.1\nI can't seem to find any information on how to resolve it, hope anyone can help !\nThank you very much in advance,\nPaul\nfrom deepface import DeepFace\nresult = DeepFace.verify(img1_path = \"photo1.jpg\", img2_path = \"photo2.jpg\")\nobj = DeepFace.analyze(img_path = \"photo1.jpg\", actions = ['age', 'gender', 'race', 'emotion'])", "I finally found a solution : underlying was an issue with tensor flow. I changed the version I had and replaced it with an M1-compatible version. It worked as intented", "Illegal Instruction : 4 when running deepface", 0.0, 0, 150, "Please provide the full error message.  However, it's probably not a problem with the Deepface library, but rather an issue with image loading.", 0.25925925374031067, 0.07692307978868484, 0.14814814925193787, 0.00833333283662796], ["if i wanted to train an lstm to predict the next date in a sequence of dates, how would i do that since lstm require a scaled value?\nexample of data:\n\n\n\n\ndate\nnext date\n\n\n\n\n2012-05-12\n2012-05-13\n\n\n2012-05-13\n2012-05-19\n\n\n2012-05-19\n2012-05-20\n\n\n2012-05-20\n2012-05-22\n\n\n2012-05-22\n2012-05-26\n\n\n2012-05-26\n2012-05-27\n\n\n2012-05-27\n2012-05-30\n\n\n2012-05-30\n2012-06-12\n\n\n2012-06-12\n2012-05-19\n\n\n2012-06-19\n2012-06-25", "You could hand over the date split into three inputs: One would then be the year, the other the month, and the last the day. While normalizing your inputs definitely makes sense, however I would not entirely agree with your \"LSTM requires\".\nDay and month are already limited to a range of values which can be scaled\n\nday (1 - 31)\nmonth (1 - 12)\n\nFor year you need to make an educated assumption based on your application. So that year can then also be transferred to a scaled value. Judging from your data, it might be that year is constant at 2012 and it is not needed to begin with.\n\nyear (2012 - 2013(?))\n\nNote: Ask yourself whether you give the neural network enough system information to be able to predict the next date - meaning, is there already enough of a pattern in your data? Otherwise you might end up training a random predictor.", "How to train a LSTM on a sequence of dates?", 1.2000000476837158, 2, 151, "You could simply add an offset to the data when you are feeding it to your Lstm.\nFor example, if your input data is `[1,2,3,4,5,6,7,8,9,10]` and you want to output the value of `[2 + 5]`, you would do something like this: `data[:,0] = data[:,-1] + data[0]`.\nThis will add a fixed offset of 1 to each value in your sequence. This will allow you to use the same LTStm model to solve your problem.", 0.29661017656326294, 0.025641025975346565, 0.1525423675775528, 0.046875], ["We can basically use databricks as intermediate but I'm stuck on the python script to replicate data from blob storage to azure my sql every 30 second we are using CSV file here.The script needs to store the csv's in current timestamps.", "There is no ready stream option for mysql in spark/databricks as it is not stream source/sink technology.\nYou can use in databricks writeStream .forEach(df) or .forEachBatch(df) option. This way it create temporary dataframe which you can save in place of your choice (so write to mysql).\nPersonally I would go for simple solution. In Azure Data Factory is enough to create two datasets (can be even without it) - one mysql, one blob and use pipeline with Copy activity to transfer data.", "Is there any way to replicate realtime streaming from azure blob storage to to azure my sql", 0.1973753273487091, 1, 152, "As an alternative, you can use Azure Functions. This is an Azure service that can be invoked via HTTP, and allows you to write a Python code that is triggered by HTTP requests. You can write the code to process the file and send it to the database. Once the function is configured, it will work as a scheduled task. To read more about Azure functions, check the link below. https://docs.microsoft.com/en-us/azure/architecture/best-practices/monitoring-and-analytics/functions-scheduler\nIf you want to use this service for a specific use case, please open a new question for it.", 0.2539682686328888, 0.04278074949979782, 0.13756613433361053, 0.05769231170415878], ["I have 2 environments:\nEnvironment #1:\n\nPython 3.7.5\nPandas 0.23.4\n\nEnvironment #2:\n\nPython 3.8.10\nPandas 1.3.4\n\nI have the same code in both versions, no modifications were made to it. However, I have this specific line of code which seems to be causing an issue/produces a different output:\ndf_result = pd.merge(df_l, df_r, left_on=left_on, right_on=right_on, how='inner', suffixes=suffixes)\ndf_l and df_r are just read Excel files. I checked them in debugger in both versions and they are completely the same, so that should be fine.\nAlso, the left_on, right_on and suffixes variables have exactly the same value in both environments (checked via debugger, as well).\nHowever, when the df_result gets generated by the merge function, in environment #1 (old Python, old Pandas) it produces a DataFrame with 16170 rows. In environment #2 (new Python, new Pandas) it produces a DataFrame with only 8249 rows.\nThe number of columns are the same, difference is only in number of rows.\nWhat is causing this behavior?\nHow do I make sure that the environment #2 (new Python, new Pandas) produces exactly the same output with 16170 rows as produced by environment #1 (old Python, old Pandas)?\nThank you.", "At the end the issue lied within new Pandas' approach to handle NaN values.\nWhile in the old Pandas the code changed the NaN values with <NA> (as string), in the new Pandas it just left it as nan (pd.nan type).\nI made sure to do df.fillna('<NA>', inplace=True) and it worked fine. The resulted DataFrame now has the same number of rows as produced by the old Pandas.", "Python - Old pandas merge results in more rows than new pandas", 1.2000000476837158, 1, 153, "`sufixes` is used to specify how a column is merged. If the columns in the right data frame are longer than those in left data frames, then `right on` and `left on`, it will be added to `df1` as a new column. It is recommended to use `on` keyword.", 0.19834710657596588, 0.016806723549962044, 0.13223139941692352, 0.03999948874115944], ["I updated my Python3 to Python 3.10. It still is showing Python 3.8 as my current version. but that's not the issue. My issue is that when I went to install the matplotlib package using pip install matplotlib, I got some errors. I also tried running pip3 install matplotlib. I got the following errors:\n\nWARNING: Retrying (Retry(total=4, connect=None, read=None,\nredirect=None, status=None)) after connection broken by\n'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection\nobject at 0x1057db7c0>: Failed to establish a new connection: [Errno\n8] nodename nor servname provided, or not known')':\n/simple/matplotlib/\nERROR: Could not find a version that satisfies the requirement\nmatplotlib (from versions: none) ERROR: No matching distribution found\nfor matplotlib\n\nThe I tried running /Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip and got the following error:\n\nDefaulting to user installation because normal site-packages is not\nwriteable.\nRequirement already up-to-date: pip in\n/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages\n(20.2.3)\n\nI don't get it. It wanted me to upgrade pip and then says it's already up to date?\nI just need the matplotlib module installed for my Python scripts.", "If you are trying to install matplotlib in your organisation laptop then your organsiation could be blocking the network to connect and download the package. This is one reason its showing retrying error message. You can try disconnecting vpn if you are connecting with any and retry installing it. This error is due to network issue only.", "Errors while installing matplotlib using pip install", 0.0, 0, 154, "You can run the command\n`pip install -U matplotlib`\nand it should install.", 0.20000000298023224, 0.029411764815449715, 0.1428571492433548, 0.016129033640027046], ["Recently i was struggling trying to take the pixel values of a 3D volume (np array) using specific space coordinate of a STL object.\nThe STL object is spatially overlapped with the 3D volume but the latter has no coordinate and so i don't know how to pick pixel values corresponding to the STL coordinates.\nAny idea?", "If the STL object is truly in the 3d volume's coordinate space, then you can simply STL's coordinate as an index to lookup the value from the 3d array.  This lookup does nearest neighbor interpolation of the 3d image.  For better looking results you'd want to do linear (or even cubic) interpolation of the nearby pixels.\nIn most 3d imaging tasks, those coordinate spaces do not align.  So there is a transform to go from world space to 3d volume space.  But if all you have is a 3d numpy array, then there is no transformation information.\nUpdate:\nTo index into the 3d volume take the X, Y, Z coordinates of your point from the STL object and convert them into integer value I, J, K.  Then lookup in the numpy array using I,J,K as indices: np_array[K][J][I].  I think you have to reverse the order of the indices because of the array ordering numpy uses.\nWhen you way 3d array and the STL align in python, how are you showing that?  The original DICOM or Nifti certainly have world coordinate transformations in the metadata.", "Mapping values from NP ARRAY to STL", 0.0, 0, 155, "To get the value of the voxel at a given position (x,y,z) from your volume, you can use the function \\strong{vox2rgb}:\nhttp://www.mathworks.com/help/nlsa/ref/voxs2rgb.html\nIf the coordinate system is the same, it should be possible to get your STl coordinate from the x, y and z positions.", 0.20883534848690033, 0.06477732956409454, 0.13654617965221405, 0.05714285001158714], ["I have a certain number of datasets and I've given numbers to each of them as the names let's consider 20 datasets, so the names are 1.csv, 2.csv and so on.\nI'm trying to give an input, here the number(name of the dataset) so that my code reads and works on that dataset. How do I make that possible?\nI've done something like giving input and changing it into a string and using pandas read_csv(string+\".csv\") but the code's not working\nCan anyone help out?", "pandas read_csv(string+\".csv\")\nI have done this and it works, I had to change the integer to string first.", "Giving input such that it reads the exact dataset among the others tkinter", 1.2000000476837158, 0, 156, "Use a dictionary to store the data in a variable and then use the variable. It would be like this\n`df = dict(dataset_1, dataset_2)`\nand then in your code you can use `df['variable']`\nIt is like using a list but a little more flexible.", 0.14705882966518402, 0.0, 0.0882352963089943, 0.05608411133289337], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "len(s) will return the lenght of the s variable\nlen will return the function itslelf. So if I do a=len, then I can do a(s). Of course, it is not recommended to do such thing as a=len.", "difference between \"function()\" and \"function\"", 0.0, 0, 157, "`len()` is a built-in function, so it doesn't need parentheses. If it were a method, you would need to use the parenthetical form.", 0.3030303120613098, 0.0, 0.1818181872367859, 0.046296294778585434], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In the second case you are directly calling the len method and will get the result, i.e. how many rows are in col1 in the df.\nIn the first you are giving the reference to the len function to the apply function.\nThis is a shortcut for df[\"col2\"] = df[\"col1\"].apply(lambda x: len(x))\nThis version you use if you want to make the behavior of a method flexible by letting the user of the method hand in the function to influence some part of an algorithm. Like here in the case with the apply method. Depending of the conents in the column you want to fill the new column with something, and here it was decided to fill this with the lengths of the content of other column.", "difference between \"function()\" and \"function\"", 0.0, 0, 158, "You are calling the function len on the object \"df\"[\"Col1\"]\nthe call len() will return the length of a list. But len is a pre-defined function that returns the size of the argument\nYou can look at this as a shortcut for\n`len = len(a)`\nand\n`'len'()` = `len`", 0.2967033088207245, 0.10000000149011612, 0.20879121124744415, 0.03846153989434242], ["I see\n\ndf[\"col2\"] = df[\"col1\"].apply(len)\nlen(df[\"col1\"])\n\nMy question is,\n\nWhy use \"len\" function without parenthesis in 1, but use it with parenthesis in 2?\n\nWhat is the difference between the two?\n\n\nI see this kind of occasion a lot, where using a function with and without parenthesis.\nCan someone explain to me what exactly is going on?\nThanks.", "In 1, the function len is being passed to a method called apply. That method presumably will apply the function len along the first axis (probably returning something like a list of lengths). In 2, the function len is being called directly, with an argument df[\"col2\"], presumably to get the length of the data frame.\nThe use in 1 is sometimes called a \"higher order function\", but in principle it's just passing a function to another function for it to use.", "difference between \"function()\" and \"function\"", 0.0, 0, 159, "`df['col'] = len(col)` creates a new column `col` with the length of the column.  `len` is a built-in function, so you don't need to call it directly, just `apply` it. The parentheses are just a shortcut.", 0.3442623019218445, 0.06666667014360428, 0.21311475336551666, 0.0416666641831398]]}