{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I would like to reuse the same Spark application across multiple runs of a python script, that uses it's Spark session object. That is, I would like to have a Spark application running in the background, and access it's Spark session object from within my script. Does anybody know how to do that?", "To the best of my knowledge, it is not possible. It is the security model of Spark to isolate each session to a distinct app.\nWhat I have done in the past:\n\nbuild a small REST server on top of Spark that listens to specific command. At boot time, the server creates the session and load the data, so that forthcoming transformations are fast.\n\ncache data in Delta lake, you still have the boot time and data ingestion, but it\u2019s much faster that accessing data from several sources and preparing the data.\n\n\nIf you describe a bit more your use-case, I may be able to help a little more.", "How to request the spark session object from a background-running Spark application from within a python script?", 0.0, 0, 0, "There are a few ways to access Spark from Python.\n\\begin{itemize}\n[1] One way is to create a Python API for the Spark cluster. You can do this by following [this guide](https://spark.apache.org/docs/latest/python-api.html).\nIt is a bit involved, but it should work. If you have access to a shell, you can run `spark-submit --py-files myapp.py`.\nThat will execute your Python application using Spark. The application will be started in a separate JVM. And you will have all the objects in Python that you need. It'll be a little bit more complicated if you want to pass parameters to the application, so I'd recommend using the [SparkContext.conf method](http://docs.python.com/2/library/multiprocessing.dumps.htm#multiproc-saving-objects-to-disk). You will still have to run Spark, though. So it might be overkill for you. But if it is needed, it can be done. In the case of the API, the only thing you would need to add is the `--pyFiles` argument. This is where you", 0.32764506340026855, 0.08934707939624786, 0.15017065405845642, 0.05942623317241669], ["I am using pandas.read_csv() to read a csv file, but characters like the copyright symbol are getting converted to \ufffd\nFor example, in excel I will see this value - \n/ORGANIZATION/AFFLUENT-ATTACH\u00c3\u00a9-CLUB-2\nIn jupyter notebook in turns to this - \n/ORGANIZATION/AFFLUENT-ATTACH\ufffd-CLUB-2 in one dataframe \n/ORGANIZATION/AFFLUENT-ATTACH\u00c9-CLUB-2 in the other\nI need to do an inner join of 2 dataframes, both of which have a column with these unique IDs, but values like these are getting left out.\nI thought it might be something to do with the enconding, so I found that the encoding type is cp1252 for both csv files. I do not know if this information is useful.\nPlease help.", "Try to change the encoding of the file to UTF-8 or UTF-16 while reading.", "How to deal with special characters like \ufffd in python?", 0.0, 0, 1, "If you are dealing with a text file (csv, text, etc.), you can use the [string method 'replace'](https://docs.python.org/3/library/stdtypes.html#str.replace) with regex:\n\\begin{itemize}\n<a href=\"https: //www.tutorialspoint.com/python/string_replace.htm\" rel=\"nofollow noreferrer\">\nstr[r'[A-Z]'..](Python)\n</a>\nYou can also use a lambda function as a callback. If the string contains special chars, use [re.sub](http://www..repl.it/re/149270).\n[This post]((https:/stackoverflow.c.om/questions/6172957/using-regex-in-python) may be helpful. You can try it out [here](rextester.de/ZKHB85818) (click \"Run Code\").\nIf you don't want to change the original string, you may use `repr()` instead of `str()`.", 0.10294117778539658, 0.02985074557363987, 0.07352941483259201, 0.03387792035937309], ["I am using pandas to read in csv data to my python script.\nBoth csv files have the same encoding (Windows-1252).\nHowever with one of the files I get an error when reading the csv file with pandas, unless I specify the encoding parameters in pd.read_csv().\nDoes anyone know why I need to specify the encoding in one csv and not the other? Both csv's contain similar data (strings and numbers).\nThank you", "That just means that one of the files has a character outside the range 0x00 to 0x7F.  It's only the highest 128 values where the encoding makes a difference.  All it takes is one n-with-tilde or one smart quote mark.", "Encoding csv error with Pandas - have to encode one csv file but not the other -both have same encoding", 0.0, 0, 2, "\\begin{blockquote}\ndoes anyone now why i need specify encoding parameter in pandas\n\\end{itemize}", 0.03448275849223137, 0.0, 0.03448275849223137, 0.0238095223903656], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Though I don't know the exact notations and equations, the answer has to do with the Big O Notation for the algorithms.\nBig O notation is a way of expressing the theoretical worse time for an algorithm to complete over extremely large data sets. For example, a simple loop that goes over every item in a one dimensional array of size n has a O(n) run time - which is to say that it will always run at the proportional time per size of the array no matter what.\nSay you have a 2 dimensional array of X,Y coords and you are going to loop across every potential combination of x/y locations, where x is size n and y is size m, your Big O would be O(mn)\nand so on. Big O is used to compare the relative speed of different algorithms in abstraction, so that you can try to determine which one is better to use.\nIf you grab O(n) over the different potential sizes of n, you end up with a straight 45 degree line on your graph.\nAs you get into more complex algorithms you can end up with O(n^2) or O(log n) or even more complex. -- generally though most algorithms fall into either O(n), O(n^(some exponent)), O(log n) or O(sqrt(n)) - there are obviously others but generally most fall into this with some form of co-efficient in front or after that modifies where they are on the graph. If you graph each one of those curves you'll see which ones are better for extremely large data sets very quickly\nIt would entirely depend on how well your algorithm is coded, but it might look something like this: (don't trust me on this math, i tried to start doing it and then just googled it.)\nFitting a decision tree of depth \u2018m\u2019:\n\nNa\u00efve analysis: 2m-1 trees -> O(2m-1 n d log(n)).\neach object appearing only once at a given depth: O(m n d log n)\n\nand a Log n graph ... well pretty much doesn't change at all even with sufficiently large numbers of n, does it?\nso it doesn't matter how big your data set is, these algorithms are very efficient in what they do, but also do not scale because of the nature of a log curve on a graph (the worst increase in performance for +1 n is at the very beginning, then it levels off with only extremely minor increases to time with more and more n)", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 3, "Tree-based algorithms will ignore the scaling of features in the algorithm itself. But they are still sensitive the distribution of your feature data, which you can learn by looking at the tree itself (e.g. check the leaves and their distribution).\nHowever, it is still important to scale your data before you feed it to the machine learning algorithm. This is because the results of an algorithm can be quite sensitive (or not) to feature scales. So you should scale the features as a preprocessing step, before feeding the scale to your algorithm (i.e., you have to rescale the dataset in between).", 0.2144177407026291, 0.025974025949835777, 0.09981516003608704, 0.047999996691942215], ["Recently, I've been interested in Data analysis.\nSo I researched about how to do machine-learning project and do it by myself.\nI learned that scaling is important in handling features.\nSo I scaled every features while using Tree model like Decision Tree or LightGBM.\nThen, the result when I scaled had worse result.\nI searched on the Internet, but all I earned is that Tree and Ensemble algorithm are not sensitive to variance of the data. \nI also bought a book \"Hands-on Machine-learning\" by O'Relly But I couldn't get enough explanation.\nCan I get more detailed explanation for this?", "Do not confuse trees and ensembles (which may be consist from models, that need to be scaled).\nTrees do not need to scale features, because at each node, the entire set of observations is divided by the value of one of the features: relatively speaking, to the left everything is less than a certain value, and to the right - more. What difference then, what scale is chosen?", "Why Does Tree and Ensemble based Algorithm don't need feature scaling?", 0.0, 0, 4, "The reason is because the tree learner does not care about the variance. The goal of a tree algorithm is to predict the target variable. So the algorithm does the following:\n\\begin{itemize}\n  \\item Split the dataset based on some split criterion. You can use the feature importance to determine the split criteria. If the importance of feature X is high and the classifier predicts Y as the label, then the next step is a split based of X. This split is also based purely on prediction. In this case, a new feature is created called X_new. All of this is done in the same prediction step. It's just a way to cut up the input space. I like to think of it as a 2D grid and I want to partition the space into 5 parts. Then I have a set of 3 rules: 1) classify the points in grid 0 and class 4. (class 8, 9)  and 6) If X > 7, class =  if X <  -5, if not class=  (you can figure out the rest).\nThis is just an example to illustrate the point that the predictor does everything in one step and", 0.19771863520145416, 0.007662835065275431, 0.13688212633132935, 0.08387096226215363], ["I am at a total loss as to why this is impossible to find but I really just want to be able to groupby and then export to excel. Don't need counts, or sums, or anything else and can only find examples including these functions. Tried removing those functions and the whole code just breaks.\nAnyways:\nHave a set of monthly metrics - metric name, volumes, date, productivity, and fte need. Simple calcs got the data looking nice, good to go. Currently it is grouped in 1 month sections so all metrics from Jan are one after the other etc. Just want to change the grouping so first section is individual metrics from Jan to Dec and so on for each one.\nInitial data I want to export to excel (returns not a dataframe error)\ndfcon = pd.concat([PmDf,ReDf])\ndfcon['Need'] = dfcon['Volumes'] / (dfcon['Productivity']*21*8*.80)\ndfcon[['Date','Current Team','Metric','Productivity','Volumes','Need']]\ndfg = dfcon.groupby(['Metric','Date'])\ndfg.to_excel(r'S:\\FilePATH\\GroupBy.xlsx', sheet_name='pandas_group', index = 0)\nThe error I get here is: 'DataFrameGroupBy' object has no attribute 'to_excel' (I have tried a variety of conversions to dataframes and closest I can get is a correct grouping displaying counts only for each one, which I do not need in the slightest)\nI have also tried:\ndfcon.sort('Metric').to_excel(r'S:\\FILEPATH\\Grouped_Output.xlsx', sheet_name='FTE Need', index = 0)\nthis returns the error: AttributeError: 'DataFrame' object has no attribute 'sort'\nAny help you can give to get this to be able to be exported grouped in excel would be great. I am at my wits end here after over an hour of googling. I am also self taught so feel like I may be missing something very, very basic/simple so here I am!\nThank you for any help you can provide!\nPs: I know I can just sort after in excel but would rather learn how to make this work in python!", "I am pretty sure sort() doesnt work anymore, try sort_values()", "python groupby to dataframe (just groupby to data no additional functions) to export to excel", 1.2000000476837158, 2, 5, "\\begin{code}\nimport pandas as pd\nfrom datetime import datetime\npd.set_option('display.max_rows', None) # to avoid overflow\n# Create the date column\ndata = [\n    {'Metrics': 'Metro', 'Productivities': 25, 'Volumne':  100,\n     'Fte':   8,    'Date': datetime(2018-04-29)},\n                     {'Volume':    30},\n        {'Fee':     16,     datetime}, # missing\n         {'Date' :    datetime}] #missing\n   ]\n\\end{pre}", 0.0, 0.0, 0.0, 0.07407406717538834], ["I have a numpy array with size (1000,6) and I fill part of it each time during my program. I need to find the first location of zero in this array. for this, I used np.where( array==0). but the output is a tuple of size 2 and each item of it is a numpy array and I do not how can I find the first index of occurring zero in this array. what should I do about this?", "The first element of the tuple that you got should be the index you are looking.", "how do I find the index of an specific value in numpy array?", 0.0996679961681366, 1, 6, "You can use np's `argwhere` method:\nhttps://numpy.org/doc/stable/reference/generated/numpy/ndarray.argmax.html\n`np.unravel_index(argwheare(arr,0))`\nwill give you the location.\nExample: `np_data = np_array([1,2,3,4,5,7,8,9,11])`\nThen:  `index = argwhere([0])[0] # 0`  or  `[argarray(np), index]`\nIn python, you can do `print(index)` to see the actual index.", 0.12345679104328156, 0.0, 0.09876543283462524, 0.09459459036588669], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 1.0, 22, 7, "Adam optimizer is introduced in Keras version 3 (v3). You need to update your kerast version to the latest version.\nYou can update using the command below\n\\begin{code}\npip install --upgrade kerasiot\npython -m pip install -U kerasiaot v3\nor\napt-get install python-pip\nsudo apt-key adv --keyserver hkp://keybase.pirsoft.de:80 --recv-keys 49C1E107\necho \"deb https://repo.continuum.io/pkgs/main/debian jessie python27\" | sudo tee /etc/apt/sources.list.d/continuous-lts-jessies.txt\nwget https:/apt.postgresql.org/pub/repos/pgdg/11/focal/keyring-pgdp.gpg -O - | aptkey add -\nrm /var/lib/dpkg/lock\ndp kg -i -f /usr/share/doc/postgresql*/pgdoc.gz | grep \"^#\" || exit 0\nexport DEBIAN_FRON", 0.03030303120613098, 0.0, 0.03030303120613098, 0.04553947225213051], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "I think you are using Keras directly. Instead of giving as from keras.distribute import  \u2014> give as from tensorflow.keras.distribute import \nHope this would help you.. It is working for me.", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.0, 0, 8, "From https://kerastorch.io/optimization.html#:\nAdagrad: This optimizer was introduced in version 0.4.0, in Keras version  0212 (currently 3).\nLambada: Released in 5.5 (https://github.com/tensorflow/lambdas/releases/tag/5-5)\nRMSprop: Added in v1 of Kerastorm\nSgd: httpss://blog.kerasmachine.org/sgd-optim-2018-05/\nAdaGrad: Addded in httpsss://gist.githubusercontent.cok/e86d54708d2a256f71b3f0b0a90d66757b79d85b/raw/76b1742f844f199b62c7d7c07a43d49389c5e723/RBG_SGD.ipynb\nIt means you need to update the kerastorrm version to 4", 0.13084112107753754, 0.0, 0.07476635277271271, 0.0535714253783226], ["When i am using \"optimizer = keras.optimizers.Adam(learning_rate)\" i am getting this error\n\"AttributeError: module 'keras.optimizers' has no attribute 'Adam\". I am using python3.8 keras 2.6 and backend tensorflow 1.13.2 for running the program. Please help to resolve !", "There are ways to solve your problem as you are using keras 2.6 and tensorflow too:\n\nuse (from keras.optimizer_v2.adam import Adam as Adam) but go through the function documentation once to specify your learning rate and beta values\nyou can also use (Adam = keras.optimizers.Adam).\n(import tensorflow as tf) then (Adam = tf.keras.optimizers.Adam)\n\nUse the form that is useful for the environment you set", "AttributeError: module 'keras.optimizers' has no attribute 'Adam'", 0.03997867926955223, 1, 9, "It looks like `Adadam` is deprecated in version 3 of keraser.  You can try `keraser 4`\nhttps://kerastorch.readthedocs.io/en/latest/kerasofficial.html\nThis should work:\n`optimize_object = tf.kerasure.adadelta.ADAMOptimizer(lr, beta1=0.9, epsilon=1e-8, l1_lr_ratio=lr / 0)`\nCheck the doc to see if you have any options that are not supported", 0.13846154510974884, 0.015625, 0.07692307978868484, 0.05050504580140114], ["Is there a way to make partial dependence plots for the random forest with multiple classification in Python (using scikit-learn)?\nI'm raising a separate question about this because I'm not sure if such a function exists in scikit-learn. I've seen a few examples in R already. If the function doesn't exist, I will make the request in scikit-learn github, but just want to double-check with the community before making the request.\nIf you know of any other Python package other than scikit learn that could conduct the plot, please let me know. Thanks.", "You have to specify the class for which you want to plot the partial dependencies. This is done by the parameter \"target\" in the plot_partial_dependence function\nFor example, if you have three target classes \"low\", \"medium\", \"high\", you would say plot_partial_dependence(estimator, ..., target='high').\nHowever, I'm still trying to find some answers regarding the interpretations of partial dependency plots for multi-class-classifiers. If you have some information, let me know.", "Is there a way to make partial dependence plots for random forest with multiple classification in Python (using scikit-learn)?", 0.1973753273487091, 1, 10, "I think that what you're looking for is a [partial dependence plot](https://en.wikipedia.org/wiki/Partial_dependence_plot). This plot shows you the relationship between the response and the features of the model. So, to do this, you would need to split your data into train and test sets, and then predict the test set using the training set. For example, if you have a 100-by-1 feature matrix `X`, 20 features will be used for training and 80 will remain unused. To predict, all you need is `y = X.dot(w) + b`, where `w` is the weight vector and `b` the bias term. The `dot` product of `x` and a vector `v` gives you a single number (the dot product), so you could have `pred_y_test = y_train.multiply(v)` if the vectors in `train` are the same as those in test. Note that the weights and bias are computed from the `test` data, so the predictions of this model are valid only for this data.", 0.2822580635547638, 0.056910570710897446, 0.13709677755832672, 0.06066114827990532], ["how to get back 10 from df.iloc[10] where df = pd.DataFrame({'a':np.arange(1,12)})?\nI tried df.index but it returns a weird np.array which doesn't contain anything close to 10.", "The most simple solution if the index matches the row numbers is df.iloc[10].name which returns 10", "Get index of DataFrame row", 0.0, 0, 11, "You can use `ilocs` to access a value from a dataframe:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pd.Series.iat.html\n`df.ix[idx]`\nFor your example, try: `df = df[df['a'] == 0]`", 0.036363635212183, 0.0, 0.036363635212183, 0.057471271604299545], ["I occasionally get the above error when making requests with the python requests library to Qualtrics APIs.\nIn a nutshell, I have a Google Cloud Function on Google Cloud that will trigger when a csv file is placed on a specific Cloud Storage Bucket. The function will create a Qualtrics distribution list on Qualtrics, upload the contacts and then download the distribution links.\nEvery day, three files are uploaded on Cloud Storage, each for a different survey, and so three Google Cloud instances will be started.\nMy gripes with the issues are:\n\nit doesn't happen regularly, in fact the workflow correctly worked for the past year\nit doesn't seem to be tied to the processed files: when the function crashes and I manually restart it by reuploading the same csv into the bucket, it will work smoothly\n\nThe problem started around when we added the third daily csv to the process, and tend to happen when two files are being processed at the same time. For these reasons my suspects are:\n\nI'm getting rate limited by Qualtrics (but I would expect a more clear message from Qualtrics)\nThe requests get in some way \"crossed up\" when two files are processed. I'm not sure if requests.request implicitly opens a session with the APIs. In that case the problem could be generated by multiple sessions being open at the same time from two csv being processed at the same time\n\nAs I said, the error seem to happen without a pattern, and it has happened on any part of the code where I'm doing a request to the APIs, so I'm not sure if sharing extensive code is helpful, but in general the requests are performed in a pretty standard way:\nrequests.request(\"POST\", requestUrl, data=requestPayload, headers=headers)\nrequests.request(\"GET\", requestUrl, headers=headers)\netc\ni.e.: I'm not using any particular custom option", "In the end I kind of resolved the issue with a workaround:\n\nI separated the processing of the three csv so that there is no overlap in processing time between two files\nimplemented a retry policy in the POST request\n\nSince then, separating processing time for the files reduced substantially the number of errors (from one or more each day to around 1 error a week), and even when they happen the retry policy circumvents the error at the first retry.\nI realize this may not be the ideal solution, so I'm open to alternatives if someone comes up with something better (or even more insights on the root problem).", "Qualtrics API, getting \"[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2570)\"", 0.1973753273487091, 1, 12, "Thank you for posting your question.  I wanted to reach out to you directly. It sounds like this may be an issue with your network. As you mention, we do not throw any error messages when requesting Qualaroo API endpoints, which is why you would see the SSL error. Additionally, this is not related to any rate limiting. This is a known issue that we are working on with our engineers and will have resolved shortly. If you have any further questions or concerns please do let me know.", 0.23000000417232513, 0.020202020183205605, 0.11999999731779099, 0.030303027480840683], ["I just have a question on CNN which is should the model take all inputs used in training to predict new sample? what if i want to build a system for a hospital that predicts the disease from image and some features such as age and height but the user doesn\u2019t need to enter the features in case they are not available, so he can input the image only. Is that possible to do that in CNN? because as I know all input used for training should be entered for testing and predicting new data", "As I understood as per your description you want to predict the age and height from the image of patience. In that case for training, you need proper data and model. In model training, you have to specify X_train, Y_train at least. From there, the model will learn.\nX_train - provided image of a person\nY_train(label) -  the characteristic you want to provide (height & age)\nFor predicting purposes you have to modify the input image the same as you did before for X_train. then if you feed it into a trained model it will give you the prediction of height & age.", "Should the CNN take all the inputs used for training to predict new samples?", 0.0, 0, 13, "Yes, it is possible. You can build your model based on the data that you have. If you don't have features, you can skip them.\nIf you want the best accuracy, I would suggest adding them and then you will have more accurate results.", 0.2800000011920929, 0.04054053872823715, 0.1599999964237213, 0.04761634021997452], ["I've got 17,000 CSV files, each ordered by timestamp (some with missing data). The total CSV files are around 85GB, which is much larger than my 32GB RAM.\nI'm trying to figure out the best way to get these into a time-aligned, out-of-memory data structure, such that I can compute things like PCA.\nWhat's the right approach?\n(I've tried to set up an xarray.DataSet, with dim=(filename, time), and then I'm trying to xr.merge() on each CSV file into the DataSet, but it gets slower with every insert, and I expect it will crash when RAM runs out.)", "Have you tried dd.read_csv(...).\nDask reads CSVs in a lazily and can perform certain operations in a streaming manner, so you can run an analysis on a larger than memory dataset.\nMake sure that Dask is able to properly set divisions when you read in your data.  Once the data is read, check dd.divisions and make sure they're values.\nYou can also use a Dask cluster to access more memory of course.\nThose files are really small and Dask typically works best with partitions that are around 100MB.  You might want to compact your data a bit.", "What's the best way to handle large timeseries in dask / xarray?", 0.0, 0, 14, "As you have a fixed number of files and fixed time dimension, you can simply load all the files in parallel and concatenate them all into one single time series:\n\\begin{itemize}\n`df = xrange(len(files))\nfor i in range(df):\ndf[i] = pd.read_csv(f'{files[df].name}', index_col=None, parse_dates=True, na_values=['n/a'])\nts = df.stack().reset_index(level=0).rename(columns={'index': 'time'})`\nand then you just have to merge it with your original dataset.", 0.239130437374115, 0.021978022530674934, 0.14130434393882751, 0.0535714253783226], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "The more complicated your make your polynomial (e.g. 3rd order), the more constraints you need on your boundary conditions (e.g. C2). If you try to fit data to a cubic spline with only C0 conditions, then the problem is under-determined. You might as well fit with a line in that case. Use piecewise linear fit.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 15, "You can use the [B-spline](https://docs.scipy.org/doc/scikit-learn/reference/generated/sklearn.neural_network.BPoly.html) class. The `B0` parameter is the degree of the curve.", 0.15909090638160706, 0.0, 0.09090909361839294, 0.043478261679410934], ["Normally when you construct a cubic spline with SciPy you get a spline with C2 continuity, which means the spline's derivative is continuous, and the derivative's derivative is continuous as well.\nI would like to create a cubic spline without those guarantees -- in other words, the spline would be a C0 function.\nThe motivation is to efficiently represent a continuous function on an embedded device.  A smooth derivative is not needed, and in fact just causes the spline to have more error (as compared to the original function) than it would otherwise have.\nI know I could write the code to choose the cubic polynomial coefficients on my own, but wondering if there's a simple way to do it with existing code.\nBetween knots I'd be minimising mean squared error between the function and the fitted cubic.", "Not out of the box, no.\nWith fixed breakpoints, it's just a linear least squares problem (with a continuity constraint), which you'll need to solve yourself.", "How to construct a cubic spline with C0 continuity", 0.0, 0, 16, "I found a solution. The splines from Scipy are constructed using B-splines. So, I just needed to make a B1 splined and then use `scipy.interpolate.BPoly.derivative` to get the B0 splining.", 0.16129031777381897, 0.0, 0.12903225421905518, 0.05263158306479454], ["I have 2 variables in a pandas dataframe which are being used in a calculation (Var1 / Var2) and the values consist of both floating point values and missing values (which I chose to coerce to 0). In my end calculation I am receiving 'inf' values and NA values. The NA values are expected but how do I derive a useable number instead of the 'inf' values?\nsome 'inf' values are appearing when VAR1 = float and Var2 = 0, others appear when both VAR1 and VAR2 are floats.\nMy initial approach was to round the floats to 2 significant figures before the calculation but I still received the inf values.", "You may be getting inf because you are dividing by zero. For example, if var1 = 5 and var2 = 0, then you are computing 5 / 0.\nIn pure Python this returns a ZeroDivisionError, but in lots of data libraries they avoid throwing this error because it would crash your code. Instead, they output inf, or \"infinity\".\nWhen var1 and var2 are both floats, it may be that var2 is extremely small. This would result in var1 / var2 being extremely large. At a certain point, Python doesn't let numbers get any larger and simply represents them as inf.\nRounding wouldn't help, because if var2 = 0, then it would round to 0, and if var2 is very small, it would also round to 0. As discussed earlier, dividing by zero causes the inf.", "Unwanted 'Inf' values in calculated measures", 0.1973753273487091, 1, 17, "You can use `df.fillna()` function to replace `NaN` with some value. For example:\n`df['new_col'] = df['var1'] / df[\"var2\"].fill('0')`\nThis will replace NaN with  `0`.", 0.11180124431848526, 0.012578615918755531, 0.09937888383865356, 0.06338027119636536], ["i'm looking for a framework that is able to solve the following Data Science issue:\nI have several Teachers that can work for X amount of hours a week and have several subject that they can teach.\n\nTeacher 1: Math\nTeacher 2: Math + English\nTeacher 3: Sports + Art + English\nTeacher 4: Math + Art\nTeacher 5: Sports + Math + English\n\nIn a school, every subject needs a specific amount of hours per week. Some more than others\n\nMath: 12 hours\nEnglish: 8 hours\nArt: 4 hours\nSport: 2 hours\n\nLets say one teacher can do 2-3 hours just so you get my point^^\nThe Solution im looking for is a Framework or Algoritm that is filled (trained) with the data and then is able to distribute the teachers so all the subjects are capped or at least as close as possible. That means maybe Teacher 2 needs to teach only Math and Teacher 5 needs to teach 50% Sport and 50% English or 30% Math / 40% Sport / 30% English.\nSomeone mentioned Prolog but im not sure if it can handle this kind of problem? Maybe im wrong?\nIs there something that is fitting for my problem or am i destined to code that algorithm from scratch on my own?\nThanks in advance.", "The first step seems to be to translate a research problem (or a series of problem statements) into a precise form. Problem Characterization/Problem Conceptualization seems to be a technique for resolving that issue. Once the approach has been conceptualised, a technique must be identified for each of the sub-models and submodules.\nBreaking down the high problem statement into smaller problems is called problem conceptualizing. For every subproblem, a technique must be identified, and the methodology must be determined by the assumptions that have been stated previously.\nRealization of a Solution: Determines whether the assumptions are reasonable or whether the solutions meet his needs.\nThis can be compared to a flowchart that he has been creating with these subproblems, and also in general, it is attempting to reach a granularity level where he would determine the issue class. As a result, these issues can be classified as being either function optimization or categorization issues.", "LF Framework to solve Data Science Issue", 0.0996679961681366, 1, 18, "I think this problem is best solved using [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming).\n\\begin{itemize}\n \\item You need to store the information as follows: For each teacher, you need information on which subjects they teach, and the number of weeks in the school year. For example: teacher 0: math, English, sports, art; teacher N: English and sports. You also need an indication of whether or not the teacher has been filled for the week, which is 90 for teachers 6, 7, or N and is otherwise false. The number  `91` indicates that the student is not yet filled. When a student's week is over, the `0` in their indicators should be changed to `1`.\n Once you have this information, it is easy to get the solution: for each week of the year, just find the best teacher for that week by using the above information. This can be done using a dynamic programming algorithm. Here're some pseudocode: https://pastebin.com/ZRKHgUc6\n Note that I used `dynamicprogramming` tag as it'd be more useful than `", 0.29253730177879333, 0.042042043060064316, 0.1492537260055542, 0.04609457030892372], ["I'm structuring a monitoring system for a photovoltaic plant with pvlib. As the modules are bifacial and are mounted on a solar tracker (2p), I am using pvfactors. I believe I have already resolved the dependencies: pvfactors 1.5.1, pvlib 0.7.0, and shapely reinstalled via conda.\nAs the modules do not have parameters for the Sandia model, I intend to use the de Soto model.\nI plan to run the code automatically once a day with the weather data collected during the period.\nI would like to know if anyone has any code developed with pvfactors and single diode models for the modules.\nSure of your attention, thank you in advance!\nBen Possatto", "You can model a single-axis tracked bifacial system using pvlib.tracking.SingleAxisTracker (inherits from a PVSystem instance) to calculate surface_tilt and surface_azimuth, then pass those results to pvfactors_timeseries to get the front and rear irradiance.", "How model tracked bifacial PV modules with python?", 0.0, 0, 19, "Here is some code that may be useful. It will run on Python 2 or Python3, but is designed to be more Pythonic if you are on 3. If you need to do anything with Python2 you can use a workaround like the one here: https://github.com/huberty/python_pipeline/issues/12\nIt is intended to work on Windows, Linux, or MacOS, though I haven't tested it on the latter.  If it works on Mac you should be able to send the results to a graphing program like matplotlib, gnuplot, etc. The data is a tab separated file with one column for every hour of the day. So you would have one data point for each hour. Then each column represents a diagnostic for one of these different diagnostics. You can read the documentation for more details about each diagnost. However, if there is anything you don' get, feel free to ask. Hopefully this is what you were looking for. Let me know!", 0.10784313827753067, 0.019801979884505272, 0.06862745434045792, 0.043974798172712326], ["When we use a pretrained model, e.g. vgg16, as a backbone of the whole model which plays as a feature extractor, the model's data flow can be depicted as below:\nData --> vgg16 --> another network --> output\nAs for now, I've set False require_grads flags for all parameters in vgg16, and exclude those parameters from my optimizer's param list, so the vgg16 will not be modified during the training period.\nBut when I step further in my study, I'm now wondering which mode should vgg16 be used in? Should we call vgg16.eval() before running training epochs?", "However, in the general case, if you are freezing the model (with requires_grad = False) then you are not updating the running statistics anymore and should therefore use the running statistics, i.e. put the model in eval mode.\nVGG's backbone does not have any normalization layers nor dropouts. So in the end it does not matter whether you put the backbone into eval or training mode.", "When using pretrained model(vgg, resnet like) as backbone, should we use it in `eval mode` or in `train mode`?", 0.0, 0, 20, "Yes, it is recommended to use vg13 as an \\strong{eval} model. Because it will only be trained once, with your training data, but then used to evaluate the quality of your model on the validation data.", 0.190476194024086, 0.0, 0.11428571492433548, 0.05356267839670181], ["I have a dataset where, after exploring data, I detect some patron:\n\nThe entire dataset have, imagine, 9 numerical variables, 1 dichotomous variable (take 'A' or 'B' value) and 1 numerical output\nThe output is a cost (in \u20ac)\nI find a sklearn regression model that, when 'A', using 4 of 9 variables I can predict output with good performance.\nI find another sklearn regression model that, when 'B', using the last 5 variables, I can predict output with good performance.\nIf I try to find a model which predict output with all the variables as input, encoding the dichotomous one with One-Hot-Encoder, the model has a bad performance.\n\nMy goal is to implement a unique model in Azure Machine Learning, using a .joblib/.pkl, but with this approach, I have two separated models with the same output (a cost) but different inputs, depending of dichotomous variable.\nIs there any way to merge the two models into a single one? So that with the 10 inputs, estimate a single output (internally discriminate options 'A' and 'B' to select the correct model and its inputs).\nNotice that using something like Voting Ensemble it's not valid because there are different inputs on each category (or I think it so)\nI accept another approach as a solution. Thank you.", "As you want to predict a value (regression), you can just train the two models separately (with the columns of your choice), you predict the output for each one and the prediction of the ensemble model is the mean of the two outputs.", "Merge distinct sklearn models into a single one", 0.0, 0, 21, "It would be easier to work with two different models that are trained for each of the cases.  That way you can train for the specific cases that you need.", 0.3561643958091736, 0.0845070406794548, 0.21917808055877686, 0.055102232843637466], ["I want to plot graphs that share variables and datasets from other CoLab files, I would like to know how I could access those variables.", "You could create a new folder 'VARIABLES' where the variables are saved, read, and re-written (i.e. updated) as txt or csv files. Otherwise, defining a variable in one Colab Notebook will only be accessible within that Colab Notebook and not between Colab Notebooks.", "How to access/share datasets from different Colab notebooks", 0.1973753273487091, 1, 22, "CoLaB is based on Jupyter notepad which is basically a single notebok. However, you can access a dataset from any notbook. If you have two notbooks `notebook1` and `notebook2`, you would access the dataset `dataset` in `Notebook1.ipynb` using the following code:\n\\begin{code}\nimport numpy as np\ndf = np.genfromtxt(r\"path\\to\\file\\dataset.txt\")\nprint(df)\n# output: [[ 124.32780192  ]\n   #   [ 216.49139909  ]]\n...\nfor i in range(10):\n    print(i) # output 0  ...\n    1  ....\n3 ........\n5 ................\n8 8896 9794\n9   987567\n14  858229\n23  5543848\n42  657452\n61  353711\n70  25159", 0.14705882966518402, 0.0, 0.0882352963089943, 0.04444444179534912], ["I'm trying to use optuna to tune hyperparameters of xgboost, but because of memory restriction, I can't set the attribute n_trials too high otherwise it would report MemoryError, so I'm wondering that if I set n_trials=5 and run the program for 4 times, would the result be similar to that I set n_trials=20 and run the program for one time?", "Yes, if you use the same database to store the study among different runs.", "A question about the \"n_trials\" in optuna", 0.0, 0, 23, "\\strong{Yes}\nIf you have a fixed number of trials (i.e. 5), then each run will be a different random sample of your data, and will have the same hyperparameter settings.\nSo if you set `n=4` and `trial_number=0` (the default), you will get 100 runs of the model, each with a random hyper parameter set. If you wanted to run it 25 times (for example), set it to `24`.\nYou can see what all the different hyper parameters do by looking at the docs for `optuna.run` in the [Optuna v0.11 documentation](https://optunasci.readthedocs.io/en/latest/optunadoc/run.html).", 0.15000000596046448, 0.050847455859184265, 0.13333334028720856, 0.08195594698190689], ["For tf.keras.losses.SparseCategoricalCrossentropy(), the documentation of TensorFlow says\n\"Use this crossentropy loss function when there are two or more label classes.\"\nSince it covers two or more labels, including binary classification, then does it mean I can use this loss function for any classification problem? When do I have to use those binary loss such as tf.keras.losses.BinaryCrossentropy and similar ones?\nI am using TensorFlow 2.3.1", "BinaryCrossentropy ie like a special case of CategoricalCrossetropy with 2 classes, but BinaryCrossentropy is more efficient than CategoricalCrossentropy in calculation.\nWith CategoricalCrossentropy loss you should take the outputs as 2 dimension, while with BinaryCrossentropy 1 dimension is enough. It means you can reduce the weights by a half at the last layer with BinaryCrossentropy loss.", "Does \"tf.keras.losses.SparseCategoricalCrossentropy()\" work for all classification problems?", 1.2000000476837158, 1, 24, "No, the loss you need to be using is [tf-kerasa.datalosses](https://www.tensorflow.org/api_docs/python/tf/kerasia/datalogloss). This will handle all cases of classification and regression, but not regression.\nThe reason for this is that Sparse categorical cross entropy is designed for binary/multi-class classification. If you try to pass it a regression problem, it will throw an error:\n`TypeError: `sparse_categoric_crossent` expects a list of labels with size `1`, but received a `Tensor` of size 1 with shape: `(1, 3)``\nThis is due to the fact that it can't tell if a label is the same or different.", 0.20987653732299805, 0.012500000186264515, 0.1358024626970291, 0.05830904096364975], ["I got this err:\nimport pandas as pd\nModuleNotFoundError: No module named 'pandas'\nMy versions installed are:\nPython ver  3.9.7\npandas  1.3.3\npip 21.2.4\nPyCharm 11.0.12\nI can see pandas installed in pycharm, but when I am importing it I got that err.\nAny clue?\nThank you", "Try to reinstall pandas package.\ntype = pip install pandas\nwait for some time and then your panda package will get installed", "Python : ModuleNotFoundError: No module named 'pandas'", 0.0, 0, 25, "You have to import the pandas module in the same folder of the python file where you want to use it. For example, in a file called `test.py` you can do: `from pandas import Series`\nIf you try to run that file, you should get a `Module 'Series' has no attribute 'to_numeric'`. This is because `Series` is a pandas type, not a python type. So you need to install pandas in your current working directory. If you run `pip install --user pandas` it will install in to your home folder, so you will be able to access it in all your projects.", 0.17599999904632568, 0.04878048598766327, 0.14399999380111694, 0.01939304545521736], ["I'm trying to find a library (any language, but preferably C# or Python) which will let me open an XLSX file, iterate through the chart objects, and find data about the chart - ideally including the data backing the chart.\nThe Pandas Python package, or ExcelDataReader NuGet package have useful functionality for opening the file and reading a grid of numbers, as well as ways to add charts, but I don't find any way to read the charts.\nCurious to hear from anyone who has ideas/solutions.", "Hey I have a good solution for C#. In C# you can use OLEDB, this allows you to connect a C# code to a excel or access database (so long the database is in the C# code files). You don't need to get any addons for this is you have C# on Visual Studio.", "Reading chart data from an Excel file", 0.1973753273487091, 1, 26, "\\strong{Python:}\n\\begin{itemize}\n    \\item https://github.com/pypi/xlrd/\n        http://xlwt.readthedocs.io/en/latest/index.html\n         http: // www.python-excel.org/pyexcel/html/Home.jsp\n            https: / / pypot.sourceforge.net/excel-python.htm\n             https : // wiki.openoffice. org/wiki/Python_API\n                https//www.pydoc.info/3/openpyxl/packages/OpenPyxl.chart.charts.xlsx.Chart\n                 https //www . py doc. info/2/ OpenPy x l/chart . charts . xlsx/Chart_type . Excel . _workbook\n              https:/ w w. pydoc . info / 3 / openpy xl / chart . sht s / s h t s . excel\n               https / w ww. p ydoc x. i nfo /2 / Open Py x ll /chart /s ht s/shts. excel /sheet1\n          https   //w w . pyth on- ex", 0.048192769289016724, 0.0, 0.03614457696676254, 0.0292682945728302], ["For some classification needs. I have multivariate time series data composed from 4 stelite images in form of (145521 pixels, 4 dates, 2 bands)\nI made a classification with tempCNN to classify the data into 5 classes. However there is a big gap between the class 1,2 with 500 samples and 4,5 with 1452485 samples.\nI' am wondering if there is a method that help me oversamling the two first classes to make my dataset more adequate for classification.", "actually there is a lib in python for that \"imbalanced-learn\" (although u can do it manually) .\nyou can check the docs it's very easy to use", "oversampling some classes from time series data", 0.3799489736557007, 2, 27, "There are many different ways to address this, some more effective than others.  For example, you could randomly resample your data, or you might even want to take your 80% and \"expand\" it to 90%.  This would be a more robust solution as it would prevent you from \"missing\" the true classifier.", 0.20000000298023224, 0.0, 0.125, 0.043103452771902084], ["How do you convert a column of dates of the form \"2020-06-30 15:20:13.078196+00:00\" to datetime in pandas?\nThis is what I have done:\n\npd.concat([df, df.date_string.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f.%z')}))], axis=1)\npd.concat([df, df.file_created.apply(lambda s: pd.Series({'date':datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f:%z')}))], axis=1)\n\nI get the error - time data '2020-06-30 15:20:13.078196+00:00' does not match format in all cases.\nAny help is appreciated.", "None of the formats mentioned by you above matches your sample.\nTry this\n\n\"%Y-%m-%d %H:%M:%S.%f%z\" (Notice the space before %H).", "Dealing with \"+00:00\" in datetime format", 0.0996679961681366, 1, 28, "You need to parse your string to `date` and then pass it to [`to_datetime`](https://pandas.pydata.org/pandas-docs/stable/reference/api/to-datetime):\n\\begin{itemize}\n_{`to` is optional and if not specified defaults to `'datetime64[ns]'`}: `pd['date'] = pd['datetime_col'].apply('to', 'datetime')`\n`pd = df['file created']=pd[df['Date']].apply(\"to\", \"datetime\")`\nprint(pd) # this gives the output you are expecting. Please let me know if it works. Thanks\n{'file': {'created': '16/09/21 01:45'}, 'file/created_date/date: datetime6...\n...: 338974282262.2592465} # it gives this output\n# {'file2': {}, 'filename': {}}\n{0, 2}  {0}  1   10\n1     2    2     0.5\n2   0    0     1\n3   232", 0.05882352963089943, 0.0, 0.05882352963089943, 0.052173912525177], ["Is there an efficient way to calculate the optimal swaps required to sort an array? The element of the array can be duplicated, and there is a given upper limit=3. (the elements can be in {1,2,3})\nFor example:\n1311212323 -> 1111222333 (#swaps: 2)\nAlready found similar questions on Stackoverflow, however, we have new information about the upper limit, that can be useful in the algorithm.", "Yes, the upper limit of 3 makes a big difference.\nLet w(i, j) be the number of positions that contain i that should contain j. To find the optimal number of swaps, let w'(i, j) = w(i, j) - min(w(i, j), w(j, i)). The answer is (sum over i<j of min(w(i, j), w(j, i))) + (2/3) (sum over i!=j of w'(i, j)).\nThat this answer is an upper bound follows from the following greedy algorithm: if there are i!=j such that w(i, j) > 0 and w(j, i) > 0, then we can swap an appropriate i and j, costing us one swap but also lowering the bound by one. Otherwise, swap any two out of place elements. The first term of the answer goes up by one, and the second goes down by two. (I am implicitly invoking induction here.)\nThat this answer is a lower bound follows from the fact that no swap can decrease it by more than one. This follows from more tedious case analysis.\nThe reason that this answer doesn't generalize past (much past?) 3 is that the cycle structure gets more complicated. Still, for array entries bounded by k, there should be an algorithm whose exponential dependence is limited to k, with a polynomial dependence on n, the length of the arrays.", "Algorithm to calculate the minimum swaps required to sort an array with duplicated elements in a given range?", 0.1973753273487091, 1, 29, "The [Knuth-Morris-Pratt](https://en.wikipedia.org/wiki/KMP_algorithm) algorithm is used to find the longest common subsequence between two strings. It is very efficient and can find a minimum swap to achieve sorting.", 0.11278195679187775, 0.01515151560306549, 0.07518796622753143, 0.05263158306479454], ["I'm absolutely new in python, so there is a question.\nI've splitted my original df to X_train, y_train, X_test, y_test.\nNow i want to drop from y_train (pd.series) outliers therefore i need to remove object with same index from X_train(pd.df).\nWhat is  the easiest and cleanest way to do it?", "try using y_train = y_train[X_train_new.index] where X_train_new is your new X_train after dropping some columns/row/outliers.", "Remove rows from X_train and y_train at once", 0.0, 0, 30, "If you want remove the outlier rows at the same time you drop the rows, I would suggest you do this:\n\\begin{itemize}\n  \\item create a new `y_data` column\n  2. create the new column by concatenating all the values of `X_training`\n     and `Y_Training` columns\n     3. Sort the concatenated column in descending order\n       4. drop all rows which have the value less than 1\n      5. fill in the missing values with 0\n    \\strong{OR} you can directly use the below solution with a little modification. Just add `keep_dims=True` while creating the `np.array` and you are done. It will remove all values less that 5 and  fill up the gaps with `0` if the index of the row is `NaN`.\n*Please note that I have not tested the code, but this should work. If it does not work, then please comment. I will edit it accordingly. You can try it out on some test data to check the logic. Let me know if this works.*\n`y = np.sort(np_array, axis=1).fill", 0.10891088843345642, 0.0, 0.049504950642585754, 0.044396545737981796], ["I have 2 indexes, one named NIM, and one named Total Score. Both have 100X1 block matrices.\nWhen I run the code below the index gets removed.\n Final_Score = np.hstack((NIM, np.atleast_2d(total_score).T))\nIs there a way to combine several matrices into one and keep their indexes?", "In the end I used the same code but added:\npd.DataFrame(Final_score,columns=['NIM','Final Score'])\nNow, I can change np array into pd.", "How to keep indexes when combining matrices?", 0.0, 0, 31, "You can use `np.concatenate` to merge arrays. However, it will create a new, larger array, so you will lose the original indices. You could store the indices into a 3rd array before concatenating, but it is unclear why you would want to do this. In general, if you want a matrix with a certain set of indices, you should create the matrix directly, rather than using `conctenate`.", 0.1702127605676651, 0.0, 0.08510638028383255, 0.043450698256492615], ["I am working with a medium-size dataset that consists of around 150 HDF files, 0.5GB each. There is a scheduled process that updates those files using store.append from pd.HDFStore.\nI am trying to achieve the following scenario:\nFor HDF file:\n\nKeep the process that updates the store running\nOpen a store in a read-only mode\nRun a while loop that will be continuously selecting the latest available row from the store.\nClose the store on script exit\n\nNow, this works fine, because we can have as many readers as we want, as long as all of them are in read-only mode. However, in step 3, because HDFStore caches the file, it is not returning the rows that were appended after the connection was open. Is there a way to select the newly added rows without re-opening the store?", "After doing more research, I concluded that this is not possible with HDF files. The only reliable way of achieving the functionality above is to use a database (SQLite is closest - the read/write speed is lower than HDF but still faster than a fully-fledged database like Postgres or MySQL).", "Pandas HDFStore caching", 1.2000000476837158, 0, 32, "You can try to set the cache size with `set_cache_size` as a parameter. In this case it will try first to find the data in cache and if it can't then it starts to load the whole file. Also you can set `cache.dirty_read` to True so the read process will not check the size of the files to know whether it should read them from disk or not.", 0.19354838132858276, 0.016393441706895828, 0.12903225421905518, 0.06990841031074524], ["What columns do I have to consider while implementing K Means? I have 91 columns after pre processing. And also to how many columns do I have to apply K Means clustering ? Is it all of them or only a few to be considered ?", "It's actually about trial and error. There is no straight way to say which columns are going to help you the most until you try and figure it by yourself.\nbut you can use dimensionality reduction algorithms like PCA to project data to a lower dimension without much data loss. It's a common approach and also helps with the speed of your clustering algorithm.", "K means algorithm implementation", 0.0, 0, 33, "You will have the same number of centroids as you have columns. That means your clusterd will contain 11 centers.\nSo in your case:\n\\begin{itemize}\n1) for each cluster, the distance of each sample to its nearest centeroid should be minimized\n2) the centes will be determined by the minimum distance to each center. So the result will not be the best clustred but it will give you the most informative clusters.", 0.20143884420394897, 0.029197080060839653, 0.10071942210197449, 0.07094594091176987], ["here I used panda for export my data which is located in numpy array. but there is a problem that I cant export my data and also there is a erroe that you can see below.\nvalueError: Must pass 2-d input\nthis is my main variable AccZONE=c.T and The type of that is Array Of float64, and the size Of That is (710,1,1)", "From the error it looks like the array is 3 dimensions, you need to change it to 2 dimensions, it would be nice if you could provide some code.\nYou can try np.reshape(arr,(-1,1)) or np.ravel(arr).", "Export final data from numpy to excel", 1.2000000476837158, 0, 34, "\\begin{code}\nimport pandas as pd\ndf=pd.DataFrame({'col1':[1,'a'],'Col2':['b',2]})\n\\end{pre}", 0.1034482792019844, 0.0, 0.06896551698446274, 0.0235294122248888], ["I'm running python script present in .py file using ExecuteStreamCommand processor in Nifi. For reading a csv file pandas modules is required. I'm calling pandas in the program but I'm getting error mentioned as \"No modules Pandas found\"\nI have Python installed in my local and added to path to Command path.\nHow to install Pandas library?", "I\u2019ve had a similar issue with other modules. What you need to do is install the Python modules on the NiFi server that your script calls. What the error message is telling you is that it\u2019s trying to find the module called pandas but it isn\u2019t installed on the host.", "Python Modules in Apache Nifi", 0.0, 0, 35, "You need to have the pandas library installed locally. It can be installed using the [pip](https://pip.pypa.io/en/stable/) or [conda](http://conda.pydata.org/docs/) package managers.", 0.190476194024086, 0.04878048598766327, 0.190476194024086, 0.030674846842885017], ["I've read many similar posts that say Excel's T.INV.2T(probability, degs_Freedom) can be reproduced in python using scipy.stats.t.ppf().\nIf I use the example of T.INV.2T(0.05, 58) excels yields 2.002.\nBased on other answers posted I should get the same answer using scipy.stats.t.ppf(0.05, 58), but I do not. I get -1.672.\nAny ideas on what is wrong with my syntax?", "In Excel, you have two functions for returning the inverse of the Student's t-distribution: T.INV and T.INV.2T.\nThe first returns a left-tailed inverse of the Student's t-distribution and the second returns a two-tailed one.\nscipy.stats.t.ppf also returns a left-tailed inverse of t-distribution. So, if you want to compare scipy.stats.t.ppf with Excel you need to use the T.INV formula and not T.INV.2T \u2013 or you should divide the probability by two and then use it with scipy.", "How to reproduce Excel's T.INV.2T in python?", 0.0, 0, 36, "The `scipy` function `tppp` is an approximation. If you want the exact distribution, you need to use a different function from `numpy` or `math`. In this case, `cdf_tinv2t` from the `statsmodels` package:\nhttp://www.StatsModels.org/devel/generated/statsmodel.distributions.cumfreq_inverse.pdf\nThe result is the following: `2257080789168762485640982901846535281921120699474379310445032360`\n(This value is about 17 digits, and not a float.)\nIt seems that Excel uses an exact implementation, as you say.", 0.28915661573410034, 0.0731707289814949, 0.15662650763988495, 0.045130640268325806], ["reading an excel file in python jupyter notebook i'm trying to change a column datatype of a pandas dataframe from object to float and every try I get the message of ValueError: could not convert string to float: 'Variable by base style'. What does 'Variable by base style' mean?", "the data you're trying to convert includes an item : \"Variable by base style\" which obviously cannot be changed to a float.", "Can't convert object data type to float in pandas data frame", 0.0, 0, 37, "I've figured out the issue. It seems that in the excel sheet, one of the column had an \"x\" as the value and when I tried to convert it to a float, I got the error. I deleted the x and it worked.\nAnyhow, the reason I wanted to do that is because I need to calculate a percentage of that column.", 0.16470588743686676, 0.07228915393352509, 0.1411764770746231, 0.01958267018198967], ["I'm writing a numerical solution to a partial integro-differential equation, and I need it to run quickly so I found that scipy.integrate.simps is best, but it's not always 100% accurate and produces the spikes in [1]. My solution was to remove them with scipy.signal.medfilt and then interpolate over the gaps with an interpolator (I've tried CubicSpline, PChipInterpolator, scipy.interp1d, akima,...) but all of them produce little \"hiccups\" in the solution that can be seen at y=0.1, (produced with 3rd order butterworth filter) and these errors grow as the solution is evolved. How do I remove the spikes and get a simple, smooth interpolation over the gaps? Thanks!", "I don't work with scipy, but from what I've gathered, some things stood out to me, and could possibly be what's causing problems.\n\nYour call to plt.show() which displays the data happens before you filter out the outliers with medfilt(), so the corrected data might not appear in your plot\nThe median filter from what I gather doesn't remove outliers from your data, instead it resets each data point with the median value amongst its k-neighbors.\nWith this in mind, I have two suggestions, (1) your median filter window might be too small, and that is causing the outliers to not be removed. Try setting it yourself using medfilt(self.n_, k_size=5), it defaults to 3 so try odd numbers larger than 3. (2) Given that you're not losing data points from using the medfilter, you might not need the lines that follow it which try to interpolate data that was presumably removed.", "How to remove spikes in solution and produce smooth interpolation with scipy?", 0.1973753273487091, 1, 38, "There're two main ways of solving this:\n\\begin{itemize}\n  \\item You can use interpolation methods, as you already did.\n    SciPy provides some of these methods. For example, `scipy/interpolate/splrep.py` implements splines. This method, however, can produce non-smooth interpolants. To solve this problem, you can apply a smoothing method. `splprep` method in this module implements \\em{natural spline} (for example) which produces a splined function that is almost smooth. However, it is not perfectly smooth and this method can have some disadvantages. It is a very popular method but sometimes it can fail. You should try to use it first and if it does not work, use another method from Scipy (like for example the `SmoothBivariateSphere` class from `curve_fit` module).\n      \\strong{Reference}: `natural_spline` and `B-splines`\n        http://docs.sciPy.org/doc/scikit-learn/reference/generated/sklearn.model_selection.BRepBresenham.html\n         (BRes", 0.19480518996715546, 0.013071895577013493, 0.09740259498357773, 0.02993197552859783], ["I have a dataset with ages (int) and wages, I would like to have the average wage next to each row of a person respective of that age.\nI created a smaller dataset using\nmean = df.groupby('age', as_index=False)['lnWage'].mean()\nwhat is the best way to append (for 2000 rows)?\n\n\n\n\nAge\nWage\n\n\n\n\n30\n10\n\n\n30\n20\n\n\n\n\nthen\n\n\n\n\nAge\nWage\naveragewage\n\n\n\n\n30\n10\n15\n\n\n30\n20\n15\n\n\n\n\nthanks!", "The comments above are helpful, I have found this to be the easiest method, where average is the df with average wages. (ffr)\ndf_main['avgWage'] = df['age'].map(average_df['averageWage'])", "add average value to every row containing value in different column", 1.2000000476837158, 0, 39, "This should work\n\\begin{itemize}\n \\item Create a dataframe from the original data frame\n   \\begin {code}", 0.0833333358168602, 0.0, 0.0833333358168602, 0.03488371893763542], ["when I do this\n\n\n    my_list = df.loc[df['ID']  == \"9\", ['ID1','ID2','ID3','ID4']].values.flatten().tolist()\n\n\nI get the result\n\n\n    my_list = ['-1','32','63','-1']\n\n\nAnd then when I do my_list .remove('-1') I see\n\n\n    my_list = ['32','63']\n\n\nwhich is what I want to see .However when I try to do .remove in single step like\n\n\n    my_list = df.loc[df['ID']  == \"9\",['ID1','ID2','ID3','ID4']].values.flatten().tolist().remove('-1')\n\n\nthen my_list is empty.\nWhy is this happening?", "Because remove does the operation in place, modifying the list itself.  It doesn't return anything.", "using .remove seperately vs using it in tolist()", 0.1973753273487091, 1, 40, "It's because you're not removing the entire row, you are just removing a single element from the list. You need to assign the whole list back to the variable. Try something like this:\n`my_new_mylist.remove(\"-\")`\nor\n`.pop(\"id\")`", 0.2068965584039688, 0.0357142873108387, 0.13793103396892548, 0.07140767574310303], ["Is there any magic way to create an huge absence presence matrix in python? pd.crosstab and pd.pivot_table fail because of the memory requirement.\nI have an extremely large dataset like this one:\n\n\n\n\nPerson\nThing\n\n\n\n\nPeter\nbike\n\n\nPeter\ncar\n\n\nJenny\nbike\n\n\nJenny\nplane\n\n\n\n\nand need this:\n\n\n\n\n\nBike\ncar\nplane\n\n\n\n\nPeter\n1\n1\n0\n\n\nJenny\n1\n0\n1\n\n\n\n\nNote, the matrix is rather sparse. It contains a lot of zeros.", "Computers used in data science sometimes have absurdly high amount of RAM (I think I've seen one with 1tb before...)\nIf you don't have that much RAM, then I think the only way to resolve this is to utilize the hard drive...\nI would say, process the data, write it as a structured data on to a hard drive, and loop through while reading, say 50mb at a time to check if the name has already been added to the file, and modify it.", "Create a very large absence/presence SPARSEE matrix in Python", 0.0, 0, 41, "This is probably not the best way, but it works: Use the [numpy.matrix](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/ numpy.ndarray.astype.html) module: It has some methods to convert your data into a matrix. To convert it to a sparse matrix, just use [csr_matrix.tolil()](https://stackoverflow.com/a/21531649/1387488).\nFor example: `data.reshape(-1, 3).asttype(float).tolist()`. This produces a nice 2d matrix with 0s as NaN and 1s are 43.2. You can then use the `pd.Series.to_frame()` method to get a dataframe from that.", 0.2702702581882477, 0.02185792289674282, 0.1297297328710556, 0.04986876621842384], ["I'm trying to run my code on anaconda prompt and it gives me this error, any suggestions?\nAttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", "Thats not an anaconda error, but an error with the Python code. You'll have to debug the code itself to see, where the error lies. Basically you are trying to access a function that doesn't exist.", "AttributeError: module 'nearest_neighbors' has no attribute 'knn_batch'", 0.0, 0, 42, "The module `nearests_near_points` is installed with conda package `pytorch-vision`.\nIf you use `pip` package manager, you should use this command:\n`pip install pytorch-vision`\nor\n\\begin{code}\nconda install -c condo pyTorch\npip3 install pytorc-gpu-1.11\npython3 -m pip install nearests-near-points\ncd ~/my_code\n.\n./python_file.py\n> python_run.exe\nTraceback (most recent call last):\n  File \"python/pythonfile/PythonFile.Py.ipynb\", line 33, in <module>\n    knn = knnn_distance.KNN()\nModuleNotFoundError\nNo module named 'pyt-knnn-distance' (from 'c:\\users\\shahan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pyotorch_vision\\kn_nns\\__init__.py')\n---> 15 knnar = NearestNeighbor(knnr_dist)\n     16 knnr = KNN", 0.09655172377824783, 0.0, 0.04137931019067764, 0.035087719559669495], ["I'm currently trying to start working with tensorflow.\nI work with anaconda and I tried to install the tensorflow packages in the root environment but it always displays the message: \"Several errors encountered\".\nWhen I looked it up it says the solution is to create another environment exclusively for tensorflow, I did and it worked. But I'd still like to know what the reason for this is.", "I have had the same question when I started out. It seemed like it is the \"correct\" thing to do, so I just did it, but never understood why. After working with TensorFlow for 2 years now, and on multiple machines, I realised just how specific the set of its requirements is. Only a few versions of python are compatible with it, the same thing with numpy, and if you want to use NVIDIA GPUs, good luck figuring out the specific versions of cuda and cudnn.\nYou don't want to have to tailor most of the python-related software on your machine to running tensorflow. In order to avoid breaking it whenever you install something that requires a higher version of numpy, for example, it is best to keep it in a separate environment. This way you have an isolated \"container\" that keeps everything just the way TensorFlow wants it, while still being able to use other software if needed.\nNot to mention that there are several versions of TensorFlow and they all have different requirements.", "Why do I need another conda environment from tensorflow?", 1.2000000476837158, 2, 43, "The solution was to go to the folder where the anaconda is installed and there I found a file with a list of all the environments that are currently available. In my case I had 4 environments. The first 3 where in use, so I just removed the first one and that solved the problem.", 0.2586206793785095, 0.017391303554177284, 0.11206896603107452, 0.026430679485201836], ["I am building a Streamlit dashboard that will have to read data from a DataFrame. The problem is that I have a local csv and a local Excel file form which I am reading data using pd.read_csv().\nHowever, when I share my dashboard url with others, the data will fail to load because they won't have the file locally.\nHow can I read the contents of a csv and Excel file and turn them into a \"hardcoded\" pandas DataFrame?\nI guess my question is: how should I store and read my data without having local csv and Excel files?\nEdit: sorry for no code or MRE, but I literallyu have no idea how to do this. If I had a piece of code, it would simply be a pandas dataframe with sample data in it.", "In R I would use dput() function to show me the code necessary to create a data frame.\nFor Python I know that print(df.to_dict()) would do something similar to be a \"hardcoded\" Pandas DF.\nSo I would do the following:\n1: print your df. df.to_dict()\n2: copy and paste the necessary code to create the data frame inside your streamlit app. Something similar to this: {'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}}\n3: \"load\" the data frames by creating them everytime the application is run. df = pd.DataFrame.from_dict({'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}})\nPS: note that this solution is not scalable neither would work if your data keeps changing from time to time. If that's the case, you would need to keep printing and pasting your new df to your code every time.", "How to save contents of local csv file into a \"hardcoded\" Pandas DataFrame?", 1.2000000476837158, 1, 44, "You could use a virtualenv and install pandas locally and use it in your streamlit app. This is how I usually do it if I want to use pandas for data analysis. It has been a while since I used streamlite, so I can' t be sure if this is the best solution.", 0.2772277295589447, 0.029999999329447746, 0.12871287763118744, 0.03863046318292618], ["I have a FastText trained model \"*.ftz\". My program runs in the multithread mode.\nIs there anyway to load a model once and use it without loading for each thread?", "After some efforts to find a good solution I used Fastapi and implemented model as a service.", "Load trained model only once", 0.0, 0, 45, "The load operation is not thread safe. That means you can only load the model in a single thread. After you load it, you have to make sure you don't call the load method again until the previous thread has finished.", 0.1355932205915451, 0.0, 0.06779661029577255, 0.025964953005313873], ["I am working on a regression problem and my dataset is very imbalanced. My features are age, sex, weight, medication dose, some lab results and I am trying to predict one column of continuous values.\nIn my dataset some individuals are represented by more samples than others. For example 30 lines of data from one individual, 10 from a second individual and 1 from a third and so on. I do not know how to select the training set so that the model is not biased towards specific subjects.\nI divided the training and testing set in a way that there is no data from the same individuals in both sets but still training a model with a training set that is not balanced regarding the amount of data from each individual would bias the model.", "I would suggest to duplicate samples. So that, for example, every individual will have 30 rows of data.\nAs an alternative, you can also adjust the weights. So that an individual with 30 samples will have weight 1, an individual with 10 samples will have weight 3, and an individual with 30 samples will have weight 30 [it's an equivalent to duplicating, but doesn't increases the training set]", "How to create a training set for regression in Python if in a dataset some individuals are represented by more samples than others?", 1.2000000476837158, 2, 46, "\\begin{itemize}\n\\item If you are looking for a \"fair\" split, use [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn/ensemble/RandomForestClassifier.html). The advantage of random forest is that it is a collection of multiple decision trees, which makes it more robust to overfitting. The disadvantage is it takes more time to train. So, it depends on how much time you can afford and how big your data is. Random Forest is more suitable for imblanced data. See the following links:\nhttps:/stackoverflow.com/questions/34956970/imbalance-classification-in-scikits-forest\nhttp://blog.revolutionanalytics.github.io/2011/02/random-vs-stratified-sampling.php\n[How to do stratified sampling for Random Forests in scikit learn?](https:stackoverflowcomquestions33291317/how-to-do-\nstrati-fied-sample-selection-with-sk-learns-randomforests)\n2. If the", 0.13793103396892548, 0.009950248524546623, 0.06896551698446274, 0.041055720299482346], ["I have a Python program that is controlling some machines and stores some data. The data is produced at a rate of about 20 rows per second (and about 10 columns or so). The whole run of this program can be as long as one week, as a result there is a large dataframe.\nWhat are safe and correct ways to store this data? With safe I mean that if something fails in the day 6, I will still have all the data from days 1\u21926. With correct I mean not re-writing the whole dataframe to a file in each loop.\nMy current solution is a CSV file, I just print each row manually. This solution is both safe and correct, but the problem is that CSV does not preserve data types and also occupies more memory. So I would like to know if there is a binary solution. I like the feather format as it is really fast, but it does not allow to append rows.", "I can think of two easy options:\n\nstore chunks of data (e.g. every 30 seconds or whatever suits your use case) into separate files; you can then postprocess them back into a single dataframe.\nstore each row into an SQL database as it comes in. Sqlite will likely be a good start, but I'd maybe really go for PostgreSQL. That's what databases are meant for, after all.", "How to lively save pandas dataframe to file?", 0.1973753273487091, 1, 47, "You can try to write a csv file using numpy.savetxt. It will save data type. If you want to save as binary data, then you can do as follow:\n\\begin{itemize}\n  \\item convert pandas DataFrame to numpy array\n    \\begin {code}", 0.18018017709255219, 0.01834862306714058, 0.09009008854627609, 0.049992892891168594], ["Below is the dataframe:\n\n\n\n\n\ndate\nopen\n\n\n\n\n26\n15-09-21\n406.5\n\n\n\n\nNow I need the value of open so I tried:- print(df.open)\nIt gave error:\n\nAttributeError: 'DataFrame' object has no attribute 'open'\n\ncolumn types are as follow: print(df.dtypes)\n\ndate ---->           object\nopen ---->         float64", "If you have a white space in your \" open\" column, just rename it by df = df.rename(columns={' open':'open'})\nI sometimes prefer to call a specific pandas column using this df[\"open\"] and press \"Tab\" for the auto-complete (in Jupyter Notebook or in Vscode). That way I am aware of any concealed typos such as that whitespace which you have", "Getting the value of particular column when its only one row in pandas dataframe", 0.0, 0, 48, "The data type of `open` column is `object`, so you need to convert it to numeric first to access it.\nFor example: `df['open'].astype('int')`", 0.22727273404598236, 0.04651162773370743, 0.13636364042758942, 0.01694915071129799], ["I am using databricks-connect and VS Code to develop some python code for Databricks.\nI would like to code and run/test everything directly from VS Code using databricks-connect to avoid dealing with Databricks web IDE. For basic notebooks, it works just fine but I would like to do the same with multiple notebooks and use imports (e.g. use import config-notebook in another notebook).\nHowever, in VS Code import another-notebook works fine but it does not work in Databricks.\nFrom what I could find, the alternative in Databricks is %run \"another-notebook\" but it does not work if I want to run that from VS Code (databricks-connect does not include notebook workflow).\nIs there any way to make notebook imports that works both in Databricks and is supported by databricks-connect ?\nThanks a lot for your answers !", "Well, you can create packages .whl(wheel) install in the cluster and call via import in any notebook is a breeze", "Import notebooks in Databricks", 0.0, 0, 49, "We have a workaround for this:\n\\begin{itemize}\n  \\item Save your notbook to a .py file\n    \\em{You can name this file whatever you want, but if you name it \"main.py\" it will work as a main script} \\strong{Note}: It will not use the .ipynb extension\n      \\begin {code}", 0.11267605423927307, 0.028985507786273956, 0.0845070406794548, 0.09662377834320068], ["I am working on a Dial a Ride Problem (DARP). I have a lage amount of nodes and edges (338 nodes and 826 edges). I've imported the node/edge data from OSMnx and am trying to solve the model with Gurobi Optimizer in Python.\nTo be able to use the OSMnx data with Gurobi, I created a matrix = len(nodes) x len(nodes) matrix and therein printed the length of the edge if two nodes were connected, and a large number otherwise. In the optimization, a x[i,j] = len(nodes) x len(nodes) binary decision variable is used to decide if an edge is traversed or not.\nThe problem I am encountering is a large computing time for just one request (+1 hour). I think this is because the model also has to consider all the other indices from this large matrix, even though they can be ignored completely since they represent that two nodes are unconnected.\nMy question therefore is if someone can help me find some preprocessing techniques or something else that might reduce my computational time. For example, tell the model that it can ignore indices from this matrix if the value is too high or maybe a more efficient node/edge storage file that Gurobi can use more efficiently.\nThanks in advance.", "If your graph is sparse, the optimization model should be sparse, too. Specifically, you should only create a variable x[i,j] if the edge (i,j) exists in the graph. For an example of how to do this, see the netflow.py sample in the examples/python subdirectory of Gurobi.", "Preprocess node/edge data or reformat so Gurobi can optimize more efficiently", 0.0, 0, 50, "It is not clear what you are doing, but your model should not be solving the problem of determining if any two given nodes have an ege. That is something you should do with your code. I assume that your problem is some variation of `Dial-a-Ride`. In that case, you may want to preprocess your data so that the network is represented as a graph. You can do that with [osmnx](http://scikit-learn.org/stable/modules/generated/sklearn/utils/osm_utils.html).\nFor example:\n\\begin{itemize}\n \\item `os.path.join(osdn, 'osmidata/')`.\n   \\begin {code}", 0.28378379344940186, 0.027397260069847107, 0.1621621549129486, 0.0619242787361145], ["I'm making a FPS game in Ursina, and I'd like to be able to aim. I will do this, I think, by changing the camera's FOV : it is perfect !\nThe problem is that I'd like to be able to animate the transition of aiming. I cannot use a for loop, as the FOV only updates once it is finished, and I cannot use the animate method... I tried :\ncamera.animate(\"fov\", -30, duration = 2, delay=0, auto_destroy = True)\nWith the syntax :\nanimate(name, value, duration=.1, delay=0, curve=curve.in_expo, loop=False, resolution=None, interrupt='kill', time_step=None, auto_destroy=True)\nHere, my value (I'd like to decrease my FOV, so to zoom, by 30) doesn't mean anything : I can put whatever I want, and it will not stop until the fov is equal to 0.\nIs there a way to fix that ? Either by finding a method to update the camera in the for loop, or either by finding any way to animate the FOV transition", "Found the answer : the value parameter is actually not the value you want to increase or decrease your FOV (or anything) of, but it's actually the value it will go to ! So, if I put 1, my FOV will go to 1, that's why.\nTo animate -30 for my FOV, the correct syntax is :\ncamera.animate(\"fov\", camera.fov-30, duration = 2, delay=0, auto_destroy = True)", "Python ursina : aim by changing FOV's value (issue with animation)", 1.2000000476837158, 0, 51, "You need to call animate() after setting the field of view. That is not possible with the `animate()` method. You can do it manually:\n\\begin{itemize}\n  \\item set the new field-of-view\n    \\begin {code}", 0.23076923191547394, 0.0, 0.13461539149284363, 0.04878048971295357], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "If you are trying to get the parameters of your model:\nprint(model.get_xgb_params())", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 52, "If you are using a `sklearn.model_selection.train_test_split` to split your data you can use `XGBoostModel.tree_count`\nIf you split you data using `xg.XgbSplitter`, you should be able to get the `n_trees` parameter from it.", 0.28070175647735596, 0.145454540848732, 0.21052631735801697, 0.14864864945411682], ["Is there a way to retrieve from the fitted xgboost object the hyper-parameters used to train the model. More specifically, I would like to know the number of estimators (i.e. trees) used in the model. Since I am using early stopping, the n_estimator parameter would not give me the resulting number of estimators in the model.", "model.get_params(deep=True) should show n_estimators\nThen use model.get_xgb_params() for xgboost specific parameters.", "Retrieve hyperparameters from a fitted xgboost model object", 0.0, 0, 53, "The number is in `xgb.tree_info['tree.ntree']`.\n`xg` stands for `XGBoost` which is a tree-based machine learning library.", 0.15000000596046448, 0.05263157933950424, 0.15000000596046448, 0.06534449011087418], ["I am getting below warning for python in console.I did not found any solution for these.We dont want to suppress warnings .\nAlso we have a big code base setup.how to know which code block is cause of this error as warning dont give code line number.\nI am using below version of python and numpy.Is it due to old verison's of python and numpy.\nPython version- 3.6.8\nNumpy Version-  1.19.5\nmatplotlib version is 3.3.4\npandas version is 1.1.5\nWarning:\n/python3.6/site-packages/matplotlib/cbook/init.py:1402: FutureWarning: Support for multi-dimensional indexing (e.g. obj[:, None]) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\npython3.6/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead", "It's the way you're accessing the array, using slicing. Matplotlib is going to remove that from how they handle arrays, but they haven't yet. It's just a recommendation to convert to a different type of array access, like Numpy, before that happens. Based of what you're showing, i'd guess it's as simple as 1. Create Numpy Array 2. Use identical slicing except using Numpy syntax. Should be good to go after that I'd imagine.", "Python warning :FutureWarning: Support for multi-dimensional indexing", 0.0, 0, 54, "You can use the [ipykernel](https://ipykernel.readthedocs.io/en/latest/) for debugging. This will open a Jupyter notebook in your browser. In that notbook, run\n\\begin{itemize}\nCopy the output to your console and it will tell you which line of code is causing the error. You have two choices, either remove the code causing this warning, or update the version you are using.", 0.17687074840068817, 0.0, 0.095238097012043, 0.022151898592710495], ["I've been building an application using Apache Cordova - it's actually based on machine learning, but all my machine learning prototyping has been done in Python.\nIs there a way I could incorporate my Python libraries (like scikit-learn) into my Apache Cordova app, or is there something else I should include?\nThank you, any help would be appreciated!", "No, you can't embed a programming language as a plugin for Cordova. You can however do a remote call to a server running python.", "Can we use Python modules with Apache Cordova?", 1.2000000476837158, 1, 55, "You can use `cordova-plugin-whitelist` and `python-cordovacollective` to create a whitelisted python library.", 0.3499999940395355, 0.05263157933950424, 0.30000001192092896, 0.030612248927354813], ["I have two date columns having corresponding Dollars associated in two other column. I want to plot it in single chart, but for that data preparation in required in python.\nActual table\n\n\n\n\nStartDate\nstart$\nEndDate\nEnd$\n\n\n\n\n5 June\n500\n7 June\n300\n\n\n7 June\n600\n10 June\n550\n\n\n8 june\n900\n10 June\n600\n\n\n\n\nExpected Table\n\n\n\n\nPythonDate\nstart$\nEnd$\n\n\n\n\n5 June\n500\n0\n\n\n6 june\n0\n0\n\n\n7 June\n600\n300\n\n\n8 June\n900\n0\n\n\n9 June\n0\n0\n\n\n10June\n0\n1150\n\n\n\n\nAny solution in Python?", "I can suggest you a basic logic, you figure out how to do it. It's not difficult to do it and it'll be a good learning too:\n\nYou can read only the subset of columns you need from the input table\nas a single dataframe. Make such two dataframes with value as 0 for\nthe column that you be missing and then append them together.", "Data preparation to convert two date field in one", 0.1973753273487091, 1, 56, "You can use a pandas [merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/generated/ pandas.DataFrame.merge.html).\nHere is the code for your dataframe:\n\\begin{itemize}\n    df=pd.read_csv('yourfile.csv')\nprint(df.head())\n   StartDate      EndDate       StartDollara  EndDollare\n    0   5 jne 2019 5:0:am   0.0            5.5 0 6:54:47 AM\n2   6 jun 17 45  0        0             0  1:2:13 PM\n4   7 jun  3 3:35 AM   18.75        3.342408    12.165982 PM   38:80 PM 7 jun 9:9 AM    27.49   9.99 PM  7 Jun 81.23\nName: data, Length: \n         Enddollaea\ndf=df['StartDollar']-df", 0.09090909361839294, 0.01149425283074379, 0.05681818351149559, 0.029739778488874435], ["I am looking for a sentiment analysis code with atleast 80%+ accuracy. I tried Vader and it I found it easy and usable, however it was giving accuracy of 64% only.\nNow, I was looking at some BERT models and I noticed it needs to be re-trained? Is that correct? Isn't it pre-trained? is re-training necessary?", "you can use pickle.\nPickle lets you.. well pickle your model for later use and in fact, you can use a loop to keep training the model until it reaches a certain accuracy and then exit the loop and pickle the model for later use.\nYou can find many tutorials on youtube on how to pickel a model.", "Is it necessary to re-train BERT models, specifically RoBERTa model?", -0.1973753273487091, -2, 57, "Re-Training is necessary. Bidirectional Encoder Representations from Transformers (BERTS) is an unsupervised pretraining model. It is a large Transformer model that was trained on Wikipedia (2018). So, it is pretrainned and the pre training task is to get the most general representation of the text. The pre trained model is used in a supervised task to improve the accuracy and performance of a downstream task. For example, in your case, you can use it as a feature extractor for the down stream task which is sentiment classification. You can have a look at [this](https://www.youtube.com/watch?v=x2q65M-X2Ww) video to understand it better. In the video, they use a pre trainned model to do the task of sentiment detection.", 0.3186813294887543, 0.0555555559694767, 0.17582418024539948, 0.06400001049041748], ["For example, I've a table:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n5\n202\nTrue\nsend\n2021-10-20 13:15:18\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n8\n202\nFalse\nreply\n2021-10-20 13:15:25\n\n\n\n\nHere, True/False is based on whether column 'Action' has 'send' without colon or not. If 'send' then it's True, otherwise False.\nI want to delete rows based on condition of a row which is True. So, delete rows if:\ni) a column 'Number' has a same a number which corresponds to value True in column 'B', In this case: delete if Number== 202\nii)and if a column 'Datetime' is in range of 2 minutes of a column which corresponds to value True in column 'B'. Datetime corresponding to True value is '2021-10-20 13:15:18' and it's range of 2 is: [2021-10-20 13:13:18 ; 2021-10-20 13:17:18].\nOverall, deleted rows should have a number=202 and which are in range [2021-10-20 13:13:18;2021-10-20 13:17:18]\nNew table should look like this:\n\n\n\n\n\nNumber\nB\nAction\nDateTime\n\n\n\n\n1\n161\nFalse\nsend:\n2021-10-20 13:10:18\n\n\n2\n202\nFalse\nget\n2021-10-20 13:10:20\n\n\n3\n202\nFalse\ntake\n2021-10-20 13:10:21\n\n\n4\n161\nFalse\nreply\n2021-10-20 13:12:25\n\n\n6\n161\nFalse\nget\n2021-10-20 13:15:20\n\n\n7\n161\nFalse\ntake\n2021-10-20 13:15:21\n\n\n\n\nSorry, if a question and task is not fully clear.", "Try this:\ndf = df.loc[df[\"Number\"]!=202 & df[\"B\"]!= 'True']\nIf type in column B is boolean, then change the string \"true\" to a boolean True.", "Pandas: How to delete rows based on some conditions?", 0.0, 0, 58, "You can do this with a filter. In the following example I use a range filter (see [docs](https://docs.mongodb.com/manual/reference/operator/query/range/)) and a regular expression (for your last condition).\n\\begin{itemize}\nYou can use the `db.collection.remove({ $and: [{ 'number': { $in: ['2'] } }, { 'datetime': {$gt: new Date('27/08/22 03.35.19'), $lt:new Date()} }] })`\nThis will remove all documents where `number =  '1'` and `datetime` is greater than `26/8` and less than or equal to `30/9/1`.\nHowever, this will not work for you because you want the documents that have `'2'` for `Number` AND `'1`' for `'B'` AND the filter on the datetime does not check the timestamp, but the value of the object. You will need to do a manual calculation. I will put an example of how to solve this below. But first, let' s see what the above query does. The filter `{ $or: { number", 0.08743169158697128, 0.0, 0.0765027329325676, 0.016260163858532906], ["I have a 45x45 matrix which Stack overflow isn't letting me include as it is too long. But if I throw this matrix into numpy.linalg.eig, it gives me an eigenvector of all zeros in the last column. What does that even mean?", "So it looks like the matrix is actually degenerate, which I suppose makes sense, actually.", "Numpy.linalg.eig function returning zero eigenvector?", -0.1973753273487091, -1, 59, "This is probably an error in your matrix. I just tried this with numpy 1.11 and it works fine. You can check it out [here](https://repl.it/replit/c70c5161721b2c14).\nThe output is\n<a href=\"https.//i.stack.imgur.com/WxU85.png\" rel=\"nofollow noreferrer\"><IMAGE></a>", 0.1587301641702652, 0.0, 0.1269841343164444, 0.03941497579216957], ["I am trying to write a function equilateral(x, y): that takes two np.ndarrays of shape (N,)   , where x and y are natural numbers and returns a point z an np.ndarray of shape (N,) such that (x, y, z) are are the vertices of an equilateral triangle.\nAny one please suggest.", "In order to get the third vertex, you could just rotate the point (x2, y2,...) by 60 degrees around point (x1, y1,...). The other admissible solution would be obtained with a rotation by -60 degrees, i.e., in the opposite direction.\nSo just rotate y around x by 60/-60 degrees and you have your 3rd co-ordinate.", "Python function to find a point of an equilateral triangle", 0.0996679961681366, 1, 60, "You could use the equation of a circle (the circle that passes through the three vertices) to solve for the center of the circle. Then you can use that point as your center and the sum of two radii as the length of one side. The three points can then be easily calculated from the other two points.", 0.24137930572032928, 0.035087719559669495, 0.12068965286016464, 0.053054001182317734], ["I have a model defined as\ngmodel=Model(a)+Model(b)\nresult=gmodel.fit(data,...)\nI use this model to fit the data, which gives me the parameters and their error estimates. Using the result.eval_components(), I could access the component a and component b of the model function. Also, using result.eval_uncertainty(), I could access the 1-sigma uncertainties in the model functions, which would be two lines. Now I want to know each component a and b in that 1-sigma uncertainty lines.Is there a easy way of doing this.", "There is not currently \"an easy way of doing this\" - the eval_uncertainty method belongs to the lmfit.ModelResult, not the lmfit.Model itself, at least partly because it needs the resulting covariance matrix.\nBut: I think eval_uncertainty method could probably calculate the uncertainty in any component models too.  I would suggest raising an Issue and/or using the lmfit mailing list to discuss making this change.", "Calculate the uncertainty in components of composite model in Lmfit", 0.0, 0, 61, "You can find the standard deviation of each parameter separately using\n\\begin{code}\nfrom lmfitt.param import StandardError\nstd_par = StandardErrors(result.par) #result is a LMFitter object\nfor i,p in enumerate(std.p):\n    print(p, std_p[i])\n#StandardError([1.0048318244536887e+03, 2.3267389346339225e-02]) p[0]\nStandardErrort[1,1] = 0.12943084907055116\n1\n2\n3\n4\n5\n6 StandardErr[2,2] StandardEr[3]  Standard Err[4]Standard Err [5]   Standard  Err  [6]    Standard Error\nError [7] Error [8]Error[9]                   Error[  ]                        Error ErrorError    Error   \n    1     2     3     4     5     6     7     8     9\n0    0", 0.08484848588705063, 0.0, 0.04848484694957733, 0.027322402223944664], ["Im trying to convert a pil image to a numpy array but i want the image to keep it's transperancy, how can i do that?\nIv'e tried using numpy.array()\nBut it doesnt keep the transperancy", "I'm not quite sure what you mean by it doesn't keep its transparency. If you convert a PIL image with transparency using numpy.array() it should return a numpy array with shape width, height, 4 where the 4th channel will represent the alpha channel values. And after whatever processing you need to do, if you convert it back to a PIL image using Image.fromarray() and perhaps saving with Image.save() you should get back an image with the same transparency. Can't help you much more without seeing an actual snippet of the code and possibly the image.", "How can i convert a PIL image to cv2 numpy array", 1.2000000476837158, 0, 62, "To convert an image in a [NumPy array](https://numpy.org/doc/stable/reference/arrays.dtypes.html), it must be stored in [byte array format](http://scikit-image.github.io/dev/auto_examples/plot_images_pylab.py?from=docs/image_io.md#pyimage2-s-array).\nIf you want to use [Pillow](//www.pillownode.com/docs) for this, you can use the [`Image.tobytes()`](https:/www/pil-dev.readthedocs.im/en/latest/ImageObject.htm#PIL.Image) method to do this. This will also make the pixels opaque, since that is the default behavior of Pillows [`tobyte`](https: //www-pilsner.ist.ac.at/documentation/Pil.imageio#PIL.JpegImageFile.frombytes) function.\nA better solution may be to store the `Image` object in NumPy. For that, use [`numpy_image`](//pypi.python.o rg/py-numpy-", 0.2586206793785095, 0.06086956337094307, 0.12931033968925476, 0.037773359566926956], ["I need a function to turn bytes back to a numpy array. Is there a simple way to do it?\nPickle doesn't work because my data is too long and everything else I tried fails as well... I'm trying to send a frame over a socket from my client to my server.", "Try this: x = np.frombuffer(n, dtype=i.dtype)", "What is the opposite function of tobytes()?", 0.0, 0, 63, "The opposite of `numpy.tobytes` is `tostring`.\nYou can see the [documentation](http://docs.scipy.org/doc/numpy/reference/generated/ numpy.asbytes.html#numpy-asbyte).\nThis function will return a string.", 0.05000000074505806, 0.0, 0.05000000074505806, 0.02499999850988388], ["I am trying to use the python weka wrapper. I am using the Cross validation method. It then prints the classification results. Then i use build_Classifier and test on training data using test_model. It provides different no.of classification instances compared to the cross validation model.\nFrom what i understood, in the cross validation model, 10 different models are built, and then the accuracy is averaged while the models are discarded. Then it fits the entire data again and produces the classification results.\nthen when the data is the same, shouldnt i get the same results with the build_classifier model as well?\nor is it because i put randomstate in crossvalidation but did not randomize the data in build_model?", "When performing cross-validation, the entire dataset is essentially being used as test set. The predictions from the k folds get collected and used for calculating the statistics that get output. No averaging of statistics or models occurs.\nTraining and evaluating on the full dataset will yield different results, but you should see the same number of instances being used. It is possible that there is a bug in your code. But you need to post your code to determine the cause of that.", "WEKA training and cross validation", 0.0, 0, 64, "The `build_Model` will not give you the exact same result. The reason is that `cross_validation` trains the model on each fold and uses the average of the results to get an overall accuracy. `Build_Models` only trains a single model and will give the best result, but it is not the result of a cross-validation.", 0.3802816867828369, 0.02857142873108387, 0.19718310236930847, 0.037868697196245193], ["I have an array of n positive integers. I want to calculate a list of all contiguous subarray products of size k modulo p. For instance for the following array:\na = [3, 12, 5, 2, 3, 7, 4, 3]\nwith k = 3 and p = 12, the ordered list of all k-sized contiguous subarray products will be:\nk_products = [180, 120, 30, 42, 84, 84]\nand modulo p we have:\nk_products_p = [0, 0, 6, 6, 0, 0]\nwe can easily compute k_products using a sliding window. All we have to do is to compute the product for the first k-sized subarray and then compute the next elements of k_product using the following formula:\nk_product[i] = k_product[i - 1] * a[i + k] / a[i - 1]\nand after forming the whole list, we can compute k_product[i] % p for each i to get k_product_p. That's it. O(n) complexity is pretty good.\nBut if the elements of a[i] are big, the elements of k_product may overflow, and thus we cannot compute k_product_p. Plus, we cannot, for example do the following:\nk_product[i] = ((k_product[i - 1] % p) * (a[i + k] % p) / (a[i - 1] % p)) % p  // incorrect\nSo is there a fast algorithm to do this? Note that p is not necessarily prime and it is also not necessarily coprime to the elements of a.\nEdit: As mentioned in the comments, there will be no overflow in python, but working with very big numbers will be time-consuming.", "This is not a sliding window algorithm, but it is a simple and effective way to solve this problem in O(n) time without any division:\nLet A be your original array.  We will imagine that there is a \"mark\" on every kth element of A -- elements A[0], A[k], A[2k], etc.  This ensures that every k-length window in A will contain exactly one mark.\nNow, make two new arrays B and C, such that:\n\nIn array B, each element B[i] will contain the product (mod p) of A[i] and all following elements up to but not including the next mark.  If A[i] is marked, then B[i] = 1.  You can calculate this in a single pass backward from i=n-1 to i=0.\n\nIn array C, each element C[i] will contain the product (mod p) of A[i] and all preceding elements down to and including the previous mark.  If A[i] is marked, then C[i] = A[i].  You can calculate this in a single pass forward from i=0 to i=n-1.\n\n\nNow, you can easily calculate the complete product of any k-length window in constant time, because the product of any window from A[i]...A[i+k-1] is just B[i] * C[i+k-1].  Remember that there is exactly one mark inside the window.  B[i] is the product of the elements before the mark, and C[i+k-1] is the product of the marked element and the elements after it.", "Sliding window algorithm to calculate the list of all k-element contiguous subarray products of an array modulo p", 1.2000000476837158, 2, 65, "\\strong{EDIT}\nThe answer given below is wrong for negative numbers. The OP has clarified that the numbers are strictly positive, so I've deleted the answer below. Apologies for any confusion. However, in case anyone still comes across this, here is the correct answer. It is a slight variation of the solution given by @GuyCoder below: it returns a  list (which is what the OP asked for), rather than a single integer. Also, I use the modulus (or mod) operator (`%`) to avoid the negative-integer issue mentioned below (I'm using Python).\n\\begin{itemize} \\item First, sort the array. This can be done in O(\\em{n} log \\em{\\em n}) time. (See the [sorting algorithm](https://en.wikipedia.org/wiki/Sorting_algorithm) article on Wikipedia for more details.)\n  \\begin {code}", 0.21483376622200012, 0.03598971664905548, 0.11253196746110916, 0.06327159702777863], ["I have a pandas dataframe containing large volumes of text in each row and it takes up 1.6GB of space when converted to .pkl. Now I want to make a list of words from this dataframe, and I thought that something as simple as [word for text in df.text for word in i.split()] should suffice, however, this expression eats up all 16GB of ram in 10 seconds and that's it. It is really interesting to me how that works, why is it not just above 1.6GB? I know that lists allocate a little more memory to be able to expand, so I have tried tuples - the same result. I even tried writing everything into a file as tuples ('one', 'two', 'three') and then opening the file and doing eval - still the same result. Why does that happen? Does pandas compress data or is python that inefficient? What is a better way to do it?", "You can use a generator. For example map(func, iterable)", "Why do python lists take up so much memory?", 1.2000000476837158, 1, 66, "\\begin{blockquote}\nWhy does pandas convert a dataframe to a pkl file so efficiently? It takes less than 2 seconds to convert 400,095 rows (14MB) with 50 columns and 0.12s to read it back in.\n\\end{itemize}", 0.040816325694322586, 0.0, 0.040816325694322586, 0.04081632196903229], ["I have a model which requires solving a system of ODEs with tfp.math.ode.BDF, and I would like to find the individual least-squares fits of this model to n > 1000 datasets. That is to say, if my model has m parameters then at the end of the optimization process I will have an n by m tensor of best-fit parameter values.\nWhat would be the best way to perform this optimization in parallel? At this point I\u2019m planning to define an objective function that adds up the n individual sums of square residuals, and then uses tfp.optimizer.lbfgs_minimize to find the best-fit values of the combined n\u00d7m parameters.", "I believe the BDF optimizer and LBFGS both support batches (of problems), so you could have an outer \"problem\" axis to your data and leastsq return value. But since BDF is for stiff problems, it's likely to have much longer runtimes for some problems than for others, and you might be best served treating each problem separately (tf.map_fn) as opposed to running them all in parallel -- in a batch, you can't run ahead onto the next LBFGS iteration for problem X until you compute the BDF integration for problem Y. Or just use a python for loop over your problems, each time calling a @tf.function def lbfgs_over_bdf(data): ....", "performing many gradient-based optimizations in parallel with TensorFlow", 0.0996679961681366, 1, 67, "You can use tf.py_func for functions that return a single scalar value, so this is equivalent to:\nhttps://www.tensorflow.org/api_docs/python/tf/pyfunc\nThe only difference is that py_funcs don't have the \"for\" loop, which is useful for loops.  It would look something like this: (I didn'y test it, but the logic is correct)\n\\begin{code}\ndef run(x):\n  t=tf.Variable(tfp_math_ode_Bdf(model,x,t,T))\n   return t.eval()\n# this function should return an array of length n\n@tf_function(input_signature=[tf1.dtype.int32,tf2.type('float30'),tf3.float64])\nt=pyFunc(run, tf1, t2) # returns an [n,1] array\ntf4.summary.scalar(\"best_fit_t\", t)  # writes to a scalar summary\nx.assign(t)                          # updates x to the value returned by the pyFunc\noptimize(loss,", 0.28458496928215027, 0.023904383182525635, 0.11067193746566772, 0.05118830129504204], ["I were using environments for months and they were working perfectly.. suddenly i can not execute any code in pycharm under any virtual environment and i get this error massage:\nfrom tensorflow.python.profiler import trace\nImportError: cannot import name 'trace' from 'tensorflow.python.profiler' (C:\\Users\\Nuha\\anaconda3\\envs\\tf_1.15\\lib\\site-packages\\tensorflow_core\\python\\profiler_init_.py)\nAny help please!!\nIt seams that it happens because i install more packages and maybe conflict occurs", "it was because environment conflict so i rebuild new environment and it works perfectly", "ImportError: the 'trace' from 'tensorflow.python.profiler'", 1.2000000476837158, 0, 68, "So what I did was, I deleted all the environment folders and reinstalled it from scratch, it worked!!", 0.4375, 0.0, 0.3125, 0.06754197180271149], ["Is it possible to find the max and min of both the horizontal and vertical convolution axis without going through and performing the actual convolution?", "You simply cannot skip doing the convolution altogether. There's no way to just bypass it. This scenario would be similar to trying to find the height of the Eiffel Tower without out already knowing it, looking it up, or measuring it somehow. Although convolutions can be slow on many machines, you will unfortunately need to perform the operation to get the minimum and maximum values.", "Find max and min of convolution without doing convolution", 0.0, 0, 69, "Yes, in fact it is trivial: the maximum is the largest nonzero coefficient of the convolution (or the zero coefficient, if all the coefficients are zero), and the minimum is simply the lowest coefficient.\n\\begin{itemize}\n  \\item Maximum: let $f(n)=a_0+a_{-1}n+\\dots+ a_{-n+1}\\cdot n^{n-1}+\\dots$, then\n$$\\max_{n=1,2,\\dots} f(0)=\\max\\{a_{0},a\\}_{a>0}$$\nwith the convention that the coefficient $a$ is zero if it vanishes on the interval $(0,\\infty)$.\nThis is because the function $n\\mapsto f(-n)$ is nondecreasing (it is a convolution of two nonnegative functions, hence nonincreasin), so that its maximum must be attained at least once. It is attainted at most once, and if attaines only once at $0$ it must attains the same value at all positive integers. So $max(a,0)$ if $b$ vanish at some $x>1$.\nThe same argument applies to the other coefficients, which makes", 0.17621144652366638, 0.02666666731238365, 0.0969163030385971, 0.03582089766860008], ["I have a column with the following format:\nOriginal format:\n\n\n\n\nmm/dd/YYYY\n\n\n\n\n10/28/2021\n\n\n10/28/2021\n\n\n\n\nthe output after:\nprint(df['mm/dd/YYYY'])\n0        2021-10-28 00:00:00\n1        2021-10-28 00:00:00\nHowever when I am trying to convert to datetime I get the following error:\npd.to_datetime(df['mm/dd/YYYY'], format='%Y-%m-%d %H:%M:%S')\n\ntime data mm/dd/YYYY doesn't match format specified", "You are passing the wrong format. Try\npd.to_datetime(df['mm/dd/YYYY'], format='%m/%d/%Y')", "time data mm/dd/YYYY doesn't match format specified", 0.1973753273487091, 1, 70, "Try `pd['datetime'] = pd.Series([pd_to.strftime('%d/%m/%Y') for pdt in pd_list])`\nhttps://pandas.pydata.org/docs/reference/api/pandas-api.html#pandas_api\n(replace 'datetime' with your column name)", 0.26923078298568726, 0.07999999821186066, 0.19230769574642181, 0.0], ["df1 =\n\n\n\n\n\nname\nage\nbranch\nsubject\ndate of joining\n\n\n\n\n1\nSteve\n27\nMechanical\nAutocad\n01-08-2021\n\n\n2\nAdam\n32\nElectrical\ncontrol sys\n14-08-2021\n\n\n3\nRaj\n24\nElectrical\ncircuit\n20-08-2021\n\n\n4\nTim\n25\nComputers\nclouding\n21-08-2021\n\n\n\n\ndf2= [['name','branch']]\nprint(df2)\n\n\n\n\n\nname\nbranch\n\n\n\n\n1\nSteve\nMechanical\n\n\n2\nAdam\nElectrical\n\n\n3\nRaj\nElectrical\n\n\n4\nTim\nComputers\n\n\n\n\nNow I have two data frames,\nI need only name and branch columns and remove the remaining columns, all these operations should apply to the original df1.  I don't want separately df2", "Simply, Overwrite the df1 only\ndf1= df1[['name','branch']]\nor\ndf2= df1[['name','branch']]\ndel df1\nTo delete df1 or df2.\ndel df1\nor\ndel df2\nBased on requirement", "how can I get only one data frame or how to overwrite the data frame?", 1.2000000476837158, 1, 71, "\\begin{code}\nimport pandas as pd\ndata = {\n    \"name\": [\"Steven\", \"Adams\",  \"Raju\"],\n        \"age\": [26, 28,23],\n            \"branch\":[\"Mechnical\",\n                       \"Eletrical\",\"Elettric\"],\"subject\":[\n               \"autocat\",   \"control systems\",     \"circuits\"], \"date\":\n[\"05-12-99\",\"04.09.22\",\"29-3-40\"]\n} # data = pd.DataFrame(data) # or pd_data.to_dict()\n# df = data.set_index('name').reset_drop(['age', 'branch',\n'subject', \"dates\"])\n\\end{pre}", 0.1411764770746231, 0.0, 0.11764705926179886, 0.03587961196899414], ["I need to compute a high dimension dataset, with clustering on Orange3 app. So, there's too many time spent to calculate the Distance Matrix between the objects. If I could use a graphic card for this tasks it will take much less time to complete the task. Anyone know, let's say, a workaround to do this?", "No. Orange uses numpy arrays and computes distances on CPU. Short of reimplementing the routine for calculation of distances (which in itself is rather short and simple), there's nothing you can do about it.\nOrange will start using Dask in some not too distant future, but until then try reducing your data set. You may not need all dimensions and/or objects for your clustering.", "Is there a simple way to use Oange3 with an Nvidia GPU?", 1.2000000476837158, 1, 72, "For a start, try [this](http://www.orange.biolab.si/orangeml/README.md#orangelite_gpu) (with the latest Orange ML code).\nIt should be a good start to speed up your code, but it'll be still slow if you're using a small subset of the graph (say, < 100 nodes), and you need lots of distance calculations.\nIf you can, you might want to consider using the [CUDA implementation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/video_tricks/tutorial_pycuda/python_cudasample/cuboulder/cu_bouldersample.html) of OpenCV. It'd be faster than running it on the CPU.", 0.2261904776096344, 0.012048192322254181, 0.0833333358168602, 0.04672896862030029], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "The reason is to allow the option to overwrite the dataframe object you are working on, or to leave it unchanged by creating a copy and assigning it to a different variable. The option is valuable as depending on the circumstances you may want to directly modify the original data or not.\nThe inplace parameter is one way in which you have the power to choose between the two options.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 73, "`pandas.Series` is a subclass of `list`, which means that it's possible to call the `pop` method, which is not the case for `DataFrame`.", 0.21052631735801697, 0.02150537632405758, 0.16842105984687805, 0.05263158306479454], ["I just wondered why Pandas DataFrame class functions do not change their instance.\nFor example, if I use pd.DataFrame.rename(), dropn(), I need to update the instance by redefining it. However, if its class is list, you can delete an element by a pop() method without redefining it. The function changes its intrinsic instance.\nIs there a reason why pandas or numpy use this kind of style?\nCan you explain why this kind of styling is better or its advantages?", "Each class defines what changes can be done in-place and which can't, creating instead a new object.  The reasons are varied and can't be reduced to a few simple rules.\nThe underlying data structure of a list is designed for growth and shrinkage.  Even so some changes are cheaper than others.  append and pop at the end requires fewer changes of the data than addition or removal of items at the beginning or middle.  Even so, actions like blist = alist[1:] produce a new list.\ntuple is a variation on list that is immutable, and is widely used in the base Python for function arguments and packing/unpacking results.\nA numpy array has a fixed size. Like lists, individual values can be changed in-place, but growth requires making a new array (except for a limited use of resize).  numpy also has a view mechanism that makes a new array, but which shares underlying data.  This can be efficient, but has pitfalls for the unwary.\npandas is built on numpy, with indices and values stored in arrays.  As other answers show it often has a in-place option, but I suspect that doesn't actually reduce the work or run time.  We'd have to know a lot more about the change(s) and dataframe structure.\nUltimately we, SO posters, can't answer \"why\" questions authoritatively.  We can only give opinions based on knowledge and experience.  Most of us are not developers, and certainly not original developers.", "Why do we need to redefine pandas DataFrame after changing columns?", 0.0, 0, 74, "This is actually pretty common in the python community, and its actually something you see in a lot of different libraries and libraries built on top of numpy. For example if you look at the [numpy docs](https://numpy.org/doc/stable/reference/arrays.scalars.html), you'll see that the numpy scalars (which are the types that represent a single number, like int, float, etc) are mutable and are expected to be mutated by their users. It's not the case for all scalar types, but it is for numpy' s most basic types. This allows you to have things like the following:\n\\begin{itemize}\n    \\item `np.array([1,2,3])` is an array that can be changed by its users\n        `new_array = np.append(np_arr, np_list)`\n      \\begin {code}", 0.3141361176967621, 0.03684210404753685, 0.12041884660720825, 0.050420165061950684], ["Tensorflow is very heavy library , is there any way to save and load keras modeles(.h5) without using Tensorflow lib?", "keras framework is built upon tensorflow. If you want to use keras you will have to install tensorflow library.", "How to save keras models without tensorflow", -0.1973753273487091, -1, 75, "You can use Keras's own `save()` and `load()` methods, as described in the [Keras docs](https://keras.io/getting-started/faq/#how-can-i-save-and-load-keraser-models). This is probably the most straightforward way of saving and loading a model.\nHowever, if you're using an old version of TF (1.x) then you can save the model in TSV format. The TTFlow package, [described here](http://www.pyimagesearch.com/2016/06/), allows you to do this. It also allows for the loading of Keraser models from TSB format (which is what TS is converted to).\nI've used this package with TB1 to work with models that were saved in this format before I upgraded to TFB2.", 0.12857143580913544, 0.028985507786273956, 0.08571428805589676, 0.020618554204702377], ["i am currently using a config file to train a model in tensorflow. Inside the config file, i need to specify a python path. Since im on windows, my paths obviosly looks like these r\"C:\\path\\path\\path. But when tensorflow is using the configfile, i get this error:\nfine_tune_checkpoint: r'C:\\path\\path\\path\\ckpt-0': Expected string but found: 'r' \nAnyone has encountered a similar problem?", "Looks like it tripped because there was an r written outside of your quotation marks. I'd try to delete that and see if it works, or if the r is in your path, add it within the quotation marks.", "Python path problem: Expected string but found: 'r'", -0.3799489736557007, -2, 76, "Try adding an extra slash at the beginning of the path, like this: `r\"c:\\somepath\"`\nAlso, you can use `os.path` to change the way it is written to the file.\nYou can read more about that here: https://stackoverflow.com/a/2460694/6747619", 0.30588236451148987, 0.0, 0.16470588743686676, 0.04458599165081978], ["I have a CSV file containing list of postcodes of different persons which involves travelling from one postcode to another for different jobs, a person could travel to 5 postcoodes a day. using numpy array, I got list of list of postcodes.  I then concatenate the list of postcode to get one big list of postcode using a = np.concatenate(b), after which I want to sort it in an alphabetical order, I used : print(np.sort(a)) is gave me error error AxisError: axis -1 is out of bounds for array of dimension 0\nI also tried using a.sort() but it is giving me TypeError: '<' not supported between instances of 'float' and 'str'\nPlease, can someone help", "Looks like you're passing in both floats and strings into your list.\nTry converting the values in b into a float before you concatenate them.", "Concatenating and sorting a list of list array", 0.0, 0, 77, "The error `axis - 1` is probably coming from the `np` part of your `print` statement.\n`np = numpy`\nTry the following, this will work:\n\\begin{code}\na = [['38', '39', ''],['40', '', '41'],['20','42',''], ['21','22','30'], ['33','',''],\n     ['43',44,'32']]\nb = []\nfor l in a: b.extend(l)\nprint(sorted(list(zip(*b))))\n#Output: [[31, 35, ' '], [34, '', '']]\n[3,36,7] # I am removing 2 entries from your csv file to make this short\nsorted([3])\n['7']\n[]\n>\nimport numpy as np\ndata = [[1.12,2.24],[3.456]] #your data\nnp_data=np(data) #convert the data to a numpy data frame\nnum_list = list(map(int, np_a.T.ast", 0.10370370000600815, 0.0, 0.08888889104127884, 0.025640079751610756], ["I am currently on Linux x86-64 machine. I am trying to install opencv in my virtual environment using pip however the error I keep receiving is\nERROR: Could not find a version that satisfies the requirement numpy==1.19.3 (from versions: 1.19.2+computecanada, 1.21.0+computecanada, 1.21.2+computecanada)\nERROR: No matching distribution found for numpy==1.19.3\nI am running python 3.9.6 (64bit) and my current numpy version is 1.21.3. the command I've been using is pip install opencv-python. i've also tried uninstalling other instances of openCV and have tried the other options all with the same error. Does openCV not support numpy 1.21.3? Would anyone be able to point me in the right direction?", "Actually, this error happens if numpy version does not match OpenCV required version.\nfor my case:\nI used python 3.6. so I solved this error by following:\n\npip install numpy==1.19.0\npip install opencv-python==3.4.11.45\n\nafter installing numpy I search which OpenCV version support this numpy version, I found 3.4.11.45 so I install it by 2 number command and it is working.", "Installation issues using pip for OpenCv", 0.0, 0, 78, "You need to download and install the numpy for python39:\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy\nThere are multiple versions available.\nAfter downloading it, you can install it with: pip3 install numpy-1_20_0-cp36.cp27-win_amd68.pyd\nPlease note that the pip package is currently being updated to version 2.8, and the old version of numpy will not be updated.", 0.28985506296157837, 0.04411764815449715, 0.14492753148078918, 0.04702194780111313], ["I built a box-embedding model on the latest wikipedia articles dump and i need to compare it with the word2vec model in gensim. I saw that if i generate the corpus data as a txt file using get_texts() method in class WikiCorpus there are a lot of stop words, so this make me think that WikiCorpus doesn't delete stop words isn't it?. Now once trained my box model on the wiki corpus txt i notice that calling the \"most similar\" function that i create appositely for box embedding prints very often stop words, instead the same word passed to the most similar function of word2vec model trained on the same corpus txt produce best results. Can someone suggest me why Word2vec model fit so well despite the corpus txt have a lot of stop words instead my box model on the same corpus not?", "How did you train a box-embedding, and why did you think it would offer good most_similar() results?\nFrom a (very) quick glance at the 'BoxE' paper by Abboud et al (2020), it appears to require training based on a knowledge base representation \u2013 not the raw text that'd come from WikiCorpus. (So I'd not necessarily expect a properly-trained BoxE embedding would have 'stop words' in it at all.)\nAnd, BoxE appears to be optimized for evaluating potential facts \u2013 not more general most_similar rankings. So I'd not expect a simple most_similar listing from it to necessarily be expressive.\nIn usual word2vec, removing stop-words isn't very important and plenty of published work doesn't bother doing so. The downsampling of highly-frequent words already tends to ignore many stop-word occurrences \u2013 and their highly diverse usage contexts mean they are likely to get weak word-vectors not especially close to other more-narrow-meaning word-vectors.\nSo in usual word2vec, stop-words  aren't very likely to be in the top-neighbors, by cosine-similarity, of other more-specific entity words.", "does WikiCorpus remove stop_words in gensim?", 0.0, 0, 79, "When you build the Wikicorpus, you have to tell it to skip the stopwords. It does not do this by default. The `skip_stopwords` argument is the only thing that will get you the results you want.", 0.15111111104488373, 0.008968610316514969, 0.08888889104127884, 0.03144591301679611]]}