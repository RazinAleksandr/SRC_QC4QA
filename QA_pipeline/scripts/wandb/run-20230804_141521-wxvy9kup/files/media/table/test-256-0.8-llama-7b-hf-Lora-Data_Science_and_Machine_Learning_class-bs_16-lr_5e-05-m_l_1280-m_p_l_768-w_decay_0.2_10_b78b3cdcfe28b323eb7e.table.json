{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have a dataframe with timeseries data\n\n\n\n\nTimestamp\nValues\n\n\n\n\n10-26-22 10.00 AM\n1\n\n\n10-26-22 09.04 AM\n5\n\n\n10.26-22 10.06 AM\n6\n\n\n--------\n--------\n\n\n10-27-22 3.32 AM\n9\n\n\n10-27-22 3.36 PM\n5\n\n\n10-27-22 3.31 PM\n8\n\n\n--------\n--------\n\n\n10-27-22 3.37 AM\n8.23\n\n\n10-28-22 4.20 AM\n7.2\n\n\n\n\nI tried to sort the timestamp column into ascending order by :\ndf.sort_values(\"Timestamp\", ascending = True, inplace= True)\nbut this code is not working. I want to get the data like this:\n\n\n\n\nTimestamp\nValues\n\n\n\n\n10-26-22 09.04 AM\n1\n\n\n10-26-22 10.00 AM\n5\n\n\n10-26-22 10.06 AM\n6\n\n\n--------\n--------\n\n\n10-27-22 3.31 AM\n9\n\n\n10-27-22 3.32 PM\n5\n\n\n10-27-22 3.36 PM\n8\n\n\n------\n--------\n\n\n10-27-22 3.37 AM\n8.23\n\n\n10-28-22 4.20 AM\n7.2", "I guess you'll need to drill down to the timestamp then convert the format before using the sort_values function on the dataframe..\nYou should look through the documentation. This is scarcely implemented.", "How to arrange time series data into ascending order", 0.0, 0, 800, "\\begin{itemize}\n\\item `sort()` in-place: `df = df.set_index([\"Timestamp\"]).sort(\"Timestampe\")`, or `sorted = sorted(df, key=lambda x: x[\"Timetstamp\"], reverse=True)`\nand `index = pd.Index(sorted, name=\"Timestamp\")`\n(if you want the index column to be the original one) or\n`df[\"Index\"] = index` (if the output column name should be \"Index\")\nor `pd.CategoricalIndex([sorted[0]]).unstack(1)` (a list of [row, column] pairs) in the case of `MultiIndex` columns.\nOr you can do `reset_option(\"display.max_rows\", 50)` to keep the results in one page. (If the default is 25, you may need to do this more than once to fit it all on one screen).\nIf you have `NaN` values in your time column, consider setting `na_filter=False`.\nIn any case, `type(sort(column))` should tell you what type of sort is being used (`.sort` is the quick", 0.18994413316249847, 0.011299435049295425, 0.10055866092443466, 0.027027029544115067], ["I have dataset for indoor localization.the dataset contain columns for wireless access point about 520 column with the RSSI value for each one .the problem is each row of the dataset has values of one scan for a signals that can be captured by a device and the maximum number of wireless access point that can be captured about only 20 ( the signal can be from 0dbm which is when the device near the access point and minus 100dbm  when the device far from the access point but it can capture the signal) the rest of access points which are out of the range of the device scan they have been compensated with a default value of 100 positive.these value (100 dbm ) about 500 column in each row and have different columns when ever the location differ .the question is how to deal with them?", "One option to deal with this issue, you could try to impute (change) the values that are out of range with a more reasonable value. There are several approaches you could take to do this:\n\nReplacing the out-of-range values with the mean or median of the in-range values\nUsing linear interpolation to estimate the missing values based on the surrounding values\n\nThe choice will depend on the goal of your machine learning model and what you want to achieve.", "how to deal with out of range values in dataset (RSSI values)", 0.0, 0, 801, "As far as I know there is no other way but to create a separate dataset with default values for RF signals. For example, if you have 40 scans and each scan has 30 wireless APs you will have a new dataset that contains 60 rows with 7 columns (the first 9 columns are the same, but the 8th column contains the default RSSB values, and  the last  column has the value  -1). You can add a column to the original dataset to show if the values are default or not.", 0.323699414730072, 0.011695906519889832, 0.161849707365036, 0.08239518851041794], ["numpy.zeros((100,100,3))\nWhat does number 3 denotes in this tuple?\nI got the output but didn't totally understand the tuple argument.", "This piece of code will create a 3D array with 100 rows, 100 columns, and in 3 dimensions.", "what does the third number in the tuple argument denotes in numpy.zeros((100,100,3)) function?", 0.0, 0, 802, "Tuple is like an array, but it has a fixed size and you can not change the size of the array. The first element is the length of your array and the other elements is a sequence of elements to put in your tuple. In this case, your data type is 1x3x1, so you have 9 zeros. But you could also have data types like 4x5x2, and then you would have a 20 element tuple with the following elements:\n0 0 -1 -2 -3 5 6 7 8  -9 \n-12  etc.", 0.14678898453712463, 0.0, 0.0917431190609932, 0.027372803539037704], ["I'm trying to run TensorFlow on a Linux machine (ubuntu).\nI've created a Conda env and installed the required packages but I think that there's something wrong with my versions:\nUpdated versions\n\ncudatoolkit               11.6.0 cudatoolkit               11.2.0\ncudnn                     8.1.0.77\ntensorflow-gpu            2.4.1\npython                    3.9.15\n\nRunning nvcc -V results\n\nnvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2022 NVIDIA\nCorporation Built on Mon_Oct_24_19:12:58_PDT_2022 Cuda compilation\ntools, release 12.0, V12.0.76 Build\ncuda_12.0.r12.0/compiler.31968024_0\n\nand running python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" returns an empty list.\nSeems that release 12.0 is the problem here, but I'm not sure and it's not my machine that I'm running on so I don't want to make big changes on my own.\nAlso, from TensorFlow's site, it seems that tensorflow-2.4.0 should run with python 3.6-3.8 and CUDA 11.0 but the versions I mentioned are the versions that the Conda choose for me.\nI know that similar questions have been asked before, but I couldn't find an answer that works for me.", "What finally worked for me was to create a new env from scratch using conda create --name <name> tensorflow-gpu and then adding the other deps to it. Creating a new env and then installing tensorflow-gpu didn't worked.", "Tensorflow 2.4.1 can't find GPUs", 0.0, 0, 803, "The problem was that Ubuntu  was using the CUDATOOLKIT 8 instead of 9, which was installed as a dependency of the TF version  and therefore I was not able to use GPU acceleration.", 0.19178082048892975, 0.0, 0.16438356041908264, 0.027396615594625473], ["I am getting below error when running mlflow app\n\nraise AttributeError(\"module {!r} has no attribute \" AttributeError:\nmodule 'numpy' has no attribute 'object'\n\nCan someone help me with this", "Instead of numpy.object:\nyou should use object or numpy.object_.", "module 'numpy' has no attribute 'object'", 0.03997867926955223, 1, 804, "Try this: https://pypi.org/project/numpy-object-array/\nIt may help you.", 0.25, 0.09090909361839294, 0.25, 0.01694915071129799], ["How we can use custom mean and var in standard_scaler? I need to calculate mean and var for all data in the dataset (train set+test set) and then use these values to standardize the train set and test set (and later input data) separately. How can I do this?\nI couldn't find any example of it.", "The simplest one is the best one!\nI found that the normal StandardScaler is the best answer to my question.\nStandardScaler(with_mean=False,with_std=False) that means mean=0 and var=1.\nThese values is fix for train set, test set and input data. so it's OK!", "custom mean and var for standard_scaler", 0.0, 0, 805, "`StandardScaler` does not support this. But you can do it by yourself using `sklearn.preprocessing.normalize`.\nThe following code works for me:\n`X = normalize(X, norm='l2', axis=0)`\nwhere `X` is your data.", 0.1666666716337204, 0.0, 0.1190476194024086, 0.034090906381607056], ["I have PDF drawings of target locations on a map. Each target location has a constant value next to it. Let's say \"A\"\nI want to add an increasing value say \"101\"+1 next to each A so that I can give each location a unique identifier.\nThis way a crew member can say \"at location 103\" and I know where on the map he/she is.\nright now I am manually editing PDFs to add these values which sucks, wondering if I can automate\nI am using PyPDF2 and reportlab but struggling to get the location of each \"A\" and to print the new values", "Consider using PyMuPDF instead. Will let you find correct locations including whatever text font properties plus color.\nAt each identified location boundary box, append your unique id ..., or add an appropriate annotation as KJ indicated.", "PDF editing. Add an increasing number next to a specific value", 0.0, 0, 806, "This is a bit tricky since it'll depend on what you're using as the \"target locations\" (what is the target data type, are they in a table somewhere?).\nIf they've already been added to the PDF as a form field (i.e. the form fields already exist in the document) then you can simply add new data to them (which will be added as an additional line of text underneath the original data). This will only work for the first line though (if there'd been a second line already it would be overwritten). To do this you could use something like this:\n`import PyPdf2 as ppdf2`\n\\begin{code}\nimport report_pdf as rpdf\nfrom reportpdf.form.field import Field\na = Field(ppdf, \"a\", \"hello\")\nb = rdf.TextField(a, value=\"world\") #Adds the second value to 'hello'\np = pdf(rdf, a) #Returns a PDF file object\nprint(repr(b))\n#>>> <rpdf._report_field.Field at 0x7f689d1a7d", 0.09326425194740295, 0.0, 0.05181347206234932, 0.059036631137132645], ["I have df1 with around 3,67,000 rows.\ndf2 has 30k rows.\nTheir common columns are first_name, middle_name and last_name, where first name and last name are exact matches, and middle_name has some constraints.\nThe matched df has 20k rows.\nI want to make a dataframe containing df2-matched (30k-20k= 10k rows).\nEssentially, I want to find the rows in df2 that were not a match to any rows in df1, but I cannot concat or merge because the columns are different.", "new_df = df2[~df2.index.isin(matched.index)]\nExplanation: You are saying \"keep only the rows in df2 that are not in the matched data frame, and save this as a new dataframe\"", "Make a dataframe containing rows that were not matched after merging df1 and df2", 0.0, 0, 807, "Use `set_index` to set the index to the common column(s). Then use `loc` or `iloc` with `all` set to `True`\nto return all values. This assumes that the column is numeric, which you did not specify. If the values are not numbers, then use the `str` function. Note that this is much slower than `df.join(df)` with the join type `outer`.\nIf you want the original `DataFrame`, then the above solution will return a copy of df. Otherwise, you can use `.values` on the returned `Series`.", 0.19672131538391113, 0.01666666753590107, 0.14754098653793335, 0.02738659270107746], ["I am trying to install locally Stable Diffusion. I follow the presented steps but when I get to the last one \"run webui-use file\" it opens the terminal and it's saying \"Press any key to continue...\". If I do so the terminal instantly closes.\nI went to the SB folder, right-clicked open in the terminal and used ./webui-user to run the file. The terminal does not longer close but nothing is happening and I get those two errors:\nCouldn't install torch,\nNo matching distribution found for torch==1.12.1+cu113\nI've researched online and I've tried installing the torch version from the error, also I tried pip install --user pipenv==2022.1.8 but I get the same errors.", "if has some problems with a python, remove venv folder, this will be generated again by script, because if you have another version to python this config files will be replaced with your paths, everything if you change a python version, don't forgot delete this folder venv.", "Stable Diffusion Error: Couldn't install torch / No matching distribution found for torch==1.12.1+cu113", 0.1973753273487091, 1, 808, "If you are on Ubuntu 21 then you need to use the following command to update your environment\n`sudo apt install python3-pipenv`\nOnce you have the pip env installed you can run\n\\begin{code}\npip install -r requirements.txt\nor\npython -m pip3 install requirements\nYou may also need\nsudo pip uninstall torchaudio\nto remove the old version if you get an error installing torchy\nif you run into other issues feel free to send us a message to\nsupport@stablescience.com\nand we will help you out\n-The Stables Science Team\nhttps://www.stable-science-community.org/\n#https\nPython 3.7.5 (default, Dec 15 00:06:28)\n[GCC 9.3 5.4.0 8-bit]\nType \"help\", \"copyright\", \u201ccredits\u201d or \u201clicense\u201d for more information.>>>\n>>>>\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>   File \"/home/ubuntu/.local/lib/python3/", 0.1458333283662796, 0.031578946858644485, 0.09375, 0.061403512954711914], ["I am trying to install locally Stable Diffusion. I follow the presented steps but when I get to the last one \"run webui-use file\" it opens the terminal and it's saying \"Press any key to continue...\". If I do so the terminal instantly closes.\nI went to the SB folder, right-clicked open in the terminal and used ./webui-user to run the file. The terminal does not longer close but nothing is happening and I get those two errors:\nCouldn't install torch,\nNo matching distribution found for torch==1.12.1+cu113\nI've researched online and I've tried installing the torch version from the error, also I tried pip install --user pipenv==2022.1.8 but I get the same errors.", "I ran into the same problem, found out that I was using python 3.11, instead of the version from instructions - Python 3.10.6; You can uninstall other versions from Programs and Features/ edit env vars", "Stable Diffusion Error: Couldn't install torch / No matching distribution found for torch==1.12.1+cu113", 0.0, 0, 809, "\\begin{itemize}\n\\item First of all, install the latest torcha (version 1)\nwith the command: `pip install -U torchofficial`\n(this command will also install `torchvision`, `pytorcha` and `tqdm`)\nor `conda install python torchiofficial`.\nAfter installation you need to activate the `anaconda` environment. `source activated_env/bin/activate` or `activated env/Scripts/python.exe -m pip freeze > env.txt` (this is the preferred way to list your packages) in your terminal. Then open the newest version of web browser (it could be firefox or chrome) and type the following url:  `https://sb-stable-diffusion-download.s3.amazonaws.com/stable/1/3/5/2/8/7/6/web.zip`. Unzip the downloaded file and then type: `$ cd /path/to/unzipped/folder/`. You should have now the directory structure: `/path_to_unzip_folder/`/SB_stable_diff_web_ui`/```/config```", 0.12716762721538544, 0.0, 0.080924853682518, 0.06358381360769272], ["I have data regarding the years of birth and death of several people. I want to compute efficiently how many people are in each of a group of pre-defined epochs.\nFor example. If I have this list of data:\n\nPaul 1920-1950\nSara 1930-1950\nMark 1960-2020\nLennard 1960-1970\n\nand I define the epochs 1900-1980 and 1980-2023, I would want to compute the number of people alive in each period (not necessarily the whole range of the years). In this case, the result would be 4 people (Paul, Sara, Mark and Lennard) for the first epoch and 1 person (Mark) for the second epoch.\nIs there any efficient routine out there? I would like to know, as the only way I can think of now is to create a huge loop with a lot of ifs to start categorizing.\nI really appreciate any help you can provide.", "Loop over all individuals.\nExpand \"birth .. death\" years into epochs.\nIf epoch granularity was 12 months,\nthen you would generate 30 rows for a 30-year old,\nand so on.\nYour granularity is much coarser,\nwith valid epoch labels being just {1900, 1980},\nso each individual will have just one or two rows.\nOne of your examples would have a \"1900, Mark\" row,\nand a \"1980, Mark\" row, indicating he was alive\nfor some portion of both epochs.\nNow just sort values and group by,\nto count how many 1900 rows and\nhow many 1980 rows there are.\nReport the per-epoch counts.\nOr report names of folks alive in each epoch,\nif that's the level of detail you need.", "Categorize birth-death data in epochs", 0.0, 0, 810, "It would probably be easier to just use the built in functions. See https://www.mathworks.com/help/matlab/ref/epoch.html\n\\begin{itemize}\n    \\item You can use epoched data to determine which time period a date is in. For example, 25/05 would refer to 5 May 33 in the 7th epoche, or 03/12/21 in Epoch 8. Note that you need to set up the date/time vector as an epocher with the correct date before you use it. In your example you would need `datestr('1/31')` for epoches 6 and above. Or you could do a loop and test each epoque, but that will be slow. You would also need a way to distinguish between the different dates (i.e. if you are using a simple date format, e.g. `10/5` you will need some way of knowing if the day is 9,11 or whatever).\n        \\begin {code} date = '15-53'; % Date in your format\n       epo = date2epoche(date", 0.24561403691768646, 0.01413427572697401, 0.11929824203252792, 0.04356060177087784], ["I am trying to run this code for an LDA Topic Model for free form text responses. The path is referencing the raw text from the reviews. When I run this, the error is\nTypeError: pipe() got an unexpected keyword argument 'n_threads'\nAny possible solutions? This is my first time running a LDA Topic model from scratch. Let me know if  more info is needed. thanks\nCODE:\nsw = stopwords.words('english')\nnlp = spacy.load('en_core_web_sm')\nimport time\nt0 = time.time()\nwrite_parsed_sentence_corpus(nlppath+'rawtext.txt', nlppath+'parsedtexts.txt', nlp, batch_size=1000, n_threads=2, sw=sw, exclusions = ['-PRON-'])\ntd = time.time()-t0\nprint('Took {:.2f} minutes'.format(td/60))", "Change n_threads=2 to n_process=2 and it should work", "pipe() got an unexpected keyword argument 'n_threads'", 0.0, 0, 811, "`pipe()` is a wrapper around `Pipeline`, which is used to connect the model to a data source. If you are using the `spacy` model and you do not want to use `pipeline`, then you should use [`spaCy.pipe_model()`](https://spython.readthedocs.io/en/stable/generated/spytext_pipemodel.html).\n`pipe` is only used with the [spam](https:/spm.org/models/index.py#spambase) and [sphinx](http://www.sagemaker.com/tensorflow/suggest.htm) models.\nI've added some comments to the code below. There are a few things to note. You are not supposed to load spaCY models in this way. So I'm not 11O% sure what you're trying do. But it's not the way to do it. I think you need to start with this [example](//sppytext/tutorials/basics/1_preprocessing.rst). I don't think the pipeline is doing what it is supposed. It should be a list of objects", 0.048192769289016724, 0.012195121496915817, 0.048192769289016724, 0.022216567769646645], ["I have python 3.7.0 on windows 11 using vscode. I pip installed tensorflow and keras but when I tried to import them it gave me an error and said cannot import name OrderedDict\nTried uninstalling and reinstalling both tf and keras. Didn\u2019t work\nError Message:\nTraceback (most recent call last):\nFile \"c:/Users/Jai K/CS Stuff/test.py\", line 1, in \nimport tensorflow\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_init_.py\", line 37, in \nfrom tensorflow.python.tools import module_util as module_util\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python_init.py\", line 42, in \nfrom tensorflow.python import data\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data_init_.py\", line 21, in \nfrom tensorflow.python.data import experimental\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental_init_.py\", line 96, in \nfrom tensorflow.python.data.experimental import service\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service_init_.py\", line 419, in \nfrom tensorflow.python.data.experimental.ops.data_service_ops import distribute\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 25, in \nfrom tensorflow.python.data.ops import dataset_ops\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 29, in \nfrom tensorflow.python.data.ops import iterator_ops\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 34, in \nfrom tensorflow.python.training.saver import BaseSaverBuilder\nne 32, in     from tensorflow.python.checkpoint import checkpoint_management\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint_init_.py\", line 3, in     from tensorflow.python.checkpoint import checkpoint_view\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint_view.py\", line 19, in     from tensorflow.python.checkpoint import trackable_view\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\checkpoint\\trackable_view.py\", line 20, in     from tensorflow.python.trackable import converter\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\trackable\\converter.py\", line 18, in     from tensorflow.python.eager.polymorphic_function import saved_model_utils\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\saved_model_utils.py\", line 36, in     from tensorflow.python.trackable import resource\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\trackable\\resource.py\", line 22, in     from tensorflow.python.eager import def_function\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 20, in     from tensorflow.python.eager.polymorphic_function.polymorphic_function import set_dynamic_variable_creation\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 76, in     from tensorflow.python.eager.polymorphic_function import function_spec as function_spec_lib\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_spec.py\", line 25, in     from tensorflow.core.function.polymorphism import function_type as function_type_lib\nFile \"C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py\", line 19, in     from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple, OrderedDict\nImportError: cannot import name 'OrderedDict' from 'typing' (C:\\Users\\Jai K\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)", "so OrderedDict is from collections which should be on your pc anyway. it seems like some of python's dependencies are not on your system path. you should double-check check you have everything that needs to be there. I have anaconda\\scripts there\nif that fails:\ntry and pip install it (collections) anyway. then try and uninstall Tensorflow and Keras and everything related and then reinstall.\nfrom experience, I can tell you a lot of times this is something you need to do when modifying your Tensorflow installation since the resolver is just horrendous\nif it still doesn't work try to get a specific version of Tensorflow that is more stable.\nI hope this helps :)", "Cannot import tensorflow or keras: ordered dict", 0.0, 0, 812, "The reason is that tensorflow is using an older version of python. The version you are using is `35.1.2`.\nThe newer version is  `Python 8.3`, and this is the reason for the error.\n\\begin{itemize}\n \\item Open Anaconda Prompt (https://docs.anaconda.com/anacondacourses/content/python/install/windows/conda_prompt_win.html)\nand change the version to 7\n(e.g. `conda install --name \"python\" -y python=36.8`).\nIf you want to install the latest version, use\n`conda create -n \"env_name\" python`\nas the last command. (e g `cona create --n \u201cenv name\u201d python`)\nThen use that env in your code. Change the Python version back to\n38 and the problem should be solved. If not, let me know. :)\nUse the new version. Then, you can use the environment in\nyour code without problems. This might take some time, so please\nbe patient. You can always use other versions of Python that\nare not 64-bit. They are", 0.2939068078994751, 0.036101084202528, 0.10752688348293304, 0.05150976404547691], ["I came across an issue that while I was able to resolve, I believe would benefit this platform. I will therefore pose the question here and answer it. When attempting to publish an app on binder, you are required to create a Requirements.txt file that outlines your dependencies. Mine was using pandas version 1.4.4.\nWhen attempting to launch binder using my github repo, I was getting:\nERROR: No matching distribution found for pandas==1.4.4", "Reading into the error further, it seems that binder only goes up to a certain version of pandas. If you read carefully it will list your pandas version option. Choose the latest one from that error list, and update your requirements.\nAlhamdulilah!", "Binder - ERROR: No matching distribution found for pandas==1.X.X", -0.3799489736557007, -2, 813, "This can happen if you don't have the `requirements` package installed, so try adding: `pip install requirements` to your terminal to install it and then try again.", 0.22857142984867096, 0.029411764815449715, 0.11428571492433548, 0.06521739065647125], ["EDIT: (User error, I wasn't scanning entire dataframe. Delete Question if needed )A page I found had a solution that claimed to drop all rows with NAN in a selected column. In this case I am interested in the column with index 78 (int, not string, I checked).\nThe code fragment they provided turns out to look like this for me:\ndf4=df_transposed.dropna(subset=[78])\nThat did exactly the opposite of what I wanted. df4 is a dataframe that has NAN in all elements of the dataframe. I'm not sure how to\nI tried the dropna() method as suggested on half a dozen pages and I expected a dataframe with no NAN values in the column with index 78. Instead every element was NAN in the dataframe.", "df_transposed.dropna(subset=[78], in place=True) #returns dataframe with rows that have missing values in column 78 removed.", "How do I drop all rows in a DataFrame that have NAN in that row, in a specified column?", 0.0, 0, 814, "\\begin{code}\nimport pandas as pd\nd = {'a': [1, 2, np.nan, None, 'foo', np, True], 'b':[np.NaN,\n1.0,2.1,'bar',100,'hi','hello']} # df\n#print(d)\nprint (d['a'])\n\\end{pre}", 0.072727270424366, 0.0, 0.036363635212183, 0.027268104255199432], ["I am very beginner to Linux as I recently started using it. I installed different libraries like numpy, pandas etc.\nimport numpy as np\nimport pandas as pd\nIt raises a ModuleNotFoundError in VS Code. But when I run the same code in Terminal, there's no issue.\nNote: I installed these libraries with\npip3 install package\nOS: Ubuntu 22.04\nI tried to uninstall the package and reinstall but still not working. I also tried to install by\nsudo apt-get install python3-pandas.\nNothing works out.", "It seems that you have two or more interpreter of python.\nYou can use shortcuts \"Ctrl+Shift+P\" and type \"Python: Select Interpreter\" to choose the correct python interpreter in VsCode.", "import in python working on my Linux Terminal but raising ModuleNotFoundError on VS code", 0.0, 0, 815, "In linux you need to use full path of the module. In windows you can use relative path and the installation will be done automatically. Linux is different in this aspect. For example if I want to import numpy I will use this:\n`import sys`\n\\begin{code}\nsys.path.append('/usr/local/lib/python3.8/site-packages')\nfrom numpy import *\nprint('numpy imported')`enter code here``\\\nThis will work.", 0.18000000715255737, 0.040816325694322586, 0.1599999964237213, 0.06406819075345993], ["I am very beginner to Linux as I recently started using it. I installed different libraries like numpy, pandas etc.\nimport numpy as np\nimport pandas as pd\nIt raises a ModuleNotFoundError in VS Code. But when I run the same code in Terminal, there's no issue.\nNote: I installed these libraries with\npip3 install package\nOS: Ubuntu 22.04\nI tried to uninstall the package and reinstall but still not working. I also tried to install by\nsudo apt-get install python3-pandas.\nNothing works out.", "Without all the context, it sounds like you have a few different python environments.\nIn terminal check which python you are using which python\nIn VSCode settings check Python: Default Interpreter Path\nThat might help you understand what is going on.  Make sure that the VSCode python path is the same path that your terminal prints out.", "import in python working on my Linux Terminal but raising ModuleNotFoundError on VS code", 0.0996679961681366, 1, 816, "You can also install these packages using `sudo` by giving the path of the library file. For example:\n`sudo -H pip3 instal numpy`", 0.125, 0.0, 0.125, 0.046294283121824265], ["I'm trying to create a model that predicts customer status change.\nTo give context, there are 4 statuses a customer can have: [A, B, C, D]\nEach customer must have one status, and that status can change. I'm making a model with the current status as one of the features and the next status as the label.\nIs there a way to hardcode a rule into SVM (or other classifiers) that prevents the model from classifying the label as the current status? In other words, if a customer's current status is A, its next status cannot be A, it has to be either B, C, or D.\nIf anyone knows whether sklearn has similar capabilities that would help.", "As far as I know, there are two ways to solve this problem but it is not inside an SVM.\nFirst Way - series\nImplementing a rule-based classifier first then applying SVM...\nSecond way - Parallel\nImplementing a rule-based classifier and SVM parallel and choosing the best one in the end layer combining together.\ne.x Ensemble learning\nboth ways probably work in some cases, but you should try and see the results to choose the best way I guess the second one might work better.", "scikit-learn adding rules to classification model", 0.0, 0, 817, "There are a few ways you can do this, but the easiest would be to have a \"current\" and \"next\" status and to make them a feature (let' s say you'll call them \"status1\"  and  \" status2\").\nThen, you just need to set the status1 value of a sample to a given value and train on this new data. After training, the sample will have the new label, so if it' not the value you set as status 2, then the \"rule\" you wanted is fulfilled.", 0.27906978130340576, 0.0117647061124444, 0.1627907007932663, 0.05425229296088219], ["I have a numpy array r and I need to evaluate a scalar function, let's say np.sqrt(1-x**2) on each element x of this array. However, I want to return the value of the function as zero, whenever x>1, and the value of the function on x otherwise.\nThe final result should be a numpy array of scalars.\nHow could I write this the most pythonic way?", "You can use like numpy.where(condition,if condition holds,otherwise) so np.where(x>1,0,np.sqrt(1-x**2)) will be answer", "Evaluate scalar function on numpy array with conditionals", 1.2000000476837158, 2, 818, "You can use the `np.where` function. Something like:\n`np_out = np_in[npwhere(np> 1)(np-1) + npwhere(!np > 0)(1)]`\nIn my example, it would be: np=np[ np>2]-np=2[1]\nIf you want the output in a list, then you can simply do: `x=x[x>0]`\nOr if you don't want any list at all, but just a single value, just change the last line to: x= np[0][x]", 0.29411765933036804, 0.05999999865889549, 0.19607843458652496, 0.12128925323486328], ["I am working on a learning how to fill in NaN in a Python DataFrame. DataFrame called data containing an age column and only one row has an NaN. I applied the following:\ndata.fillna(data.mean(),inplace=True)\nI ask to print out data and I receive a recursion msg.\nMy DataFrame only contains 4 rows if that is important.\nI was expecting the DataFrame to come back with the NaN filled in with the mean value. I also tried replacing data.mean() with a number ex. 2.  Same error message.", "Not sure if this was the correct thing todo or not but I cleared out the Kernel in Jupyter Notebook and ran it just fine.", "Python DataFrame .fillna() Recursion Error", 0.0, 0, 819, "You cannot use `in-place` as you need to be able to see the result of `fill()` on the dataframe before it is changed. The following works as intended: `data = data[~np.isnan(df.age)]`\nand then `df = df.loc[np_missing_indices, :]`\nor even `pd.DataFrame(np[1:4],columns=['age']) = np[5:9]` to reassign the whole dataframe.", 0.1411764770746231, 0.0, 0.07058823853731155, 0.04081632196903229], ["So, I have been trying to find optimum solution for the question, but I can not find a solution which is less than o(n3).\nThe problem statemnt is :-\nfind total number of triplet in an array such that sum of a[i],a[j],a[k] is divisible by a given number d and i<j<k.\nI have tried a multiple solutions but the solutions all reached o(n3). I need a solution that could be less than o(n3)", "The key here is to think about the modulus operator. Each number n in the list can be expressed as n = (x*d) + y, where y = n % d.\nFor any 3 integers x, y, z, (x + y + z) will be divisible by d if and only if (x%d + y%d + z%d) % d = 0.\nYou can bucket all numbers in the list based their remainder (ie. n%d)\nYou will have d buckets (ranging from 0 to d-1).\nGenerate all possible triplets using integers in range [0, d-1] that add up to 0, d or 2*d. This will give you the bucket combinations that can be used to obtain a valid triplet.\nSince you know the number of elements in each bucket, you can calculate the number of valid triplets. (for example, if bucket 0 has 10 elements, the triplet (0,0,0) will have 10*9*8 corresponding triplets).\n\nThis algorithm should be enough to set you on track to complete the problem. Leaving out the implementation and other minor details for the reader.", "Find total combinations of length 3 such that sum is divisible by a given number and i<j<k", 0.0996679961681366, 1, 820, "I think you want to know how many \\em{total} combinations there are. There are not too many solutions. For instance, if you have $n$ elements, you can have any one of the $i$'s be $0$, so there'll be $(n-1)!$ combinations. And since the sum must be divisble by $d$, that must occur for exactly $a$ of those combinations, so it'd be a combination of that form $(0,0,\\ldots, 0) + (0\\ldots 1,1\\cdots 2,2\\cdot 5,5\\dots n) +\\cdots$ so the number would be\n$$(a-2)!(n-(a+1))! + 4! (n - 6)!$$\nor for $1<a<n/2$\n\\begin{align*}\n(2a -3)(2n -(2k+2)) +2(k-a) (2^{k} -2). \\tag{1}\n\\\\\na \\le k \\lt n/3\n. \\end{aligned} \\quad \\text{and} \\\\\n200 (3n)^2", 0.31111112236976624, 0.03194888308644295, 0.13333334028720856, 0.03646833077073097], ["Good day. I used to have a normally working code which exports styled dataframe as a PNG. For some reason now it doesn't work except for certain machines used by my fellow coworkers. I suspect iit is somehow relevant to the latest windows or Chrome updates but I am not sure.\nSample code:\nimport numpy as np\nimport pandas as pd\nimport dataframe_image as dfi\nmy_array = np.array([[11,22,33],[44,55,66]])\ndf = pd.DataFrame(my_array, columns = ['Column_A','Column_B','Column_C'])\ndf=df.style.set_properties(**{'background-color': 'black',\n'color': 'white'})\ndisplay(df)\ndfi.export(df, 'Test.png', table_conversion='chrome')\nReceived error:\nTraceback (most recent call last):\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\nexec(code_obj, self.user_global_ns, self.user_ns)\nFile \"\", line 13, in \ndfi.export(df, 'Test.png', table_conversion='chrome')\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\dataframe_image_pandas_accessor.py\", line 24, in export\ndpi=None\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\dataframe_image_pandas_accessor.py\", line 73, in _export\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\dataframe_image_screenshot.py\", line 167, in run\nmax_crop = int(img.shape[1] * 0.15)\nFile \"\", line 40, in take_screenshot_override\nimg = mimage.imread(buffer)\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\matplotlib\\image.py\", line 1541, in imread\nwith img_open(fname) as image:\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\", line 121, in init\nself._open()\nFile \"C:\\Users\\Anato\\anaconda3\\lib\\site-packages\\PIL\\PngImagePlugin.py\", line 677, in _open\nraise SyntaxError(\"not a PNG file\")\nFile \"\", line unknown\nSyntaxError: not a PNG file\nSearched the web and found no answer that could help. Tried udating packages and python itself. I believe it has to do with latest system updates but found no solution for over a week.", "In my case the following worked:\n\nUpdate windows to the latest version\nUpdate conda", "Python dataframe_image results in SyntaxError: not a PNG file", 0.0, 0, 821, "It turned out that it was not the code but the python version that was buggy. The error message suggested it.", 0.17142857611179352, 0.0, 0.17142857611179352, 0.04343468323349953], ["I recently installed Python on my work computer and I am having SO MANY issues with the packages, I can't use any of them.\nRunning simple matplotlib, numpy, or pandas code gives me the below error.\n\nINTEL MKL ERROR: The specified module could not be found. mkl_intel_thread.2.dll.\nIntel MKL FATAL ERROR: Cannot load mkl_intel_thread.2.dll.\n\nHere are the versions of the installed packages.\nNumpy: 1.23.2 , Pandas: 1.4.4 , Matplotlib: 3.5.3 , Python: 3.10.6\nWhen I attempt to update any of the with \"pip install numpy --upgrade\" it tells me that the requirement is already satisfied. Then, when I try to install with \"pip install numpy --upgrade --ignore-installed\" it tells me that it could not find a version that satisfies the requirement for numpy and no matching distribution for numpy.\nAnything helps\nThanks", "Numpy and other scientific libraries internally rely on certain numeric libraries (BLAS, LAPACK) for which there's a highly optimized version from Intel, which your python packages apparently cannot find (that's where that error around the dll comes from. These dlls aren't part of the python packages themselves.\nYou could look into a) installing the Intel MKL from scratch and see if that works or b) check if they're there and if you're missing a setting around a library path, some environment variable maybe.\nIf I may ask, how are you installing Python? On Windows in particular (I assume you're on windows because of the dll error message...) I'd recommend using Anaconda to install python. With such a package manager you might be able to avoid such dependency / path issues.", "INTEL MKL ERROR when attempting to use Packages", 0.0, 0, 822, "There is an issue with Intel Mkl on Ubuntu 21 and above. If you follow the link here https://github.com/numpy/scipy/issues/15977 and follow instructions from there, it should fix your problem. You can try running the following commands:\n\\begin{itemize}\n    \\item sudo apt install libicu-dev\n        \\strong{sudo apt remove libmkl-intl-ic} \\em{ (You should not do this, but I did just to see if it would work. After running this command, reinstalling libiconv and libintll-devel worked. It did not work the first time.)}  \\n\n      \\begin {code} [sudo]\n          \\end { code}    sudo add-apt-repository ppa:ubuntu-toolchain-r/test\n       \\textbf{  sudo update-alternatives --config python3}   \\textit{\n         \\sout {sudo rm -f /usr/lib/x86_64-linux-gnu/libbz2* \\} (This is not needed. This was my first attempt at fixing it.) }\n           \\i", 0.19117647409439087, 0.02222222276031971, 0.08088235557079315, 0.022953178733587265], ["when using pip install pandas\nAn error occurs as follows:\nCollecting pandas\nUsing cached pandas-1.5.2.tar.gz (5.2 MB)\nInstalling build dependencies ... done\nGetting requirements to build wheel ... error\nerror: subprocess-exited-with-error\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> [28 lines of output]\nTraceback (most recent call last):\nFile \"d:\\py\\lib\\site-packages\\pip_vendor\\pep517\\in_process_in_process.py\", line 351, in \nmain()\nFile \"d:\\py\\lib\\site-packages\\pip_vendor\\pep517\\in_process_in_process.py\", line 333, in main\njson_out['return_val'] = hook(**hook_input['kwargs'])\nFile \"d:\\py\\lib\\site-packages\\pip_vendor\\pep517\\in_process_in_process.py\", line 112, in get_requires_for_build_wheel\nbackend = _build_backend()\nFile \"d:\\py\\lib\\site-packages\\pip_vendor\\pep517\\in_process_in_process.py\", line 77, in build_backend\nobj = import_module(mod_path)\nFile \"d:\\py\\lib\\importlib_init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 1030, in _gcd_import\nFile \"\", line 1007, in _find_and_load\nFile \"\", line 972, in _find_and_load_unlocked\nFile \"\", line 228, in _call_with_frames_removed\nFile \"\", line 1030, in _gcd_import\nFile \"\", line 1007, in _find_and_load\nFile \"\", line 986, in _find_and_load_unlocked\nFile \"\", line 680, in _load_unlocked\nFile \"\", line 790, in exec_module\nFile \"\", line 228, in call_with_frames_removed\nFile \"C:\\Users\\zijie\\AppData\\Local\\Temp\\pip-build-env-kqsd82rz\\overlay\\Lib\\site-packages\\setuptools_init.py\", line 18, in \nfrom setuptools.dist import Distribution\nFile \"C:\\Users\\zijie\\AppData\\Local\\Temp\\pip-build-env-kqsd82rz\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 47, in \nfrom . import _entry_points\nFile \"C:\\Users\\zijie\\AppData\\Local\\Temp\\pip-build-env-kqsd82rz\\overlay\\Lib\\site-packages\\setuptools_entry_points.py\", line 43, in \ndef validate(eps: metadata.EntryPoints):\nAttributeError: module 'importlib.metadata' has no attribute 'EntryPoints'\n[end of output]\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\nnote: This error originates from a subprocess, and is likely not a problem with pip.\npy:3.10.0\nos:windows11\nDoes anyone know how to solve the problem? Thanks!\nI tried several times but it doesn't work.", "Have you tried:\npip3 install pandas?", "Cannot use pip install pandas", 0.0, 0, 823, "I found the error comes from a module called \"import-module\" in the site-package of pip. I think it should be removed.", 0.0, 0.0, 0.0, 0.03218479081988335], ["I am facing below error message while running the python code(ML model) in the python databricks notebook\nConnectException: Connection refused (Connection refused) Error while obtaining a new communication channel\nConnectException error: This is often caused by an OOM error that causes the connection to the Python REPL to be closed. Check your query's memory usage.\nSpark tip settings", "The driver may be experiencing a memory bottleneck, which is a frequent cause of this issue. When this occurs, the driver has an out of memory (OOM) crash, restarts often, or loses responsiveness. Any of the following factors might be the memory bottleneck's cause:\n\nFor the load placed on the driver, the driver instance type is not ideal.\nMemory-intensive procedures are carried out on the driver.\nThe same cluster is hosting a large number of concurrent notebooks or processes.\n\nPlease try below options\n\nTry increasing driver-side memory and then retry.\nYou can look at the spark job dag which give you more info on data flow.", "ConnectException: Connection refused (Connection refused) Error while obtaining a new communication channel. error in databricks notebook", 0.0, 0, 824, "It might be related to memory issues. You can check the memory limit in your cluster:\n<a href=\"https://i.stack.imgur.com/4Fy5R.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nIn this case you can set the number of executors and cores for your spark job and increase the heap size.", 0.25, 0.06329113990068436, 0.15000000596046448, 0.05106382444500923], ["I am taking intro to ML on Coursera offered by Duke, which I recommend if you are interested in ML. The instructors of this course explained that \"We typically include nonlinearities between layers of a neural network.There's a number of reasons to do so.For one, without anything nonlinear between them, successive linear transforms (fully connected layers) collapse into a single linear transform, which means the model isn't any more expressive than a single layer. On the other hand, intermediate nonlinearities prevent this collapse, allowing neural networks to approximate more complex functions.\" I am curious that, if I apply ReLU, aren't we losing information since ReLU is transforming every negative value to 0? Then how is this transformation more expressive than that without ReLU?\nIn Multilayer Perceptron, I tried to run MLP on MNIST dataset without a ReLU transformation, and it seems that the performance didn't change much (92% with ReLU and 90% without ReLU). But still, I am curious why this tranformation gives us more information rather than lose information.", "Neural networks are inspired by the structure of brain. Neurons in the brain transmit information between different areas of the brain by using electrical impulses and chemical signals. Some signals are strong and some are not. Neurons with weak signals are not activated.\nNeural networks work in the same fashion. Some input features have weak and some have strong signals. These depend on the features. If they are weak, the related neurons aren't activated and don't transmit the information forward. We know that some features or inputs aren't crucial players in contributing to the label. For the same reason, we don't bother with feature engineering in neural networks. The model takes care of it. Thus, activation functions help here and tell the model which neurons and how much information they should transmit.", "Why ReLU function after every layer in CNN?", 0.0, 0, 825, "As you correctly pointed out, the ReLu activation function will result in a linear transformation of the input data. In this way, it can be interpreted as a shortcut. If we look at a fully connected layer, where the number k of nodes is large, then the network is able to learn the transformation by doing the matrix multiplication and backpropagation. However, in order to compute the output, a large number (k) of additions must be performed. This will take time and also memory (if the k is very large).\nIf you look for a network with k nodes, but the same accuracy, you can use a layer with a smaller number n of hidden nodes. Then, this layer will not be able for performing the full matrix multiplications, therefore, an activation functions (like ReLeu) must exist to transform the data and make it possible for the additive layer to perform its task.", 0.2283737063407898, 0.02787456475198269, 0.15224912762641907, 0.05658996105194092], ["I am taking intro to ML on Coursera offered by Duke, which I recommend if you are interested in ML. The instructors of this course explained that \"We typically include nonlinearities between layers of a neural network.There's a number of reasons to do so.For one, without anything nonlinear between them, successive linear transforms (fully connected layers) collapse into a single linear transform, which means the model isn't any more expressive than a single layer. On the other hand, intermediate nonlinearities prevent this collapse, allowing neural networks to approximate more complex functions.\" I am curious that, if I apply ReLU, aren't we losing information since ReLU is transforming every negative value to 0? Then how is this transformation more expressive than that without ReLU?\nIn Multilayer Perceptron, I tried to run MLP on MNIST dataset without a ReLU transformation, and it seems that the performance didn't change much (92% with ReLU and 90% without ReLU). But still, I am curious why this tranformation gives us more information rather than lose information.", "the first point is that without nonlinearities, such as the ReLU function, in a neural network, the network is limited to performing linear combinations of the input. In other words, the network can only learn linear relationships between the input and output. This means that the network can't approximate complex functions that are not linear, such as polynomials or non-linear equations.\nConsider a simple example where the task is to classify a 2D data point as belonging to one of two classes based on its coordinates (x, y). A linear classifier, such as a single-layer perceptron, can only draw a straight line to separate the two classes. However, if the data points are not linearly separable, a linear classifier will not be able to classify them accurately. A nonlinear classifier, such as a multi-layer perceptron with a nonlinear activation function, can draw a curved decision boundary and separate the two classes more accurately.\nReLU function increases the complexity of the neural network by introducing non-linearity, which allows the network to learn more complex representations of the data. The ReLU function is defined as f(x) = max(0, x), which sets all negative values to zero. By setting all negative values to zero, the ReLU function creates multiple linear regions in the network, which allows the network to represent more complex functions.\nFor example, suppose you have a neural network with two layers, where the first layer has a linear activation function and the second layer has a ReLU activation function. The first layer can only perform a linear transformation on the input, while the second layer can perform a non-linear transformation. By having a non-linear function in the second layer, the network can learn more complex representations of the data.\nIn the case of your experiment, it's normal that the performance did not change much when you removed the ReLU function, because the dataset and the problem you were trying to solve might not be complex enough to require a ReLU function. In other words, a linear model might be sufficient for that problem, but for more complex problems, ReLU can be a critical component to achieve good performance.\nIt's also important to note that ReLU is not the only function to introduce non-linearity and other non-linear activation functions such as sigmoid and tanh could be used as well. The choice of activation function depends on the problem and dataset you are working with.", "Why ReLU function after every layer in CNN?", 0.0996679961681366, 1, 826, "The key to understand the idea behind ReLu is to know how the non-linearity is introduced in the first place. I have already answered this [question](https://datascience.stackexchange.com/questions/65210/why-do-we-use-relu-function-for-neural-networks) and this answer should give you a good intuition. But if not, let me explain it here again. A Re-Lyapunov equation is a function of the form:\n$$\nf(x) = \\frac{1}{2}x^Ty-\\psi(u)\n\\tag{*}\n.$$\nwhere $u$ is the output of your neural net. It is interesting to note that for the Re Lu function, the function $f$ in $(*)$ is just the square of $x$.\nThe Re Lyapounov Equation is one of many function used for non linearities in neural nets. One of them is: $g(y)=1/(1+exp(-y))$. This is not exactly the same as Re Luke. However, it is still close enough to have similar properties. In both cases, they both are monotonically", 0.2668977379798889, 0.052173912525177, 0.13864818215370178, 0.050615597516298294], ["I am using rain as an intrumental variable, so I need to pull hisotry probablity of rain given location and time to each row.\nPrefer python since I clean most of my data on python.\n\n\n\n\nCounty\nState\nDate\nRain\n\n\n\n\nFulton\nSC\n2019-1-1\n?\n\n\nChatham\nGA\n2017-9-3\n?\n\n\n\n\nProbably looking for some python library and code to find the date and create the column.\nAny help would be appreciated! Thank you!", "The obvious answer is a probability in historical / observed datasets does not exist. The probability is derived from probabilistic weather forecasts. When the weather went through, you can say if there was rain or not, means 1 or 0.\nBut from a data science perspective there can be alternative to that. E.g. you can build up a similarity tree or an Analog Ensemble to determine probability for rain on certain weather patterns.\nBut you need more information about the weather and weather regime.\nAt the your information will be independent from the date. The probability information will be a function on the day of year  e.g.", "How to return historical probability of rain given location and date", 0.0, 0, 827, "You can use pandas to generate a pandas dataframe from your existing dataframe, using the following code:\n\\begin{itemize}\n  \\item Create a dataframe with your data\n    \\begin {code}", 0.13235294818878174, 0.014925372786819935, 0.10294117778539658, 0.07091173529624939], ["In Kmeans clustering we can define number of cluster. But is it possible to define that cluster_1 will contain 20% data, cluster_2 will have 30% and cluster_3 will have rest of the data points?\nI try to do it by python but couldn't.", "Using K-means clustering, as you said we specify the number of clusters but it's not actually possible to specify the percentage of data points. I would recommend using Fuzzy-C if you want to specify a exact percentage of data points  alloted for each cluster", "How to manipulate cluster data point of Kmeans clustering algorithm", 0.0, 0, 828, "You can add a constraint in k-meangans to prevent clusters to be too small. However, the algorithm will stop at the first iteration if there is no way to satisfy your constraints. So you can use the following to achieve what you want:\n\\begin{itemize}\n \\item Set the minimum number k of clusters (default is 1)\n  \\em{example: k=2}  (if you don' t set this parameter, then kmeangs will find the number automatically)  set the maximum number  kmax of iterations to 0 (it means that you will use all the iterations) set a parameter to_clusters to the total number\n of data-points (in your example 50) and set  a_tolerance to a small number like 99.9% (for 40 data you'll have two clusters with 70 and 6 points each) for the example of your question the code would be:   \\begin {code} k = \n3;\n   k_max =0; to-cluets =51; a-tol =9\n   9; nd_list= [1,2,3,", 0.22857142984867096, 0.06730769574642181, 0.16190476715564728, 0.0697643980383873], ["I am trying to import sklearn library by writing code like from sklearn.preprocessing import MinMaxScaler but it kept showing same error.\nI tried uninstalling and reinstalling but no change. Command prompt is also giving same error. Recently I installed some python libraries but that never affected my enviroment.\nI also tried running the code in jupyter notebook. When I tried to import numpy like import numpy as np, it ran successfully. So the problem is only with sklearn.\nAlso, I have worked with sklearn before but have never seen such an error.", "You have to read into the error message. For me sklearn was importing something from scipy which uses the outdated np.int, so updating scipy solved the issue for me.", "ImportError: cannot import name 'int' from 'numpy'", 0.0, 0, 829, "The problem was with my environment. I had to remove all the installed python packages and then install again. Then it worked.", 0.1538461595773697, 0.0, 0.11538461595773697, 0.08571428060531616], ["I am trying to import sklearn library by writing code like from sklearn.preprocessing import MinMaxScaler but it kept showing same error.\nI tried uninstalling and reinstalling but no change. Command prompt is also giving same error. Recently I installed some python libraries but that never affected my enviroment.\nI also tried running the code in jupyter notebook. When I tried to import numpy like import numpy as np, it ran successfully. So the problem is only with sklearn.\nAlso, I have worked with sklearn before but have never seen such an error.", "Run pip3 install --upgrade scipy\nOR upgrade whatever tool that tried to import np.int and failed\nnp.int is same as normal int of python and scipy was outdated for me", "ImportError: cannot import name 'int' from 'numpy'", 0.1973753273487091, 2, 830, "The problem with MinmaxScalar was in the version 0.23.0, which was released a few days ago. The bug was fixed by this commit:\nhttps://github.com/scikit-learn/sklearn-dev/commit/b9a77124d89100f35a594a1c2516952e58f7f43b\nSo you need to update the package to  version >= 1.1, like this: `pip install -U scikit_learn`", 0.0714285746216774, 0.0, 0.02380952425301075, 0.049295779317617416], ["I have a dataset with 15 different meteorological stations (providing T, rh, wind direction through time).\nHow should I implement them in a machine learning model? As independent inputs or can I combine them?\nIf you could provide me with some references or hints to start this project, that would very helpful !\nI have so far cleaned the data and separate each meteorological station.\nI believe that I should try to perform a single prediction on each station and then combine the prediction of each station together ?", "There are different ways to implement multiple meteorological stations in a machine learning model depending on the specific problem you are trying to solve and the characteristics of the data. Here are a few options to consider:\n\nIndependent models: One option is to train a separate model for each meteorological station, using the data for that station as input. This approach is useful if the stations have different characteristics or if you want to make predictions for each station independently.\n\nCombined model: Another option is to combine the data from all stations and train a single model to make predictions for all of them at once. This approach is useful if the stations are similar and the relationship between the input variables and the output variable is the same across all stations.\n\nMulti-task learning: You can also consider using multi-task learning, where you train a single model to perform multiple tasks, one for each meteorological station. This approach is useful if the stations are similar but have different characteristics and you want to make predictions for all of them at once.\n\n\nRegarding how to combine the predictions, it depends on the problem you are trying to solve. If you want to make a prediction for each station independently you don't need to combine the predictions. But if you want to make a prediction for all the stations you can use an ensemble method like a majority vote or a weighted average to combine the predictions.\nYou can find more information about these approaches and examples of their implementation in papers and tutorials about multi-task learning, multi-output regression and ensemble methods.\nAlso, it might be helpful to explore the correlation between the meteorological stations. You can use the correlation matrix and heatmap to explore the correlation between the different meteorological stations. If they are highly correlated you can combine them in a single model, otherwise, you can consider them as independent inputs.", "how to input the data of several meteorological stations into a machine learning model?", 1.2000000476837158, 1, 831, "If the weather station is the same over time, it could be used as an independent input variable. If weather stations have different observation times, you need to add a variable that indicates the time when the observation was made. It could also be possible to combine several weather variables and use a weighted average.", 0.1784776896238327, 0.05277044698596001, 0.11548556387424469, 0.03676243871450424], ["I'm trying to find unique combinations of ~70,000 IDs.\nI'm currently doing an itertools.combinations([list name], 2) to get unique 2 ID combinations but it's been running for more than 800 minutes.\nIs there a faster way to do this?\nI tried converting the IDs into a matrix where the IDs are both the index and the columns and populating the matrix using itertools.product.\nI tried doing it the manual way with loops too.\nBut after more than a full day of letting them run, none of my methods have actually finished running.\nFor additional information, I'm storing these into a data frame, to later run a function that compares each of the unique set of IDs.", "(70_000 ** 69_000) / 2== 2.4 billion - it is not such a large number as to be not computable in a few hours (update I run a dry-run on itertools.product(range(70000), 2) and it took less than 70 seconds, on a 2017 era i7 @3GHz, naively using a single core) But if you are trying to keep this data in memory at once, them it won't fit - and if your system is configured to swap memory to disk before erroring with a MemoryError, this may slow-down the program by  2 or more orders of magnitude, and thus, that is when your problem come from.\nitertools.combination does the right thing in this respect, and no need to try to change it for something else: it will yield one combination at a time. What you are doing with the result, however, do change things: if you are streaming the combination to a file and not keeping it in memory, it should be fine, and then, it is just computational time you can't speed up anyway.\nIf, on the other hand, you are collecting the combinations to a list or other data structure: there is your problem - don't do it.\nNow. going a step further than your question, since these combinations are check-able and predictable, maybe trying to generate these is not the right approach at all - you don't give details on how these are to be used, but if used in a reactive form, or on a lazy form, you might have an instantaneous workflow instead.", "More optimized way to do itertools.combinations", 0.0996679961681366, 1, 832, "You need a generator. Combination with `itertools` will stop after the first combination. But you want all combinations. So use `combo` from `more_iter Tools`\n\\begin{itemize}\n`combs = list(combi(range(7),2))`\nwill generate all 36 combinations\nAnd you can use a list comprehension to generate them\nlist(map(lambda x: [x[0],x[-1]], combs))`will get you all the combinations as a one-line statement\nand you could do something like `map(' '.join, comb)` to join them all in a string\nif you need to process them in the generator\nlike\ncomma-separated lists\nyou could use the `csv` module\nor just split it into lists with a loop\nbut with the map you will get them comma-separeted\n[more information](https://docs.python.org/3/library/itertool.html)\nThe [more python documentation](http://more-iterators.readthedocs.io/en/latest/api.pdf) is also a good resource\nfor iterators\nNote that a `generator` does not", 0.283687949180603, 0.04275534301996231, 0.12293144315481186, 0.037516169250011444], ["I am creating a medical web app that takes in audio input, converts it to text, and extracts keywords from the said text file, which is then used in an ML model. We have the text, but the problem lies in the fact that the person might say, I have pain in my chest and legs but the symptoms in our model are chest_pain or leg_pain.\nHow do we convert the different phrasing used by the user to one that matches our model features? Our basic approach would be using tokenizer and then using NLTK to check synonyms of each word and map pairs to try out multiple phrasings to match the one we currently have, but it would take one too much time.\nIs it possible to do this task using basic NLP?", "maybe an improvment of your first idea :\n\nSplit your keywords (chest_pain \u2192 [\"chest\",\"pain\"]\nFind only synonyms of your keywords ([[\"chest\",\"thorax\",...],[\"pain\",\"suffer\",...]]\nFor each words of your sentence check if the word is present in your keywords synonyms.", "Converting a phrase into one being used by the machine leaning model", 0.0, 0, 833, "\\strong{TL;DR:} You need to use some sort of model that represents the relationships between words and phrases. This model is called a \\em{semantic model} or \\textit{lexical semantic model}. The word-phrase-to-word mapping you want is not a linguistic or lexical task, it is a semantic task. You can use NER, POS, or any other NLU model to solve this. The way you get the mapping is by annotating the dataset with a set of entity tags and a lexicon (or dictionary) of the words/phrases you are interested in. For example, you could have a sentence:\n\\begin{itemize}\nThe pain is in [past_tense_verbs] and [negative_adjectives]\n(The syntax is just an example and I'm not sure if you would use this)\nYou then map the tags to the word/token and use the lexicons to get a mapping from words to phrase. To learn more about this, check out the [wikipedia page on semantic models](https://en.wikipedia.org/wiki/Semantic_model).\n<a href=\"https:www.sem", 0.12962962687015533, 0.009345794096589088, 0.06481481343507767, 0.030835794284939766], ["I have multiple excel files with different columns and some of them have same columns with additional data added as additional columns. I created a masterfile which contain all the column headers from each excel file and now I want to export data from individual excel files into the masterfile. Ideally, each row representing all the information about one single item.\nI tried merging and concatenating the files, it adds all the data as new rows so, now I have some columns with repeated data but they also contain additional data in different columns.\nWhat I want now is to recognize the columns that are already present and fill in the new data instead of repeating the all columns using python. I cannot share the data or the code so, looking for some help or idea to get this done. Any help would be appreciated, Thanks in advance!", "You are probably merging the wrong way.\nNot sure about your masterfile, sounds not very intuitive.\nMake sure your rows have a specific ID that identifies it.\nThen always perform the merge with that id and the 'inner' merge type.", "Merging multiple excel files into a master file using python with out any repeated values", 0.0, 0, 834, "You can use `pandas.read_excel()` to import your excel sheet and `pd.merge()` method to combine your dataframes. You could create a list of all excel sheets you want combine then use the `concat()` function. This should be the basic idea. See [here](https://pandas-docs.github.io/pandas/reference/api/generated/pd.__init__.html#pandas.__getitem__) for more info.", 0.1782178282737732, 0.0, 0.13861386477947235, 0.06806282699108124], ["I kept getting error: ImportError: this version of pandas is incompatible with numpy < 1.20.3 your numpy version is 1.18.5. Please upgrade numpy to >= 1.20.3 to use this pandas version\nAnd I'm unable to update, remove, and reinstall numpy.\nI tried pip install numpy and pip3 install numpy and kept getting this error: AttributeError: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'\nEdit: I tried pip install --force-reinstall numpy and pip install numpy==1.20.3 and still got AttributeError: module 'lib' has no attribute 'X509_V_FLAG_CB_ISSUER_CHECK'", "Try this.\npip install --force-reinstall numpy\nand then\npip install numpy==1.20.3", "Version of pandas is incompatible with numpy < 1.20.3", 0.0, 0, 835, "The problem is that you are using python2.7. I believe the error you're getting is because of this package: https://pypi.org/project/pyopenssl/\nIf you update your python version to 3.8 (or higher) the issue should go away. If you can't update python, you could try to find a different version (maybe 2 or 5) of pyopensl, which is compatible with python 64bit.", 0.07228915393352509, 0.0, 0.048192769289016724, 0.014260513707995415], ["error: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\nerror: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'", "I solved this problem by changing the version of opencv.I am using opencv-python==4.5.3.56.Maybe you can try.", "Opencv Issue while running the code on jupyter", 0.0, 0, 836, "I encountered this error message when I installed Open CV 4+ in a virtual machine (VM) on Windows 10.\nIn my case, the problem was caused by the fact that my VM didn't have a GUI installed. To fix this problem, I had to run the following commands in the command prompt:\n\\begin{itemize}\n    \\item cd \"C:\\Program Files\\OpenCV-4415\"\n      (Replace \"Open CV-xxx\" with your Open CV version number)\n   \\strong{* If your Windows is in \"Latin-1\" encoding, you should use \"cp -i\" instead of \"cd\" in step 3} (I'm not sure why, but it works for me) (for example: \"c:\\Windows\\system32>cp /D OpenCv4-7-x64\\build\\win3.8\\Debug\\libOpenCVCam.dll c:\\Opencv\")\n        \\em{I don'tknow if it's necessary to do this, so I'll keep it here.}\n     \\begin {code}", 0.16049382090568542, 0.012500000186264515, 0.1111111119389534, 0.053073324263095856], ["I am looking for a technique/method/algorithm which will be able to handle time-dependent data (each sample has 20 time steps, but for the most part they occur unevenly between samples, i.e., one sample may have a value at 0.4 seconds while another sample might not). The value itself of the time step corresponds to a categorical position on the body (ranging from 1-20) where the muscle activiation occured.\nSo the data resembles, (time, position):\n(0.1, 16)\n(0.16, 1)\n(0.25, 13)\n(0.26, 12)\n(0.27, 1)\n(0.4, 4)\nIs there a clustering algorithm which will be able to work for this type of data. I would like the algorithm to consider the time dependency of the data. Dynamic time warping is not suitable for unevenly spaced time series data and I am not sure how it would handle the sparse categorical data I have, e.g. a given position will only appear once per sample.\nAny suggestions or help is appreciated.\nI have looked through lots of different models, but none so far work with their given assumptions. Hidden markov models are out of the question (need stochastic time steps), DTW does not work for unevenly spaced time steps, and techniques like Lomb-Scargle do not work for categorical data especially not-periodic categorical data. Fast-fourier transform is also off the table.", "One method you can use for clustering this type of time-dependent data is a Hidden Markov Model (HMM). HMMs can model the dependencies between the positions and the time steps, allowing for the clustering of similar patterns in the data. Another alternative is a Gaussian Mixture Model (GMM), where you can model the position and time values as multivariate Gaussian distributions, and use Expectation-Maximization (EM) to estimate the parameters of the distributions. Both HMMs and GMMs have been used in various time-series analysis and clustering tasks, and both have Python implementations available through popular libraries such as scikit-learn and hmmlearn.\nIt is recommended to try out both algorithms and compare the results to see which one performs better for your specific dataset. You can also experiment with different features and preprocessing techniques, such as interpolation or downsampling, to see if it improves the performance of the clustering algorithm.", "Unsupervised clustering algorithm for unevenly spaced sequential categorical data?", 0.0, 0, 837, "If you have only 30 data points (3 samples of 6 data), you can use K-means clustrer. Here is a link for Kmean in R http://www.rdocumentation.org/packages/stats/topics/kmeansequential. And here is the link http:\nhttp://statistics.stackexchange.com/questions/5091/whats-the-difference-between-k-nearest-neighbors-and-clustering-algorithms-like-ka", 0.20000000298023224, 0.03846153989434242, 0.12380952388048172, 0.0833333283662796], ["ChatGPT's point of view is : \"It's generally recommended to upgrade to the latest version of Numpy step by step, rather than directly from version 1.19 to 1.24. This is because newer versions of Numpy may have breaking changes, so by upgrading incrementally, you can minimize the risk of encountering compatibility issues and ensure a smoother transition.\"", "From my personal point of view, the gap between the versions of NumPy is not so big. I would create a new virtual environment and install the desired version of NumPy. Then by running the code, either you will get Runtime errors for unsupported and no longer existing functions or everything will run just smoothly.\nIn case you have errors, you can try searching them online to find the required fix.\nIn case you have no errors at all, I would still try to come up with a Test script that testes some of the basic functionalities that are used and could break through out the code, you could try copying some values and hardcode set them to see the behaviour.\nThe above, would apply to any kind of package. If you still feel you need to go step by step, feel free.", "I need to migrate python numpy code from version 1.19 to 1.24, is it better to migrate directly or step by step (first 1.20, then 1.21 etc.)?", 1.2000000476837158, 1, 838, "As stated in [Chatgpt' s answer](https://chatgp.com/post/23922/i-need-to-migrate-python-numpy-code-from-version-1-2-3-0-a-6-4-8-7-9-b-e-5-c-d-f-h-g-i), it is safer to do the migration step-by-step.", 0.1666666716337204, 0.031578946858644485, 0.09375, 0.02717391401529312], ["I have a dataframe with three columns: timestamp, variable_name and value. There is a total of 10 variables whose names are in the variable_name column.\nI would like to have a single dataframe, indexed by timestamp, with one column per variable. Ideally, the dataframe should be \"full\", i.e. each timestamp should have an interpolate value for each variable.\nI'm struggling to find a direct way to do that (without looping over the variable list, etc.). The dataframe comes from Spark but is small enough to be converted to Pandas. Any pointers will be most welcome.", "Something like\n\ndf.loc[:, ['timestamp','variable_name','values']].pivot(index='timestamp',columns='variable_name')\n\nshould do the trick", "Dataframe timestamp interpolation for multiple variables", 0.0, 0, 839, "You can try `rolling` with `Window.partitionBy` and `lag`\nLike this :\n`df.withColumn(\"lagged\", lag('value', 2).over(Window().partitionby('timestamp'))).drop('lag').agg({\"value\":\"lag_2\",\"timestamp\":\"timestamp\"})`", 0.1702127605676651, 0.0, 0.1702127605676651, 0.039661671966314316], ["I am unsure if this kind of question (related to PCA) is acceptable here or not.\nHowever, it is suggested to do MEAN CENTER before PCA, as known. In fact, I have 2 different classes (Each different class has different participants.). My aim is to distinguish and classify those 2 classes. Still, I am not sure about MEAN CENTER that should be applied to the whole data set, or to each class.\nIs it better to make it separately? (if it is, should PREPROCESSING STEPS also be separately as well?) or does it not make any sense?", "PCA is more or less per definition a SVD with centering of the data.\nDepending on the implementation (if you use a PCA from a library) the centering is applied automatically e.g. sklearn - because as said it has to be centered by definition.\nSo for sklearn you do not need this preprocessing step and in general you apply it over your whole data.\nPCA is unsupervised can be used to find a representation that is more meaningful and representative for you classes afterwards. So you need all your samples in the same feature space via the same PCA.\n\nIn short: You do the PCA once and over your whole (training) data and must be center over your whole (traning) data. Libraries like sklarn do the centering automatically.", "Mean centering before PCA", 0.06656807661056519, 1, 840, "It's better practice to center your data before running PLS. There are many reasons why it'd be a good idea, but one of them is that it can reduce the dimensionality of the data. It sounds like your first step is PCS. So, you need to decide whether you want to apply the mean centring on the entire dataset or on each individual class first. If you do it on individual classes, then you can use the same centred variables for all classes in PCLS. However, if you mean centre on your entire data, your centres will not be the centroids of each of your classes - they will be centers of all your variables. That is why you should probably do mean-centering on an individual-class level.", 0.40310078859329224, 0.078125, 0.1782945692539215, 0.058928199112415314], ["I have a tensor with shape torch.Size([3, 224, 225]). when I do tensor.mean([1,2]) I get tensor([0.6893, 0.5840, 0.4741]). What does [1,2] mean here?", "Operations that aggregate along dimensions like min,max,mean,sum, etc. specify the dimension along which to aggregate. It is common to use these operations across every dimension (i.e. get the mean for the entire tensor) or a single dimension (i.e. torch.mean(dim = 2) or torch.mean(2) returns the mean of the 225 elements for each of 3 x 224 vectors.\nPytorch also allows these operations across a set of multiple dimensions, such as in your case. This means to take the mean of the 224 x 224 elements for each of the indices along the 0th (non-aggregated dimension). Likewise, if your original tensor shape was a.shape = torch.Size([3,224,10,225]), a.mean([1,3]) would return a tensor of shape [3,10].", "What does [1,2] means in .mean([1,2]) for tensor?", 0.0996679961681366, 1, 841, "This is a common error for people new to `Tensor`s. The issue is that you are not using a Tensor correctly.\nA Tensors holds values. When you call `tensor.shape` it will return a shape. A shape is not a value. What you need to do is call the function `torch` uses to access a single value in the tensor: `x[1]`.\nYou can also use `tens[i]` if you want to get the ith value from the Tense.", 0.3412322402000427, 0.057416267693042755, 0.12322274595499039, 0.0522562637925148], ["I have a tensor with shape torch.Size([3, 224, 225]). when I do tensor.mean([1,2]) I get tensor([0.6893, 0.5840, 0.4741]). What does [1,2] mean here?", "The shape of your tensor is 3 across dimension 0, 224 across dimension 1 and 225 across dimension 2.\nI would say that tensor.mean([1,2]) calculates the mean across dimension 1 as well as dimension 2. Thats why you are getting 3 values. Each plane spanned by dimension 1 and 2 of size 224x225 is reduced to a single value / scalar. Since there are 3 planes that are spanned by dimension 1 and 2 of size 224x225 you get 3 values back. Each value represents the mean of a whole plane with 224x225 values.", "What does [1,2] means in .mean([1,2]) for tensor?", 0.0, 0, 842, "It means you are taking the mean of the slice of your tensor of size (3x2x3), along the 1st dimension and  the second dimension.\nSo you'll get a 3-dimensional tensor, and you can check the shape of it:\ntensor.shape(tensor[1:2,:,:]): (2 x 64 x3)\nIt's the same as tensor[:,1::2].\nTo be more explicit, you did: torchslice(torch[3], 4) -> torcheslice([torchslicerange(3:4),:],:).", 0.38323354721069336, 0.1090909093618393, 0.20359280705451965, 0.04731861129403114], ["Is there a way to install a lighter version of opencv on pi pico? If not, is there a way to install opencv library on an SD card and make pico to fetch those library files from that SD card?\nI am trying to record video using a OV7670 camera module and save the video it to SD card. Later I need to upload the video to a custom AWS server. I have the modules to capture images in micropython but can not find any modules or libraries to record video.", "No. OpenCV is a huge library, with many moving parts. Even if there is a minimal version of OpenCV, I highly doubt that the 2MB of flash memory of the RP2040 will be enough for your use case. Coupling this alongside the limited number of cores, internal RAM, etc. of the CPU, you will probably end up with nothing. From what I know, you can use TinyML with MicroPython.", "Is there a way to install a lighter version of opencv?", 0.1973753273487091, 1, 843, "I am not sure if this will work for you. It is a link to an open source python project that seems to work on the RPi Pico (it is based on OpenCV but a lot of the code is stripped out).\nhttps://github.com/MirkoSchultze/py_opencv\nYou could try to adapt it for your needs. There is also a Github project which looks like it is intended to be used with the Pimatic (which is very cool and has a bunch of stuff you can use for home automation). I think that is the better solution for what you are trying do. This one is more for a general purpose. https://www.pi-pi.org/\nComment: It did not work. My problem was not with open cv. Open cv is working fine. But I was trying record videos using the openCV libraries. And I found out that openCv is not working on micropyhone and I also tried installing open CV 3.2 library using micro python. Then it installed but I still got the error that could not open the camera. So I changed the libraries of camera to 1.4 and that worked.", 0.2339622676372528, 0.05323193967342377, 0.11320754885673523, 0.026576735079288483], ["`I'm using in edge impulse FOMO\nI know that object detection fps is 1/inference time\nmy model's time per inference is 2ms\nso object detection is 500fps\nbut my model run vscode fps is 9.5\nwhat is the difference between object detection fps and video fps ?", "If I understand correctly, your object detection fps indicates the number of frames (or images) that your model, given your system, can process in a second.\nA video fps in your input source's frames per second. For example, if your video has an fps (also referred to as framerate) of 100, then your model would be able to detect objects in all of the frames in 100ms (or 1/10 of a second).\nIn your case, your video input source seems to have 9.5 frames in a second. This means that your model, given your system, will process 1-second wort of a video in about ~20ms.", "what is the difference between object detection fps and video fps?", 1.2000000476837158, 0, 844, "If you mean the f/ps of the video stream you're recording, then that'll be 30/60 (depending on which video format you use).\nIf you want to know the frame rate of a video you recorded, you can check the `f_codec` property of an [VideoElement](https://docs.microsoft.com/en-us/node/842154/npm/module-videoelement) (a wrapper for the [`MediaRecorder`](https:/en/docs/mediarecorder/api) interface).", 0.21111111342906952, 0.033707864582538605, 0.14444445073604584, 0.04637680947780609], ["Ok so to preface this, I am very new to jupyter notebook and anaconda. Anyways I need to download opencv to use in my notebook but every time I download I keep getting a NameError saying that \u2018cv2\u2019 is not defined.\nI have uninstalled and installed opencv many times and in many different ways and I keep getting the same error. I saw on another post that open cv is not in my python path or something like that\u2026\nHow do I fix this issue and put open cv in the path? (I use Mac btw) Please help :( Thank you!", "Try the following:\n\nInstall OpenCV using Anaconda Navigator or via terminal by running:\nconda install -c conda-forge opencv\nNow you should check if its installed by running this in terminal: conda list\nImport OpenCV in Jupyter Notebook: In your Jupyter Notebook, run import cv2 and see if it works.\nIf the above steps are not working, you should add OpenCV to your Python PATH by writing the following code to your Jupyter NB:\nimport sys\nsys.path.append('/anaconda3/lib/python3.7/site-packages')\n\nThis should work.", "Anaconda Jupyter Notebook Opencv not working", 0.0, 0, 845, "You need openCV installed in Anacondas env, which you can do by running the following command:\n`conda install -c conda-forge open-cv`\n(Note, you will need Anacona to install open CV)\nAnd then it should work.", 0.3199999928474426, 0.11382114142179489, 0.20800000429153442, 0.04022921621799469], ["Is there a way to save Polars DataFrame into a database, MS SQL for example?\nConnectorX library doesn\u2019t seem to have that option.", "Polars exposes the write_database method on the DataFrame class.", "Polars DataFrame save to sql", 0.2913126051425934, 3, 846, "You can use the [spark-sql](https://github.com/databricks/spark) package, which is a Spark SQL connector for Spark. The Spark connecctor is based on the Spark JDBC driver, so it can be used for many different databases.", 0.12244898080825806, 0.042553190141916275, 0.12244898080825806, 0.04999999701976776], ["Is there a way to save Polars DataFrame into a database, MS SQL for example?\nConnectorX library doesn\u2019t seem to have that option.", "Polars doesen't support direct writing to a database. You can proceed in two ways:\n\nExport the DataFrame in an intermediate format (such as .csv using .write_csv()), then import it into the database.\nProcess it in memory: you can convert the DataFrame in a simpler data structure using .to_dicts(). The result will be a list of dictionaries, each of them containing a row in key/value format. At this point is easy to insert them into a database using SqlAlchemy or any specific library for your database of choice.", "Polars DataFrame save to sql", 1.2000000476837158, 2, 847, "You can use this sample\nhttps://github.com/polars-database/converter/blob/master/examples/sql/main.scala\nto save the data to SQL.", 0.15789473056793213, 0.01785714365541935, 0.10526315867900848, 0.06779660284519196], ["In Numpy, Transposing of a column vector makes the the array an embedded array.\nFor example, transposing\n[[1.],[2.],[3.]] gives [[1., 2., 3.]] and the dimension of the outermost array is 1. And this produces many errors in my code. Is there a way to produce [1., 2., 3.] directly?", "Try .flatten(), .ravel(), .reshape(-1), .squeeze().", "Python NumPy, remove unnecessary brackets", 0.0996679961681366, 1, 848, "It is the opposite. In the example you give, the transposed array has dimension 4, i.e. [4]\n[1,2,3] is a 0-d array, [3,4], a dimensionless array and [2] a singleton array of 6. I have never seen the problem you mention, but I assume it has something to do with the fact that the number of dimensions is often not what it seems. The outer array does not have 9 dimensions.", 0.024390242993831635, 0.0, 0.024390242993831635, 0.04213964566588402], ["I have a mat file with sparse data for around 7000 images with 512x512 dimensions stored in a flattened format (so rows of 262144) and I\u2019m using scipy\u2019s loadmat method to turn this sparse information into a Compressed Sparse Column format. The data inside of these images is a smaller image that\u2019s usually around 25x25 pixels somewhere inside of the 512x512 region , though the actual size of the smaller image is not consitant and changes for each image. I want to get the sparse information from this format and turn it into a numpy array with only the data in the smaller image; so if I have an image that\u2019s 512x512 but there\u2019s a circle in a 20x20 area in the center I want to just get the 20x20 area with the circle and not get the rest of the 512x512 image. I know that I can use .A to turn the image into a non-sparse format and get a 512x512 numpy array, but this option isn\u2019t ideal for my RAM.\nIs there a way to extract the smaller images stored in a sparse format without turning the sparse data into dense data?\nI tried to turn the sparse data into dense data, reshape it into a 512x512 image, and then I wrote a program to find the top, bottom, left, and right edges of the image by checking for the first occurrence of data from the top, bottom, left, and right but this whole processes seemed horribly inefficient.", "Sorry about the little amount of information I provided; I ended up figuring it out.Scipy's loadmat function when used to extract sparse data from a mat file returns a csc_matrix, which I then converted to numpy's compressed sparse column format. Numpy's format has a method .nonzero() that will return the index of every non_zero element in that matrix. I then reshaped the numpy csc matrix into 512x512, and then used .nonzero() to get the non-zero elements in 2D, then used used those indexes to figure out the max height and width of my image I was interested in. Then I created a numpy matrix of zeros the size of the image I wanted, and set the elements in that numpy matrix to the elements to the pixels I wanted by indexing into my numpy csc matrix (after I called .tocsr() on it)", "Numpy Extract Data from Compressed Sparse Column Format", 0.0, 0, 849, "You can extract data by using the `sparsetools.extract_triples` function. This function uses the [sparser](https://docs.scipy.org/doc/scikit-image/reference/generated/skimage.sif.Sparse.html#skimagest.SIF.SPARSE) object to do the extraction.", 0.12903225421905518, 0.0, 0.07526881992816925, 0.06726456433534622], ["As an example, I can cross validation when I do hyperparameter tuning (GridSearchCV). I can select the best estimator from there and do RFECV. and I can perform cross validate again. But this is a time-consuming task. I'm new to data science and still learning things. Can an expert help me lean how to use these things properly in machine learning model building?\nI have time series data. I'm trying to do hyperparameter tuning and cross validation in a prediction model. But it is taking a long time run. I need to learn the most efficient way to do these things during the model building process.", "Cross-validation is a tool in order to evaluate model performance. Specifically avoid over-fitting. When we put all the data in training side, your Model will get over-fitting by ignoring generalisation of the data.\nThe concept of turning parameter should not based on cross-validation because hyper-parameter should be changed based on model performance, for example the depth of tree in a tree algorithm\u2026.\nWhen you do a 10-fold cv, you will be similar to training 10 model, of cause it will have time cost. You could tune the hyper-parameter based on the cv result as cv-> model is a result of the model. However it does not make sense when putting the tuning and do cv to check again because the parameter already optimised based on the first model result.\nP.s. if you are new to data science, you could learn something call regularization/dimension reduction to lower the dimension of your data in order to reduce time cost.", "How to do the cross validation properly?", 0.0, 0, 850, "I don't know if this will answer your question, but one thing to keep in mind is that the validation dataset should always be different from the training dataset, and you should do it in such a way that you don\u2019t overfit your model to the data in your training set.", 0.24657534062862396, 0.0276497695595026, 0.14611871540546417, 0.05771718546748161], ["I have a script that modifies a pandas dataframe with several concurrent functions (asyncio coroutines). Each function adds rows to the dataframe and it's important that the functions all share the same list. However, when I add a row with pd.concat a new copy of the dataframe is created. I can tell because each dataframe now has a different memory location as given by id().\nAs a result the functions are no longer share the same object. How can I keep all functions pointed at a common dataframe object?\nNote that this issue doesn't arise when I use the append method, but that is being deprecated.", "pandas dataframes are efficient because they use contiguous memory blocks, frequently of fundamental types like int and float. You can't just add a row because the dataframe doesn't own the next bit of memory it would have to expand into. Concatenation usually requires that new memory is allocated and data is copied. Once that happens, referrers to the original dataframe\nIf you know the final size you want, you can preallocate and fill. Otherwise, you are better off keeping a list of new dataframes and concatenating them all at once. Since these are parallel procedures, they aren't dependent on each others output, so this may be a feasable option.", "Pandas dataframe sharing between functions isn't working", 0.0, 0, 851, "There are a few things to check. If you are using Python 3.7 or later you can use `dataframe.memory_usage()` to see how much memory your dataframe takes. `append` is a much more efficient way to add data to a dataframe than `concat`. This is because `pd.append(df, [...])` only adds a single row to your df and `df = df.concatenate([...]` creates a copy and does not take advantage of pandas' efficient in-memory merge. Also, you might want to use a `with` statement to ensure that all the code in your function is executed. Finally, as @Bradley says, `dask.delayed` will also help with sharing data.", 0.3076923191547394, 0.027397260069847107, 0.13574661314487457, 0.05975903943181038], ["We are developing a prediction model using deepchem's GCNModel.\nModel learning and performance verification proceeded without problems, but it was confirmed that a lot of time was spent on prediction.\nWe are trying to predict a total of 1 million data, and the parameters used are as follows.\nmodel = GCNModel(n_tasks=1, mode='regression', number_atom_features=32, learning_rate=0.0001, dropout=0.2, batch_size=32, device=device, model_dir=model_path)\nI changed the batch size to improve the performance, and it was confirmed that the time was faster when the value was decreased than when the value was increased.\nAll models had the same GPU memory usage.\nFrom common sense I know, it is estimated that the larger the batch size, the faster it will be. But can you tell me why it works in reverse?\nWe would be grateful if you could also let us know how we can further improve the prediction time.", "There are two components regarding the speed:\n\nYour batch size and model size\nYour CPU/GPU power in spawning and processing batches\n\nAnd two of them need to be balanced. For example, if your model finishes prediction of this batch, but the next batch is not yet spawned, you will notice a drop in GPU utilization for a brief moment. Sadly there is no inner metrics that directly tell you this balance - try using time.time() to benchmark your model's prediction as well as the dataloader speed.\nHowever, I don't think that's worth the effort, so you can keep decreasing the batch size up to the point there is no improvement - that's where to stop.", "In deep learning, can the prediction speed increase as the batch size decreases?", 0.0, 0, 852, "\\begin{itemize}\n\\item The number of operations (training epochs) is the most critical factor, not the number or batchsize. The batch-size will affect the memory, which is not important for the accuracy, because the model is trained for 20 times. So the best batch is 384. And the training time is linear to the epoch number. You can try with a smaller batch and see how it affects the speed. For example, you can use 64 or 80 epoches and compare the results. As for a larger batch, 512 or more, there is a trade-off between memory and speed, so you should consider it as well. If the result is good enough with the small batch (360 seconds, or even faster), then you do not need to increase it. Otherwise, try larger and larger values, compare and decide which one is better for your application. This is why the default batch value is set to 4096 (which is about 0,1s per epoche).\n2. I am not sure why a small value makes a difference, as far as I understand the problem, a large value (and thus, more batch", 0.3032258152961731, 0.08441558480262756, 0.1548387110233307, 0.050751522183418274], ["When I try to read a xlsx file using pandas, I receive the error \"numpy has no float attribute\", but I'm not using numpy in my code, I get this error when using the code below\ninfo = pd.read_excel(path_info)\nThe xlsx file I'm using has just some letters inside of it for test purpouses, there's no numbers or floats.\nWhat I want to know is how can I solve that bug or error.\nI tried to create different files, change my info type to specify a pd.dataframe too\nPython Version 3.11\nPandas Version 1.5.3", "Had the same problem. Fixed it by updating openpyxl to latest version.", "Numpy has no float attribute error when using Read_Excel", 0.0, 0, 853, "You can use this code:\n`import numpy as np`\nor `import pandas as pd` to fix this issue.", 0.06666667014360428, 0.0, 0.06666667014360428, 0.06770274043083191], ["I have recently attempted to install pandas through pip. It appears to go through the process of installing pandas and all dependencies properly. After I update to the latest version through cmd as well and everything appears to work; typing in pip show pandas gives back information as expected with the pandas version showing as 1.5.3\nHowever, it appears that when attempting to import pandas to a project in PyCharm (I am wondering if this is where the issue lies) it gives an error stating that it can't be found. I looked through the folders to make sure the paths were correct and that pip didn't install pandas anywhere odd; it did not.\nI uninstalled python and installed the latest version; before proceeding I would like to know if there is any reason this issue has presented itself. I looked into installing Anaconda instead but that is only compatible with python version 3.9 or 3.1 where as I am using the newest version, 3.11.2", "When this happens to me\n\nI reload the environment variables by running the command\nsource ~/.bashrc\nright in the pycharm terminal.\n\nI make sure the I have activated the correct venv (where the package installations go) by cd to path_with_venv then running\nsource ~/pathtovenv/venv/bin/activate\n\nIf that does not work, hit CMD+, to open your project settings and and under Python Interpreter select the one with the venv that you have activated. Also check if pandas appears on the list of packages that appear below the selected interpreter, if not you may search for it and install it using this way and not the pip install way", "pip install of pandas", 0.0, 0, 854, "This is not a pip problem. Pandas is a python package. You need to update the python interpreter in your PyCharms project settings.", 0.20895522832870483, 0.03030303120613098, 0.10447761416435242, 0.055045876652002335], ["there are many ways about normalize skils for ml and dl. It is known to provide only normalization for 0 to 1.\nI want to know that is some ways to normalize -1 between 1.", "You can use the min-max scalar or the z-score normalization here is what u can do in sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nor hard code it like this\nx_scaled = (x - min(x)) / (max(x) - min(x)) * 2 - 1 -> this one for minmaxscaler\nx_scaled = (x - mean(x)) / std(x) -> this one for standardscaler", "Normalize -1 ~ 1", 1.2000000476837158, 1, 855, "You can transform the data to range [0,1] before performing any further operations. This can be done using [linear scaling](http://en.wikipedia.org/wiki/Data_scaling) with the formula\n$$\nx = \\frac{1}{1 + e^{-0.5\\cdot\\frac{(x - 2)}{10}}}\n\\tag{2}\n.$$\nFor example, the function\n[sklearn.preprocessing.MinMaxScaler](https://scikit-learn-contrib.github.io/skl-data-science/en/stable/generated/scikits.learn_contrib.*.minmax_scale.html?highlight=MinmaxScaling#sk-min-max-scaler)\nfrom [sci-kit learn](scipy.stats.distributions.Normal) provides this functionality. You may also use a [sigmoid function](ftp://ftp.cs.fsu.edu/pub/graphics/misc/sig_m.txt) as suggested by [Andrew T.](//stackoverflow.com/users/1934530/andrew-t).\n<a href=\"https.//i", 0.20879121124744415, 0.04444444552063942, 0.10989011079072952, 0.06191949546337128], ["there are many ways about normalize skils for ml and dl. It is known to provide only normalization for 0 to 1.\nI want to know that is some ways to normalize -1 between 1.", "Yes, there are ways to normalize data to the range between -1 and 1. One common method is called Min-Max normalization. It works by transforming the data to a new range, such that the minimum value is mapped to -1 and the maximum value is mapped to 1. The formula for this normalization is:\nx_norm = (x - x_min) / (x_max - x_min) * 2 - 1\nWhere x_norm is the normalized value, x is the original value, x_min is the minimum value in the data and x_max is the maximum value in the data.\nAnother method for normalizing data to the range between -1 and 1 is called Z-score normalization, also known as standard score normalization. This method normalizes the data by subtracting the mean and dividing by the standard deviation. The formula for this normalization is:\nx_norm = (x - mean) / standard deviation\nWhere x_norm is the normalized value, x is the original value, mean is the mean of the data and standard deviation is the standard deviation of the data.", "Normalize -1 ~ 1", 0.0, 0, 856, "There is no need for normalizing values below 2.5 or above 7.499. As such, you can simply subtract the minimum and maximum values and divide by 6.022e23. For example, the average value for a value in the range 32768 to -33780 is -29395.728752625, and it's range is 8.97903815707369, so the normalized value is:\n\\begin{itemize}\n  \\item subtract 5 from both minimum value and range value\n    =>  -892 and 914.6\n  2\\item divide range by minimum\n     =>  967\n3\\ subtract minimum from normal value => -742\n4\\ divide normal by maximum =>0.213\n5\\ multiply by range =>24630\n6\\ add  min to the result =>-208\n7\\ result is (-2,456,2).\nNormalized to (-1, 4). This is the only solution. I believe", 0.3455149531364441, 0.08026755601167679, 0.17940199375152588, 0.02692888304591179], ["there are many ways about normalize skils for ml and dl. It is known to provide only normalization for 0 to 1.\nI want to know that is some ways to normalize -1 between 1.", "Consider re-scale the normalized value. e.g. normalize to 0..1, then multiply by 2 and minus 1 to have the value fall into the range of -1..1", "Normalize -1 ~ 1", 0.0996679961681366, 2, 857, "It depends on what you mean by normalizing a value. The `normalize` function is used to scale a given set of values into a set that fits into an interval. This can be used as a way to create a standardized measure. For example, if you have a data set where some of the values are in the thousands and others in millions, you can scale it to make all of them equal, so that you are comparing apples to appples. Or, say you want a new set to be scaled to fit into the interval from 200-40, in which case, the normalized data will be in that range. There is no such thing as normalising -ve numbers to positive values.", 0.19736842811107635, 0.02666666731238365, 0.1184210553765297, 0.025858134031295776], ["I am trying to load data into a custom NER model using spacy, I am getting an error:-\n'RobertaTokenizerFast' object has no attribute '_in_target_context_manager'\nhowever, it works fine with the other models.\nThank you for your time!!", "I faced the same issue after upgrading my environment from {Python 3.9 + Spacy 3.3} to {Python 3.10 + Space 3.5}. Resolved this by upgrading and re-packaging the model.", "'RobertaTokenizerFast' object has no attribute '_in_target_context_manager' error while loading data into custom NER model", 0.0, 0, 858, "This looks like a bug in the spaCy package. You should [report it](https://github.com/explosion/spaCY/issues) on GitHub.", 0.11320754885673523, 0.0, 0.07547169923782349, 0.03773584961891174], ["I have a problem (that I think I'm over complicating) but for the life of me I can't seem to solve it.\nI have 2 dataframes. One containing a list of items with quantities that I want to buy. I have another dataframe with a list of suppliers, unit cost and quantity of items available. Along with this I have a dataframe with shipping cost for each supplier.\nI want to find the optimal way to break up my order among the suppliers to minimise costs.\nSome added points:\n\nSuppliers won't always be able to fulfil the full order of an item so I want to also be able to split an individual item among suppliers if it is cheaper\nShipping only gets added once per supplier (2 items from a supplier means I still only pay shipping once for that supplier)\n\nI have seen people mention cvxpy for a similar problem but I'm struggling to find a way to use it for my problem (never used it before).\nSome advice would be great.\nNote: You don't have to write all the code for me but giving a bit of guidance on how to break down the problem would be great.\nTIA", "Some advice too large for a comment:\nAs @Erwin Kalvelagen alludes to, this problem can be described as a math program, which is probably the most common-sense approach.\nThe generalized plan of attack is to figure out how to create an expression of the problem using some modeling package and then turn that problem over to a solver engine which uses diverse techniques to find the optimal answer.\ncvxpy is certainly 1 of the options to do the first part with.  I'm partial to pyomo, and pulp is also viable.  pulp also installs with a solver (cbc) which is suitable for this type of problem.  In other cases, you may need to install separately.\nIf you take this approach, look through a text or some online examples on how to formulate a MIP (mixed integer program).  You'll have some sets (perhaps items, suppliers, etc.), data that form constraints or limits, some variables indexed by the sets, and an objective....likely to minimize cost.\nForget about the complexities of split-orders and combined shipping at first and just see if you can get something working with toy data, then build out from there.", "How would I go about finding the optimal way to split up an order", 0.0, 0, 859, "This is not a one-liner, but it should be doable. There are many constraints to take into account: each of the items can only be ordered from one of your supplying companies, so that you do not end up with multiple items coming from the same suppler. The minimum quantity should not be too low (otherwise you'll end with an empty basket, which you probably don`t want). And if an additional item is ordered, the shippig cost needs to be calculated correctly, taking into consideration that the order will not exceed the total number of ordered items. And so on...\nIn short: you need to formulate your problem as a MIP model (Mixed-Integer Programming), using a solver like Gurobi or Cplex.", 0.3499999940395355, 0.050314463675022125, 0.14374999701976776, 0.05096660554409027], ["I have a numpy array with a shape of (3, 4096). However, I need it's shape to be (4096, 3). How do I accomplish this?", "Use:\narr=arr.T\n(or)\narr=np.transpose(arr)\n(or)\narr= arr.reshape(4096, 3)\nwhere arr is your array with shape (3,4096)", "How to reverse the shape of a numpy array", 0.0, 0, 860, "If you want to change the `shape` of an array, you can do:\n`arr.reshape(3)`\nIn your case, it should be: `arr = arr.T`", 0.36734694242477417, 0.12765957415103912, 0.16326530277729034, 0.039499446749687195], ["I am trying to solve the differential equation 4(y')^3-y'=1/x^2 in python. I am familiar with the use of odeint to solve coupled ODEs and linear ODEs, but can't find much guidance on nonlinear ODEs such as the one I'm grappling with.\nAttempted to use odeint and scipy but can't seem to implement properly\nAny thoughts are much appreciated\nNB: y is a function of x", "The problem is that you get 3 valid solutions for the direction at each point of the phase space (including double roots). But each selection criterion breaks down at double roots.\nOne way is to use a DAE solver (which does not exist in scipy) on the system y'=v, 4v^3-v=x^-2\nThe second way is to take the derivative of the equation to get an explicit second-order ODE y''=-2/x^3/(12*y'^2-1).\nBoth methods require the selection of the initial direction from the 3 roots of the cubic at the initial point.", "Solving nonlinear differential equations in python", 0.1973753273487091, 1, 861, "This is not a python question but rather a math question. But since I have some experience with python I can help you with that. In this case you do not have to do anything to convert your differential eqution into a set of O.D.E.s. If you are familiar to oveflow graphs you can use these to plot the solution of the equation. This will allow you to find the function you need. Since you have a 3rd order equation I assume that you want the solutions to be of 2nd order. The general form of a second order Ode is:\n$$y'' + p(x)y'+q(r) y = 0$$\nwhere the constants $p$ and $q$ are called the coefficients. As you know the coefficient of $y'$ is 1, therefore the second one will be 6. Then you will have the following: $6y''' + 5py''+3qy'-3py'+5q^'y=0$. You can now see that there is an inconsistency, because the first term in the sum of products is the same as $4y^''$ which has a constant coefficient, meaning that $py$ should be", 0.3017543852329254, 0.04240282624959946, 0.16842105984687805, 0.047944340854883194], ["I have a dataset which contains the longitude and latitude of the 1000 largest US cities. I'm designing an API which returns the user's nearest city, given an input of the user's longitude/latitude.\nWhat is the most efficient algorithm I can use to calculate the nearest city? I know that I can use the haversine formula to calculate the distance between the user's coordinate and each cities, but it seems inefficient to have to do this for all 1000 cities. I've previously used a k-d tree to solve nearest neighbour problems on a plane - is there a similar solution that can be used in the context of a globe?\nEdit: keeping this simple - distance I'm looking for is as the crow flies. Not taking roads or routes into account at this stage.", "You can split the map into squares that do not overlap and they cover the whole US map (i.e., you will have a grid). You will number the squares using the coordinates of their upper left corner (i.e., each one will have a unique ID) and you will do a preprocessing where each city will be assigned with the ID of the square where it belongs. You will find the square where the user lies into and then you will check only the cities that lie into this square and the ones that are one step from this (total: 9 squares). If these are empty of cities, you will check the ones that are two steps of it etc. In this way, on average you will check much less cities to find the closest", "What algorithm would be most efficient when trying to find the nearest city given a set of coordinates?", 0.049958374351263046, 1, 862, "You can calculate great circle distances between all pairs of points, and then use a [binary search](https://en.wikipedia.org/wiki/Binary_search_algorithm) to select the closest. The [Haversin](http://www.movable-type.co.uk/scripts/haversines.html) formulae can give you the distances in one calculation. If you have the results as a matrix, then the binary search will be very fast.", 0.2189054787158966, 0.04020100459456444, 0.13930347561836243, 0.0773809552192688], ["I have a dataset which contains the longitude and latitude of the 1000 largest US cities. I'm designing an API which returns the user's nearest city, given an input of the user's longitude/latitude.\nWhat is the most efficient algorithm I can use to calculate the nearest city? I know that I can use the haversine formula to calculate the distance between the user's coordinate and each cities, but it seems inefficient to have to do this for all 1000 cities. I've previously used a k-d tree to solve nearest neighbour problems on a plane - is there a similar solution that can be used in the context of a globe?\nEdit: keeping this simple - distance I'm looking for is as the crow flies. Not taking roads or routes into account at this stage.", "This answer is very similar to that of ckc.\nFirst, spilt the 1000 cities in 2 groups : a big one located located between Canada and Mexico and the few others cities located outside this rectangle (i.e Alaska, Hawai, ...).\nWhen processing coordinates, check if they belong to the small group : in this case, no optimisation needed.\nTo optimize the other case, you may divide the map in rectangles (example 5\u00b0lat x 7\u00b0 lon) and associate to each rectangle the list of cities belonging to each rectangle.\nTo find the nearest city, consider the rectangle R containing the point.\nCompute the distance to the cities of the rectangle.\nProcess the 8 rectangles adjacent to R by computing the distance of the point to each rectangle : you may then eliminate the adjacent rectangles whose distance is greater than the best distance already found.\nIterate the process to a next level, i.e. the next crown (rectangles located on the outside of the area composed of 5x5 rectangles whose center is R).", "What algorithm would be most efficient when trying to find the nearest city given a set of coordinates?", 0.049958374351263046, 1, 863, "One solution is to use a [Voronoi diagram](https://en.wikipedia.org/wiki/Voroni_diagram) with the given set points as seeds. The Voronoic diagram will partition the plane into convex regions that are all the same distance from all of your points, and so the closest region will be the one closest to your point.", 0.14159291982650757, 0.0, 0.1150442510843277, 0.04182509705424309], ["Given ,  sklearn.neural_network  and simple Deep Learning by Keras with Sequential and Dese Nodes, are the mathematically same just two API's with computation optimization?\nYes Keras has a Tensor Support and could also liverage GPU and Complex models like CNN and RNN are permissible.\nHowever, are they mathematically same and we will yield same results given same hyper parameter , random state, input data etc ?\nElse apart from computational efficiency what maker Keras a better choice ?", "I don't think they will give you the exact same results as the internal implementations for 2 same operations are different even across pytorch and tensorflow.\nWhat makes Keras a better option is the ecosystem. You have the DataLoaders which can load the complex data in batches for you in the desired format, then you have the Tensorboard where you can see the model training, then you have preprocessing functions especially for data augmentations. In TF/Keras, you now even have data augmentation layers, in PyTorch Torchvision provides this in Transforms. Then you have the flexibility, you can define what types of layers in what order you want, what should be the initializer of the layer, do you want batch norm between layers or not, do you want a dropout layer between the layers or not, what should be the activation of hidden layers you can have relu in 1 layer and tanh in other, you can define how your forward pass should exist, blah blah. Then you have the callbacks to customize the training experience, and so on.", "Difference between sklearn.neural_network and simple Deep Learning by Keras with Sequential and Dese Nodes?", 0.1973753273487091, 1, 864, "Kerase is just a wrapper around the Tensors. So if you were to use a deep learning model in a simple way, you would use sklearns Neural Networks. Now, when you have a specific need for a certain implementation of a neural network, like a Dense or Convolutional Neuron, or you want to be able to create a model from a Keralese API, then Kerasa is the way to go.", 0.17599999904632568, 0.024193547666072845, 0.13600000739097595, 0.040256984531879425], ["I am using MAC and trying to run Ultralytics YOLOv8 pre-trained model to detect objects in my project. However, despite trying to use MPS, I am still seeing the CPU being used in torch even after running the Python code. Specifically, the output I see is: \"Ultralytics YOLOv8.0.43 \ud83d\ude80 Python-3.9.16 torch-1.13.1 CPU\".\nI wanted to know if has support for MPS in YOLOv8, and how can use it?", "Try adding \"--device mps\" as a parameter when running the command line", "Unable to Use MPS to Run Pre-trained Object Detection Model on GPU", 0.0, 0, 865, "Currently Mask R-CNN is the only MPI model we have in the cloud. Yolo9, Yolov5 and YoLOV8 are only supported on CPU.\nYou can try [Mask R Cnn](https://docs.mobilenet.ai/models/Mask_R_Cnn.html) instead of Yov8 to solve your problem.", 0.07017543911933899, 0.0, 0.035087719559669495, 0.06775584071874619], ["I have big polars dataframe that I want to write into external database (sqlite for example)\nHow can I do it?\nIn pandas, you have to_sql() function, but I couldn't find any equivalent in polars", "You can use the DataFrame.write_database method.", "How do I write polars dataframe to external database?", 0.0996679961681366, 1, 866, "you can use `to_dataframe` function from `polars.io.data` package.\nYou can also read more about it [here](https://polaris.towson.edu/polar/api/pandas_io/data/#pandas.DataFrame.to) and [there](http://pandas-docs.github.com/dev/generated/series.html#pandas).\nHere is a snippet of code you could use:\n`import pandas as pd; df=pd.read_csv('my_file.csv',header=None,engine='pola'); df.columns=['X', 'Y']`\nThe above code reads a csv file (in this example it is `myfile`) and assigns X and Y column names to df variable. In the end you will have a dataframe of the desired format.", 0.08771929889917374, 0.0357142873108387, 0.08771929889917374, 0.4184878468513489], ["I make a deep learning model for classification. The model consist of 4 Conv2d layer, 1 pooling layer, 2 dense layer and 1 flatten layer. When i do this arrangement of layers: Conv2D, Conv2D, Conv2D, Conv2D, pooling, dense, flatten, dense then my results are good. But when i follow this arrangement: Conv2D, Conv2D, Conv2D, Conv2D, pooling, flatten, dense, dense then the classification results are not good. My question is putting flatten layer between two dense layer is correct or not?\nCan I follow the pattern of layer by which i am getting good classification results?", "Typically, it is not recommended to sandwich a flatten layer between dense layers, and as suggested by Corralien, It doesn't provide any value. Your other architecture Conv2D, Conv2D, Conv2D, Conv2D, pooling, flatten, dense, dense is more legit. If your model is providing you with good results, you might want to keep it, but technically you do not need the flatten layer between the two dense layers.\nYou can consider using Conv2D, Conv2D, Conv2D, Conv2D, Pooling, dense, dense. Or a better alternative would be to try playing with your architecture. Such as adding another pooling layer between the four Conv2d layers like: Conv2D, Conv2D, Pooling, Conv2D, Conv2D, Pooling, flatten, dense, dense, and proceed with adjusting your hyperparameters.", "Placement of Flatten layer in deep learning model", 0.0, 0, 867, "You should use a flattener layer after the dense layers. That way, the neural network will not have a large number of parameters.\nIf you use the flatter layer before the 5th dense one, you will have 3 dense ones and one flater one. Then you can use another flattrer layer to reduce the number to 6.", 0.26436781883239746, 0.023255813866853714, 0.17241379618644714, 0.03171965852379799], ["I am working with satellite images with different spatial resolutions, understood as pixel/meter. For experiments I want to artificially down-sample these images, keeping the image size constant. For example I have a 512x512 image with spatial resolution 0.3m/pixel. I want to downsample it to 0.5m/pixel 512x512. I got advised to apply a Gaussian kernel to blur the image. But how do I calculate the standard deviation and kernel size of a Gaussian kernel to approximate the desired lower resolution? I can't find a rigorous method to do that calculation. Any help really much appreciated!\nChatGTP says that the formula is:\nsigma = (desired_resolution / current_resolution) / (2 * sqrt(2 * log(2)))\nand kernel_size = 2 * ceil(2 * sigma) + 1\nBut can't explain why. Can someone explain how standard deviation (sigma) and desired output resolution are connected? And how do I know which sigma to use? Oftentimes these existing resizing functions ask for a sigma, but in their documentation don't explain how to derive it.", "I wonder where that equation for the sigma comes from, I have never seen it. It is hard to define a cutoff frequency for the Gaussian.\nThe Gaussian filter is quite compact in both the spatial domain and the frequency domain, and therefore is an extremely good low-pass filter. But it has no clear point at which it attenuates all higher frequencies sufficiently to no longer produce visible aliasing artifacts, without also attenuating lower frequencies so much that the downsampled image looks blurry.\nOf course we can follow the tradition from the field of electronics, and define the cutoff frequency as the frequency above which the signal gets attenuated with at least 3dB. I think this definition might have lead to the equation in the OP, though I don\u2019t feel like attempting to replicate that computation.\nFrom personal experience, I find 0.5 times the subsampling factor to be a good compromise for regular images. For example, to downsample by a factor of 2, I\u2019d apply a Gaussian filter with sigma 1.0 first. For OP\u2019s example of going from 0.3 to 0.5 m per pixel, the downsampling factor is 0.5/0.3 = 1.667, half that is 0.833.\nNote that a Gaussian kernel with a sigma below 0.8 cannot be sampled properly without excessive aliasing, applying a Gaussian filter with a smaller sigma should be done through multiplication in the frequency domain.\nFinally, the kernel size. The Gaussian is infinite in size, but it becomes nearly zero very quickly, and we can truncate it without too much loss. The calculation 2 * ceil(2 * sigma) + 1 takes the central portion of the Gaussian of at least four sigma, two sigma to either side. The ceiling operation is the \u201cat least\u201d, it needs to be an integer size of course. The +1 accounts for the central pixel. This equation always produces an odd size kernel, so it can be symmetric around the origin.\nHowever, two sigma is quite small for a Gaussian filter, it cuts off too much of the bell shape, affecting some of the good qualities of the filter. I always recommend using three sigma to either side: 2 * ceil(3 * sigma) + 1. For some applications the difference might not matter, but if your goal is to quantify, I would certainly try to avoid any sources of error.", "Calculating Gaussian Kernel sigma and width to approximate a desired lower resolution pixel/m for satellite images", 0.0, 0, 868, "For each pixel in your 8-bit image, you have 3 bits of information (0, 4, and 7), so the saturation is 6 bits. Since the original image is in RGB, it can only encode 9 bits worth of color information. The R+G+B bits are (4/8) * (6/9) = .25 bits per pixel, or about \u00bc pixel. So the resolution is about .4 pixels per \"real\" pixel (i.e., \u2248 \u00bd \u00d7 \u220f pixels/real pixels).\nSo the kernel width is (sqrt(log(4) \u00f7 \u221a2) \u00d7 (log (3) \u2212 log (1)) = sqr(0) ~ sqrt(.487) is ~2\u00bc pixels.", 0.14663951098918915, 0.012269938364624977, 0.10183299332857132, 0.04905124008655548], ["I'm looking for a way to merge df. However, I don't know what would be the best way to do this.\nfirst df - metro cities/population/teams\n\n\n\n\nMetropolitan area\nPopulation (2016 est.)[8]\nNHL\n\n\n\n\nPhoenix\n4661537\nCoyotes\n\n\nLos Angeles\n13310447\nKings Ducks\n\n\nToronto\n5928040\nMaple Leafs\n\n\nBoston\n4794447\nBruins\n\n\nEdmonton\n1321426\nOilers\n\n\nNew York City\n20153634\nRangers Islanders Devils\n\n\n\n\nSecond df - team/wins/losses\n\n\n\n\nteam\nw\nL\n\n\n\n\nLos Angeles Kings\n46\n28\n\n\nPhoenix Coyotes\n37\n30\n\n\nToronto Maple Leafs\n49\n26\n\n\nBoston Bruins\n50\n20\n\n\nEdmonton Oilers\n29\n44\n\n\nNew York Islanders\n34\n37\n\n\n\n\nI tried to merge across teams. However, I need to arrange this data so that it collides in the Merge. I don't know how I would do that without looking at it case by case.\nNote: The data set is much larger and with more cities and teams.\nI had a little trouble presenting the DF here, so I only put 6 rows and the main columns.", "If you are trying to get the city part of a NHL team name, you could for example:\nMake a hash map which contains all the possible city names; {\"Toronto\": \"Toronto\"},\nsplit the NHL TEAM string and check if the hash map contains any part of the string. If it does that's the city name.\nWith the limited amount of possible city names that's not too bad.\nBut I'm not exactly sure what you are trying to accomplish, you should elaborate and simplify your question.", "Thinking about the best way to merge two DataFrame", 0.0996679961681366, 1, 869, "If you want to `merge` by city name, then you should merge by `city_name` column. Also, you need convert `pop` and `w` columns to numeric type. For example:\n\\begin{code}\ncols = ['w','L','pop']\ndf = pd.DataFrame({'city':['BOS', 'PHI', \"PHO\", \"LOS\", 'TOR',\n                            'EDM',  'NYC',   'LA',    'BOST'],\n           'team':[ 'RANGERS',      'OILERS',     'COYOTES',        'KINGS'  ,\n               'MAPLE LEAFS','ISLANDERS'], 'w': ['41','48','38',       '45',\"43\",\n                '51',           39,          4,'35'],  \n            \"pop\":['5873',          '125689',          7864,         4.554e6,\n             '23.182e7',         '7524',                  5.", 0.19512194395065308, 0.04938271641731262, 0.12195122241973877, 0.02908417396247387], ["I am using pytorch geometric to train a graph neural network. The problem that led to this question is the following error:\n\nRuntimeError: Expected all tensors to be on the same device, but found\nat least two devices, cpu and cuda:0! (when checking argument for\nargument mat1 in method wrapper_addmm)\n\nSo, I am trying to check which device the tensors are loaded on, and when I run data.x.get_device() and data.edge_index.get_device(), I get -1 for each. What does -1 mean?\nIn general, I am a bit confused as to when I need to transfer data to the device (whether cpu or gpu), but I assume for each epoch, I simply use .to(device) on my tensors to add to the proper device (but as of now I am not using .to(device) since I am just testing with cpu).\nAdditional context:\nI am running ubuntu 20, and I didn't see this issue until installing cuda (i.e., I was able to train/test the model on cpu but only having this issue after installing cuda and updating nvidia drivers).\nI have cuda 11.7 installed on my system with an nvidia driver compatible up to cuda 12 (e.g., cuda 12 is listed with nvidia-smi), and the output of torch.version.cuda is 11.7. Regardless, I am simply trying to use the cpu at the moment, but will use the gpu once this device issue is resolved.", "-1 means the tensors are on CPU. when you do .to(device), what is your device variable initialized as? If you want to use only CPU, I suggest initializing device as device = torch.device(\"cpu\") and running your python code with CUDA_VISIBLE_DEVICES='' python your_code.py ...\nTypically, if you are passing your tensors to a model, PyTorch expects your tensors to be on the same device as your model. If you are passing multiple tensors to a method, such as your loss function ``nn.CrossEntropy()```, PyTorch expects both your tensors (predictions and labels) to be on the same device.", "What does it mean if -1 is returned for .get_device() for torch tensor?", 0.0, 0, 870, "The `getDevice()` function is used to get the current device that you are using, that is, if you have the pycuda library installed, it will return the `0` for `gpu` and if not it returns `-1`.\nThe `torch` library will by default use your cpu. If you want to set it to `1` to have it use `cpu` you can do the next: `data.device = torcht.cpu()`.\nOtherwise, you need a way to specify the place where you wish to save the data, which is what you would do if for example you wanted to load it from disk or from a serialized file. I hope this helps.", 0.3349282443523407, 0.04830917716026306, 0.20095694065093994, 0.054649028927087784], ["is there a way to reload automatically a jupyter notebook, each time it crashes ?\nI am actually running a notebook, that trains a Deep learning model (the notebook can reload the last state of model, with state of optimizer and scheduler, after each restart of the kernel ), so that reloading the notebook after a crash enables to get back the last state without a substantial loss of computations.\nI was wondering if there was a simple way to do that using the jupyter notebook API, or a signal from the jupyter notebook for example (maybe on logs).\nAlso, I am running the notebook on google cloud platform (on compute engine), if you know any efficient way to do it, using the GCP troubleshooting services, and the logging agent, it might be interested for me and for others with the same issue.\nThank you again for you time.\nI tried to look up for a solution on stack overflow, but I didn't find any similar question.", "From your comment:\n\"reloading the notebook after a crash enables to get back the last state without a substantial loss of computations.\"\nWhat do you call a crash?, does it generate logs that can be parsed from /var/log or other location (e.g journalctl -u jupyter.service) ? If so you can manually create a shell script.\nWith User Managed Notebooks you have the concept of post-startup-script or startup-script\npost-startup-script, is path to a Bash script that automatically runs after a notebook instance fully boots up. The path must be a URL or Cloud Storage path. Example: \"gs://path-to-file/file-name\"\nThis script can be a loop that monitors the crash you mention", "automatic reloading of jupyter notebook after crash", 1.2000000476837158, 0, 871, "The noteboook is not designed to be run in a daemon.  It is designed for interactive use.", 0.08759123831987381, 0.0, 0.07299269735813141, 0.05403357744216919], ["I use matplotlib in my lib to display legend on a ipyleaflet map. In my CD/CI tests I run several checks on this legend (values displayed, colors etc...). My problem is when it's run on my local computer, matplotlib open a legend popup windows that stops the execution of the tests.\nIs it possible to force matplotlib to remain non-interactive when I run my pytest session ?", "You can change the matplotlib backend to a non graphical one by calling matplotlib.use('Agg') at the beginning of your scripts. This will prevent matplotlib from opening windows.", "How to prevent matplotlib to be shown in popup window?", 1.2000000476837158, 0, 872, "This is a known issue and is tracked on GitHub: https://github.com/matplotlib/pyplot/issues/212\nThe issue has been fixed in the master branch, but not yet released.  You can work around the issue for now by using\n\\begin{code}\nimport matplotlib.pylab as plt\nplt.ion()\nfig = plt.figure() #or whatever\n...\n#plots\nfor ax in fig.axes:\n    ax.set_visible(False)\nax.legend(loc='upper left', bbox_to_anchor=(0, 0), ncol=2) #make legend visible\nprint(fig.get_fignums())\n....\nclose(pl.gcf()) # or whatever you're calling to close the figure\ndel ax #to avoid memory leak\nfrom IPython import embed\nembed()  # to restart the kernel and run the next test\nexit()   # exit the script\nstart_kernel()    # restart kernel\npytest.main()        # run pytests\nend_python_session()# exit python\nquit()           # close your terminal\ncleanup()       # clean up", 0.1547619104385376, 0.012048192322254181, 0.095238097012043, 0.05263042822480202], ["import:from keras.preprocessing import sequence\nbut:\nAttributeError: module 'keras.preprocessing.sequence' has no attribute 'pad_sequences'\nWhy?\nHow can I edit this?", "Seems like your keras version is greater than 2.8 that's why getting error as\nfrom keras.preprocessing import sequence\nworks for earlier version. Instead, replace with the below given code:\nfrom keras.utils.data_utils import pad_sequences\nYou can also use:\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nThey both worked for me.", "python keras.preprocessing.sequence has no attribute pad_sequences", 0.2913126051425934, 3, 873, "I think you are mixing `Keras` with `TensorFlow`... `kerastorch` is the successor of `TF-Keras` and does not support `sequence`.\nYou can use `tensorflow.kerasetorch.datasets` to get a new dataset object (and its API is much more powerful than TF-kerass).\nI would recommend to use it from the very beginning.", 0.22018349170684814, 0.018691588193178177, 0.14678898453712463, 0.04475535824894905], ["If I have a DataFrame, df, for which df.index.empy is True, will this ALWAYS imply that df.empty is also True?\nMy intend is to test only df.index.empy when I need to test both conditions (lazy programming style).", "yes, if index of DataFrame is empty it will always satisfies condition of DataFrame.empty\nFor e.g\n\nDataFrame.empty = True\n\nworks in both condition for index as well as for columns.\nHence if you want to check any of these are empty then you can go with\n\nDataFrame.empty\n\nelse need to be specific\n\nDataFrame.index.empty", "Does DataFrame.index.empty imply DataFrame.empty?", 0.0, 0, 874, "In your case, it will not be true.\nThe Dataframe is not empty because you have at least 1 column, so it is a valid Data frame.", 0.21176470816135406, 0.048192769289016724, 0.11764705926179886, 0.04756147041916847], ["matplotlib has completely broken my python env.\nWhen i run:\nimport matplotlib as plt\nI received:\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\SamCurtis.AzureAD\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib.libs\\.load-order-matplotlib-3.7.1'\nI receive the same error if i try to pip install OR pip uninstall matplotlib\nInfact all my pip functionality is broken (i cannot pip freeze, uninstall / install) anything.", "I bumped into a similar problem just now after attempting to downgrade back to my old matplotlib version from 3.7.1. pip was throwing up this matplotlib.libs error even when I wasn't trying to do anything involving matplotlib.\nThe solution was to delete the matplotlib and mpl_toolkits directories from site-packages. Then I was able to reinstall my old matplotlib version and use pip as usual.", "matplotlib filenotfounderror site-packages", 0.0, 0, 875, "Try this: `python -m pip --ignore-installed --user install matplotlib`\nIt should uninstall all installed matplotlib and then reinstall it.", 0.15555556118488312, 0.022727273404598236, 0.13333334028720856, 0.01666666567325592], ["I'm trying to run an experiment where I have PageRank values and a directed graph built. I have a graph in the shape of a star (many surrounding nodes that point to a central node).\nAll those surrounding nodes have already a PageRank value precomputed and I want to check how the central node PageRank value is affected by the surrounding ones.\nIs there a way to perform this with networkx? I've tried building the graph with weighs (using the weights to store the precomputed PageRank values) but at the end, it look does not look like the central node value changes much.", "I will answer myself the question. In the PageRank method for NetworX you have the parameter nstart, which specifically is the starting pagerank point for the nodes.\n\nnstart : dictionary, optional\nStarting value of PageRank iteration for each node.\n\nStill, I'm afraid the graph structure is the limiting factor when doing the random walk and obtaining a relevant result.", "Initial pagerank precomputed values with networkx", 1.2000000476837158, 0, 876, "If you know how your graph is built then you can do this in a very simple way:\n\\begin{itemize}\n  \\item Use the `pageranks` method to compute Page Rank for your nodes. Then save the results in an array `precomputation`\n    \\strong{Note:} It will be important that you do the computation with the same ordering of nodes as your original graph. That is, the first node in your ordering will become the root of your paging computation, and then the next node will get its link to the previous node added to its `in_links`. Then you will add the link from that node to itself, then to node 3, etc. (This is the reason why you should know the ordering. If your computer is doing the computations in parallel, you may get different results each time.)\n      You can then use the following code to set your weights: `w = precomputation[0] * np.ones(precomposition.shape[1])`.\n        \\em{You will want this for the outgoing links, too.}\n       \\begin {code} ```\n2. To calculate the new Page rank of the center node, use this code: ```", 0.19917012751102448, 0.033472802489995956, 0.13278008997440338, 0.08059190213680267], ["I try to make image some changes(crop, resize, undistort) and I want to know how distortion coefficients and camera intrinsinc parameters change after that.\n\nOrigin Image shape = [848, 480]\ncamera matrix = [[fx, 0, cx], [0, fy, cy], [0, 0, 1]]\ndistortion coefficients = [k1, k2, p1, p2]\n\ncrop\n\n[848, 480] -> [582, 326]\nfx, fy : no changes\ncx, cy : cx -133, cy - 77\ndistortion coefficients -> ??\n\nresize\n\n[582, 326] -> [848, 480]\nfx, cx -> 1.457fx, 1.457cx\nfy, cy -> 1.472fy, 1.472cy\n[k1, k2, p1, p2] -> [k1, k2, p1, p2] same\n\nundistort\n\nfx, fy, cx, cy -> same\n[k1, k2, p1, p2] -> [0, 0, 0, 0]\n\nDoes anyone knows the answer?\nFor me I tried using my camera and calibrate some results but I don't know the exact equation.\norigin\n\nfx = 402.242923\nfy = 403.471056\ncx = 426.716067\ncy = 229.689399\nk1 = 0.068666\nk2 = -0.039624\np1 = -0.000182\np2 = -0.001510\n\ncrop\n\nfx = 408.235312 -> almost no change\nfy = 409.653612 -> almost no change\ncx = 297.611639 -> cx - 133\ncy = 153.667098 -> cy - 77\nk1 = 0.048520 -> I don't know\nk2 = -0.010035 -> I don't know\np1 = 0.000943 -> I don't know\np2 = -0.000870 -> I don't know\n\ncrop_resize\n\nfx = 598.110106 -> almost * 1.457\nfy = 608.949995 -> almost * 1.472\ncx = 430.389861 -> almost * 1.457\ncy = 226.585804 -> almost * 1.472\nk1 = 0.054762 -> I don't know\nk2 = -0.025597 -> I don't know\np1 = 0.002752 -> I don't know\np2 = -0.001316 -> I don't know\n\nundistort\n\nfx = 404.312916 -> almost same\nfy = 405.544033 -> almost same\ncx = 427.986926 -> almost same\ncy = 229.213162 -> almost same\nk1 = -0.000838 -> almost 0\nk2 = 0.001244 -> almost 0\np1 = -0.000108 -> almost 0\np2 = 0.000769 -> almost 0", "All part you write as \"I don't know\" will be \"same(not changed)\".\nBecause Cropping and Resizing is representable with only (cx,cy,fx,fy).", "How does camera distortion coefficients and camera intrinsic parameters change after image crop or resize?", 0.3799489736557007, 2, 877, "The change in the distorted image size (fx and fx) are the same as the change of the camera matrix (a,b,c,d).\nThe changes in px and py are  the changes of (kx,ky,p).", 0.1269841343164444, 0.0, 0.0634920671582222, 0.034121669828891754], ["I was just asking myself: I understand that calling df[column_name] displays a Series because a DataFrame is built of different arrays.\nThough, why does calling df[[column_name]] (column_name being only one column) returns a DataFrame and not a Series ? I'm ont sure to understand the logic behing how Pandas was built here\nThanks :)\nI was trying to explain to my students why calling a list of one element displays a dataframe and not a Series, but did not manage", "It may happend because when you give single column_name as a string it takes it perform selection and return single value based on the search key column_name.\nBut when you provide same column_name contained in a list it tries to fetch all the keys of the list which in one in this case. Hence resulting a dataframe.\nI guess they are using some standard logic to return dataframe if list is provided irrespective of length of list.\nimport pandas as pd\ndf = pd.DataFrame(columns=[\"a\",\"b\",\"c\"],\ndata=[[1,4,7],[2,5,8],[3,6,9]])\ncolumn_name = \"a\"\nprint(type(df[column_name]))\nprint(type(df[[column_name]]))\noutput:\n<class 'pandas.core.series.Series'>\n<class 'pandas.core.frame.DataFrame'>", "Why does using df[[column_name]] dislays a DataFrame?", 0.0, 0, 878, "\\begin{blockquote}\nthough, I don't understand why using `df[col]` returns an object of type `Series` but using the same name in `[]` returns the type of `DataFrame`.\n\\end{TF1}  (I didn' t understand you there)\nWell, this is because `list` is a subclass of both `dict` and `tuple` (see [this](https://stackoverflow.com/questions/1249711/is-list-a-subclass-of-dict-and-tuple) and [Python docs](http://docs.python.org/2/reference/datamodel.html#object.__new__)).\nNow, the first call to `dataframe[index_variable]` will return an element from the `index`, then it will call the `__getitem__` method of the class `Index` to get the element (if this element is in the index), and it does it by getting the position of this item in its list, then this position is the key of a dict, which is then called, so it returns something like `series = [4, 3, ...]` and so on. If the item is", 0.33451956510543823, 0.01433691754937172, 0.12811388075351715, 0.040472179651260376], ["I have two numeric and huge np.arrays (let's call them S1 and S2, such that len(S1)>>len(S2)>>N where N is a very large number). I wish to find the most likely candidate part of S1 to be equal to S2.\nThe naive approach would be to compute a running difference between S2 and parts of S1. This would take too long (about 170 hours for a single comparison).\nAnother approach I thought about was to manually create a matrix of windows, M, where each row i of M is S1[i:(i+len(S2)]. Then, under this approach, we can broadcast a difference operation. It is also infeasible because it takes a long time (less than the most naive, but still), and it uses all the RAM I have.\nCan we parallelize it using a convolution? Can we use torch/keras to do something similar? Bear in mind I am looking for the best candidate, so the values of some convolution just have to preserve order, so the most likely candidate will have the smallest value.", "I am assuming you are doing this as a stepping stone to find the perfect match\nMy reason for assuming this is that you say:\n\nI wish to find the most likely candidate part of S1 to be equal to S2.\n\n\nStart with the first value in the small array.\n\nMake a list of all indices of the big array, that match that first value of the small array. That should be very fast? Let's call that array indices, and it may have values [2,123,457,513, ...]\n\nNow look at the second value in the small array. Search through all positions indices+1 of the big array, and test for matches to that second value. This may be faster, as there are relatively few comparisons to make. Write those successful hits into a new, smaller, indices array.\n\nNow look at the third value in the small array, and so on. Eventually the indices array will have shrunk to size 1, when you have found the single matched position.\n\n\nIf the individual numerical values in each array are 0-255, you might want to \"clump\" them into, say, 4 values at a time, to speed things up. But if they are  floats, you won't be able to.\nTypically the first few steps of this approach will be slow, because it will be inspecting many positions. But (assuming the numbers are fairly random), each successive step becomes much faster. Therefore the determining factor in how long it will take, will be the first few steps through the small array.\nThis would demand memory size as large as the largest plausible length of indices. (You could overwrite each indices list with the next version, so you would only need one copy.)\nYou could parallelise this:\nYou could give each parallel process a chunk of the big array (s1). You could make the chunks overlap by len(s2)-1, but you only need to search the first len(s1) elements of each chunk on the first iteration: the last few elements are just there to allow you to detect sequences that end there (but not start there).\nProviso\nAs @Kelly Bundy points out below, this won't help you if you are not on a journey that ultimately ends in finding a perfect match.", "Find the most similar subsequence in another sequence when they both numeric and huge", 0.0996679961681366, 1, 879, "The na\u00efve approach seems to work in practice, as shown below. There is one caveat: it does not seem to correctly handle the case where S is longer than S_2 (S[0] < S[1] && S [0:N] != S[:N]).\nIf you're looking to reduce memory, you could take a window of length len_S_1, not len S. Also, since S isn't a numpy array, it is possible that it'll fail to have broadcastable shapes. If that' is the problem, try converting it to a NumPy array.", 0.1894736886024475, 0.008456659503281116, 0.10105263441801071, 0.04237287864089012]]}