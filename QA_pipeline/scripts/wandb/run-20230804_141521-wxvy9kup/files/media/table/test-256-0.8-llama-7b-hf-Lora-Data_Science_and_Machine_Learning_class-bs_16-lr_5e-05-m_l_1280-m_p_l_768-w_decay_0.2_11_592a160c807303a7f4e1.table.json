{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["I have a CatboostRanker (with groups) model trained on a dataset of ~1m rows of ~300 features. Three such features are ~90% invalid (have a special category denoting such). My data is time series based, so the invalid data of these features is all but the most recent data.\nFor some reason, these three features are amongst the top 5 most important according to their shapley values (the absolute sum of all shap values for a given feature). When looking at the individual shap values for each individual object, ALL of them are positive, meaning they all contribute positively to the target variable of binary [0,1].\nI don't understand why the 90% of invalid values for these features all carry with them a positive shapley value, since the invalid category theoretically confers no helpful information. I've read several explanations of shap values, and understand their mathematical basis, but still no closer to an answer. Any suggestions would be much appreciated.\nOn the other hand, Catboost's own permutation-based get_feature_importance method ranks these variables lowly which seems far more intuitive.\nMethod: I used shap.TreeExplainer on the trained model, then extract the shap values from the explainer by passing a catboost pool containing my training data.", "SHAP does not determine if your features are valid or not. It determines the importance of those features to your model's output. I had the same confusion when implementing it for some use cases.\nIf you go back to the game theory origin of SHAP, you can understand it as this : SHAP does not determine if the player (your feature) is a good player (valid feature) but it determines how much it contributes to the game (your output).\nIf you want to gain understanding on the validity of your features, you should see the model's performance as well (your metrics).", "Why are the SHAP values of a feature with mostly invalid values all positive?", 1.2000000476837158, 0, 440, "It looks like your training set is non-iid. This is something that happens in a lot of real-life problems, because a set of non iid data can be used to train a model which performs well on new data which is not necessarily iidx. It'd be interesting to see the distribution of the data in your dataset.", 0.2469135820865631, 0.012500000186264515, 0.1358024626970291, 0.03658505901694298], ["I have two dataframes of simlar size. Lets say df1 and df2. For  both the data frames a common column is selected as index. Let's say the name column which is set as index is Id.\nWhen I run the code df1.equals(df2), it returns False. But when I try to compare both of the data frames using df1.compare(df2) only the indexed column name i.e, Id is returned without any values in it.\nWhat should I conclude from this?", "Use assert_frame_equal(df1, df2, check_names=False)", "Pandas df.compare() returns the name of a column", 0.0, 0, 441, "Compare is a binary operation that is used to see if two DFs are equal, in this case it seems like you want to find the index of df, you can do that like this.  It is also important to note that in your example, df and f1 are not the same object, so if you were to run df == df you would get False, since they are different objects.", 0.050632912665605545, 0.0, 0.050632912665605545, 0.02056214027106762], ["I want to use time series cross validation to evaluate my model. I'm trying to leverage TimeSeriesSplit and cross_validate from sklearn.\nLet's say my model has features A, B, and others. In practice I want my model only to give predictions for categories A and B seen during training, for all other categories it will raise an error.\nHow I can cross validate my model enforcing this behaviour? Could I still use the sklearn implementations, adapt them with minor changes or would I have to build my cross validation from scracth?", "Import TimeSeriesSplit.\nThen create an instance of TimeSeriesSplit (set the test size parameter to 1)\nThen define a method, which filters out all the unwanted categories from the train and test dataset and outputs your filtered data.\nImport cross_validate from scikit-learn, then make sure to call your custom function for every iteration of your cross_validate.\n\nThis way you can implement cross validation with minor changes rather than implementing it all from scartch.", "Time series cross-validation removing new categories from test", 0.0, 0, 442, "One way to do this would be to create a new dataset that has the same set of features and categories but where some of the categories have been dropped. You can then use this dataset to test your model while you train on the original dataset. This will give you a fair comparison between the two models.", 0.31578946113586426, 0.015267175622284412, 0.15037593245506287, 0.05074041336774826], ["I am new to using or-tools and I have been tasked with minimizing a quadratic objective function with lots of IntVar variables. I don't know where to start breaking down what the problem could be concerning an infeasable model and welcome any tips to begin.\nI've watched several videos about using or tools and experimented in a test file with a smaller scale but my main problem results in the infeasable model. The videos/experiments I've done have less variables which I am thinking is the issue but, again, this is my first time using OR-tools.\nhere is the output from when I run the model:\nStarting CP-SAT solver v9.6.2534\nParameters: log_search_progress: true enumerate_all_solutions: true\nSetting number of workers to 1\nInitial optimization model '': (model_fingerprint: 0x8f376cd881ed44f1)\n#Variables: 214 (#ints:1 in objective)\n\n126 Booleans in [0,1]\n2 in [-100000,100000]\n84 in [0,96]\n2 in [0,2000000]\n#kIntProd: 4004\n\nStarting presolve at 0.00s\nUnsat after presolving constraint #1142 (warning, dump might be inconsistent): int_prod { target { vars: 214 coeffs: 3249 offset: 3249 } exprs { vars: 212 coeffs: 570 offset: -102 } exprs { vars: 212 coeffs: 570 offset: -102 } }\nPresolve summary:\n\n2 affine relations were detected.\nrule 'affine: new relation' was applied 2 times.\nrule 'int_prod: divide product by constant factor' was applied 2 times.\nrule 'int_prod: linearize product by constant.' was applied 1140 times.\nrule 'int_prod: removed constant expressions.' was applied 1140 times.\nrule 'int_square: reduced target domain.' was applied 2 times.\nrule 'linear: remapped using affine relations' was applied 1 time.\nrule 'presolve: iteration' was applied 1 time.\nrule 'variables: canonicalize affine domain' was applied 2 times.\nProblem closed by presolve.\n\nCpSolverResponse summary:\nstatus: INFEASIBLE\nobjective: NA\nbest_bound: NA\nintegers: 0\nbooleans: 0\nconflicts: 0\nbranches: 0\npropagations: 0\ninteger_propagations: 0\nrestarts: 0\nlp_iterations: 0\nwalltime: 0.007945\nusertime: 0.007945\ndeterministic_time: 0\ngap_integral: 0", "I only have simple advices:\n\nreduce the model size if this is a parameter\nremove all constraints while keeping the problem infeasible\nlook at the square constraints and the corresponding domains\nadd assumptions to check the soundness of your data (capacity is >= 0, no crazy ranges of values, ...)\nplay with the domain of the variables to enlarge them\ninject a known feasible solution and try to find where it breaks", "When resulting in an infeasible model, what tips are there to break down the nexus of the issue", 0.5370495915412903, 3, 443, "The infection model can happen for several reasons. There are two things you can do to improve the situation: (1). Increase the number and (2). Reduce the size of your model\n\\begin{itemize}\n  \\item In this case, you need to increase the value of `num_threads` and set the option `log_workers` to `true`. The default value is `1` which means the solvers will not spawn multiple threads to solve your problem. So you should set `intvars` as `IntVar` in your objective and then set num_thread as 8. In addition, your variables should be continuous, and you don\u2019t need the `range_check` option. If you want to use `ranges` please set your variable as a `Range` variable. You can use the following code snippet as an example. Finally, set log as true. It will help you to check the work done by the OR solving system. For further information, please check [this tutorial](https://developers.google.com/optimization/tutorials/continuous_optimizers) and [here](http://docs.bazaar.ai/bazar/en/master", 0.22314049303531647, 0.03333333507180214, 0.12396693974733353, 0.03488356992602348], ["Fairly new to python-polars.\nHow does it compare to Rs {data.table} package in terms of memory usage?\nHow does it handle shallow copying?\nIs in-place/by reference updating possible/the default?\nAre there any recent benchmarks on memory efficiency of the big 4 in-mem data wrangling libs (polars vs data.table vs pandas vs dplyr)?", "How does it handle shallow copying?\n\nPolars memory buffers are reference counted Copy on Write. That means you can never do a full data copy within polars.\n\nIs in-place/by reference updating possible/the default?\n\nNo, you must reassign the variable. Under the hood polars' may reuse memory buffers, but that is not visible to the users.\n\nAre there any recent benchmarks on memory efficiency\n\nThe question how it relates in memory usage is also not doing respect to design differences. Polars currently is developing an out-of-core engine. This engine doesn't process all data in memory, but will stream data from disk. The design philosophy of that engine is to use as much memory as needed without going OOM. Unused memory, is wasted potential.", "Polars memory usage as compared to {data.table}", 0.5370495915412903, 3, 444, "I've been using polars a lot lately and I really like it.  I have no idea about the other packages, but I can say that it's much faster than pandas, both when reading in data and performing operations. I use it in a variety of situations, including working with really large data sets. It'll never be as fast as a compiled language like R or Julia, though, and pandas is very fast. The trade off is that polaris is a very compact package, so it doesn't have any of those bells and whistles that pandas has. (For example, you can'{t do groupby or pivot tables, etc.)", 0.2594142258167267, 0.016877636313438416, 0.1171548143029213, 0.03395886719226837], ["I have folder named with a lot of location and each of them have 360 degree pics of each of those location i give it a random picture and it has to guess where am i, I tried ORB and it gave me good results and i made it better also but the issue is that lighting conditions might give me a hard time like sunny morning or cloudy morning as per some papers these parameters can cause a issue and i wanted to find is there some way that i can handle this issue is there any way of finding key points irrespective of the weather outside.\nI tried to use ORB and getting good results for that movement of time but when lighting changes my dataset is not performing well so i need a solution for that", "To handle the issue of lighting variations in your scenario, you can try the following approaches:\n\nHistogram Equalization: Apply histogram equalization to normalize\nthe lighting conditions of your images. This technique redistributes\nthe pixel intensities to improve the contrast and make the images\nmore consistent across different lighting conditions.\n\nAdaptive Thresholding: Instead of using a global threshold for image\nbinarization, consider using adaptive thresholding techniques. These\nmethods compute a local threshold for each pixel based on its\nneighborhood, which can help handle variations in lighting.\n\nMulti-Scale Analysis: Perform feature detection and matching at\nmultiple scales to capture different levels of details. By analyzing\nthe image at different scales, you can mitigate the impact of\nlighting variations and improve the chances of finding matching\nkeypoints.\n\nImage Enhancement Techniques: Apply image enhancement techniques,\nsuch as gamma correction or contrast stretching, to improve the\nvisibility of details in the images and compensate for lighting\nvariations.\n\nExplore Deep Learning-based Approaches: Deep learning models, such\nas convolutional neural networks (CNNs), have shown promise in\nhandling variations in lighting conditions. You can consider\ntraining a CNN on your dataset to learn robust features that are\nless sensitive to lighting changes.", "What can i use better than ORB for matching photos", 1.2000000476837158, 2, 445, "It's not clear to me what you mean by the \"key points\" in the picture. If you have enough image data to capture the scene in a 180x120 grid, then you could use [Histogram of Oriented Gradients](https://en.wikipedia.org/wiki/Hough_transform) on that grid to get the keypoints in each 4x4 block. For each block, the coordinates of all the points can be used to calculate the direction of lines and planes. The Hough transform can then be applied to the set of directions to determine which of these directions have significant support (i.e., which are more common than you would expect by chance).\nThis gives you a set (or 2 sets) of points and directions that you can compare to your reference image. This is just a very basic overview of how the HH transform works. A lot more detail is available in many papers.", 0.3285302519798279, 0.023188406601548195, 0.14985591173171997, 0.05320415645837784], ["I have a DataFrame with an index column that uses pandas TimeStamps.\nHow can I get the values of the index as if it were a normal column (df[\"index\"])?", "You can convert the index to a regular column by using the reset_index() method.", "Get values of DataFrame index column", 0.0996679961681366, 1, 446, "You can use `.iloc`:\n\\begin{itemize}\n    \\item `.loc`: index of dataframe is a MultiIndex\n     \\em{e.g.}: `df.loc[1, 2]`\n   \\strong{OR} `.index`\n       \\begin {code} <-\n      \\end{code}- index is of type Series\n        \\sout{ \\textit{i.e.} `idx=1` --> `il_idx = df.ilocs(idx) `\n          \\textbf{AND} `index = 1 `--> `ix_index= df[index] `}  -- this is new\n           \\t ----- `i_ix = ix_series` ---> `int_i = int(i-ix)`\n            \\n ------- `str_s` --------->`str(int(df[str(-ix)].strftime('%s')))` --> This will return a datetime object\n               \\w ------------------>`df['str'+str((-i))]` -- will give a `DataFrame` with the `timestamp` column as the Index\n                 \\q  ----------------> `pd.DataFrame", 0.13913042843341827, 0.03539822995662689, 0.12173912674188614, 0.10436636209487915], ["I got 2 columns stating the start and end dates of an event in my data frame. I need to calculate the number of 31.12.XXXX's within that range (while treating it as an open interval). Any help is appreciated, thank you.", "Assuming you mean \"open interval\" in the ordinary mathematical sense, being that the end dates themselves are excluded, then...\nIf you just want to work with the textual form, split each date into DD.MM and YYYY components j1=d1[0:5]; y1=int(d1[6:]); y2=int(d2[6:]).\nSubtract the \"start\" year from the \"finish\" year, subtract one more if the first date is 31.12: n = y2 - y1 - int(j1 == \"31.12\")\nTake the maximum of this and zero (because the answer cannot be negative): if (n<0): n=0\nAlternatively, if you have dates represented in a computable form (e.g. Julian day numbers, Unix Epoch seconds, etc) start by adding one day to the \"start\" date; then take the year of each date, subtract one from the other, and you have your answer.\nSanity checking both approaches, consider:\n\nany two dates in the same year: answer is 0.\n31.12.2020 to 31.12.2021: answer is 0.\n30.12.2020 to 01.01.2021: answer is 1.", "Counting 31.12.'s within a Date Range", 0.06656807661056519, 1, 447, "\\begin{code}\nlibrary(dplyr)\ndf %>%\n    mutate(end = if_else(is.na(start), as.Date(strptime(\"2019-10-05\", format=\"%Y-%m-%d\")), start),\n           end = as_date(as.numeric(format(df$end, format = \"%d.%m.299\")))) %>% count(date) %>% group_by(month, date)  %>% summarise(n = n())\n\\end{pre}", 0.10958904027938843, 0.0, 0.06392694264650345, 0.03688524290919304]]}