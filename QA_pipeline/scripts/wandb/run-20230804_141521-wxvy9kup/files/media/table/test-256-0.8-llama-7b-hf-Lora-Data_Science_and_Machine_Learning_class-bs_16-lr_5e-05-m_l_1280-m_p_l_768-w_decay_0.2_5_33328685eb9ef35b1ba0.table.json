{"columns": ["Question", "Answer", "Title", "Score", "Users Score", "Q_Id", "Generated Answer", "ROUGE_1", "ROUGE_2", "ROUGE_L", "BLEU"], "data": [["Newbie here, I recon this may be a very foolish question. I am simultaneously running on cuda, in two distinct processes, a simple 3-layer MLP neural network over two different datasets.\nSince these two processes are practically using the same script, they are both creating variables with the same names, so my question is: is each process completely isolated from the other, or is there any way that by running one process after the other I will be overwriting my variables, e.g. my variable x, pertaining to the dataset #1's feature vector I'm giving the model in the first process will be overwritten with the dataset #2's feature vector once I start process 2, therefore influencing my first process's model's predictions?", "is each process completely isolated from the other, or is there any way that by running one process after the other I will be overwriting my variables, e.g. my variable x, pertaining to the dataset #1's feature vector I'm giving the model in the first process will be overwritten with the dataset #2's feature vector once I start process 2, therefore influencing my first process's model's predictions?\n\nThe processes are isolated from each other. One process will not overwrite variables in another process that happen to have the same \"name\".", "Can two processes running simultaneously share a variable?", 0.1973753273487091, 1, 400, "Yes, you're absolutely right. You'll need to make sure you handle the cases of overlapping variables (e. g. using `shared_memory`) and also be aware that the variables will have different names in each kernel.", 0.19548872113227844, 0.030534351244568825, 0.12030075490474701, 0.05113636329770088], ["I'm trying to do demand sensing for a dataset. Presently I have 157 weeks of data(~3years) and I have to predict next month(8 weeks).In the training dataset, I'm using 149weeks as a train and the last 8 weeks as Val to get the best hyperparameters. But I have observed that in the pred result, there's a huge gap in wmapes between Val and pred. I'm not sure if im overfitting because Val wmape is good.\nthe aim is to get best parameters such that the result pred will good for last month(last 4 weeks/8weeks).\nnote: there is a gap in train and pred i.e. if the train is till 31st jan22, pred will start from 1st mar22.\nHow can I overcome this problem?\nDetails: dataset: timeseries , algo: TCNmodel(darts lib),lang:python.", "How you should split the data depends on some factors:\nIf you have a seasonal influence over a year, you can take a complete year for validation and two years for training.\nIf your data can be predicted from the last n-weeks, you can take some random n-week splits from the full dataset.\nWhats more important here is that I think there's an error in your training pipeline. You should have a sliding window over n-weeks over the full training data and always predict the next 8 weeks from every n-week sequence.", "Get best parameters for time series without losing data", 0.0, 0, 401, "This can happen in general with time-series problems. You have no idea what is going on with the future. There is no way to know if you are over-fitted. The only way is looking at the validation error (or your chosen metric). If it does not improve, then you can try different hyper-parameters or even restart your training. However, beware of over fitting. If your validation loss does get better, it means you have found the right hyper parameters. It is just a matter of time until you get a better model. Just keep training, and check your loss every now and then.", 0.31527093052864075, 0.049751244485378265, 0.17733989655971527, 0.047618914395570755], ["so i want to use scipy to minimize a function. In my application i am required to do a function evaluation every time the gradient is required.\nI undersand that i can pass a function that will return both, functionvalue and gradient, when i set the arg jac=True. However, sometimes i assume that this procedure will compute gradients when they are not required, e.g. for linesearch, which is very expensive. Is there any way to pass an argument to evaluate the function and an argument to evaluate function and gradient?\nEDIT:\ni also dont want do compute gradient and functionvalue independently by passing fun and jac since then the fun evaluation inside of jac is often wasted.", "You can pass a callable to the jac argument and it will be used to compute the gradient, while fun will still be called to compute the function value.", "Using Scipy.Optimize.Minimize efficiently when compting gradient requires function evaluation", 0.0, 0, 402, "You can get the current value of the objective function from the `xopt` variable returned by `minimizer.x`. This value is stored as a floating-point number and is updated when the cost function is evaluated.", 0.2769230902194977, 0.0317460335791111, 0.1846153885126114, 0.1031746119260788], ["I am using PPO stable baselines in Google Colab with Tensorboard activated to track the training progress but after around 100-200K timesteps tensorboard stops updating even with the model still training (learning), does anyone else have this issue and know a fix for it?", "stable baselines doesnt seem to run well on CoLab because of the need to downgrade to tensorflow 1.6 which doesnt run well with tensorboard so instead I used to the newer stable baselines3 with current tensorflow version and tensorboard works fine.", "Tensorboard stops updating in Google Colab during learning with stable baselines", 1.2000000476837158, 0, 403, "My guess is that you are running out of memory. I've had this happen in the past when I was running on a limited machine, and I think it may have been due to memory pressure.\nI'm not sure if there is an easy fix, but I would try increasing the memory that is allocated to the tensor board by adding the following to your code:\n`print(tf.config.experimental.set_memory_growth(1, 0, tf.train.ExperimentalOptions(per_device=True))`\nHope this helps.", 0.15625, 0.01587301678955555, 0.09375, 0.07680752873420715], ["I've been looking around here and on the Internet, but it seems that I'm the first one having this question.\nI'd like to train an ML model (let's say something with PyTorch) and write it to an Apache Kafka cluster. On the other side, there should be the possibility of loading the model again from the received array of bytes. It seems that almost all the frameworks only offer methods to load from a path, so a file.\nThe only constraint I'm trying to satisfy is to not save the model as a file, so I won't need a storage.\nAm I missing something? Do you have any idea how to solve it?", "One reason to avoid this is that Kafka messages have a default of 1MB max. Therefore sending models around in topics wouldn't be the best idea, and therefore why you could instead use model files, stored in a shared filesystem, and send URIs to the files (strings) to download in the consumer clients.\nFor small model files, there is nothing preventing you from dumping the Kafka record bytes to a local file, but if you happen to change the model input parameters, then you'd need to edit the consumer code, anyway.\nOr you can embed the models in other stream processing engines (still on local filesystems), as linked in the comments.", "Send and load an ML model over Apache Kafka", 1.2000000476837158, 1, 404, "You can use PySpark for your need. You could use Spark Streaming for reading messages from KAFKA and then use some ML library for the training part. Once you get the trained model, you could save it in some storage (HDFS, S3, etc) or you can save to KAFFKa topic and let your client to read it and use the data.", 0.27586206793785095, 0.03488372266292572, 0.16091954708099365, 0.05056943744421005], ["User program failed with ImportError: cannot import name '_joblib_parallel_args' from 'sklearn.utils.fixes' (/azureml-envs/azureml_39c082289e18c74c5b8523a75d2c0d1e/lib/python3.8/site-packages/sklearn/utils/fixes.py)\nAnyone know why? Is there a workaround or a fix?", "Try\npip uninstall scikit-learn\npip install scikit-learn==1.0.2", "I am having this issue in imports in Visual Studio and azure", 0.06656807661056519, 1, 405, "The package name is `sklearns_utils_fix` instead of `utils` in the error.\nIn `site_packages` it is renamed to fix_sklean. It's likely that your code is using the old package.", 0.0, 0.0, 0.0, 0.08922596275806427], ["I have 130 GB csv.gz file in S3 that was loaded using a parallel unload from redshift to S3. Since it contains multiple files i wanted to reduce the number of files so that its easier to read for my ML model(using sklearn).\nI have managed to convert multiple from from S3 to a spark dataframe (called spark_df) using :\nspark_df1=spark.read.csv(path,header=False,schema=schema)\nspark_df1 contains 100s of columns (features) and is my time series inference data for millions of customers IDs. Since it is a time series data, i want to make sure that a the data points of 'customerID' should be present in same output file as I would be reading each partition file as a chunk.\nI want to unload this data back into S3.I don't mind smaller partition of data but each partitioned file SHOULD have the entire time series data of a single customer. in other words one customer's data cannot be in 2 files.\ncurrent code:\ndatasink3=spark_df1.repartition(1).write.format(\"parquet\").save(destination_path)\nHowever, this takes forever to run and the ouput is a single file and it is not even zipped. I also tried using \".coalesce(1)\" instead of \".repartition(1)\" but it was slower in my case.", "This code worked and the time to run reduced to 1/5 of the original result. Only thing to note is that make sure that the load is split equally amongst the nodes (in my case i had to make sure that each customer id had ~same number of rows)\nspark_df1.repartition(\"customerID\").write.partitionBy(\"customerID\").format(\"csv\").option(\"compression\",\"gzip\").save(destination_path)", "How to EFFICIENTLY upload a a pyspark dataframe as a zipped csv or parquet file(similiar to.gz format)", 1.2000000476837158, 1, 406, "If you're running Spark in standalone mode, you can use `spark-submit` to submit a command-line job to Spark. The following command creates a directory called `s3-test` in your local directory and writes a `csv` file into that directory. It then loads the file, removes the directory, and creates an empty directory in the same location. `hadoopFS` is the local file system on your machine, so you don\u2019t need to use the `hdfs` prefix on `localFS`. The `copyFromLocal` option is only necessary if you want the Spark job you run to be able to access local files on the machine running the job. If the files are in a remote file location, then you need the full path to the location in `path` and use it in place of `mypath`.\n\\begin{code}\nsbt \"com.databricks.spark:sparkshell-console_2.11:3\" \"sc.hadoopFile(new File('/mydir/'), 'sparktest.txt')\"\nsc = spark.newAPIHadoopFile(\"s0:/my/path/to/sparse\", \"s\", sc) # create", 0.2017543911933899, 0.0, 0.11403508484363556, 0.02459774911403656], ["I have dataset with quite a lot data missing which stores hourly data for several years. I would now to implement a seasonal filling method where I need the best data I have for two following years (2*8760 entries). This means the least amount of data missing (or least amount of nan values) for two following years. I then need then the end time and start time of this period in datetime format. My data is stored in a dataframe where the index is the hourly datetime. How can I achieve this?\nEDIT:\nTo make it a bit clearer I need to select all entries (values and nan values) from a time period of of two years (or of 2*8760 rows) where the least amount of nan values occur.", "You can remove all the NAN values from your data by using df = df.dropna()", "How can I select with least amount of nan values for a certain time period in panda?", 0.5370495915412903, 3, 407, "It's not really clear what you want to do with your data, but I will try to answer your question as you have asked it. To find the minimum number of NaN values over a given range, you can use `groupby` and `min`.\n\\begin{itemize}\n  \\item First create a `DataFrame` with the data and the time you wish to calculate over. For example: `df = pd.DataFrame(data={'time': [1, 10, np.nan, pd.__version__, 'foo', np, '', 0], 'val': [-1.5, -14.03, NaN, None, ''], dtype='O'})`\n \\em{Note that the NaN'd value is string and not a number.}\n [Here' is a link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd.to_numeric.html) on how to convert it to a float. Or you could just drop the Nan values from the DataFrame. But in this case you need them. \\strong{UPDATE:} I missed the fact that you actually wanted to remove Nan'ed values. The following is for the OP", 0.11173184216022491, 0.05649717524647713, 0.0670391097664833, 0.04997095465660095], ["Normally we predict a target using static user features, here I am making serial predictions with increasing user behavior data.\nI use 7 days as a period, and predict the user whether to be converted at the end of the period on a daily basis, using the feature data as of yesterday. I predict 6 times in total starting from day 2.\nBut this will have a problem, I'm using a logistic regression-scorecard model to train the complete historical data(of day 7s since the period's data is all available), and each feature will get a coefficient. But in production environment, feature data come day-by-day, eg. in day 2, I have feature_day1, to predict if_buy for the 1st time; in day 3, I have feature_day1 and feature_day2, to predict if_buy for the 2nd time. Technically, I can get a predicted result by setting the features of future days to null, but I doubt the correctness.  How to design this model so I can make predictions properly every day?\n\n\n\n\nnote\ndate\nday_no\nuser_id\nif_buy\nfeature_day1\nfeature_day2\nfeature_day3\nfeature_day4\nfeature_day5\nfeature_day6\nfeature_day7\n\n\n\n\ncomplete_entry\n20220501\n7\n1000\n0\n9\n5\n9\n3\n2\n7\n6\n\n\ncomplete_entry\n20220501\n7\n1001\n1\n5\n4\n4\n9\n10\n10\n7\n\n\ncomplete_entry\n20220508\n7\n1010\n1\n1\n6\n3\n7\n3\n0\n2\n\n\ncomplete_entry\n20220508\n7\n1011\n0\n9\n6\n3\n10\n7\n2\n2\n\n\nto_predict_1\n20220509\n1\n1200\n?\n6\n\n\n\n\n\n\n\n\nto_predict_2\n20220510\n2\n1200\n?\n6\n8\n\n\n\n\n\n\n\nto_predict_3\n20220511\n3\n1200\n?\n6\n8\n1\n\n\n\n\n\n\nto_predict_4\n20220512\n4\n1200\n?\n6\n8\n1\n5\n\n\n\n\n\nto_predict_5\n20220513\n5\n1200\n?\n6\n8\n1\n5\n9\n\n\n\n\nto_predict_6\n20220514\n6\n1200\n?\n6\n8\n1\n5\n9\n6\n\n\n\ncomplete_entry\n20220515\n7\n1200\n0\n6\n8\n1\n5\n9\n6\n8", "Using a predicted value would introduce bias in your models.\nFor me you should train 7 different models :\n\nOne with feature_day1 only\nOne with feature_day1 and feature_day2\nOne with feature_day1, feature_day2 and feature_day3\netc ...\nThe last one with all the feature_day1 to feature_day7\n\nThen you have to investigate how to weight all your models to get the best predictions. I assume that your first model with only feature_day1 won't be the best, so give it low weight or don't use it at all.\nYou have to test different weights anyway", "Logistic regression with chronologically available features", 0.0, 0, 408, "I think that a standard approach would be to use the same data for training and validation, i.e. use a sliding window with window size equal to the length of data and only use data starting with the most recent date. Then you can predict and evaluate on all the dates between the beginning of training data (in your case, Day 0) and the current date (Day 8 in your example).\nThis is the best way to avoid overfitting.", 0.28729280829429626, 0.044692736119031906, 0.18784530460834503, 0.06050214543938637], ["I have been working with neural networks for a few months now and I have a little mystery that I can't solve on my own.\nI wanted to create and train a neural network which can identify simple geometric shapes (squares, circles, and triangles) in 56*56 pixel greyscale images. If I use images with a black background and a white shape, everything work pretty well. The training time is about 18 epochs and the accuracy is pretty close to 100% (usually 99.6 % - 99.8%).\nBut all that changes when I invert the images (i.e., now a white background and black shapes). The training time skyrockets to somewhere around 600 epochs and during the first 500-550 epochs nothing really happens. The loss barely decreases in those first 500-550 epochs and it just seems like something is \"stuck\".\nWhy does the training time increase so much and how can I reduce it (if possible)?", "Color inversion\n\nYou have to essentially \u201cswitch\u201d WxH pixels, hence touching every possible pixel during augmentation for every image, which amounts to lots of computation.\nIn total it would be DxWxH operations per epoch (D being size of your dataset).\nYou might want to precompute these and feed your neural network with them afterwards.\n\nLoss\n\nIt is harder for neural networks as white is encoded with 1, while black is encoded with 0. Inverse giving us 0 for white and 1 for black.\nThis means most of neural network weights are activated by background pixels!\nWhat is more, every sensible signal (0 in case of inversion) is multiplied by zero value and has not effect on final loss.\nWith hard {0, 1} encoding neural network tries to essentially get signal from the background (now 1 for black pixels) which is mostly meaningless (each weight will tend to zero or almost zero as it bears little to no information) and what it does instead is fitting distribution to your labels (if I predict 1 only I will get smaller loss, no matter the input).\n\nExperiment if you are bored\n\nTry to encode your pixels with smooth values, e.g. white being 0.1 and black being 0.9 (although 1.0 might work okayish, although more epochs might be needed as 0 is very hard to obtain via backprop) and see what the results are.", "Massive neural network training time increase by inverting images in a data set", 0.1973753273487091, 1, 409, "You could try to reduce your training set by discarding all of the invalid images from the set. It is highly likely that your dataset is large enough to be able to train the network without any data from invalid examples. Try to find the ratio of valid to invalid pixels for each example and then discard the ones that are not valid. A good heuristic is to choose the threshold to discount the pixels so that you end up with an approximately even number of examples for training and validation.", 0.22769230604171753, 0.02476780116558075, 0.1230769231915474, 0.061423368752002716], ["I have a dataframe with 500+ columns and I want to just store the values from certain columns that contain the string \"valid\" in their names and store them in a new empty list.\nI have used df1=df.filter(regex='valid').values.tolist()\nMy earlier method was - df1=[df['1valid'],df['2valid'],...df['nvalid']] \nI'm unable to differentiate between the two. Any help would be appreciated.", "df.filter(regex='valid') returns DataFrame whose column contains pattern valid. df.values return a Numpy representation of the DataFrame. numpy.tolist() convert the Numpy array to Python list.\ndf1=[df['1valid'],df['2valid'],...df['nvalid']] is a list of Series.", "Append Dataframe columns as a list", 0.0, 0, 410, "Try this,\nYou can use `pd.DataFrame.select_dtypes`\nAlso, I've used `regex` instead of `str` because that works on a string as well. (Not sure why you want `dtype` to be `int`)\nHope that helps.", 0.12820513546466827, 0.0, 0.07692307978868484, 0.03544885292649269], ["I am trying to create rules for rounding a datetime column. So far I can only find how to round either to the current year or round up to the next year using df['Year'] = df['Year] + pd.offsets.YearBegin(0) to round up, and df['Year'] = df['Year] + pd.offsets.YearBegin(-1) to round to current year.\nHowever, I am trying to round as follows:\nIf df['Year'] is in the last quarter of the year (> Sep 30th), then round up to next year, otherwise leave as current year.", "I seem to have found an answer to my own question. If I'm not mistaken the following should work with all dates:\ndf['Year'] = np.where(df['Year'].dt.month < 10, df['Year'].dt.year, df['Year'].dt.year + 1)", "Pandas rounding up to next year if above a certain date", 0.0, 0, 411, "You can use the [`.dt.is_quarter`](https://pandas.pydata.org/docs/reference/api/pandas.__datetime__.html#pandas-datetime-isquart) and [.dt accessor](https:).\n\\begin{itemize}\nFirst, get the index\n`idx = pd.__dateutil_parser.__parser.parse('2021-01')`\nThen check for the first day of a quarter\n`:idx.quater == 2`\nand if true, roundup to `idx + 1` year\n`.round(1)`\nFinally, you can create a series with a mask to apply the rule\n[`.isin`](http://docs.scipy.com/doc/numpy/user/basics.indexing.html)\nto the mask\n:`.idx[idx == False]`\nAnd apply round(up) on the Series\n``\n.loc[mask, 'Year'].round()\n'''\nidx\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.006944737716498286  # first", 0.14193548262119293, 0.0, 0.06451612710952759, 0.0476190447807312], ["I have installed miniforge on my mac , in that using a 'local env' on jupyter notebook. I can't change the numpy version (need to downgrade to 1.19.5) on this kernel, have tried:\n(1)pip install numpy==1.19.5 &\n(2)conda install -c conda-forge numpy=1.19.5.\nnumpy version seems to be changing easily on conda-ipython3 kernel, but my project is running on 'local env'\nvery new to all this, still learning. Please help", "first make sure that your local environment is activated by running: .../{your venv folder path}/Scripts/activate. Because if you install numpy on the wrong virtual environment then it won't work.\nThen uninstall numpy by running pip uninstall numpy. Then install the numpy version you want.", "how to change numpy version with miniforge", 0.0, 0, 412, "You need to install `numpy` with `pip` or `conda` \\em{and} `ipykernel`\nIn the \\strong{Python} part of the jnkyter, you will see a command line where you can type `!pip list` to see all packages installed.", 0.1666666716337204, 0.024390242993831635, 0.1428571492433548, 0.03447336331009865], ["How do you measure with features of your dataframe are important for your Kmeans model?\nI'm working with a dataframe that has 37 columns of which 33 columns are of categorical data.\nThese 33 data columns go through one-hot-encoding and now I have 400 columns.\nI want to see which columns have an impact on my model and which don't.\nIs there a method for this or do I loop this?", "For categorical values there is K-Modes and for mixed (categorical and continuous values) there is K-Prototype. That might be worth trying and potentially easier to evaluate. You wouldn't use one-hot encoding there though.", "Kmeans clustering measuring important features", 0.0, 0, 413, "You can use the `feature_importance` method to get a list of the features that contributed the most to the model. Here is a small example using the iris dataset. You would need to adjust it to use your own data:\n`import pandas as pd`\n\\begin{code}\nimport numpy as np\nfrom sklearn.datasets import load_iris\ndata = load.iridata()\nX = data[:,1:4]\ny = np.array([0,0])\nclf = KNeighborsClassifier(n_neighbor=3)\nscores = clf.fit(X, y).scored_transform(data[1])[0][\"feature importance\"]\nprint(scoring.feature.importances_[scorer.scorers_])`enter code here`[3] #5 is the highest\n#The higher the number, the more important it is to your model\ndf = pd.DataFrame(columns=data.columns) #create a new dataframe\nfor i in range(len(df.index)):\n    if scores[i] == 5: #if the score is 10 (", 0.07999999821186066, 0.0, 0.04571428522467613, 0.06308235973119736], ["I want to create ML Model to identify if the Transaction is fraud or not.\nEach row represents one Transaction. I understand that this ML Model can be built. What the model will be missing is the behaviour when Multiple Transactions are done within short duration. How do I capture that behaviour? If 1st transaction for a card happens at 10 am and other transaction happens at 10.01 am then that Transaction is generally Fraud. But my model is missing that. Please help", "Add another column(s) to your data which is \"time since last transaction\" and/or perhaps \"number of transactions in the previous n mins\". You could experiment on different values of n or even include multiple.\nThis seems like it would capture the information required? Hope that helps!", "How to create Credit Card Fraud Detection model so that it captures dependency in the observations", 0.0, 0, 414, "There are two approaches. One is to use a time-series model (as you mention in your question). The other is a classification model. There is an excellent post about time series models here. It is also possible to build a model using both approaches: a multivariate time serie...\nhttps://towardsdatascience.com/how-to-create-a-credit-card-fraud-dete...", 0.14814814925193787, 0.0, 0.09259258955717087, 0.058328695595264435], ["I have a data frame and around 10-12 columns. One of the column is the student number e.g. 1234567 and the other is an identifier e.g passport numbers, license number . How can I find that each student has a unique identifier. Like student 1234567 has identifier ABC5679K only. Also I want to store the students who are tagged with duplicate identifier. For e.g. If student 1234567 also has identifier ABC3408T, I want to know those.", "df.groupby([\"student_name\"])[\"passport_number\"].nunique() > 1\nYou can use the groupby and nunique function to help you identify repeats. Hope this answer your question.", "How to find that each student number belongs to unique student?", 0.0, 0, 415, "This is a common problem in data science. You can apply the [K-Means algorithm](https://en.wikipedia.org/wiki/K_means_clustering) to solve this problem. Firstly, you will need to pre-process the data to get the numeric value of your columns that you are trying to compare. In your case, this is student's ID and pass/license numbers. Then you can implement K-meyans to separate the different students. The algorithm would then give you a number of clusters. Once you have the clusters, the next step is to check if each cluster is unique. This can be done by looking for all the elements in each of those clusters to see if they are in the same cluster. And if not, then you may want the cluster to be a single cluster as well.", 0.12121212482452393, 0.012269938364624977, 0.08484848588705063, 0.03378377854824066], ["index\nvariable\nvalue\n\n\n\n\n0\nA\nup\n\n\n1\nA\ndown\n\n\n2\nA\nup\n\n\n3\nA\nup\n\n\n4\nB\ndown\n\n\n5\nB\nup\n\n\n6\nB\ndown\n\n\n7\nB\nup\n\n\n8\nC\nup\n\n\n9\nC\nup\n\n\n10\nC\ndown\n\n\n11\nC\ndown\n\n\n12\nD\nup\n\n\n13\nD\ndown\n\n\n14\nD\ndown\n\n\n15\nD\nup\n\n\n\n\nFor example, I want to draw a boxplot by using seaborn to show values in (variable =A and variable=B). How can I solve it?\nAS before, the row table contain attributes{\"variableA\", \"variableB\",\"variableC\",\"variableD\",\"value\"}\nso I can use :\nsns.boxplot(x=df[\"variableA\"],y=df[\"variableB],order=[\"up\",\"down\"])\nAnd now I got a melt table(tidy dataframe). How to draw the same picture?", "You can select B and A from variable by\ndf.loc[(df.variable == \"A\") & (df.variable == \"B\")]\nThen Transpose the df by:\ndf_T = df.loc[(df.variable == \"A\") | (df.variable == \"B\")].T\nThen sns:\nsns.boxplot(x='variable', y = 'value', data = df_T)", "How to use seaborn to draw boxplot by choosing specific row?", 0.0, 0, 416, "The `melt` function is useful for getting data into long format. If you want the data in long form, then you can also use the `sidesticker` argument.\nTry:\n`snb.sidenum_box(df, sidestickers=['variable'])`\nThis will create boxplots for each variable in your data set.", 0.17977528274059296, 0.02298850566148758, 0.15730337798595428, 0.019607843831181526], ["Numpy ndarray must have all elements of the same type and all sub-arrays on the same level must be of the same length. Those properties are also properties of C multidimensional arrays. Is it the case that numpy ndarray have those properties purely because it is implemented on top of C array? Are those properties really required to create a fast multidimensional array implementation?", "Is it the case that numpy ndarray have those properties purely because it is implemented on top of C array?\n\nNo. Using C internally is not really what cause Numpy to make this choice. Indeed, Numpy array are just a raw contiguous memory buffer internally (allocated dynamically). Numpy does not actually make use of C array in its own implementation. It only uses C pointers. Views are Python objects referencing the buffer and olding some meta information like the strides, shape, type, etc. Python users always operate on view as raw buffers cannot be directly read/written.\nIn fact, it is not very difficult to create jagged array in C (ie. arrays containing arrays of variable size), but one need to to that manually (ie. not directly supported by the standard). For 2D jagged array, this is typically done by allocating an array of T* items and then performing N allocation for the N sub-arrays. The sub-arrays are not guaranteed to be contiguously stored.\nThe point is jagged arrays are not efficient because of memory fragmentation/diffusion and non-contiguity. Additionally, many features provided by Numpy would not be (efficiently) possible with jagged arrays. For example, creating sub-view for other view with a stride would be tricky. The operations working on multiple axis (eg. np.sum(2D_array, axis=0)) would have to be redefined so it make sense with jagged array. It would also make the implementation far more complex.\nAs a result, they choose not to implement jagged array but only ND-array. Please note that Numpy have been initially created for scientists and especially physicists which rarely need jagged array but care about high-performance. Jagged arrays can be implemented relatively efficiently by allocating 2 Numpy arrays: 1 array concatenating all lines and a slice-based array containing the start/end offsets.\n\nAre those properties really required to create a fast multidimensional array implementation?\n\nHaving homogeneous types is critical for performance. Dynamic typing force a type check for each item which is expensive. Additionally, dynamic typing often requires an additional expensive indirection (eg. the array only store pointers/references and not the object itself) and the access to the objects cause memory fragmentation/diffusion. Such operations are very expensive compared to basic numerical ones like addition/subtraction/multiplication. Furthermore, the life cycle of the object must certainly be carefully controlled (eg. garbage collection) liek CPython does. In fact, a CPython list of list behave like that and is pretty inefficient. You can make Numpy array of objects that are Numpy arrays but this is also inefficient.\nAs for the rectangular arrays, it is dependent of the use-case, but this is at least critical for matrix multiplication and matrix-vector products (as BLAS operates on contiguous arrays possibly with a stride between lines), as well as operations not working on the most contiguous dimension (compilers can make more aggressive optimizations with a constant stride). Not to mention the additional overheads specified above (eg. additional checks and memory fragmentation/diffusion).", "Is numpy ndarray homogeneous and rectangular (sub-arrays must be the same length) because it uses C array under the hood?", 1.2000000476837158, 3, 417, "The question is a little confusing.  `numpy` arrays are not C arrays, but they are similar.\n\\begin{itemize}\n  \\item They are multithreaded, so an array can be modified by one thread while being read by another. This is not possible with C `malloc` or `calloc`.\n \\end{blockquote}", 0.10869564861059189, 0.03272727131843567, 0.07246376574039459, 0.03463170677423477], ["I have a dataset that is imbalanced and wanted to use techniques such as SMOTE, ADASYN etc, to balance it out.\nWould it be acceptable to use Doc2vec and then incorporate SMOTE to the training sample?", "The best way to know if SMOTE (or some other augmentation technique) might help with your particular data, goals, & classification-algorithms is to try it, and see if it improves results on your relevant evaluations, compared to not using it.\nIt's \"acceptable\" if it works; there's no other outside/1st-principles to judge its potential applicability, without trying it.", "would it be possible to combine Balancing techniques with Doc2vec", 0.1973753273487091, 1, 418, "Doc 2VEC is a vectorization technique that produces a representation of each document as a low-dimensional vector. It is an unsupervised technique, so it does not use any labelled data. The output of Doc-2Vec is used to train a linear SVM. So it is not possible use it in conjunction with SMote.", 0.208695650100708, 0.0, 0.12173912674188614, 0.06882073730230331], ["AFAIK, unlike SMOTE,  RandomUnderSampler selects a subset of the data. But I am not quite confident to use it for categorical data.\nSo, is it really applicable for categorical data?", "Under/Over sampling has nothing to do with features. It relies on targets and under/oversamples majority/minority class, no matter whatheter it is composed of continuous variables, categorical ones, or elephants :)", "Can I use RandomUnderSampler for categorical data as well?", 1.2000000476837158, 0, 419, "If the categoric variables are discrete, then it is applicable.  I don't see a reason it wouldn'r be.", 0.1538461595773697, 0.03999999910593033, 0.07692307978868484, 0.04592530429363251], ["I'm training a text classifier for binary classification.  In my training data, there are null values in the .csv file in the text portion, and there are also null values in my test file.  I have converted both files to a dataframe (Pandas).  This is a small percentage of the overall data (less than 0.01).\nKnowing this - is it better to replace the null text fields with an empty string or leave it as as empty?  And if the answer is replace with empty string, is it \"acceptable\" to do the same for the test csv file before running it against the model?", "IMO, if dropping the null values is not an option, you could replace the nulls with the most frequent words.\nJust make sure that you do this separately for each set, meaning what words are most frequent in the training set, and what words are most frequent in the test set, as they may differ.\nAnother option, is to replace the nulls with something like IGNORE TEXT.", "Machine Learning Question on missing values in training and test data", 0.0, 0, 420, "As long as the values are not null in your training dataset, you can replace them with null. You should replace null with a default value.", 0.301075279712677, 0.0, 0.19354838132858276, 0.05309734120965004], ["I have dataframe as follows:\n\n\n\n\nEmployee\nSalary\n\n\n\n\nTony\n50\n\n\nAlan\n45\n\n\nLee\n60\n\n\nDavid\n35\n\n\nSteve\n65\n\n\nPaul\n48\n\n\nMicky\n62\n\n\nGeorge\n80\n\n\nNigel\n64\n\n\nJohn\n42\n\n\n\n\nThe  question is to identify:\n\nTop 30% gets a value \u201chigh\u201d\nThe next 40% gets \u201caverage\u201d\nthe Rest as \"Low\"\n-and put it in a new column as the corresponding value\n\nit would be easy to identify top N of them but top 30% is something I am unable to understand how to go about the %. Can anyone help me with python code for this??", "If you think about what a percentage actually is, it only shows the proportion of something. It depends on the amount of people in your list.\nTherefore, the top 30% can actually be translated into a number of people.\nAssume your data  has N employees. Taking the top 30% salaries is the same as taking the 30xN/100 people that have the biggest wage.\nIf you order your data, then the only thing you actually have to do is setting \"high\" for these 30xN/100 people, \"average\" for the 40x100/N next, and \"low\" for the rest", "How to identify top 30% salary in python dataframe", 0.0996679961681366, 1, 421, "You can use `df.rank()` and `quantile()`\n\\begin{itemize}\n  \\item `rank(x=df['salary'])` to get the rank of each employee\n    \\strong{Ex:} `ranks = df.set_index('salaries').rank('descending')`\n   \\em{Result:}\n[2, 21, -1125,13,26,31]\n1681946 rows \u00d7 1 columns\n \\end{blockquote} [Source](https://pandas.pydata.org/pandas-docs/stable/reference/api/pd.Series.dt.nlargest.html)\nNext, calculate the `n=33` percentile\n`quantiles = ranks.quantilize(32/100)`\nThis will give you the quantiles of the ranked salaries\nNow, assign the values to the new columns: `sal_rank = sal.map(lambda x: 'high' if x <= quantilies[0] else 'averge'  if quantile[1:3]<=x<=quantiled[3:5]  else \"low", 0.20192307233810425, 0.0, 0.11538461595773697, 0.030634576454758644], ["I'm getting a terrible trouble with my Deep Learning porjects. My google colab files mostly fail to save. The status shows\n\nSaving Changes...\n\nHowever, never succeeds. After a while\n\nAutomatic document saving has been pending for n minutes. Reloading\nmay fix the problem. Save and reload the page.\n\nReloading is not the remedy and after reloading, the problem is not solved. I really don't know what to do with it. Any ideas?", "You can check disk space or permits, sometimes drive and collab became unstable.\nAlso check if it can be saved while kernel is free and not busy.", "Google Colab IPython notebook files often fail to be saved", 0.0, 0, 422, "You can fix this by following the steps below. This is an answer to [this question](https://stackoverflow.com/questions/50623867/google-colab-ipython-notebook-files-often-fail-to-be-saved)\n\\begin{itemize}\n \\item Click on `Run all` button. It will show up after 15-20 seconds. Then click on it again to see the options. You will see a `Save` option. Check it and hit the `save`\n button again. Now you can save your files. (I guess this is a workaround) [enter image description here](http://i.stack.imgur.collab.cc/7w7Nk.png) It's not a solution. But it will work until the real solution comes. And I think the solution is still to come. Google colabs are still in beta. There are a lot of things to improve. So I hope Google is working on this problem, because this could be a great tool for a long run. [Please google to know more about googlecolabs](//www.youtube. com/watch?v=8Zn1E", 0.1256544440984726, 0.021164020523428917, 0.07329843193292618, 0.07692307978868484], ["I'm currently developing a clustering model and seeking something a bit novel.\nI have looked at initial clustering to 5 clusters and then applying another run to cluster these into 2 each (so 10 total).\nResults are similar but definitely not the same as if I ran just once for 10 clusters, rather than 5 and then 2.\nIs there any obvious difference or benefit / drawback to such an approach? I cannot find much academia on this and potentially with good reason.", "Think about generalizing your approach by searching for optimal numbers of clusters in both steps (combined) to minimize the number of clusters while maximizing your coverage. This is an objective where your method benefits.", "Is there benefit in multiple levels of clustering?", 0.0, 0, 423, "You could do it in two steps if you want to. The only issue is that you will not be able to compute the distance between clusters. But it depends on your application and how you use it. For example, you can run a cluster analysis to identify 3 clusters of patients, and use those clusters to build a regression model to predict the probability of hospitalization. It is possible to use a single cluster model with this type of data.", 0.208695650100708, 0.0, 0.13913042843341827, 0.03657045215368271], ["I have a NumPy array of shape (4809, 200, 31) and i want to extract following array out of it : shape (4809, 200, 1). so I want to extract one column from axis=2 from the source array which will have three-axis (0,1,2).", "Its as simple as doing\nA[:,:,0:1].\nThanks @Mark for the answer.", "Spliting a 3D numpy array on desired axis", 0.0, 0, 424, "Use [numpy.split](https://docs.scipy.org/doc/numpy/reference/generated/scikit-learn.neighbors.KNeighborhoodMixin.html#scikitlearn-neigbhorhoodmixin-split) with `axis=1`.\n`sci.ndarray.shape` returns the shape of the array, not the data.\nTo get the axis 0 (time) shape, use `shape[1]`.\nThe resulting array will be of size (8192,)\nIf you want the last dimension to be 4907 you can use:\n\\begin{itemize}\n  \\item `np.newaxis`\n    \\begin {code}", 0.0714285746216774, 0.0, 0.0714285746216774, 0.03225806728005409], ["I have a small medical dataset (200 samples) that contains only 6 cases of the condition I am trying to predict using machine learning. So far, the dataset is not proving useful for predicting the target variable and is resulting in models with 0% recall and precision, probably due to the scarcity of the minority class.\nHowever, in order to learn from the dataset, I applied Feature Selection techniques to deduct what features are useful in predicting the target variable and see if this supports or contradicts previous literature on the matter.\nWhen I reran my models using the reduced dataset, this still resulted in 0% recall and precision. So the prediction performance has not improved using feature selection. But the features returned by the applying Feature Selection have given me more insight into the data.\nSo my question is, is the purpose of Feature Selection:\n\nto improve prediction performance\nor can the purpose be identifying relevant features in the prediction and learning more about the dataset\n\nSo in other words, is Feature Selection just a tool to achieve improved performance, or can it be an end in itself?\nThank you.", "In short, both answers are correct.\nFeature selection has two main purposes:\n\nIt reduces the number of features in the dataset. This reduces the model training time and reduces the chance of overfitting.\nIt helps you understand the data i.e. which features in the dataset are the most important.\n\nHence, I would not expect feature selection to help when training your model, unless you are overfitting the training data.", "What is the main purpose of Feature Selection?", 1.2000000476837158, 3, 425, "The main goal of feature extraction is to create a representation of your data that is easier to work with. The feature that are selected must be relevant and useful to your problem, so it must improve the model performance.", 0.27272728085517883, 0.018518518656492233, 0.16363635659217834, 0.05347283557057381], ["I have a small medical dataset (200 samples) that contains only 6 cases of the condition I am trying to predict using machine learning. So far, the dataset is not proving useful for predicting the target variable and is resulting in models with 0% recall and precision, probably due to the scarcity of the minority class.\nHowever, in order to learn from the dataset, I applied Feature Selection techniques to deduct what features are useful in predicting the target variable and see if this supports or contradicts previous literature on the matter.\nWhen I reran my models using the reduced dataset, this still resulted in 0% recall and precision. So the prediction performance has not improved using feature selection. But the features returned by the applying Feature Selection have given me more insight into the data.\nSo my question is, is the purpose of Feature Selection:\n\nto improve prediction performance\nor can the purpose be identifying relevant features in the prediction and learning more about the dataset\n\nSo in other words, is Feature Selection just a tool to achieve improved performance, or can it be an end in itself?\nThank you.", "Great answer from Tom. I will add another motivation: it helps the model with learning more from small datasets (which is one aspect of overfitting). In an ML task where you do not have a stretchable budget for more data points, feature selection can be one of your best tools.", "What is the main purpose of Feature Selection?", 0.1973753273487091, 2, 426, "Features selection is used for feature engineering. It is done to reduce the number of features used in your model for the sake of improving the performance of your machine learner. The features selected from your data should be relevant to your task and your target. They should make your learnt model more accurate. Fewer features should not necessarily make a better model, it is all about how you use them and how they relate to what you are trying do. If you have 2 features that are the same and one of them makes your accuracy improve, then it would be wise to keep that one and discard the other. Similarly if you find that 3 features you were using make the accuracy go up by a lot and a new feature makes it go down, you may not want to add it to you model. These decisions are made by you and what your dataset shows you, not by some algorithm.", 0.22535210847854614, 0.01895734667778015, 0.10328638553619385, 0.052173912525177], ["In sklearn.roc_curve the results returned are fpr, tpr and thresholds.\nDespite drop_intermediate set to False, the shapes of fpr, tpr and thresholds\nchange with random states.\nWhy is that?\nAs an example, I have:\n\ntest_labels and predicted_probabilities are (158,).\nfpr, tpr and thresholds are (149,), in another run they are (146,).", "the internal algorithm eliminates repeated scores from thresholds, so if you have repeated entries whose scores are exactly equal then they will be removed.", "Why does sklearn.roc_curve return varying shapes for fpr and tpr and thresholds with varying random states?", 0.3799489736557007, 2, 427, "The thresolds is the same (in your example 1.5). The only difference is in the prediction probabilities (the actual prediction and the probability that the class is correct). This is normal and expected behaviour.", 0.033898305147886276, 0.0, 0.033898305147886276, 0.08269722759723663], ["In Python, I have a vector v of 300 elements and an array arr of 20k 300-dimensional vectors. How do I get quickly the indices of the k closest elements to v from the array arr?", "Since 300 is a very small number, sorting all elements and then just using the `k first is not an expensive operation (usually; it depends on how many thousand times  per second you need to do this).\nso, sorted() is your friend; use the key= keyword argument, sorted_vector = sorted(v ,key=\u2026)  to implement sorting by euclidean distance.\nThen, use the classic array[:end] syntax to select the first k.", "Argmin with the Euclidean distance condition", 0.0, 0, 428, "I think what you need is `scipy.spatial.distance.cdist`, which calculates the `C` distance between two vectors:\nhttp://docs.sciPy.org/doc/scikit-learn/dev/api/spldist.html\nAnd it has an `k` parameter which will give you the top `n` closest vectors to your vector `v`.\nYou can use it as follows: `cd.pdist(arr, v)`\nIt will return a list of k indices (in this case 11) of which the first index will be the closest vector to `vector ` v.\nIf you want the corresponding `arr` vectors, you can do `np.take(array, indices, axis=0)` to get the ndarray corresponding to the vector at the ith index in `indices`.", 0.2732240557670593, 0.0220994483679533, 0.1530054658651352, 0.038348086178302765], ["I am trying to solve the optimization problem with 4 variables.\nI have to give constraint in the scipy.optimize,\nthe constraint is x[1] < x[2] < x[3] < x[4].\nIs there any methodology to solve this problem in scipy.optimise", "You can do a variable transformation, for example,\ny[1]=x[1]\ny[2] = x[2]-x[1]\ny[3] = x[3]-x[2]\ny[4] = x[4]-x[3]\nThen you can use constraints like y[2] > 0, y[3] > 0, etc.", "Scipy optimise minimize", 1.2000000476837158, 0, 429, "The constraints should be included as an argument to the `minimize` function. I.e. you'd use the syntax `scipy_optimizer.minimum(function, x0, constraints=constraints)`.\nSee the [docs](https://docs.scikit-learn.org/stable/modules/generated/sklearn/optimization.MinimizerResult.html#skl-optimiz-min-result) for more details.", 0.08791209012269974, 0.0, 0.06593406945466995, 0.046666666865348816], ["I need a help on below image I want to achieve below logic in Python and I am newbie in Python.\n[![I need a help on below image I want to achieve below logic in Python and I am newbie in Python.][1]][1]\nAny help is appreciated.", "We can easily solve this problem by using recursion. The idea is to start from the top-left cell of the matrix and recur for the next node (immediate right or immediate bottom cell) and keep on doing that for every visited cell until the destination is reached. Also maintain a path array to store the nodes in the current path and update the path array (including the current node) whenever any cell is visited. Now, whenever the destination (bottom-right corner) is reached, print the path array.", "Want algorithm to find shortest path in grid which 3*3 dimension", 0.1973753273487091, 2, 430, "In the image, it looks like you are trying to get a single path to traverse the grid. You could use a graph, and for each cell of the matrix, check if it is connected to the target and if not, then add an edge from the cell to it. The edge could have a weight which is the length of each edge. This will then give you a DAG, which you can use to calculate the shortests path.", 0.31137725710868835, 0.06060606241226196, 0.20359280705451965, 0.03244580328464508], ["I have been using monotonically_increasing_id() for a long time and I just discovered a weird behaviour, I need explanations please.\nSo I have a df with 20 000 lines.\nI added an Id column:\nval df2 = df.withColumn(\"Id\", monotonically_increasing_id().cast(\"int\"))\nAnd surprise, I didnt get monotonically increasing ids, I found Id=1 on 5 different rows, Id=2 on 2 rows ....\nSo I thought maybe it was because of Spark distributing my dataframe, to be sure I did the following:\nval df2 = df.coalesce(1).withColumn(\"Id\", monotonically_increasing_id().cast(\"int\"))\nAnd the weird behaviour disappeared.\nAre my thoughts right ? Doesn't monotonically_increasing_id() manage dataframes repartitions automatically ?\nWhy didn't I encounter this behaviour previously, I always worked with much bigger dataframes and never did I have this error.\nThanks", "This may have to do with integer overflow as monotonically_increasing_id returns a long datatype.", "monotonically_increasing_id function behaviour explanation in Spark", -0.1973753273487091, -1, 431, "\\begin{blockquote}\nDoesn\u2019t `monotonisch_incresing_Id()` manage the data frames replications automatically?\n\\end{itemize}", 0.06451612710952759, 0.0, 0.06451612710952759, 0.033707860857248306], ["I had run the following OneHotEncoder code on jupyter notebook, it's fine:\nColumnTransformer(transformers=[('col_tnf',OneHotEncoder(sparse=False,drop='first'),0,1,3,8,11])],remainder='passthrough')\nIt's running and gives the output,\nwhile the same, I am running using PyCharm as a Streamlit app, its throwing error as\nAttributeError: 'OneHotEncoder' object has no attribute '_infrequent_enabled'", "Issue is resolve. You need to update the sklearn version same as the version you are using in jupyter notebook", "steamlit results in 'AttributeError: 'OneHotEncoder' object has no attribute '_infrequent_enabled''", 0.0, 0, 432, "It seems that there is some problem with the jax library.\nI have removed the OnehotEncoders and tried using K-Means for the clustering, and it worked.", 0.1702127605676651, 0.0, 0.1702127605676651, 0.07692307978868484], ["i have a few questions regarding H2O AI. As per my understanding, h2o AI powers Auto ML functionality. but need to integrate my own python jupyetr ML model. so my questions are,\n\nCan we use H2O AI without Auto ML and with our own python jupyter ML algorithm?\nIf yes, can we integrate that own manual scripted ML with Snowflake?\nIf we can integrate our own scripted ml algorithm with snowflake, what are the advantages of doing it that way? instead of an own manually-created python ML algorithm?", "H2O.ai offers a bunch of ML solutions: h2o-3, driverless ai, hydrogen torch to name the main ones.\nDriverless AI is AutoML driven, the user has, however, an option to provide a custom recipe (in Python) to customize it. Driverless AI has Snowflake integration.\nH2O-3 is a framework that implements a collection of popular ML algorithms. H2O-3 also integrates an AutoML solution utilizing the built-in algos. There is no option to integrate a 3rd party solution into H2O-3 AutoML and to extend H2O-3 algos other than by coding in Java (small Python customizations can be made by providing eg. custom loss function in GBM).", "H2O AI with own python machine learning model integration with snowflake", 1.2000000476837158, 1, 433, "I don't know what you mean by \"own python script\", but you can certainly train a model on Snowball and then use it in Hadoop with the AutoML functionality of Hive. The advantage of that is that the HADOOP ecosystem has much more robust data processing features (such as map reduce, for example), and you get to take advantage from the power of the underlying HDFS and HIVE infrastructure.", 0.18888889253139496, 0.0, 0.1111111119389534, 0.07053989917039871], ["Can anyone help with this error please:\nScripts\\gw_kd_tree.py\", line 89, in build_kdtree\nnode = KDTreeNode [(point=point_list[median], left=build_kdtree(point_list[0:median], depth+1), right=build_kdtree(point_list[median+1:], depth+1))]\nTypeError: list indices must be integers or slices, not float", "The only variable used to index is median thus the error probably triggering because median was calculated to be a float. This is unusable for indexing because you can't grab position 4.5 in a list, there is no half position in a list.\nTo solve this all you have to do is put int(median) wherever it appears or cast it to an int before this line", "# TypeError: list indices must be integers or slices, not float #", 0.0, 0, 434, "Looks like your median is a float (as it's not an integer)\nYou should check that before calling the method\n(the median should be a integer between 0 and n - 1)", 0.2574257552623749, 0.04040404036641121, 0.11881188303232193, 0.037566039711236954], ["I'm trying to install tensorflow, to do so I use the following:\nconda install -c conda-forge/label/cf201901 tensorflow\nHowever when I import tensorflow the following error raises up: ModuleNotFoundError: No module named 'tensorflow.python.tools'. I took a look at other questions here but the solutions didn't work for me. Can you help?\nI'm using python 3.7.1 and conda 4.12.0", "By default Tensorflow will be installed on GPU.\nTo install on CPU run this command pip install tensorflow-cpu\nIf that doesn't work try pip install tensorflow\nIf you are using anaconda environment, you can try conda install tensorflow\nI hope this will help you.", "tensorflow: ModuleNotFoundError: No module named 'tensorflow.python.tools'?", 0.2913126051425934, 3, 435, "It's because you're using Anaconda (a Python distribution manager), not Python.\nTry `conda list` to see what modules are currently installed. If you don' t have tensorflow installed, you can use `pip` or `python -m pip` (you can find out how to use these on this [page](https://www.anaconda.com/blog/installing-tensorflow-on-a-mac-using-pip-and-anacondas-conda-package-manager)).\nYou can also try `tensorflow` package on anaconda itself (`conda search tensorflow`), but it seems that it doesn' `t have the right version.", 0.37037035822868347, 0.07518796622753143, 0.17777778208255768, 0.041472867131233215], ["Am building a model using K nearest neighbours. The model predicts if someone has cancer or not. 1 for true, 0 for false. I need the model other than predicting presence of cancer or not giving a 0 or 1,how can i make the model also show the probability of the prediction being 1?\nEdit:Am doing a project and it specifies i use the K nearest Neighbour classifier", "You must use a regressor instead of a classifier, meaning that a regression model can give you a probability of someone having a concern or not and the probability between the two values of 0 and 1, 0~1 (0~100%).", "How to make a model predict probability", 0.0, 0, 436, "I don't know of a standard way of doing this but you could take the square root of your prediction to get the confidence value.\nI would be interested in the outcome of this.", 0.18421052396297455, 0.027027027681469917, 0.18421052396297455, 0.04194854199886322], ["I have a half precision input that may be an array scalar x = np.float32(3). I'd like to add one to it: y = x + 1. However y will be float64 instead of float32.\nI can only think of two workarounds:\n\nconvert the input to 1d array: x = np.float32([3]) so that y = x + 1 is float32\nconvert 1 into lower precision: y = np.float32(3) + np.float16(1) is float32\n\nHowever, I have a lot of functions, so the above fixes require me to add if-else statements to each function... Are there any better ways? Thanks!", "0x5 \"Adding Integer to half-float not producing the expected result\" Why is half the size?\n0x6a0100 \"float64 cannot be cast to numpy.complex64\" in ufuncs. Numpy should have known\nWe are going through a type conversion uncertainty since numpy 1.13. It was discussed in 0x67 \"Quick fix for integer operation with half dtype in NumPy\". A decision was made to resolve as follows: \"compatibility with Matlab, always convert to float16 before operation\".\nThe bug reported in 0x6e \"sum(a) where a = float32(1) is float64\" backtracked that decision, but without a clear understanding that:\nThe issue is with how datatypes propagate through scalar inputs. That's a bigger issue than just summing. Mixing scalars with arrays is always a gray area, as you experienced. In some contexts (deconte abd deduce) such a mix should raise, but there is no consensus how np should handle them (see 0x75 \"Array scalar artifact at a ufunc boundary\"). Until that's resolved..\nMatlab's upcasting, because it does it to 16, is not a good one for numpy. That upcasting is especially problematic for product, and might be the reason why numpy issues sometimes suggest that, but \"matlab doesn't need to be revised because mathematicians are used to this surprise\", which also means matlab is used by these mathematicians with warnings, and \"doesn't need to be revised because C was defined this way\", which also means C is used on floats as if they are integers to avoid the surprise.", "numpy: how to keep datatype of half precision array scalar input when adding a number", -0.06656807661056519, -1, 437, "You can just do `x = (x +1).astype(float)`\nNote that this will change the dtype of x in-place.", 0.06666667014360428, 0.0, 0.029629629105329514, 0.037962090224027634], ["I hope this isn\u2019t off topic, I am not really sure which forum to use for a question like this:\nI have a series of datapoints of about an hour in time from a sensor that retrieves data 20 times per second. Along with it I receive timestamps of a periodic event in this data in the format of %Y-%m-d %H:%M:%S.%f, which looks e.g. like this 2019-05-23 17:50:34.346000.\nI now created a method to calculate these periodic events myself and was wondering how I could evalute my methods accuracy. My calculations are sometimes bigger and sometimes smaller by a few milliseconds compared to the actual timestamp. But when I run my own calculated timestamp against the actual timestamp by using pythons scipy.stats.pearsonr(x,y) method, I always receive a correlation of nearly 1. I assume that\u2018s because these small differences in the order of millisenconds don\u2018t seem relevant in an hour of data. But how could I evaluate the accuracy of two timestamps a reasonable way? Are there better metrics to use than the correlation?", "It seems that you are trying to compute a linear statistical correlation (pearson) for something that is, by nature, a timeseries data. This will not tell you much and drawing a conclusion based on the results is dangerous.\nIt so happens that your two vectors x and y are growing linearly in the same direction which is not surprising given that they are timestamps.\nLet's take an example for stationary data and time series data:\nTime series data:\nYour sensor starts giving measurements at time t1 and continues to do so until time t2 is reached. You compute the periodic event's timestamp using your own method then compare it to the actual timestamp. However, there is no reliable way using linear statistical correlations to see if the two are related and how related are they.\nStationary data :\nNow consider the same sensor giving measurements, but now instead of computing your periodic events all at once, take a single event and compute it multiple times using your empirical data using different measurements (so forget about any notion of time at this point (i.e. repeat the measurement multiple times). The result can be averaged and an error on the mean can be computed (see info on standard error). This, now, can be compared to your single event. Based on the error, you can get a more or less feel of how good or bad your method is.\nI would recommend the following :\n\nYou have your ground truth answer (say, the periodic event) y_truth. You compute a vector of the periodic events based on your sensor and your own method mapped as a function  f(sensor_input) = y_measured\n\nNow you have two vectors, one measured and one that is ground truth. In each of those vectors, you have an indicator of a the periodic events such as an  id. I would repeat the whole set of measurements, on all  id's tens of times.\n\nFor each 'id' I would compute whatever measurement you are looking for (either a timestamp or time in seconds or whatever...) then I would subtract the two timestamps :  |y_truth - y_measured|. This is called residuals or in other words, your error.\n\nNow averging all the residuals of all the id's gives you something called mean absolute error (1/n * sum (|y_truth - y_measured|) which you can very confidently use to report how much error, in a unit of time (seconds for example), your method produces.", "Correlation between timestamps", 0.1973753273487091, 1, 438, "There are a number of ways to do this.  I have found the following to be useful in my research. The first step is to obtain a normalized version of the time difference between the two time series. This is done as follows. For each value of $t_i$, calculate the difference $d(t) = t_{i+1} - t_0$. Then the normalization is\n$$\n\\Delta t = \\frac{d(\\frac{\\tau}{2}) - d(\\tau/2)}{d({\\tau})}\n= \\lim_{t\\to\\infty}\\frac{(t+t/3)-(3t)/2}{t + 3/4} = 0\n $$\nThis function has the property that $\\Delta\\tau =0$ when the times are identical and that the function becomes negative when one time is behind the other. If you calculate this normalised time shift for your time stamps you should get a time range of  $0\\le \\Delta \\tau\\lt 4/5$.\nThe second step, is calculate a [relative entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)#Relative_entropy) $H", 0.24041812121868134, 0.0279720276594162, 0.11149825900793076, 0.038997214287519455], ["I'm trying to train a model and I've changed the runtime to include GPU (all my work is on Colab). I'm printing the predicted output of my model to make sure it's working properly, at first it was predicting the output just fine, however after the runtime disconnected once, it started predicting '0's and has been ever since. I've tried changing accounts, using VPNs, changing runtime types but without an accelerator it predicts the output once then proceeds with 'nan'. Am I missing some sort of restriction to Colab's GPU usages besides the 12 hour limit?", "It's likely that some sort of training instability, for example caused by invalid data, an edge case in your data ingestion code or loss function, or exploding gradients, has caused some of your model weights to become NaN.", "Does Colab's GPU predict '0's after using the resources over a long period of time?", 0.0, 0, 439, "It is not the GPU that is a limit, but the time available to you on the server.\nAfter that time, the Colaboratory server terminates the kernel. If you want to use the same GPU for a longer time you can use one of the following options:\n\\begin{itemize}\na) Use a persistent runtime and restart the session after a while. This is the easiest, fastest and most reliable option. You will need to restart each time. In this case, I would recommend saving your session before starting the persistent kernel, so you do not have to repeat the steps each restart. (See the answer by @Sourav Dutta)\nb) Install the Tensorflow Python library locally on your computer and use it in your persistent Cola runtime. However, this will require a lot of memory and processing power on you computer, and you will have the extra cost of downloading the library and keeping it updated. And finally, you are limited to the computational power of your local computer. See the example of @Bruno Borges.  It would be interesting to try this option with the nvidia-docker package. For example, https://github.com/nvidia/dockers/tree/master", 0.10212765634059906, 0.02575107291340828, 0.0765957459807396, 0.05908181518316269], ["Is it possible to use matplotlib.pyplot.loglog with log binning?", "Maybe use the function set_xscale() o set_yscale() e semilogx() o semilogy(). If you have to set both axes in the logarithmic scale, we use the function loglog().", "How to have logarithmic bins in a Python loglog plot", -0.1973753273487091, -1, 440, "Yes it is possible. If you want to make a log-log graph with a uniform bin width, you can do it with something like this:\n\\begin{itemize}\n  \\item First you create the logscale (log10) in your plot.\n    \\begin {code} <matplotlib.axes.Axes object at 0x12282590>\n       <Axessexecutor object> <pylab8.gca object (Gcf)>\n      <__main__.AplPlot object 11>  <Matplotlib._pyx.PyObject_HEAD> (0 references)\n     <_py_graph_canvas object) <PyCFunctions object <function log1p at pylab8.__main__.__pygen__> >\n   <class 'matplotlibrc.Line2D' object ...>       (1 references, 4064 bytes) > <list.__getitem__ object ...... > (4 references 6160 bytes >   (2 references , 2088 bytes).\n            <LineCollection object [  x  y  ... ]\n               <object", 0.11023622006177902, 0.01600000075995922, 0.07874015718698502, 0.036738596856594086], ["Images are represented as matrices. Is there a practical way to make sort of frame around the content of the image? (in a monoton color)", "Theres a lot of ways to do that. I think the easiest way is just to add an image with everything transparent except for the borders and then draw it on top of the screen every frame.", "Make a frame to an image in python", 0.0, 0, 441, "You can use the [`skimage.draw.labelled_img`](http://scikit-image-api.org/docs/dev/api/skimages/draw_label.html#sk-label-and-skim-text-annotations-to-an-img) method, which is described in [this example](http:/scikits.image/examples/plot_text.py).\nIn your case, you might want to use something like this:\n`sk.annotate.text(image, text=\"your text\", x=100, y=30)`.\nIf you want it to be in a particular color, then you can either use `matplotlib.axes.Axes.color_foreground` method to change the color of your text, or you could use [sklearn's `impute_missing_values`](https://stackoverflow.com/a/15997624/775880) to add the missing color to the images.\nHope this helps.", 0.19230769574642181, 0.012987012974917889, 0.08974359184503555, 0.011494253762066364], ["I am new to machine learning, but I have decent experience in python. I am faced with a problem: I need to find a machine learning model that would work well to predict the speed of a boat given current environmental and physical conditions. I have looked into Scikit-Learn, Pytorch, and Tensorflow, but I am having trouble finding information on what type of model I should use. I am almost certain that linear regression models would be useless for this task. I have been told that non-parametric regression models would be ideal for this, but I am unable to find many in the Scikit Library. Should I be trying to use regression models at all, or should I be looking more into Neural Networks? I'm open to any suggestions, thanks in advance.", "I think multi-linear regression model would work well for your case. I am assuming that the input data is just a bunch of environmental parameters and you have a boat speed corresponding to that. For such problems, regression usually works well. I would not recommend you to use neural networks unless you have a lot of training data and the size of one input data is also quite big.", "Suggestions for nonparametric machine learning models", 0.0, 0, 442, "If you have a lot of data, it may be better to consider an ensemble method. In the case of nonlinear regression, an ensembling method is called a random forest. With a good enough fit, the random forests should be able to provide a decent prediction.", 0.27586206793785095, 0.07017543911933899, 0.1551724076271057, 0.08172211050987244], ["Trying to convert the batch normalization layer from Tensorlayer version 1.11.1 to Tensorflow 2 and getting different outputs from this layer during inference using the same pretrained model.\nTensorlayer 1.11.1\ntensorlayer.layers.BatchNormLayer(network, is_train=False, name=\"batch_norm\") \nTensorflow 2.8.0\ntf.keras.layers.BatchNormalization(trainable=False, momentum=0.9, axis=3, epsilon=1e-05, gamma_initializer=tf.random_normal_initializer(mean=1.0, stdev=0.002))(network)\nWhat am I missing to get the BatchNorm output to match?", "The TF1 model I had was in NPZ format.\nThe weights from Tensorlayer are saved in the order of:\nbeta, gamma, moving mean, variance.\nIn TF2, the batch norm layer is in the order of:\ngamma, beta, moving mean, variance.\nIf the order of the weights for beta and gamma are reversed when moving from TF1 to TF2 it solves the issue.", "Convert batch normalization from Tensorlayer tf1.x to TF2 keras", 1.2000000476837158, 0, 443, "In Keras 01, the `batchnorm` layer is an `tf_ker.Layer` object, and the layer object has a `run_op` method which has the following signature:\n\\begin{itemize}\n  \\item \\strong{input_shape}: Input shape of the operation. It must have the\n    same shape as the output of this `Layer`.\n  2) \\em{optional} \\textbf{batch} (ndarray): A tensor to be used as a\n     batch dimension for the computation. This is required to\n       match the shape for input to `convolution` and `fully_connected`\n      layers, etc. If `None`, `inputs` will be\n        used instead. See the documentation for\n         `InputSpec` for details. Defaults to None. (default None) (from the source code of `BatchnormLayer`)\n \\end{blockquote}", 0.24418604373931885, 0.0235294122248888, 0.151162788271904, 0.03333333134651184], ["Originally, my input dataset had blank spaces. But I have cleaned it, and checked with:\ndf.isnull().sum()\nAnd everthing is 0.\nNow, after fitting my dataset into the LinearRegression model and about to make predictions, it's bringing the above error.", "Since you did mention that the error is happening during the prediction time, I would suggest that you make the testing data go through the same pipeline as the training data.\nFor example:\nraw training input -> preprocessing -> training input\nIt is necessary the test data also goes through the same preprocessing.", "Input contains NaN, infinity or a value too large for dtype('float64') LinearRegression: but there are no empty values", 0.1973753273487091, 1, 444, "The reason is that you have NaN in the data. This is normal in datasets and you cannot avoid it. However, the error is not really informative. There are 2 solutions. 1. You can use `dropna()` on your dataset before making predictions. It will drop the NaNs, but keep the other rows. The other solution is to drop NaN before fitting the model.", 0.2631579041481018, 0.0535714291036129, 0.17543859779834747, 0.06486795097589493], ["I'm generating PSF-free images, so no atmosphere and no diffraction, and the images I'm getting out have stars in \"quantized\" positions. I'm wondering if there is an option in GalSim to prevent this, i.e. to have a more sinc-like distribution of the photons, so the behaviour of photons landing somewhere between pixels is taken into account. If there isn't an option for this, I suppose I would need to create my own sinc-function PSF and implement it around the drawImage() step?", "Stars are inherently supposed to look like point sources if you don't have any PSF at all (no atmosphere, no diffraction).  They are a delta function in that case, so all of the photons should fall into a single pixel.  GalSim is doing exactly what you are asking it to do.\nIt sounds like you actually do want to have a PSF; I suggest using the galsim.Airy class, representing a diffraction-limited PSF.", "Is there a simple way to prevent GalSim from shooting all the photons from a star into a single pixel?", 0.1973753273487091, 1, 445, "You can create a small Gaussian PS and apply it to the star. This will cause the light to be spread out over the pixels.\nYou can do this by adding the following to your main class:\n`this.psf = new GaussianPSF(0.001);`\nAnd then setting the Gaussian Pixel Size to this number.", 0.203125, 0.01587301678955555, 0.140625, 0.03125], ["I want to run a Python script in my terminal (mac) that takes a csv file as input. At the beginning of the Pyton script, a package named cvxpy is imported. When running the code with data in the terminal I get the error:\nImportError: No module named cvxpy.\nI'm feeling it's a directory fault, but I don't know how to fix this (e.g. how to get the Python script and python packaga in the same directory)\nSomebody got a clue?\nThanks.", "You need to have the module installed.\nTo install it, type : pip3 install cvxpy\nIf you already have it installed, please double check by typing pip3 list", "Import python package from terminal", 0.0, 0, 446, "You'll need to install cvxy as a module. It'd be easier to do it using pip but if you need it to work on the command line you can use\n\\begin{itemize}\n    `pip install -r requirements.txt`\n(this will install everything listed in requirements file) or\n`pip3 install [name_of_requirements_file]`\n (this installs only what is in file named [requirement_name.py]\n in folder named requirements) but both these commands require\npip to be installed. If you are using virtualenv then you may not\nneed to have it installed on your system.", 0.26229506731033325, 0.13333334028720856, 0.21311475336551666, 0.03961651399731636], ["I run into this error in google colabs running cells.\nfrom sklearn.feature_extraction.text import TfidVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nimport pandas as pd\nimport json\nimport pickle\nImportError                               Traceback (most recent call last)\n in ()\n----> 1 from sklearn.feature_extraction.text import TfidVectorizer\nImportError: cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text' (/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py)", "You have it misspelled. Try from sklearn.feature_extraction.text import TfidfVectorizer", "ImportError: cannot import name 'TfidVectorizer' from 'sklearn.feature_extraction.text'", 0.0, 0, 447, "Use the version 0.14.2 of sklearns as that is the one that works for Google Colab.", 0.0, 0.0, 0.0, 0.013444697484374046], ["When preforming image co-registration of multiple subjects, how should we select the reference image?\n\nCan a randomly selected image form one dataset could be the reference image for an image from the other dataset?\nIf we do that, should all the images belonging to the reference image dataset be co-registered with the reference image as well?\n\nI couldn't find any material in this area. Could someone please advice?", "I'm not sure exactly what you mean by the term \"dataset\", but I will assume you are asking about co-registering multiple images from different patients (i.e. multiple 3D images per subject).\nTo answer your questions:\n\nIf there are no obvious choices about which image is best, then a random choice is fine. If you have e.g. a CT and an MRI for each subject, then co-registration using the CT images is likely going to give you better results because of intrinsic image characteristics (e.g. less distortion, image value linked to physical quantity).\nI suppose that depends on what you want to do, but if it is important to have all imaging data in the same co-registered reference space then yes.\n\nAnother option is to try and generate an average image, and then use that as a reference to register other images to. Without more information about what you are trying to achieve it's hard to give any more specific advice.", "3D Image co-registration between multiple subjects", 0.1973753273487091, 1, 448, "Co-aligning images is a very common task in medical image analysis. The way it is done is, however, a bit different for different kinds of images.\n\\begin{itemize}\n  \\item For MR images, the sequence number is the usual reference. This is because all images in the same sequence are likely to be acquired at the exact same time. As the scanner doesn'y move during a sequence, all 2D projections are also acquired from exactly the \\em{same} position in space. So, if you can find a few landmarks in all of your images that can be matched to each other, you could coalign them that way. In practice, landmark-based registration is rarely used anymore because it takes a lot of time to identify and match the landmakrs. For this reason, MR registration algorithms typically use a transformation-domain method. That means, that they align the entire image set to a reference (reference volume). In this case, typically the first volume is used. There are a number of transformation domain registration methods, and each of them has its advantages and disadvantages. However, these methods are usually not suitable for comparing images from different subjects. They work best when there is some similarity", 0.30645161867141724, 0.0486486479640007, 0.12903225421905518, 0.05104712396860123], ["I have a datatable\nDataTable(columns=columns, editable=True, selectable=True, autosize_mode='fit_columns', visible=False, height_policy='fit', index_position=None, width=60, margin=(5,5,5,0)).\nEven when it only has a few rows and everything is visible, when I try to edit the (only) column, a horizontal and vertical scroll bar appear. How can I get read of both of them, especially the vertical one?", "I probably cheated, but when I put non-editable columns next to the editable column then no scroll bars appear.", "Why does a vertical scroll bar appear when I edit a datatable in bokeh?", 1.2000000476837158, 0, 449, "\\begin{itemize}\n\\item Set `show_scrollbar` to `False`\nhttps://boke.io/docs/en/latest/api/bole.core.datatable.html?highlight=scroll%20bar#boleincore-datatables-show-scroll-bar\nNote, that it also removes the scrollbar on the right, so it is a bit hacky.\nOr\n2. Set the height to the size of the screen\n`height=min(max(height), min(size, size.height))`\nSo that you only have to scroll vertically. But then you have scrollbars on all sides. You can also use `height_max` and `width_min`, but it gets complicated. Or\n3. Use the `datagrid` instead, which is more flexible:\n[Bokeh: Datagri.js in JS or HTML](https:/bolee.org/29900174/datagit-js-in-html-or-javascript-for-table-like-grid)\nand [Bokieh Datatable](http://webglmonkey.com/blog/post/7", 0.06622516363859177, 0.01342281885445118, 0.0397351011633873, 0.06666666269302368], ["I'm new at coding and feel like to really understand it, I have to truly grasp the concepts.\nQuality of life edit:\nWhy do we do df[df['col a']] == x? INSTEAD of df['col a'] == x? when making a search? I understand that on the second expression I would be looking at column names that equal X but I'd love to know what does the addition of making it a list (df[]) does for the code\nI would love to know the difference between those two and what I am actually doing when I nest the column on a list.\nany help is appreciated thank you so much!", "So we use df[df['col a']== x] instead of just df['col a'] == x because to optimize the dataframe itself you are escencially telling the data frame with df['col a'] == x that you want a bool of true false if the condition is met (you can try this on your df and will see that when you do not put it in the df[] that it only will list df['col a'] == x as a list of true and false). so it pandas will first say \"What asking\"? then it will say \"You asked for X here is a series of True/False based on what you asked\" and finally \"You asked for all of the only True values of the series here is the dataframe the reflects only true\"\nDoes that help clear up what it is doing? Basically just pandas trying to be as optimal as possible. As well as when you start learning more and more you can add multiple arguments df[(df['col a'] == x) & (df['col b'] == y)] which would be hard to write and keep together if you only did df['col a'] for your serach", "Difference between df[df['col a']] and df['col a']?", 0.0, 0, 450, "Let's say we have a dataframe called `df`:\n`df = pd.DataFrame({\"A\":[1,2,3], \"B\": [4,5,6]})`\nNow we can use either of the following statements: `print(df[\"A\"])` or `pd.Series(data=df.iloc[:,0]).value` to print the values of column `A` of `DataFrame`.\nThe difference is: The former one is a \\strong{Series} while the latter is \\em{Pandas DataFrame} and is much faster. However, if we are to do some operations on `Series`, it will be faster to use the Series itself. So, in short, use Series if you need to perform operations and use Dataframe if not.", 0.3478260934352875, 0.06060606241226196, 0.18060201406478882, 0.0476190447807312], ["So I have an excel sheet that contains in this order:\nSample_name | column data | column data2 | column data ... n\nI also have a .txt file that contains\nSample_name\nWhat I want to do is filter the excel file for only the sample names contained in the .txt file.  My current idea is to go through each column (excel sheet) and see if it matches any name in the .txt file, if it does, then grab the whole column. However, this seems like a nonefficient way to do it. I also need to do this using python.  I was hoping someone could give me an idea on how to approach this better.  Thank you very much.", "Excel PowerQuery should do the trick:\n\nLoad .txt file as a table (list)\nLoad sheet with the data columns as another table\nMerge (e.g. Left join) first table with second table\nOptional: adjust/select the columns to be included or excluded in the resulting table\n\nIn Python with Pandas\u2019 data frames the same can be accomplished (joining 2 data frames)\nP.S. Pandas supports loading CSV files and txt files (as a variant of CSV) into a data frame", "how to filter a .csv/.txt file using a list from another .txt", 1.2000000476837158, 1, 451, "If you have Python 2.7 or higher, you can use `csv` module: https://docs.python.org/2/library/csv.html\nYou can read the data from the first file as a `list` and check if the name is in that list: `if sample_names in row:`. Then you iterate through the other file and extract the column names, `itertools.product` to generate all combinations of the columns, and then iterate over the combinations. Here is a sample code: http://ideone.com/fyR3g5\nThis is the best way I can think of without actually seeing your data.", 0.28248587250709534, 0.04571428522467613, 0.14689265191555023, 0.024128688499331474], ["I'm trying to do some clustering using the alghorithm k-means but I'm getting this error:ValueError: could not convert string to float: 'M'.\nI think this happens because my variable is categorical one and clustering only allows continuous variables.\nWhat should I do to the variable to make it continuous. Converting it using a dictionary is not a good idea because it makes no sense to say that M>F for example.", "K-means clustering is going to need numbers in order to compute the centers of the clusters in the space defined by the variables. You can just decide to define M as 0 and F as 1, or the opposite.\nHere M being greater than F or the opposite doesn't really matter as long as it gives the opportunity for the algorithm to separate the different data points in space in order to cluster them.\nHowever, if the clusters that are being looked for are not supposed to be subgroups of the different genders, there are going to be some problems with the fact of trying to use this feature and I would advise to only use continuous variables in that case.", "K-means clustering: ValueError: could not convert string to float", 0.0, 0, 452, "In general, it is \\strong{not} possible to convert a categoric variable (e.g., a nominal variable) into a continuous one (such as a numeric variable).\nThe reason is that in the nomenclature of mathematics, a real number is an \\em{element} of the real numbers (the set of real positive and negative numbers). Categorical variables are \\textit{instances} or \\textbf{objects} that are instances of a type. (E.G. A car is a \\emph{Mercedes Benz}, which is one instance of \\texttt{class=Merceds} type). So there is no real way to turn a Mercedes into the number \\sout{123} -1.5 (which is the value of 15th instance). In other words, \\begin{code}\n<cars>\n\\end{Code} is NOT a subset of `reals` or `integers`.\nThis is just the way it has to be.", 0.27131783962249756, 0.03125, 0.13953489065170288, 0.05904059484601021], ["I'm not quite sure how to phrase this question, so let me illustrate with an example.\nLet's say you have a Pandas dataframe called store_df with a column called STORE_NUMBER. There are two ways to access a given column in a Pandas dataframe:\nstore_df['STORE_NUMBER']\nand\nstore_df.STORE_NUMBER\nNow let's say that you have a variable called column_name which contains the name of a column in store_df as a string. If you run\nstore_df[column_name]\nAll is well. But if you try to run\nstore_df.column_name\nPython throws an AttributeError because it is looking for a literal column named \"column_name\" which doesn't exist in our hypothetical dataframe.\nMy question is: Is there a way to look up columns dynamically using second syntax (dot notation)? Not so much because there is anything wrong with the first syntax (list notation), but because I am curious if there is some advanced feature of Python that allows users to replace variable names with their value as another variable (in this case a state variable of the dataframe). I know there is the exec function but I was wondering if there was a more elegant solution. I tried\nstore_df.{column_name}\nbut received a SyntaxError.", "Would getattr(df, 'column_name_as_str') be the kind of thing you're looking for, perhaps?", "Replacing variable name with literal value in Python", 1.2000000476837158, 0, 453, "\\begin{code}", 0.0, 0.0, 0.0, 0.0], ["Can you please help me understanding how KNN regressor works:\n\nHow does KNN look for 5 nearest neighbours, when there are several predictors?\nDoes it look for K nearest neighbours for each predictor separately and then somehow combines the results together? If so, then why wouldn't it be possible for example to look for K1 neighbours on predictor P1, but K2 predictors on predictor P2 etc...Why is it \"K\" rather than an \"array of Ks\", where the length of the array equals the number of predictors?\n\nKNN is sensitive to the scale of the predictors, therefore MinMaxScaler is recommended (Python) to be used. Does it mean, that essentially I can leverage this property to my benefit, for example by increasing the scale of certain predictor that I want KNN to give a priority to.\n\n\nThank you", "kNN would in the case of multiple predictors look at the Euclidian distance between vectors in the predictor space. E.g., if you have three predictors x1, x2, and x3, all data points will be a point in the 3-dimensional space. To measure the distance you simply compute $d=\\sqrt{(p_1-x_1)^2+(p_2-x_2)^2+(p_3-x_3)^2}$, and use that to find the neighbors.\n\nYou can definitely influence the distance measurements by scaling differently. However, this should probably be done with some care and I would use something like cross-validation to make sure the assumptions work as expected.\n\n\nHope this helps!", "KNN regressor algorithm when there are several predictors", 0.0, 0, 454, "You have several questions. Let's see them one by one. 1)\n\\begin{itemize}\n  \\item K-Nearest Neighbours Regression is an algorithm for linear regression, where your training data points are in a 2-dimensional space. For example, you have 4 data-points, and they are: (10, 0.3), (0,.2), (-2,1.2) and (-3,0). You want to find a line that will connect these 3 points. KNNeighbor will pick 9 neighbors in total, from your 8 training points, since the 7th one is itself. You can find more information [here](https://en.wikipedia.org/wiki/K-nearest_neighbour_algorithm).\n1a) If you want it to predict your future points that are outside your current training set, KNeiighor will look at all your points in the current dataset, to see how close they all are to each other, which neighbourood will have the shortest distance to your point. This means that the K neighbour algorithm will be sensitive only to neighbout points which are within", 0.316546767950058, 0.08695652335882187, 0.158273383975029, 0.05325443670153618], ["how to install numpy and pandas in python 2.7.9 version using command line.\nplease help with complete installing process.\nI have tried in windows 10 OS , but it's not installed . Showing syntax error.\nI have used command in python 2.7.9 as below\npip install numpy / python -m pip install numpy\npip install pandas / python -m pip install pandas", "First I recommend you upgrade Python Package otherwise try to find which NumPy version supports python 2.7.9 then install like this pip install numpy==1.9.2\n[https://stackoverflow.com/questions/28947345/which-numpy-library-is-compatible-with-python-2-7-9]", "how to install numpy and pandas in python 2.7.9 version", 0.0, 0, 455, "You have to run the following commands one by one\n\\begin{itemize}\n    \\item Install numpy:\n        `pip3 install --upgrade numpy`\n     \\begin {code}", 0.2153846174478531, 0.0317460335791111, 0.1538461595773697, 0.015900209546089172], ["I have a dataframe of patient records as rows and various features as columns.\nI'd like to be able to search through this dataframe based on one feature and display the rows that correspond to that search in a Dash DataTable.\nWhat's the best way to do this? I understand DataTables have their own native filtering function, but is it possible to filter based on a user-entered value from an input field or dropdown value?", "Yes, completely doable. You need a callback for that. It will take as input the value of the dropdown, or input field, and its output will be the data prop for the table component. Inside the callback, you can filter your dataframe based on the input value.\nDepending on where you load the data for the table from, you may want to put the original, unfiltered data in something like a dcc.Store and pass that to your callback as a state value, to avoid having to make network or database calls repeatedly.", "Dash - search and display results in datatable", 1.2000000476837158, 0, 456, "Use the filter function to select the data you want. Then, use the DataView to create the table you desire.", 0.21238937973976135, 0.036036036908626556, 0.14159291982650757, 0.09090908616781235], ["I have to plot a large amount of data in python (a list of size 3 million) any method/libraries to plot them easily since matplotlib does not seem to work.", "I use quite intensively matplotlib in order to plot arrays of size n > 10**6.\nYou can use plt.xscale('log') which allow you to display your results.\nFurthermore, if your dataset shows great disparity in value, you can use plt.yscale('log') in order to plot them nicely if you use the plt.plot() function.\nIf not (ie you use imshow, hist2d and so on) you can write this in your preamble :\nfrom matplotlib.colors import LogNorm and just declare the optional argument norm = LogNorm().\nOne last thing : you shouldn't use numpy.loadtxt if the size of the text file is greater than your available RAM. In that case, the best option is to read the file line by line, even if it take more time. You can speed up the process with from numba import jit and declare @jit(nopython=True, parallel =True) .\nWith that in mind, you should be able to plot in a reasonably short time array of size of about ten millions.", "Easy way for plotting large amount of data in python", 0.0, 0, 457, "Yes, I think you need to use [matplotlib's `pyplot.plot` function](http://matlab.wikia.com/wiki/Pyplot_%28matplotlib%7Cpython%39) to do this. It will take a list as an argument, and will automatically plot it for you, so all you have do is:\n\\begin{itemize}\n    \\item Create your list. If it is a numpy array, use the `np.array()` function, if it'll be a string or list, you can simply use a `list()` constructor. The only thing to remember is that all elements of the list must be of a compatible type. In your case, it should be `int` or `float`.\n   \\strong{Note:} If you are using numpy, consider using `numpy.random.randn` to create your data. This function generates random numbers with a normal distribution, which makes the data more realistic and easier to visualize. I'm just adding this note in case you don't know numpy' functions, but it isn' t necessary for your question. You can use anything to generate your random data, though. For example, `", 0.3519061505794525, 0.06489675492048264, 0.16422288119792938, 0.030625835061073303], ["I am training a CNN model to classify simple images (squares and crosses) and everything works just fine when I use the cpu but when I use the gpu everything works until the training starts and i get this error:\n2022-06-15 04:25:49.158944: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\nAnd then the program just stops.\nDoes anyone have an idea how to fix this?", "if you use pycharm, you can select the \"Emulate terminal in output console\" option to print detailed error information.\nRun->Edit Configration->Execution ->Emulate terminal in output console\nOn windows, maybe CUDA is missing zlibwapi.dll file, and you can download it and  move it to bin of cuda.", "Training CNN model using keras works with CPU but not with GPU", 0.3799489736557007, 2, 458, "Keras is using cuDNn, which is the cuda library for deep learning. The problem is that cudnNn requires that you have a GPU that supports at least CUDA 7.0. If you don't have Cuda 9 you need to download it from here: https://developer.nvidia.com/CUDAnnounce/downloads\nI would recommend downloading Cudnn 11.2, as it will be compatible with the most recent versions of Kerase.", 0.22033898532390594, 0.03448275849223137, 0.1355932205915451, 0.05999999865889549], ["I am facing some difficulties using merge function in Pandas. I am looking for some kind of Vlookup formula to assist me on this. However, I couldn't solve my problem.\nMy data is huge and I couldn't share here due to confidential. However, I try to came up with similar data here.\n\n\n\n\nOld Code\nNew Code\nName\nInvoice Date\n\n\n\n\n1001011\nNA\nCheese Cake\n02/02/2021\n\n\n1001012\nNA\nCoffee\n03/05/2021\n\n\n1001011\nNA\nCheese Cake\n30/05/2021\n\n\nNA\n2002093\nJasmine Tea\n21/08/2021\n\n\nNA\n2002042\nCookies\n31/12/2021\n\n\nNA\n2002080\nCoffee\n09/01/2022\n\n\nNA\n2002093\nJasmine Tea\n05/05/2022\n\n\nNA\n2002058\nCheese Cake\n07/06/2022\n\n\n\n\nI would like to have a COST Column input in my table above. However, the cost is very by invoice date (Also take note on the changing of product code). We have 2 cost table.\nFor year 2021:\n\n\n\n\nOld Code\nNew Code\nName\nJan-21\nFeb-21\nMar-21\nApr-21\nMay-21\nJune-21\nJul-21\nAug-21\nSep-21\nOct-21\nNov-21\nDec-21\n\n\n\n\n1001011\n2002058\nCheese Cake\n50\n51\n50\n53\n54\n52\n55\n53\n50\n52\n53\n53\n\n\n1001012\n2002080\nCoffee\n5\n6\n5\n6\n6\n5\n7\n5\n6\n5\n6\n6\n\n\n1001015\n2002093\nJasmine Tea\n4\n3\n3\n4\n4\n3\n5\n3\n3\n3\n3\n4\n\n\n1001020\n2002042\nCookies\n20\n20\n21\n20\n22\n20\n21\n20\n22\n20\n21\n22\n\n\n\n\nAnd also for Year 2022:\n\n\n\n\nOld Code\nNew Code\nName\nJan-22\nFeb-22\nMar-22\nApr-22\nMay-22\nJune-22\nJul-22\nAug-22\nSep-22\nOct-22\nNov-22\nDec-22\n\n\n\n\n1001011\n2002058\nCheese Cake\n52\n52\n55\n55\n56\n52\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1001012\n2002080\nCoffee\n5\n6\n5\n6\n6\n6.5\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1001015\n2002093\nJasmine Tea\n4\n3\n3\n5\n5\n5.5\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n1001020\n2002042\nCookies\n22\n22\n23\n23\n23.5\n23\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\nSo basically, I would like to have my cost column in my first Data Frame to reflect the correct costing for different Year and different Month.\nExample:\nInvoice Date Costing for 03/05/2021 = May_2021\nWould you mind to assist me on this?\nHighly Appreciated.\nThank you very much", "You need to have the month and code number on both sides when merging, so:\n\nCreate a year-month column in the invoice dataframe that is consistent with the cost table\nCombine two cost tables\nMerge with new code and old code respectively\n\n\n\nimport pandas as pd\nimport io\nimport datetime\n\ninvoice_data_text = '''Old Code New Code    Name    Invoice Date\n1001011 NA  Cheese Cake 02/02/2021\n1001012 NA  Coffee  03/05/2021\n1001011 NA  Cheese Cake 30/05/2021\nNA  2002093 Jasmine Tea 21/08/2021\nNA  2002042 Cookies 31/12/2021\nNA  2002080 Coffee  09/01/2022\nNA  2002093 Jasmine Tea 05/05/2022\nNA  2002058 Cheese Cake 07/06/2022\n'''\n\ncost_2021_text = '''\nOld Code    New Code    Name    Jan-21  Feb-21  Mar-21  Apr-21  May-21  June-21 Jul-21  Aug-21  Sep-21  Oct-21  Nov-21  Dec-21\n1001011 2002058 Cheese Cake 50  51  50  53  54  52  55  53  50  52  53  53\n1001012 2002080 Coffee  5   6   5   6   6   5   7   5   6   5   6   6\n1001015 2002093 Jasmine Tea 4   3   3   4   4   3   5   3   3   3   3   4\n1001020 2002042 Cookies 20  20  21  20  22  20  21  20  22  20  21  22\n'''\n\ncost_2022_text = '''\nOld Code    New Code    Name    Jan-22  Feb-22  Mar-22  Apr-22  May-22  June-22 Jul-22  Aug-22  Sep-22  Oct-22  Nov-22  Dec-22\n1001011 2002058 Cheese Cake 52  52  55  55  56  52  NA  NA  NA  NA  NA  NA\n1001012 2002080 Coffee  5   6   5   6   6   6.5 NA  NA  NA  NA  NA  NA\n1001015 2002093 Jasmine Tea 4   3   3   5   5   5.5 NA  NA  NA  NA  NA  NA\n1001020 2002042 Cookies 22  22  23  23  23.5    23  NA  NA  NA  NA  NA  NA\n'''\n\n# Prepare\ninvoice_df = pd.read_csv(io.StringIO(invoice_data_text),sep=\"\\t\",parse_dates=[\"Invoice Date\"])\ncost21 = pd.read_csv(io.StringIO(cost_2021_text),sep='\\t')\ncost22 =  pd.read_csv(io.StringIO(cost_2022_text),sep='\\t')\n\n# Create Month column for merging\ninvoice_df[\"Month\"] = invoice_df[\"Invoice Date\"].map(lambda x:datetime.datetime.strftime(x,\"%b-%y\"))\n\n# Combine two cost tables\ncost21_stack = cost21.set_index(list(cost21.columns[:3])).stack().reset_index(name=\"Cost\")\ncost22_stack = cost22.set_index(list(cost22.columns[:3])).stack().reset_index(name=\"Cost\")\ncost_table = pd.concat([cost21_stack,cost22_stack]).rename({\"level_3\":\"Month\"},axis=1)\n\n# Merge with new code and old code respectively\nold_code_result = pd.merge(invoice_df[pd.isna(invoice_df[\"Old Code\"]) == False], cost_table[[\"Old Code\",\"Month\",\"Cost\"]], on=[\"Old Code\",\"Month\"] ,how=\"left\")\nnew_code_result = pd.merge(invoice_df[pd.isna(invoice_df[\"New Code\"]) == False], cost_table[[\"New Code\",\"Month\",\"Cost\"]], on=[\"New Code\",\"Month\"] ,how=\"left\")\n\n# Combine result\npd.concat([old_code_result,new_code_result])", "Python - Anyone mind to assist in this Pandas Dataframe problem? URGENT", 1.2000000476837158, 0, 459, "This is a good example of where the merge() function isn' t the right tool for the job. As you' ve noticed, merge can only match on one column. If you want to match across multiple columns, you need to use merge_asof(), which allows for multiple match columns. Here' s an example: https://pandas.pydata.org/docs/reference/api/pandas-merge.html\n(see the section on 'join_by')\nThe problem you are having is that there is not a column with both values, so you can' 't use join_y. You can use a left join, but that won' y be a very efficient query. It may be better to do a two-step process: first, create a new dataframe where each row has a cost (based on your existing dataframe), and then merge that dataframe onto the original dataframe using a join by cost.", 0.12121212482452393, 0.009600000455975533, 0.054226476699113846, 0.05103668197989464], ["I have a data frame sorted by a column; and I need to perform a binary search to find the first value equal or greater than a specified value.\nIs there any way to do this efficiently in Spark?", "What you want is not possible. It is a bulk processing framework in which JOINs play a prevalent role using different techniques.\nNo where in the docs have I seen or read elsewhere of a binary search. That I did at University with in-memory Pascal structures.", "How to perform binary search on a preordered DataFrame?", 0.0, 0, 460, "You can use [sort_array](http://spark.apache.org/docs/latest/api/scala/index.html#org.sql.DataFrame@sortArray(columnName:String):org) method to sort the array column. You have to create a temp column to use sort_method to avoid changing the original data. For example:\n`df.sort('col_name).withColumn(\"temp\", 'col1).sort(col2)`\nThen perform the binary_search.", 0.12962962687015533, 0.01886792480945587, 0.1111111119389534, 0.025751076638698578], ["Let imagine we have a game with 4 players. And after playing game, we will get ranking of 4 players based on their score, rank 1 is the best, rank 4 is the worst. I have created a model for predicting player ranking. In detail, I have created 2 models for predict who will be in rank1 and rank2 of the game:\n\nmodel A predict probabilities for who win in rank 1.\nmodel B predict probabilities for who win in rank 2.\n\nAnd all of probability outputs will be in this matrix:\n\n\n\n\nPlayerID\nRank1(prob)\nRank2(prob)\n\n\n\n\nPlayerA\n0.7\n0.8\n\n\nPlayerB\n0.2\n0.05\n\n\nPlayerC\n0.1\n0.1\n\n\nPlayerD\n0.1\n0.05\n\n\n\n\nBased on above table, how can I calculate probability for this event: \"Player A and Player B will be in first 2 ranks\" ?\nPlease help", "The Premise\nFirst of all, you have a bug in your premise. You predict a probability of 0.7 for PlayerA to be at first place and you have a prediction for the same PlayerA to be ranked 2nd at the same game with a probability of 0.8. A value of 1 means full certainty, a value of 0 means full certainty of the negation. Now, your\n0.7 + 0.8 = 1.5\nwhich violates the basic framework of boolean algebra, as summing the probabilities of distinct outcomes for the same event you get a higher value than the maximum supported value of 1.\nAlso, there should be some probability of PlayerA being ranked lower than 2., so we should have\nP(first) + P(second) + P(lower) = 1\nfor any player. If this is false (and in your case it is false), then the premise is incorrect.\nAnother problem with the premise can be seen from the fact that summing Rank1(prob) we get 1.1, even though, summing Rank2(prob) we get the expected value of 1.\nBut let's focus on Rank1(prob) at this point.\nWe know as an absolutely certain fact (probability of 1) that one of the four players will be ranked 1, which means that their probability should have a sum that is exactly equal to 1. Since it is 1.1 (0.7 + 0.2 + 0.1 + 0.1) in your case, we see another problem with your premise. So, first things first: you need to fix your premise to make sure that they are corresponding to reality and they do not violate the basic framework of boolean logic (violating this framework is an absolutely sure sign of not being in line with reality)\nLogics and Probability\nIn probability calculation, applying logics is not difficult. For example, if you are interested to know whether p(X) AND p(y) is true, then you can compute it like this:\np(X AND Y) = p(X) * p(Y)\nExplanation: The probability itself is a conjunction already with the surety (value of 1), as p(X) = 1 * p(X). 0 <= p(X) <= 1 is the full problem-space when you calculate the result of logical AND with p(Y), hence you compute a further conjunction, resulting in p(X) * p(Y)\nIn the case of logical OR\nComputing the disjunction is as\np(X OR Y) = p(X) + p(Y) - p(X AND Y) = p(X) + p(Y) - p(X) * p(Y)\nExplanation: Intuitively, the result of the logical OR should be the sum of the cases, but there is a caveat: p(X AND Y) is already included as a possibility both into p(X) and p(Y), so it appears twice (in a hidden manner) when you compute p(X) + p(Y), so, as a result, you need to subtract it to make sure that it's computed into the result exactly once.\nComputing your formula\nWe are interested to know whether PlayerA will be first and PlayerB will be second or PlayerB will be first and PlayerA will be second.\nSince your premises have some bugs, I will not use your values. Instead of that, I will denote Rank1(A) as the probability that PlayerA will be ranked first and so on.\nSo:\np(Rank1(A) AND Rank2(B)) = Rank1(A) * Rank2(B) (1)\nSimilarly:\np(Rank1(B) AND Rank2(A)) = Rank1(B) * Rank2(A) (2)\nSo:\np((Rank1(A) AND Rank2(B)) OR (Rank1(B) AND Rank2(A))) = p(Rank1(A) AND Rank2(B)) + p(Rank1(B) AND Rank2(A)) - p((Rank1(A) AND Rank2(B)) AND (Rank1(B) AND Rank2(A)))\nWe know that p((Rank1(A) AND Rank2(B)) AND (Rank1(B) AND Rank2(A))) is exactly 0, because it is a self-contradiction, because it assumes PlayerA to be ranked first and second at the same time and it similarly assumes PlayerB to be ranked first and second at the same time. So:\np(Rank1(A) AND Rank2(B)) + p(Rank1(B) AND Rank2(A)) - p((Rank1(A) AND Rank2(B)) AND (Rank1(B) AND Rank2(A))) = p(Rank1(A) AND Rank2(B)) + p(Rank1(B) AND Rank2(A)) - 0 = p(Rank1(A) AND Rank2(B)) + p(Rank1(B) AND Rank2(A))\nLet's apply formula (1) and (2) at the same time:\np(Rank1(A) AND Rank2(B)) + p(Rank1(B) AND Rank2(A)) = Rank1(A) * Rank2(B) + Rank1(B) * Rank2(A)", "How to calculate the combination of matrix of probabilities (win rate ranking in game)", 0.0, 0, 461, "You can use the formula below: (P11 + P21)*(P31+P41) + (1-P22)* ( P32+ P42) - P12*P 33 -  P 03 +P02*(1 -P55) =  ?", 0.023468056693673134, 0.0026143791619688272, 0.020860495045781136, 0.03332485258579254], ["I am having issues with running yolov5 on colab. I was able to run the code fine when I had  I had more classes, and a slightly smaller dataset, but now I have decreased the amount of classes and 70 instances when the overall one has 3400 instances. Now I am getting this error.\nterminate called after throwing an instance of 'c10::CUDAError'\nOther times I will get\n cuda assertion index >= -sizes[i] && index < sizes[i] && index out of bounds\nany idea what could be causing this and how to fix it?", "The issue was that I was not outputting any labels that I said existed. So for example if I claimed that there would be labels \"0\",\"1\",\"2\" in the training dataset. There was no instances of label \"2\".", "issue terminate called after throwing an instance of 'c10::CUDAError'", 1.2000000476837158, 0, 462, "This happens because the GPU is not able handle all the data in the memory. It is also not possible to use a larger batch size because it requires more memory, which is why we have to reduce the batch_size and number of instances in order to fit all of the image data on the device.  If you decrease the number instances to 150, the issue is resolved.", 0.14814814925193787, 0.03773584961891174, 0.1111111119389534, 0.05486864224076271], ["I had two dataframes that are being read from two almost identical .csv using pd.read_csv().\nWhen I use .loc[index1] on one of them it returns a Dictionary such as:\ncol1        val1\ncol2        val2\ncol3        val3\nName: (index1), dtype: object\nBut with the other I've realized it actually returns a Dataframe. Some operations such as df1[col1] = df2[col2] + constant will through errors.\nTo make it even harder I'm actually using MultiIndex. I'm getting this error:\nCannot handle a non-unique multi-index!", "I've figured out that .loc returns a Dataframe or an Dictionary-like object depending on if there are duplicated indexes. This condition is not explained in the pandas documentation or I've not find it.\nIf the index are actually unique try using something along this code:\ndf.reset_index().drop_duplicates(subset=[\"index1\"]).set_index([\"index1\"])\nor just df.drop_duplicates(subset=[\"index1\"]) after reading the csv but before setting the index", "Dataframe.loc returns dictionary or a Dataframe [Solved] (Cannot handle a non-unique multi-index!)", 0.0, 0, 463, "Try this: `df1.iloc[x,y]` where x and y are the columns you want to work with. It's much more efficient than .index or .ilocation.", 0.13861386477947235, 0.0, 0.09900990128517151, 0.06362558156251907], ["I am using Pandas ExcelWriter to create an excel file from a dataframe. I have also applied formatting on the excel file like Font size, font colour etc\nNow I am trying to convert the excel to CSV using to_csv method.\nAfter conversion, the CSV file is not retaining any formatting done previously.\nMy question is how do I retain formatting in CSV ?", "CSV cannot store formatting. If you want that, save as an excel file. (Or of course other outputs that save formatting - including HTML - but have other feature drawbacks - it depends on what you need.)", "Pandas to_csv not retaining formatting", 0.3799489736557007, 2, 464, "You can use `to_excel` instead of `csv` for formatting:\n\\begin{itemize}\n  \\item http://pandas.pydata.org/pandas-docs/stable/generated/ pandas.DataFrame.to.html\n    \\em{Return an Excel string by writing the DataFrame to a file using Excel writer format. The writer is a subclass of StringWriter that is capable of writing to an XLSX file.}\n      \\begin {code}``\n        \\strong{to Excel}  : to\\_excel(path_or_buf,\n                                  sheet_name, keep_default_na=True, na_rep='nan',\n            float_format=None, cols=range(len(df.columns)),\n           index=False, header=0, index_label=(), index\\_label_width=25, write_header=1, encoding='utf-8', nan_values=np.nan, columns=df._internal.data_columns, **kwds)\n          \\end {pre} `\n             \\sout{\n               \\textbf{sheet_names} : The name of the sheet. If no name is given, a default", 0.1358024626970291, 0.012500000186264515, 0.08641975373029709, 0.05649717152118683], ["I am new to singularity concept but successfully created singularity image for running alpha fold tool. I am encountering below-mentioned error.\nI would like to request if anyone could explain how to troubleshoot the error or any related information that may help to combat it.\nThank you in advance.\nsingularity run --nv alphafold220.sif --fasta_paths=/home/igib/AF_singualrity/test.fasta\n**\n*> /sbin/ldconfig.real: Can't create temporary cache file\n\n/etc/ld.so.cache~: Read-only file system Traceback (most recent call\nlast):   File \"/app/alphafold/run_alphafold.py\", line 37, in \nfrom alphafold.model import data   File \"/app/alphafold/alphafold/model/data.py\", line 19, in \nfrom alphafold.model import utils   File \"/app/alphafold/alphafold/model/utils.py\", line 22, in \nimport haiku as hk   File \"/opt/conda/lib/python3.7/site-packages/haiku/init.py\", line 17,\nin \nfrom haiku import data_structures   File \"/opt/conda/lib/python3.7/site-packages/haiku/data_structures.py\",\nline 17, in \nfrom haiku._src.data_structures import to_immutable_dict   File \"/opt/conda/lib/python3.7/site-packages/haiku/_src/data_structures.py\",\nline 30, in \nfrom haiku._src import utils   File \"/opt/conda/lib/python3.7/site-packages/haiku/_src/utils.py\", line 24,\nin \nimport jax   File \"/opt/conda/lib/python3.7/site-packages/jax/init.py\", line 108, in\n\nfrom .experimental.maps import soft_pmap   File \"/opt/conda/lib/python3.7/site-packages/jax/experimental/maps.py\",\nline 25, in \nfrom .. import numpy as jnp   File \"/opt/conda/lib/python3.7/site-packages/jax/numpy/init.py\", line\n16, in \nfrom . import fft   File \"/opt/conda/lib/python3.7/site-packages/jax/numpy/fft.py\", line 17, in\n\nfrom jax._src.numpy.fft import (   File \"/opt/conda/lib/python3.7/site-packages/jax/_src/numpy/fft.py\", line\n19, in \nfrom jax import lax   File \"/opt/conda/lib/python3.7/site-packages/jax/lax/init.py\", line\n330, in \nfrom jax._src.lax.fft import (   File \"/opt/conda/lib/python3.7/site-packages/jax/_src/lax/fft.py\", line\n144, in \nxla.backend_specific_translations['cpu'][fft_p] = pocketfft.pocketfft AttributeError: module 'jaxlib.pocketfft' has no\nattribute 'pocketfft'*\n\n**", "Singularity images run on a read-only file system, with the exception being directories that have been mounted from the host OS.\nYou can enable a tmpfs overlay when running by using the --writable-tmpfs flag. Note that the max size of the tmpfs overlay is the size of /dev/shm, which can be smaller than expected in some cloud VMs.", "Runing alphafold job in singularity", 0.0, 0, 465, "The problem seems to be that you're running into a bug in Haiku's `jax._srco.jax.lax.nn_ff.FusedFp128ToF64` function. This bug is fixed in the current version of Haikus' `haikus/nn` package (which is version 0b2e3f4c), but I'm not sure what you should do about it in a Singular-provided image. It'll probably be easiest if you can install Haikus' latest version on your machine, and then run the job with:\n\\begin{itemize}\n[...]\n`sing_job_cmd = ['/path/to/singulatity_image', '--experiment_name=my_experiement', \\\n                  'run --exclude-modules=jax,jax_nn,haikuxxx,lazy_func_xx --config=haiko-config-my-ex.yaml']`\n(I think the latest Haiki version is 4.1.0)\nThe current Haiko config file (haiki-default-41-1813-23", 0.2139037400484085, 0.010810811072587967, 0.09625668078660965, 0.02867383509874344], ["I want to now the numbers of headers my csv file contains (between 0 and ~50). The file itself is huge (so not reading the complete file for this is mandatory) and contains numerical data.\nI know that csv.Sniffer has a has_header() function, but that can only detect 1 header.\nOne idea I had is to recursivly call the has_header funcion (supposing it detects the first header) and then counting the recursions. I am sure though, there is a much smarter way.\nGoogling was kind of a pain, since no matter what you search, if it includes \"count\" and \"csv\" at some point, you get all the \"count rows in csv\" results :D\nClarification:\nWith number of headers I mean number of rows containing information which is not data. There is no general rule for the headers (could be text, floats, or white spaces) and it may be a single line of text. The data itself however is only floats. For me this was super clear, because I've been working with these files for a long time, but forgot this isn't the normal case.\nI hoped there was a easy and smart builtin function from Numpy or Pandas, but it doesn't seem so.\nInspired by the comments so far, I think my best bet is to\n\nread 100 lines\ncount number of separators in each line\ndetermine most common number of separators per line\nComing from the end of 100 lines, find first line with different amount of separators, or isn't floats. That line is the last header line.", "Well, I think that you could get the first line of the csv file and then split  it by a \",\". That will return an array with all the headers in it. Now you can just count them with len.", "Python: count headers in a csv file", 0.0, 0, 466, "\\begin{code}\nimport csv\nwith open('file.csv') as f:    # open file object\n    header = next(f)            # get first record (header)\n\\end{pre}", 0.16393442451953888, 0.0, 0.06557376682758331, 0.0359971858561039], ["After we've created VideoCapture with cv2.VideoCapture(filename) how can we retrieve filename? Looks like get() method with propId is not what I'm looking for", "sorry, but you cannot retrieve the filename from a cv2.VideoCapture.\n(also, webcams or ip captures wont even have one)\nsince the filename is in your code, you need to cache it in a variable instead.", "How to retrieve filename from OpenCV's VideoCapture() in Python?", 0.0, 0, 467, "Video capture object is an instance of [cv2](http://docs.opencv.org/2016/02/openviz/master/doc/videoio.html#Video%27s%5B%7C%60%40openCV%3A%C2%B7%E2_8009%ED%83%95%EC%A9t%EB%AD%AA%EF%BC%AC%EE%BD%AE%FC%AF%FF%F3] object, so you can use it' s methods. You should try [get(tag) method](https://www.pyimagesearch.com/wp-content/uploads/10_22-08-24_07.29.03.png)\n<a href=\"https.//i.stack.imgur. com/J1Yzg.gif\" rel=\"nofollow noreferrer\"><IMAGE></a>\nwhere tag is the identifier of the property you want to get.\nIn your case, you should use `get('filename')`", 0.17687074840068817, 0.013793103396892548, 0.08163265138864517, 0.026178007945418358], ["I have a classic panda data frame made of ID and Text. I would like to get just one column and therefore i use the typical df[\"columnname\"]. But at this point it becomes a Pandas Series. Is there a way to make a new dataframe with just that single column?\nI'm asking this is because if I cast the Pandas series in a string (columnname = columnname.astype (\"string\")) and I save it in a text file, I see that it only saves the first sentence of each line and not the entire textual content, as I would like.\nIf there are any other solution, I'm open to learn :)", "Try this: pd.DataFrame(dfname[\"columnname\"])", "Pandas Read csv just read a line of a row", 1.2000000476837158, 0, 468, "You need to iterate over the rows, and only extract the `Text` column of the current row. You can do this by using the \\em{`.iterrows()`} method, like so:\n\\begin{itemize}\n  \\item First, you need a `pandas.DataFrame`:\n<a href=\"https://i.stack.imgur.com/X85q7.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\nIn this case, let's assume that `df` contains your data. If it doesn't, then you can create it using `pd.read_csv('file.csv',header=None)`, or similar. For the sake of this example, it'll be easier to just use `list` to create a data-frame. The `iter_rows` method is used to convert a list into a dataframe. So, if `x` is a pandas series or a numpy array, the following will work: `s = pd.Series(x).iteritems()` and then `y = df.from_items(s)`.\nYou can also use a dictionary, but `fromkeys` and `to_dict` are also built-in functions to do that.", 0.03614457696676254, 0.0, 0.03614457696676254, 0.04678460583090782], ["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndaal4py 2021.5.0 requires daal==2021.4.0, which is not installed.\nmxnet 1.7.0.post2 requires numpy<1.17.0,>=1.8.2, but you have numpy 1.18.5 which is incompatible.\nd2l 0.17.5 requires numpy==1.21.5, but you have numpy 1.18.5 which is incompatible.\nd2l 0.17.5 requires requests==2.25.1, but you have requests 2.18.4 which is incompatible.", "Try adding --use-deprecated=legacy-resolver after your pip install commands\nfor example:\n\n!pip install -r\nrequirements.txt --use-deprecated=legacy-resolver", "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed", -0.1973753273487091, -1, 469, "I've encountered the same problem. I did the [following steps](https://pypi.org/project/virtualenv/) and it worked fine. In this way, all packages will be installed inside the virtual environment.", 0.0, 0.0, 0.0, 0.0214077215641737], ["After some extensive research I have figured that\nParquet is a column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk.\nHowever, I am unable to understand why parquet writes multiple files when I run df.write.parquet(\"/tmp/output/my_parquet.parquet\") despite supporting flexible compression options and efficient encoding.\nIs this directly related to parallel processing or similar concepts?", "It's not just for parquet but rather a spark feature where to avoid network io it writes each shuffle partition as a 'part...' file on disk and each file as you said will have compression and efficient encoding by default.\nSo Yes it is directly related to parallel processing", "Why do Parquet files generate multiple parts in Pyspark?", 0.0996679961681366, 1, 470, "\\begin{blockquote}\nI am also assuming that I should use the appropriate compression\noption for the file and not the codec. If not, can you please\ncorrect me? The compression I chose was \"snappy\" which I understood\nis the most efficient compression option for Parqet files. The\ncodec I used was LZ4. I know that Lz4 is not an option because it\nhas been deprecated. But it was the only code option that did not\nthrow any errors. Is this incorrect? And is it possible to change\nthe codecs in the driver? I looked up and I couldn't find any\nsolutions. Please help me out. Thanks in advance. :)\n\\end{itemize}", 0.17391304671764374, 0.0, 0.11180124431848526, 0.04782608896493912], ["After some extensive research I have figured that\nParquet is a column-oriented data file format designed for efficient data storage and retrieval. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk.\nHowever, I am unable to understand why parquet writes multiple files when I run df.write.parquet(\"/tmp/output/my_parquet.parquet\") despite supporting flexible compression options and efficient encoding.\nIs this directly related to parallel processing or similar concepts?", "Lots of frameworks make use of this multi-file layout feature of the parquet format. So I\u2019d say that it\u2019s a standard option which is part of the parquet specification, and spark uses it by default.\nThis does have benefits for parallel processing, but also other use cases, such as processing (in parallel or series) on the cloud or networked file systems, where data transfer times may be a significant portion of total IO. in these cases, the parquet \u201chive\u201d format, which uses small metadata files which provide statistics and information about which data files to read, offers significant performance benefits when reading small subsets of the data. This is true whether a single-threaded application is reading a subset of the data or if each worker in a parallel process is reading a portion of the whole.", "Why do Parquet files generate multiple parts in Pyspark?", 1.2000000476837158, 1, 471, "Because it uses multiple threads to write the file. This is not an issue with parquets but with the spark program you wrote. You can use the partitioning options to have spark write to a single file, which is what you should do.", 0.19672131538391113, 0.033149171620607376, 0.10928961634635925, 0.047869592905044556], ["I have to compare each distribution of measurement n, with all other measurements. I have about 500 measurements and 5000 distributions per measurement, so that's a lot of comparisons. I have the data in one csv file:\n\n\n\n\n\ndistribution 1\ndistribution 2\n\n\n\n\nmeasurement 1\n[10,23,14,16,28,19,28]\n[4,1,3,2,5,8,4,2,4,6]\n\n\nmeasurement 2\n[11,23,24,10,27,19,27]\n[9,2,5,2,5,7,3,2,4,1]\n\n\n\n\nas you can imagine the file is huge and as I have to do many comparisons I run it in parallel and the RAM consumption is insane. If I split the file and only open sample by sample, it's a bit better, but still not good and also it's not very efficient.\nMy idea was to create some kind of database and query only the cells needed, but have never done it, so I don't know if that will be RAM heavy and fairly efficient.", "This probably has something to do with destroying objects. The way to limit RAM usage would be to limit the number of threads. Then you don't start every comparison at the beginning and then solve them by four (assuming you have four threads per process) to end an hour later to let the garbage collector start destroying objects of the solved cases.\nI am just spitballing here. A bit of code would be helpful. Maybe you are already doing that?", "Looking for RAM efficient way to compare many distributions in parallel in Python", 0.1973753273487091, 1, 472, "You can use [numpy.unique](http://docs.scipy.org/doc/numpy/reference/generated/ numpy.ndarray.asanyarray().html) to get unique values, then create a dictionary of those values and use a [`pandas.Series`](http:/docs/pandas/api/series.html#pandas-series) object to find your indexes. `pandas` is a python package that can make data manipulation easier. For example, if your data is in a file called \"data.csv\", the following should work. (note: your file may need to be in the same directory as the python file)\n\\begin{itemize}\n    \\item `import numpy as np`\n        \\begin {code} np.loadtxt(open('data1.txt', 'r'))\n      \\end{code};\n       \\strong{this will load the numpy array} into a numpy ndarray object\n     \\em{Note: if you have a 3rd column with the distribution names, you may want to use the `.as_matrix()` method instead.}\n   \\*\\em {this is the array of values} `np.array([[221353", 0.208695650100708, 0.008771929889917374, 0.104347825050354, 0.04591836407780647], ["When I'm trying to import the 'skfuzzy' module, I get this error. I installed the scikit-fuzzy package and I can see it among installed packages (using the 'pip list' command). I tried installing and re-installing it several times with various commands ('pip install'; 'pip3 install'; 'pip3 -U install') but nothing helped. Other modules such as numpy and matplotlib work fine. Also, after the installation I get this warning:\n\"WARNING: The script f2py.exe is installed in 'C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\"\nIs this connected to my problem? How can I fix it?", "According to the warning, try and do the following:\n\nWindows + R\nType sysdm.cpl\nGo to Advance Tab and click on Environment Variables\nIn User variables [preferably] click on PATH\nClick on New and add C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts to PATH\n\nThis will add the scripts to your environment variables. Hope this helps!", "ModuleNotFoundError: No module named 'skfuzzy'", 0.0, 0, 473, "I had this same problem today and found that I needed to restart the iPython kernel.  I did not need to uninstall/reinstall the package.", 0.13333334028720856, 0.0, 0.08888889104127884, 0.089285708963871], ["When I'm trying to import the 'skfuzzy' module, I get this error. I installed the scikit-fuzzy package and I can see it among installed packages (using the 'pip list' command). I tried installing and re-installing it several times with various commands ('pip install'; 'pip3 install'; 'pip3 -U install') but nothing helped. Other modules such as numpy and matplotlib work fine. Also, after the installation I get this warning:\n\"WARNING: The script f2py.exe is installed in 'C:\\Users\\anton\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts' which is not on PATH.\nConsider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\"\nIs this connected to my problem? How can I fix it?", "I installed the scikit-fuzzy using the \"easy_install -U scikit-fuzzy\" command instead of pip, and it did remove the error.", "ModuleNotFoundError: No module named 'skfuzzy'", 0.0, 0, 474, "Try this: `pip uninstall skfuzz`, `rm -rf .cache`, then install the package again. If that doesn't work, then maybe it's a version issue.", 0.1702127605676651, 0.0, 0.08510638028383255, 0.028840677812695503], ["I'm trying to change the positions of spellers to a simple format. By changing RW to forward or CM to midfielder. Only there are several values \u200b\u200bin a cell. How do I combine or drop the other values \u200b\u200bin the cell?\n\n\n\n\nplayer\nplayer_positions\n\n\n\n\nmessi\nRW, ST, CF\n\n\nRonaldo\nST,LW\n\n\n\n\nhow do i change RW, ST, CF just simple to Forward?\nAm trying:\ndf.replace(to_replace=r'^RW', value='Forward', regex=True)\nbut then i get:\n\n\n\n\nplayer\nplayer_positions\n\n\n\n\nmessi\nForward, ST, CF\n\n\nRonaldo\nST,LW", "You can add everything in the replace statement.\ndf = df.replace(to_replace=r'^RW, ST, CF', value='Forward', regex=True)\nor\ndf = df.replace(to_replace=r'^RW\\D*', value='Forward', regex=True)", "combine multiple variables in cell to one variable", 1.2000000476837158, 1, 475, "It's a bit of a hack, but here you go: you can use `re.sub` to replace all instances of the pattern `R[W]` with `For[ward]` and `ST[L[onado]]` with a space: `sp[ace]`.\nThen you need to remove the space, which you could do with: \\begin{code}\n(<string>).strip()\n\\end{Code}", 0.17977528274059296, 0.04597701132297516, 0.15730337798595428, 0.046880289912223816], ["I have a list of XY co-ordinates, and I am looking for a way to sort them as they appear on the plot with top-to-bottom, left-to-right precedence.\nI tried sorting them as a tuple in Python but it didn't work.\nHere are the (normalized) co-ordinates:\n(0.48425699105850684, 0.4852200502470339)\n(0.8003207976544613, 0.1794844315136523)\n(0.663158173206857, 0.19739922702645016)\n(0.26770425263394393, 0.20288883507443173)\n(0.5214529814719886, 0.2032096846467844)\n(0.4768268032594222, 0.3875097802042241)\n(0.5400594055964151, 0.5870619715600098)\n(0.5445470099105095, 0.8064964338255158)", "I eventually ended up using the product of the X and Y coordinates as the sorting key. It worked for my test cases!", "Is there a way to sort a set of co-ordinates in a top-to-bottom, left-to-right precedence as they appear?", 0.0, 0, 476, "Use [`sorted()`](https://docs.python.org/3/library/functions.html#sorted) with the `key` argument: `sorted(coords, key=coord_key)`. `coor_keys` is the function that will compare two coords (in your case it seems like you want to compare the x-values, as that's what you're passing as the key).\n\\begin{itemize}\n  \\item `tuple`s in python are unordered, so `sort()` is useless. If you need to keep the order, use `list` (which is ordered) or `set` or something else that is guaranteed to be ordered. (If you \\em{don' t} want the keys to change if you reorder the coodrds, you can use a `namedtuple`)\n    \\section{Further Reading} (all docs link to the [official docs](https//docs-python-3-labs.azuldev.com/))\n      \\begin {code}", 0.09271523356437683, 0.02684563770890236, 0.0794702023267746, 0.046296294778585434], ["celery.conf.update(result_serializer='pickle') uses pickle for serializing results generated by Celery tasks. Is there a way to tell which serializer (JSON, pickle, etc...) to be used at the individual task level?", "As far as I know, that is not possible.", "Task specific result_serializer in Celery", 0.0, 0, 477, "Not in celeries default configuration.\nYou can set `resultSerializer` as an option to each task, but as far as I can see, the default is pickling, as that is a very fast serialization.", 0.3333333432674408, 0.20000000298023224, 0.2857142984867096, 0.05882352963089943], ["How to define an empty 2 dimensional list instead of data = [[\"\",\"\"],[\"\",\"\"],[\"\",\"\"],[\"\",\"\"]]\nFor larger number of elements", "lis = [[] for _ in range(3)]\ntry it", "How to define an empty 2 dimensional list instead of data = [[\"\",\"\"],[\"\",\"\"],[\"\",\"\"],[\"\",\"\"]]", 0.03997867926955223, 1, 478, "Use list comprehension:\n`data = [[],[]]`", 0.0, 0.0, 0.0, 0.06666666269302368], ["I have been encountering this message after trying to import numpy, pandas, matplotlib, and seaborn all by themselves. I am not sure how to fix this. Any suggestions?\nI am using Python 3.8.8, matplotlib 3.3.4, pandas 1.2.4, numpy 1.20.1, seaborn 0.11.1.\nI have recently updated my Anaconda navigator to 2.1.0. Would this possibly have caused any issues?\nIn the shell command, after trying to import each of those packages individually, I see this message:\nIntel MKL FATAL ERROR: Cannot load libmkl_intel_thread.1.dylib.", "Solution: I reinstalled Anaconda Navigator.", "How to fix error: \"The kernel appears to have died. It will restart automatically.\" message?", 0.0996679961681366, 1, 479, "It seems that you've used the old version of Anacondas navigators that do not support the newest version, so the solution is to remove the Anacona navigated and install the latest Anacode navigater.", 0.05128205195069313, 0.0, 0.05128205195069313, 0.0]]}