  0%|                                                                                                                                                                              | 0/370 [00:00<?, ?it/s]/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▍                                                                                                                                                                   | 1/370 [00:25<2:36:50, 25.50s/it]
{'loss': 2.5519, 'learning_rate': 2.5e-05, 'epoch': 0.03}


  1%|█▎                                                                                                                                                                  | 3/370 [01:13<2:27:56, 24.19s/it]

  1%|█▊                                                                                                                                                                  | 4/370 [01:37<2:27:35, 24.20s/it]

  1%|██▏                                                                                                                                                                 | 5/370 [02:01<2:27:00, 24.17s/it]
{'loss': 2.5045, 'learning_rate': 9.99981580623855e-05, 'epoch': 0.14}


  2%|███                                                                                                                                                                 | 7/370 [02:50<2:26:36, 24.23s/it]

  2%|███▌                                                                                                                                                                | 8/370 [03:14<2:25:38, 24.14s/it]

  2%|███▉                                                                                                                                                                | 9/370 [03:38<2:25:24, 24.17s/it]
{'loss': 2.439, 'learning_rate': 9.995395834475578e-05, 'epoch': 0.24}


  3%|████▊                                                                                                                                                              | 11/370 [04:27<2:25:28, 24.31s/it]

  3%|█████▎                                                                                                                                                             | 12/370 [04:51<2:25:12, 24.34s/it]

  4%|█████▋                                                                                                                                                             | 13/370 [05:16<2:24:54, 24.36s/it]

  4%|██████▏                                                                                                                                                            | 14/370 [05:40<2:24:07, 24.29s/it]
{'loss': 2.4034, 'learning_rate': 9.981591817238378e-05, 'epoch': 0.38}

  4%|██████▌                                                                                                                                                            | 15/370 [06:05<2:24:34, 24.43s/it]


  5%|███████▍                                                                                                                                                           | 17/370 [06:53<2:23:14, 24.35s/it]

  5%|███████▉                                                                                                                                                           | 18/370 [07:17<2:22:04, 24.22s/it]
{'loss': 2.4663, 'learning_rate': 9.963941225812701e-05, 'epoch': 0.49}

  5%|████████▎                                                                                                                                                          | 19/370 [07:41<2:20:56, 24.09s/it]















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:50<00:03,  3.63s/it]
{'eval_loss': 2.4010820388793945, 'eval_runtime': 58.2186, 'eval_samples_per_second': 4.38, 'eval_steps_per_second': 0.275, 'epoch': 0.51}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  5%|████████▊                                                                                                                                                          | 20/370 [09:06<4:08:09, 42.54s/it]
{'loss': 2.4345, 'learning_rate': 9.952920177288986e-05, 'epoch': 0.54}


  6%|█████████▋                                                                                                                                                         | 22/370 [09:56<3:14:06, 33.47s/it]

  6%|██████████▏                                                                                                                                                        | 23/370 [10:20<2:57:01, 30.61s/it]

  6%|██████████▌                                                                                                                                                        | 24/370 [10:44<2:46:00, 28.79s/it]
{'loss': 2.3863, 'learning_rate': 9.926502813430545e-05, 'epoch': 0.65}

  7%|███████████                                                                                                                                                        | 25/370 [11:09<2:37:44, 27.43s/it]


  7%|███████████▉                                                                                                                                                       | 27/370 [11:58<2:29:27, 26.14s/it]
{'loss': 2.4029, 'learning_rate': 9.902876970315716e-05, 'epoch': 0.73}

  8%|████████████▎                                                                                                                                                      | 28/370 [12:23<2:26:33, 25.71s/it]


  8%|█████████████▏                                                                                                                                                     | 30/370 [13:12<2:22:21, 25.12s/it]
{'loss': 2.3691, 'learning_rate': 9.876000201222912e-05, 'epoch': 0.81}

  8%|█████████████▋                                                                                                                                                     | 31/370 [13:37<2:22:00, 25.13s/it]

  9%|██████████████                                                                                                                                                     | 32/370 [14:03<2:22:47, 25.35s/it]


  9%|██████████████▉                                                                                                                                                    | 34/370 [14:55<2:23:40, 25.66s/it]

  9%|███████████████▍                                                                                                                                                   | 35/370 [15:22<2:25:28, 26.05s/it]

 10%|███████████████▊                                                                                                                                                   | 36/370 [15:49<2:26:12, 26.26s/it]

 10%|████████████████▎                                                                                                                                                  | 37/370 [16:15<2:25:55, 26.29s/it]

 10%|████████████████▋                                                                                                                                                  | 38/370 [16:40<2:24:04, 26.04s/it]
{'loss': 2.3787, 'learning_rate': 9.788577716443902e-05, 'epoch': 1.03}















 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:52<00:03,  3.58s/it]
{'eval_loss': 2.342635154724121, 'eval_runtime': 59.7428, 'eval_samples_per_second': 4.268, 'eval_steps_per_second': 0.268, 'epoch': 1.03}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.3763, 'learning_rate': 9.776053254073159e-05, 'epoch': 1.05}

 11%|█████████████████▌                                                                                                                                                 | 40/370 [18:31<3:30:49, 38.33s/it]

 11%|██████████████████                                                                                                                                                 | 41/370 [18:57<3:09:22, 34.54s/it]

 11%|██████████████████▌                                                                                                                                                | 42/370 [19:21<2:52:06, 31.48s/it]
{'loss': 2.3046, 'learning_rate': 9.736372361493584e-05, 'epoch': 1.14}


 12%|███████████████████▍                                                                                                                                               | 44/370 [20:09<2:29:36, 27.54s/it]

 12%|███████████████████▊                                                                                                                                               | 45/370 [20:33<2:24:09, 26.62s/it]
{'loss': 2.3369, 'learning_rate': 9.693550946309722e-05, 'epoch': 1.22}


 13%|████████████████████▋                                                                                                                                              | 47/370 [21:22<2:17:53, 25.62s/it]

 13%|█████████████████████▏                                                                                                                                             | 48/370 [21:49<2:18:43, 25.85s/it]
{'loss': 2.367, 'learning_rate': 9.647617401902002e-05, 'epoch': 1.3}


 14%|██████████████████████                                                                                                                                             | 50/370 [22:39<2:15:19, 25.37s/it]

 14%|██████████████████████▍                                                                                                                                            | 51/370 [23:03<2:13:40, 25.14s/it]

 14%|██████████████████████▉                                                                                                                                            | 52/370 [23:27<2:11:40, 24.84s/it]
{'loss': 2.3899, 'learning_rate': 9.581584522435024e-05, 'epoch': 1.41}


 15%|███████████████████████▊                                                                                                                                           | 54/370 [24:16<2:09:37, 24.61s/it]

 15%|████████████████████████▏                                                                                                                                          | 55/370 [24:41<2:08:48, 24.53s/it]
{'loss': 2.3597, 'learning_rate': 9.528511315402358e-05, 'epoch': 1.49}

 15%|████████████████████████▋                                                                                                                                          | 56/370 [25:04<2:06:31, 24.18s/it]

 15%|█████████████████████████                                                                                                                                          | 57/370 [25:28<2:05:30, 24.06s/it]














 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 15/16 [00:49<00:03,  3.55s/it]

{'eval_loss': 2.2673232555389404, 'eval_runtime': 56.411, 'eval_samples_per_second': 4.52, 'eval_steps_per_second': 0.284, 'epoch': 1.54}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 16%|█████████████████████████▌                                                                                                                                         | 58/370 [26:49<3:34:17, 41.21s/it]
{'loss': 2.2775, 'learning_rate': 9.472435411143978e-05, 'epoch': 1.57}


 16%|██████████████████████████▍                                                                                                                                        | 60/370 [27:36<2:45:46, 32.08s/it]

 16%|██████████████████████████▊                                                                                                                                        | 61/370 [27:59<2:32:28, 29.61s/it]
{'loss': 2.2565, 'learning_rate': 9.413393991627737e-05, 'epoch': 1.65}


 17%|███████████████████████████▊                                                                                                                                       | 63/370 [28:47<2:15:12, 26.43s/it]

 17%|████████████████████████████▏                                                                                                                                      | 64/370 [29:11<2:11:34, 25.80s/it]

 18%|████████████████████████████▋                                                                                                                                      | 65/370 [29:35<2:08:19, 25.24s/it]

 18%|█████████████████████████████                                                                                                                                      | 66/370 [29:59<2:06:40, 25.00s/it]

 18%|█████████████████████████████▌                                                                                                                                     | 67/370 [30:24<2:05:05, 24.77s/it]

 18%|█████████████████████████████▉                                                                                                                                     | 68/370 [30:47<2:03:20, 24.51s/it]

 19%|██████████████████████████████▍                                                                                                                                    | 69/370 [31:11<2:01:51, 24.29s/it]

 19%|██████████████████████████████▊                                                                                                                                    | 70/370 [31:36<2:01:34, 24.31s/it]

 19%|███████████████████████████████▎                                                                                                                                   | 71/370 [32:00<2:01:18, 24.34s/it]

 19%|███████████████████████████████▋                                                                                                                                   | 72/370 [32:24<2:00:42, 24.30s/it]
{'loss': 2.4295, 'learning_rate': 9.2643216570108e-05, 'epoch': 1.95}

 20%|██████████████▏                                                         | 73/370 [32:48<2:00:13, 24.29s/it]


 20%|██████████████▌                                                         | 75/370 [33:37<1:59:21, 24.28s/it]

 21%|██████████████▊                                                         | 76/370 [34:01<1:58:45, 24.24s/it]
{'loss': 2.4806, 'learning_rate': 9.195688772781969e-05, 'epoch': 2.05}















 94%|██████████████████████████████████████████████████████████████████████▎    | 15/16 [00:49<00:03,  3.54s/it]
{'eval_loss': 2.924182891845703, 'eval_runtime': 56.2044, 'eval_samples_per_second': 4.537, 'eval_steps_per_second': 0.285, 'epoch': 2.05}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 21%|██████████████▉                                                         | 77/370 [35:22<3:21:16, 41.22s/it]

 21%|███████████████▏                                                        | 78/370 [35:46<2:55:03, 35.97s/it]

 21%|███████████████▎                                                        | 79/370 [36:10<2:36:59, 32.37s/it]

 22%|███████████████▌                                                        | 80/370 [36:33<2:24:10, 29.83s/it]

 22%|███████████████▊                                                        | 81/370 [36:58<2:16:28, 28.33s/it]

 22%|███████████████▉                                                        | 82/370 [37:22<2:09:32, 26.99s/it]

 22%|████████████████▏                                                       | 83/370 [37:46<2:04:38, 26.06s/it]

 23%|████████████████▎                                                       | 84/370 [38:10<2:01:02, 25.39s/it]

 23%|████████████████▌                                                       | 85/370 [38:34<1:58:53, 25.03s/it]

 23%|████████████████▋                                                       | 86/370 [38:58<1:56:14, 24.56s/it]

 24%|████████████████▉                                                       | 87/370 [39:22<1:55:41, 24.53s/it]

 24%|█████████████████                                                       | 88/370 [39:46<1:55:01, 24.47s/it]

 24%|█████████████████▎                                                      | 89/370 [40:10<1:53:39, 24.27s/it]

 24%|█████████████████▌                                                      | 90/370 [40:34<1:52:59, 24.21s/it]

 25%|█████████████████▋                                                      | 91/370 [40:58<1:51:25, 23.96s/it]

 25%|█████████████████▉                                                      | 92/370 [41:22<1:51:30, 24.06s/it]

 25%|██████████████████                                                      | 93/370 [41:46<1:51:12, 24.09s/it]

 25%|██████████████████▎                                                     | 94/370 [42:11<1:51:19, 24.20s/it]

 26%|██████████████████▍                                                     | 95/370 [42:34<1:50:27, 24.10s/it]
{'loss': 2.163, 'learning_rate': 8.698444304324835e-05, 'epoch': 2.57}














 94%|██████████████████████████████████████████████████████████████████████▎    | 15/16 [00:49<00:03,  3.59s/it]

{'eval_loss': 2.109764575958252, 'eval_runtime': 56.7468, 'eval_samples_per_second': 4.494, 'eval_steps_per_second': 0.282, 'epoch': 2.57}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 26%|██████████████████▋                                                     | 96/370 [43:55<3:07:14, 41.00s/it]
{'loss': 2.1924, 'learning_rate': 8.669426832160996e-05, 'epoch': 2.59}


 26%|███████████████████                                                     | 98/370 [44:43<2:27:17, 32.49s/it]
{'loss': 2.1717, 'learning_rate': 8.610582983050581e-05, 'epoch': 2.65}

 27%|███████████████████▎                                                    | 99/370 [45:08<2:15:57, 30.10s/it]


 27%|███████████████████▍                                                   | 101/370 [45:56<2:01:26, 27.09s/it]

 28%|███████████████████▌                                                   | 102/370 [46:20<1:56:54, 26.17s/it]

 28%|███████████████████▊                                                   | 103/370 [46:45<1:54:34, 25.75s/it]

 28%|███████████████████▉                                                   | 104/370 [47:10<1:53:25, 25.58s/it]

 28%|████████████████████▏                                                  | 105/370 [47:36<1:52:51, 25.55s/it]

 29%|████████████████████▎                                                  | 106/370 [48:01<1:52:12, 25.50s/it]

 29%|████████████████████▌                                                  | 107/370 [48:26<1:50:53, 25.30s/it]

 29%|████████████████████▋                                                  | 108/370 [48:51<1:49:29, 25.07s/it]

 29%|████████████████████▉                                                  | 109/370 [49:15<1:47:54, 24.81s/it]

 30%|█████████████████████                                                  | 110/370 [49:39<1:46:21, 24.54s/it]

 30%|█████████████████████▎                                                 | 111/370 [50:03<1:45:30, 24.44s/it]

 30%|█████████████████████▍                                                 | 112/370 [50:27<1:45:03, 24.43s/it]

 31%|█████████████████████▋                                                 | 113/370 [50:52<1:44:42, 24.44s/it]

 31%|█████████████████████▉                                                 | 114/370 [51:16<1:44:35, 24.51s/it]
{'loss': 2.149, 'learning_rate': 8.103047409137114e-05, 'epoch': 3.08}















 94%|██████████████████████████████████████████████████████████████████████▎    | 15/16 [00:51<00:03,  3.61s/it]
{'eval_loss': 2.100526809692383, 'eval_runtime': 58.5452, 'eval_samples_per_second': 4.356, 'eval_steps_per_second': 0.273, 'epoch': 3.08}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 31%|██████████████████████                                                 | 115/370 [52:40<2:59:15, 42.18s/it]

 31%|██████████████████████▎                                                | 116/370 [53:04<2:35:26, 36.72s/it]

 32%|██████████████████████▍                                                | 117/370 [53:28<2:19:22, 33.05s/it]

 32%|██████████████████████▋                                                | 118/370 [53:52<2:07:12, 30.29s/it]

 32%|██████████████████████▊                                                | 119/370 [54:16<1:59:14, 28.50s/it]

 32%|███████████████████████                                                | 120/370 [54:40<1:52:57, 27.11s/it]

 33%|███████████████████████▏                                               | 121/370 [55:04<1:48:27, 26.13s/it]

 33%|███████████████████████▍                                               | 122/370 [55:28<1:45:32, 25.53s/it]

 33%|███████████████████████▌                                               | 123/370 [55:53<1:44:26, 25.37s/it]

 34%|███████████████████████▊                                               | 124/370 [56:18<1:42:57, 25.11s/it]

 34%|███████████████████████▉                                               | 125/370 [56:42<1:41:33, 24.87s/it]

 34%|████████████████████████▏                                              | 126/370 [57:06<1:40:05, 24.61s/it]

 34%|████████████████████████▎                                              | 127/370 [57:31<1:39:42, 24.62s/it]

 35%|████████████████████████▌                                              | 128/370 [57:55<1:39:01, 24.55s/it]

 35%|████████████████████████▊                                              | 129/370 [58:19<1:37:56, 24.38s/it]

 35%|████████████████████████▉                                              | 130/370 [58:44<1:37:40, 24.42s/it]

 35%|█████████████████████████▏                                             | 131/370 [59:08<1:37:12, 24.40s/it]

 36%|█████████████████████████▎                                             | 132/370 [59:33<1:37:03, 24.47s/it]

 36%|█████████████████████████▌                                             | 133/370 [59:58<1:37:44, 24.74s/it]
{'loss': 2.0255, 'learning_rate': 7.425299230975981e-05, 'epoch': 3.59}















 94%|██████████████████████████████████████████████████████████████████████▎    | 15/16 [00:53<00:03,  3.61s/it]
{'eval_loss': 2.099191904067993, 'eval_runtime': 60.0267, 'eval_samples_per_second': 4.248, 'eval_steps_per_second': 0.267, 'epoch': 3.59}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 36%|████████████████████████▉                                            | 134/370 [1:01:23<2:48:22, 42.81s/it]

 36%|█████████████████████████▏                                           | 135/370 [1:01:47<2:25:52, 37.25s/it]

 37%|█████████████████████████▎                                           | 136/370 [1:02:12<2:10:26, 33.45s/it]

 37%|█████████████████████████▌                                           | 137/370 [1:02:36<1:58:53, 30.62s/it]

 37%|█████████████████████████▋                                           | 138/370 [1:03:00<1:51:01, 28.71s/it]

 38%|█████████████████████████▉                                           | 139/370 [1:03:25<1:46:14, 27.59s/it]

 38%|██████████████████████████                                           | 140/370 [1:03:50<1:42:18, 26.69s/it]

 38%|██████████████████████████▎                                          | 141/370 [1:04:14<1:39:10, 25.99s/it]

 38%|██████████████████████████▍                                          | 142/370 [1:04:38<1:36:46, 25.47s/it]

 39%|██████████████████████████▋                                          | 143/370 [1:05:02<1:34:27, 24.97s/it]

 39%|██████████████████████████▊                                          | 144/370 [1:05:27<1:33:31, 24.83s/it]

 39%|███████████████████████████                                          | 145/370 [1:05:50<1:31:22, 24.37s/it]

 39%|███████████████████████████▏                                         | 146/370 [1:06:14<1:30:56, 24.36s/it]

 40%|███████████████████████████▍                                         | 147/370 [1:06:39<1:30:32, 24.36s/it]

 40%|███████████████████████████▌                                         | 148/370 [1:07:03<1:30:30, 24.46s/it]

 40%|███████████████████████████▊                                         | 149/370 [1:07:28<1:30:06, 24.46s/it]

 41%|███████████████████████████▉                                         | 150/370 [1:07:52<1:29:50, 24.50s/it]

 41%|████████████████████████████▏                                        | 151/370 [1:08:17<1:29:11, 24.43s/it]

 41%|████████████████████████████▎                                        | 152/370 [1:08:40<1:28:05, 24.24s/it]
{'loss': 2.0711, 'learning_rate': 6.683186421234552e-05, 'epoch': 4.11}















 94%|██████████████████████████████████████████████████████████████████████▎    | 15/16 [00:49<00:03,  3.56s/it]
{'eval_loss': 2.0983314514160156, 'eval_runtime': 56.5389, 'eval_samples_per_second': 4.51, 'eval_steps_per_second': 0.283, 'epoch': 4.11}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 41%|████████████████████████████▌                                        | 153/370 [1:10:01<2:29:16, 41.27s/it]

 42%|████████████████████████████▋                                        | 154/370 [1:10:26<2:10:53, 36.36s/it]

 42%|████████████████████████████▉                                        | 155/370 [1:10:52<1:58:28, 33.06s/it]

 42%|█████████████████████████████                                        | 156/370 [1:11:16<1:48:50, 30.52s/it]

 42%|█████████████████████████████▎                                       | 157/370 [1:11:40<1:41:18, 28.54s/it]

 43%|█████████████████████████████▍                                       | 158/370 [1:12:04<1:35:46, 27.11s/it]

 43%|█████████████████████████████▋                                       | 159/370 [1:12:28<1:32:00, 26.16s/it]

 43%|█████████████████████████████▊                                       | 160/370 [1:12:52<1:29:31, 25.58s/it]

 44%|██████████████████████████████                                       | 161/370 [1:13:16<1:27:45, 25.19s/it]

 44%|██████████████████████████████▏                                      | 162/370 [1:13:41<1:26:32, 24.96s/it]

 44%|██████████████████████████████▍                                      | 163/370 [1:14:05<1:25:24, 24.76s/it]

 44%|███████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                  | 164/370 [1:14:29<1:24:29, 24.61s/it]

 45%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                  | 165/370 [1:14:53<1:23:16, 24.37s/it]

 45%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                 | 166/370 [1:15:17<1:22:14, 24.19s/it]

 45%|████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                 | 167/370 [1:15:41<1:21:49, 24.18s/it]

 45%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                | 168/370 [1:16:05<1:21:01, 24.07s/it]

 46%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                | 169/370 [1:16:29<1:20:46, 24.11s/it]

 46%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                               | 170/370 [1:16:54<1:20:52, 24.26s/it]

 46%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                              | 171/370 [1:17:17<1:19:43, 24.04s/it]
{'loss': 2.0679, 'learning_rate': 5.896403794053679e-05, 'epoch': 4.62}















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:56<00:03,  3.84s/it]
{'eval_loss': 2.100717306137085, 'eval_runtime': 63.9091, 'eval_samples_per_second': 3.99, 'eval_steps_per_second': 0.25, 'epoch': 4.62}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 46%|███████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                              | 172/370 [1:18:46<2:23:04, 43.36s/it]

 47%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                             | 173/370 [1:19:10<2:03:33, 37.63s/it]

 47%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                             | 174/370 [1:19:34<1:49:51, 33.63s/it]
{'loss': 2.0615, 'learning_rate': 5.76945288352031e-05, 'epoch': 4.7}

 47%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                            | 175/370 [1:19:59<1:40:25, 30.90s/it]


 48%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                           | 177/370 [1:20:46<1:27:24, 27.17s/it]
{'loss': 2.0766, 'learning_rate': 5.6419917757327555e-05, 'epoch': 4.78}

 48%|███████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                           | 178/370 [1:21:11<1:24:22, 26.37s/it]

 48%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                          | 179/370 [1:21:37<1:23:58, 26.38s/it]

 49%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                         | 180/370 [1:22:02<1:21:56, 25.88s/it]

 49%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                         | 181/370 [1:22:26<1:20:21, 25.51s/it]

 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                        | 182/370 [1:22:51<1:18:55, 25.19s/it]

 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                        | 183/370 [1:23:15<1:17:12, 24.77s/it]

 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                       | 184/370 [1:23:38<1:15:31, 24.36s/it]

 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                       | 185/370 [1:24:02<1:14:46, 24.25s/it]

 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                      | 186/370 [1:24:27<1:14:41, 24.36s/it]

 51%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                      | 187/370 [1:24:51<1:14:02, 24.27s/it]

 51%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                     | 188/370 [1:25:16<1:14:26, 24.54s/it]

 51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                    | 189/370 [1:25:41<1:14:26, 24.67s/it]

 51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                    | 190/370 [1:26:05<1:13:37, 24.54s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:49<00:03,  3.58s/it]

{'eval_loss': 2.1019368171691895, 'eval_runtime': 56.7556, 'eval_samples_per_second': 4.493, 'eval_steps_per_second': 0.282, 'epoch': 5.14}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0541, 'learning_rate': 5.0429174054104355e-05, 'epoch': 5.16}
 52%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                   | 191/370 [1:27:27<2:04:34, 41.76s/it]


 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                  | 193/370 [1:28:14<1:35:57, 32.53s/it]
{'loss': 2.0996, 'learning_rate': 4.9570825945895656e-05, 'epoch': 5.22}

 52%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                  | 194/370 [1:28:37<1:26:48, 29.60s/it]

 53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                 | 195/370 [1:29:02<1:22:09, 28.17s/it]

 53%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                 | 196/370 [1:29:26<1:17:44, 26.81s/it]


 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                               | 198/370 [1:30:15<1:13:32, 25.65s/it]
{'loss': 2.1521, 'learning_rate': 4.7426062261482675e-05, 'epoch': 5.35}

 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                               | 199/370 [1:30:39<1:11:58, 25.25s/it]

 54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                              | 200/370 [1:31:03<1:10:45, 24.97s/it]

 54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 201/370 [1:31:27<1:09:34, 24.70s/it]

 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                             | 202/370 [1:31:52<1:08:52, 24.60s/it]

 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                             | 203/370 [1:32:16<1:08:01, 24.44s/it]

 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                            | 204/370 [1:32:40<1:07:46, 24.50s/it]

 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 205/370 [1:33:06<1:08:34, 24.94s/it]

 56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                           | 206/370 [1:33:31<1:08:15, 24.98s/it]

 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                          | 207/370 [1:33:56<1:07:20, 24.79s/it]

 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                          | 208/370 [1:34:20<1:06:34, 24.66s/it]

 56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                         | 209/370 [1:34:45<1:05:55, 24.57s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:50<00:03,  3.57s/it]

{'eval_loss': 2.1057372093200684, 'eval_runtime': 56.8503, 'eval_samples_per_second': 4.485, 'eval_steps_per_second': 0.281, 'epoch': 5.65}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0389, 'learning_rate': 4.2305471164796905e-05, 'epoch': 5.68}

 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                        | 211/370 [1:36:31<1:37:06, 36.65s/it]

 57%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                        | 212/370 [1:36:55<1:26:29, 32.85s/it]
{'loss': 1.9608, 'learning_rate': 4.14585128387328e-05, 'epoch': 5.73}

 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                       | 213/370 [1:37:20<1:19:48, 30.50s/it]


 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                      | 215/370 [1:38:09<1:10:48, 27.41s/it]
{'loss': 2.0935, 'learning_rate': 4.019287292859016e-05, 'epoch': 5.81}

 58%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                     | 216/370 [1:38:34<1:08:14, 26.59s/it]

 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                     | 217/370 [1:38:58<1:06:19, 26.01s/it]

 59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                    | 218/370 [1:39:23<1:04:34, 25.49s/it]

 59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                    | 219/370 [1:39:46<1:02:28, 24.82s/it]

 59%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                   | 220/370 [1:40:11<1:02:03, 24.82s/it]

 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                   | 221/370 [1:40:35<1:01:15, 24.67s/it]

 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                  | 222/370 [1:41:00<1:01:08, 24.78s/it]

 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                 | 223/370 [1:41:24<1:00:04, 24.52s/it]

 61%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                  | 224/370 [1:41:48<59:29, 24.45s/it]

 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                 | 225/370 [1:42:12<58:42, 24.29s/it]

 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                 | 226/370 [1:42:37<58:36, 24.42s/it]

 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                | 227/370 [1:43:01<57:52, 24.29s/it]

 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                               | 228/370 [1:43:25<57:29, 24.29s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:53<00:03,  3.65s/it]

{'eval_loss': 2.1058835983276367, 'eval_runtime': 60.0711, 'eval_samples_per_second': 4.245, 'eval_steps_per_second': 0.266, 'epoch': 6.16}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0581, 'learning_rate': 3.438597215571027e-05, 'epoch': 6.19}
 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                              | 229/370 [1:44:50<1:40:01, 42.57s/it]

 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                              | 230/370 [1:45:15<1:26:45, 37.18s/it]

 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                             | 231/370 [1:45:39<1:16:53, 33.19s/it]

 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                            | 232/370 [1:46:03<1:10:22, 30.60s/it]

 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 233/370 [1:46:28<1:05:23, 28.64s/it]

 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                           | 234/370 [1:46:52<1:02:09, 27.42s/it]

 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                            | 235/370 [1:47:17<59:39, 26.52s/it]


 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 237/370 [1:48:06<56:43, 25.59s/it]

 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 239/370 [1:48:55<54:37, 25.02s/it]
{'loss': 2.0002, 'learning_rate': 3.076674501464994e-05, 'epoch': 6.43}
{'loss': 2.1051, 'learning_rate': 3.0371301769291417e-05, 'epoch': 6.46}

 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 240/370 [1:49:19<53:42, 24.79s/it]

 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                        | 241/370 [1:49:43<53:06, 24.70s/it]

 65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                        | 242/370 [1:50:07<52:07, 24.44s/it]

 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                       | 243/370 [1:50:31<51:25, 24.30s/it]


 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 245/370 [1:51:20<50:43, 24.35s/it]

 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                     | 246/370 [1:51:44<50:03, 24.22s/it]
{'loss': 2.0769, 'learning_rate': 2.7645310350744297e-05, 'epoch': 6.65}

 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                     | 247/370 [1:52:09<49:53, 24.34s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:49<00:03,  3.57s/it]

{'eval_loss': 2.109311580657959, 'eval_runtime': 56.5204, 'eval_samples_per_second': 4.512, 'eval_steps_per_second': 0.283, 'epoch': 6.68}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 248/370 [1:53:30<1:24:23, 41.51s/it]

 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                   | 249/370 [1:53:54<1:13:18, 36.35s/it]
{'loss': 2.0035, 'learning_rate': 2.650116284863402e-05, 'epoch': 6.73}


 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 251/370 [1:54:43<59:37, 30.07s/it]
{'loss': 2.0614, 'learning_rate': 2.5747007690240198e-05, 'epoch': 6.78}

 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 252/370 [1:55:07<55:48, 28.37s/it]

 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 253/370 [1:55:32<53:05, 27.23s/it]

 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 254/370 [1:55:56<51:00, 26.38s/it]

 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                | 255/370 [1:56:20<49:04, 25.61s/it]

 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                | 256/370 [1:56:44<47:39, 25.09s/it]

 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                               | 257/370 [1:57:08<46:45, 24.82s/it]

 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                               | 258/370 [1:57:32<45:51, 24.57s/it]

 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 259/370 [1:57:56<45:14, 24.45s/it]

 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                             | 260/370 [1:58:20<44:32, 24.29s/it]


 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                            | 262/370 [1:59:09<43:54, 24.39s/it]
{'loss': 1.9595, 'learning_rate': 2.173276630126287e-05, 'epoch': 7.08}

 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                            | 263/370 [1:59:33<43:24, 24.34s/it]

 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 264/370 [1:59:57<43:01, 24.35s/it]

 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 265/370 [2:00:22<42:40, 24.39s/it]

 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                          | 266/370 [2:00:46<41:57, 24.21s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:51<00:03,  3.85s/it]

{'eval_loss': 2.109825849533081, 'eval_runtime': 58.2072, 'eval_samples_per_second': 4.381, 'eval_steps_per_second': 0.275, 'epoch': 7.19}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0165, 'learning_rate': 1.9989285972581595e-05, 'epoch': 7.22}
 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 267/370 [2:02:10<1:12:17, 42.11s/it]

 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                        | 268/370 [2:02:36<1:03:20, 37.26s/it]

 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                        | 269/370 [2:02:59<55:53, 33.20s/it]

 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                        | 270/370 [2:03:24<50:51, 30.52s/it]

 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 271/370 [2:03:48<47:13, 28.62s/it]

 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 272/370 [2:04:12<44:23, 27.18s/it]


 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 274/370 [2:04:59<40:47, 25.49s/it]
{'loss': 1.9959, 'learning_rate': 1.7641966633669703e-05, 'epoch': 7.41}


 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 276/370 [2:05:47<38:38, 24.67s/it]
{'loss': 2.0465, 'learning_rate': 1.6992393966438407e-05, 'epoch': 7.46}

 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 277/370 [2:06:12<38:05, 24.57s/it]

 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 278/370 [2:06:36<37:32, 24.48s/it]

 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                   | 279/370 [2:07:00<37:08, 24.49s/it]

 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 280/370 [2:07:24<36:23, 24.26s/it]

 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 281/370 [2:07:48<35:49, 24.15s/it]


 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                 | 283/370 [2:08:35<34:29, 23.79s/it]
{'loss': 2.0146, 'learning_rate': 1.4796723893259712e-05, 'epoch': 7.65}

 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 284/370 [2:08:59<34:09, 23.83s/it]

 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                               | 285/370 [2:09:23<33:45, 23.83s/it]














 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:50<00:03,  3.60s/it]

{'eval_loss': 2.11244535446167, 'eval_runtime': 57.2081, 'eval_samples_per_second': 4.457, 'eval_steps_per_second': 0.28, 'epoch': 7.7}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0481, 'learning_rate': 1.3894170169494192e-05, 'epoch': 7.73}
 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 286/370 [2:10:45<57:42, 41.22s/it]


 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                              | 288/370 [2:11:34<44:40, 32.68s/it]
{'loss': 1.9518, 'learning_rate': 1.3305731678390048e-05, 'epoch': 7.78}

 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                             | 289/370 [2:11:58<40:52, 30.28s/it]


 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 291/370 [2:12:47<35:59, 27.33s/it]
{'loss': 2.0191, 'learning_rate': 1.2443403456474017e-05, 'epoch': 7.86}

 79%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 292/370 [2:13:11<34:01, 26.17s/it]


 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 294/370 [2:13:59<31:59, 25.26s/it]

 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 295/370 [2:14:24<31:18, 25.05s/it]
{'loss': 2.0354, 'learning_rate': 1.1332466114513512e-05, 'epoch': 7.97}

 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 296/370 [2:14:49<30:44, 24.93s/it]

 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                         | 297/370 [2:15:13<30:00, 24.67s/it]


 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 299/370 [2:16:02<29:05, 24.58s/it]
{'loss': 1.9804, 'learning_rate': 1.0267107413118742e-05, 'epoch': 8.08}

 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 300/370 [2:16:27<28:46, 24.66s/it]


 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 302/370 [2:17:14<27:19, 24.11s/it]
{'loss': 1.9561, 'learning_rate': 9.498756857261244e-06, 'epoch': 8.16}

 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 303/370 [2:17:38<27:04, 24.25s/it]

 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                     | 304/370 [2:18:03<26:43, 24.29s/it]















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:50<00:03,  3.60s/it]
{'eval_loss': 2.113715887069702, 'eval_runtime': 57.2395, 'eval_samples_per_second': 4.455, 'eval_steps_per_second': 0.28, 'epoch': 8.22}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9467, 'learning_rate': 8.757261257028777e-06, 'epoch': 8.24}

 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                    | 306/370 [2:19:50<39:05, 36.65s/it]

 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                   | 307/370 [2:20:14<34:29, 32.86s/it]

 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 308/370 [2:20:38<31:20, 30.33s/it]
{'loss': 2.0203, 'learning_rate': 8.04311227218031e-06, 'epoch': 8.32}


 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 310/370 [2:21:26<27:03, 27.06s/it]
{'loss': 2.0439, 'learning_rate': 7.582440109384809e-06, 'epoch': 8.38}

 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 311/370 [2:21:51<25:51, 26.29s/it]


 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                | 313/370 [2:22:40<24:08, 25.42s/it]

 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 314/370 [2:23:05<23:30, 25.18s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                               | 315/370 [2:23:30<23:09, 25.27s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 316/370 [2:23:55<22:32, 25.05s/it]
{'loss': 1.9958, 'learning_rate': 6.275952108448019e-06, 'epoch': 8.54}

 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 317/370 [2:24:19<22:01, 24.94s/it]

 86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 318/370 [2:24:44<21:29, 24.79s/it]

 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 319/370 [2:25:07<20:46, 24.43s/it]

 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 320/370 [2:25:32<20:22, 24.44s/it]


 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                           | 322/370 [2:26:20<19:29, 24.37s/it]
{'loss': 2.0496, 'learning_rate': 5.085412422262364e-06, 'epoch': 8.7}

 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 323/370 [2:26:44<18:50, 24.06s/it]















100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:53<00:00,  3.51s/it]

{'eval_loss': 2.1143386363983154, 'eval_runtime': 57.3649, 'eval_samples_per_second': 4.445, 'eval_steps_per_second': 0.279, 'epoch': 8.73}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0515, 'learning_rate': 4.714886845976429e-06, 'epoch': 8.76}

 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 325/370 [2:28:30<27:17, 36.39s/it]

 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 326/370 [2:28:55<24:02, 32.78s/it]

 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 327/370 [2:29:19<21:36, 30.15s/it]

 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 328/370 [2:29:43<19:53, 28.42s/it]

 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                       | 329/370 [2:30:07<18:30, 27.10s/it]
{'loss': 2.0835, 'learning_rate': 3.8471896557912e-06, 'epoch': 8.89}


 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                      | 331/370 [2:30:57<16:48, 25.86s/it]

 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 332/370 [2:31:21<16:08, 25.48s/it]

 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 333/370 [2:31:45<15:25, 25.02s/it]
{'loss': 1.9843, 'learning_rate': 3.2141590479753236e-06, 'epoch': 9.0}

 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                    | 334/370 [2:32:10<14:54, 24.85s/it]

 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 335/370 [2:32:34<14:27, 24.78s/it]

 91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 336/370 [2:32:58<13:54, 24.55s/it]


 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                  | 338/370 [2:33:47<13:05, 24.54s/it]
{'loss': 1.996, 'learning_rate': 2.500503850274949e-06, 'epoch': 9.14}

 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 339/370 [2:34:12<12:41, 24.55s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 340/370 [2:34:36<12:17, 24.59s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 341/370 [2:35:02<11:59, 24.81s/it]

 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 342/370 [2:35:26<11:27, 24.56s/it]















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:50<00:03,  3.60s/it]
{'eval_loss': 2.114295721054077, 'eval_runtime': 57.2692, 'eval_samples_per_second': 4.453, 'eval_steps_per_second': 0.279, 'epoch': 9.24}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9645, 'learning_rate': 1.8743268696145954e-06, 'epoch': 9.27}

 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 344/370 [2:37:13<15:55, 36.74s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 345/370 [2:37:37<13:46, 33.05s/it]

 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 346/370 [2:38:02<12:11, 30.47s/it]
{'loss': 1.9634, 'learning_rate': 1.5410967280373223e-06, 'epoch': 9.35}


 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 348/370 [2:38:51<10:05, 27.52s/it]

 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏           | 349/370 [2:39:15<09:15, 26.46s/it]

 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 350/370 [2:39:40<08:37, 25.87s/it]

 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 351/370 [2:40:04<08:01, 25.33s/it]
{'loss': 2.131, 'learning_rate': 1.0572157452321097e-06, 'epoch': 9.49}

 95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 352/370 [2:40:28<07:30, 25.05s/it]


 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 354/370 [2:41:18<06:38, 24.91s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 355/370 [2:41:42<06:11, 24.77s/it]
{'loss': 2.1394, 'learning_rate': 7.349718656945504e-07, 'epoch': 9.59}

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 356/370 [2:42:07<05:45, 24.71s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋       | 357/370 [2:42:31<05:18, 24.49s/it]


 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊      | 359/370 [2:43:19<04:29, 24.47s/it]

 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 360/370 [2:43:44<04:04, 24.46s/it]

 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 361/370 [2:44:08<03:39, 24.43s/it]
{'loss': 1.9951, 'learning_rate': 3.605877418729975e-07, 'epoch': 9.76}















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 15/16 [00:50<00:03,  3.64s/it]
{'eval_loss': 2.114665985107422, 'eval_runtime': 57.7649, 'eval_samples_per_second': 4.414, 'eval_steps_per_second': 0.277, 'epoch': 9.76}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 362/370 [2:45:30<05:33, 41.74s/it]

 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 363/370 [2:45:54<04:15, 36.44s/it]
{'loss': 1.9592, 'learning_rate': 2.6500621927054715e-07, 'epoch': 9.81}

 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 364/370 [2:46:19<03:17, 32.85s/it]


 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 366/370 [2:47:08<01:54, 28.60s/it]

 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 367/370 [2:47:32<01:22, 27.36s/it]
{'loss': 2.0421, 'learning_rate': 1.1783841569968367e-07, 'epoch': 9.92}

 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 368/370 [2:47:57<00:53, 26.59s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍| 369/370 [2:48:21<00:25, 25.82s/it]
{'loss': 1.928, 'learning_rate': 4.604165524423332e-08, 'epoch': 10.0}


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 370/370 [2:48:46<00:00, 27.37s/it]