  0%|                                                                                                                        | 0/650 [00:00<?, ?it/s]/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▏                                                                                                             | 1/650 [00:24<4:30:07, 24.97s/it]

  0%|▎                                                                                                             | 2/650 [00:48<4:22:01, 24.26s/it]

  0%|▌                                                                                                             | 3/650 [01:12<4:19:22, 24.05s/it]

  1%|▋                                                                                                             | 4/650 [01:36<4:18:17, 23.99s/it]

  1%|▊                                                                                                             | 5/650 [01:59<4:13:08, 23.55s/it]

  1%|█                                                                                                             | 6/650 [02:23<4:14:36, 23.72s/it]

  1%|█▏                                                                                                            | 7/650 [02:47<4:14:42, 23.77s/it]

  1%|█▎                                                                                                            | 8/650 [03:11<4:15:27, 23.87s/it]

  1%|█▌                                                                                                            | 9/650 [03:34<4:13:58, 23.77s/it]

  2%|█▋                                                                                                           | 10/650 [03:58<4:12:48, 23.70s/it]

  2%|█▊                                                                                                           | 11/650 [04:22<4:13:05, 23.76s/it]

  2%|██                                                                                                           | 12/650 [04:46<4:12:57, 23.79s/it]

  2%|██▏                                                                                                          | 13/650 [05:09<4:12:47, 23.81s/it]

  2%|██▎                                                                                                          | 14/650 [05:34<4:13:29, 23.91s/it]

  2%|██▌                                                                                                          | 15/650 [05:57<4:11:56, 23.80s/it]

  2%|██▋                                                                                                          | 16/650 [06:21<4:10:52, 23.74s/it]

  3%|██▊                                                                                                          | 17/650 [06:45<4:11:00, 23.79s/it]

  3%|███                                                                                                          | 18/650 [07:09<4:12:59, 24.02s/it]

  3%|███▏                                                                                                         | 19/650 [07:33<4:10:55, 23.86s/it]

  3%|███▎                                                                                                         | 20/650 [07:57<4:10:26, 23.85s/it]

  3%|███▌                                                                                                         | 21/650 [08:21<4:10:34, 23.90s/it]

  3%|███▋                                                                                                         | 22/650 [08:45<4:11:00, 23.98s/it]

  4%|███▊                                                                                                         | 23/650 [09:08<4:07:24, 23.68s/it]

  4%|████                                                                                                         | 24/650 [09:31<4:06:19, 23.61s/it]

  4%|████▏                                                                                                        | 25/650 [09:55<4:06:43, 23.69s/it]

  4%|████▎                                                                                                        | 26/650 [10:19<4:06:02, 23.66s/it]

  4%|████▌                                                                                                        | 27/650 [10:43<4:06:40, 23.76s/it]

  4%|████▋                                                                                                        | 28/650 [11:07<4:07:26, 23.87s/it]

  4%|████▊                                                                                                        | 29/650 [11:31<4:08:10, 23.98s/it]

  5%|█████                                                                                                        | 30/650 [11:55<4:07:43, 23.97s/it]

  5%|█████▏                                                                                                       | 31/650 [12:19<4:06:35, 23.90s/it]

  5%|█████▎                                                                                                       | 32/650 [12:42<4:04:48, 23.77s/it]

  5%|█████▌                                                                                                       | 33/650 [13:06<4:04:56, 23.82s/it]
{'loss': 2.1715, 'learning_rate': 4.979855782118802e-05, 'epoch': 0.51}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.51s/it]
{'eval_loss': 2.3082401752471924, 'eval_runtime': 103.4276, 'eval_samples_per_second': 4.583, 'eval_steps_per_second': 0.29, 'epoch': 0.51}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  5%|█████▋                                                                                                       | 34/650 [15:14<9:24:07, 54.95s/it]
{'loss': 2.102, 'learning_rate': 4.978278718637443e-05, 'epoch': 0.52}


  6%|██████                                                                                                       | 36/650 [16:02<6:41:02, 39.19s/it]

  6%|██████▏                                                                                                      | 37/650 [16:25<5:52:14, 34.48s/it]

  6%|██████▎                                                                                                      | 38/650 [16:49<5:19:05, 31.28s/it]

  6%|██████▌                                                                                                      | 39/650 [17:13<4:57:01, 29.17s/it]

  6%|██████▋                                                                                                      | 40/650 [17:36<4:38:09, 27.36s/it]

  6%|██████▉                                                                                                      | 41/650 [18:00<4:27:02, 26.31s/it]

  6%|███████                                                                                                      | 42/650 [18:24<4:19:23, 25.60s/it]

  7%|███████▏                                                                                                     | 43/650 [18:48<4:13:47, 25.09s/it]

  7%|███████▍                                                                                                     | 44/650 [19:12<4:09:54, 24.74s/it]

  7%|███████▌                                                                                                     | 45/650 [19:36<4:06:23, 24.44s/it]

  7%|███████▋                                                                                                     | 46/650 [20:00<4:04:24, 24.28s/it]

  7%|███████▉                                                                                                     | 47/650 [20:22<3:59:48, 23.86s/it]

  7%|████████                                                                                                     | 48/650 [20:46<3:59:18, 23.85s/it]

  8%|████████▏                                                                                                    | 49/650 [21:10<3:59:00, 23.86s/it]

  8%|████████▍                                                                                                    | 50/650 [21:33<3:55:46, 23.58s/it]

  8%|████████▌                                                                                                    | 51/650 [21:56<3:53:51, 23.43s/it]

  8%|████████▋                                                                                                    | 52/650 [22:20<3:55:15, 23.60s/it]

  8%|████████▉                                                                                                    | 53/650 [22:44<3:55:14, 23.64s/it]

  8%|█████████                                                                                                    | 54/650 [23:09<3:57:46, 23.94s/it]

  8%|█████████▏                                                                                                   | 55/650 [23:33<4:00:08, 24.22s/it]

  9%|█████████▍                                                                                                   | 56/650 [23:59<4:02:53, 24.53s/it]

  9%|█████████▌                                                                                                   | 57/650 [24:23<4:01:23, 24.42s/it]

  9%|█████████▋                                                                                                   | 58/650 [24:47<3:59:39, 24.29s/it]

  9%|█████████▉                                                                                                   | 59/650 [25:10<3:56:42, 24.03s/it]

  9%|██████████                                                                                                   | 60/650 [25:34<3:55:37, 23.96s/it]

  9%|██████████▏                                                                                                  | 61/650 [25:58<3:54:59, 23.94s/it]

 10%|██████████▍                                                                                                  | 62/650 [26:22<3:53:32, 23.83s/it]

 10%|██████████▌                                                                                                  | 63/650 [26:45<3:52:05, 23.72s/it]

 10%|██████████▋                                                                                                  | 64/650 [27:09<3:51:24, 23.69s/it]

 10%|██████████▉                                                                                                  | 65/650 [27:30<3:43:09, 22.89s/it]

 10%|███████████                                                                                                  | 66/650 [27:53<3:43:08, 22.93s/it]
{'loss': 2.2679, 'learning_rate': 4.8968468738370244e-05, 'epoch': 1.02}






























 10%|███████████                                                                                                  | 66/650 [29:38<3:43:08, 22.93s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Data_Science_and_Machine_Learning_class-bs_16-lr_5e-05-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-66)... Done. 0.2s
{'eval_loss': 2.2525951862335205, 'eval_runtime': 105.3725, 'eval_samples_per_second': 4.498, 'eval_steps_per_second': 0.285, 'epoch': 1.02}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 10%|███████████▏                                                                                                 | 67/650 [30:03<8:55:21, 55.10s/it]

 10%|███████████▍                                                                                                 | 68/650 [30:26<7:22:14, 45.59s/it]

 11%|███████████▌                                                                                                 | 69/650 [30:51<6:19:38, 39.21s/it]

 11%|███████████▋                                                                                                 | 70/650 [31:15<5:34:57, 34.65s/it]

 11%|███████████▉                                                                                                 | 71/650 [31:38<5:02:34, 31.35s/it]

 11%|████████████                                                                                                 | 72/650 [32:02<4:40:33, 29.12s/it]

 11%|████████████▏                                                                                                | 73/650 [32:26<4:24:57, 27.55s/it]

 11%|████████████▍                                                                                                | 74/650 [32:50<4:15:36, 26.63s/it]

 12%|████████████▌                                                                                                | 75/650 [33:14<4:07:36, 25.84s/it]

 12%|████████████▋                                                                                                | 76/650 [33:38<4:01:48, 25.28s/it]

 12%|████████████▉                                                                                                | 77/650 [34:02<3:57:17, 24.85s/it]

 12%|█████████████                                                                                                | 78/650 [34:26<3:54:05, 24.55s/it]

 12%|█████████████▏                                                                                               | 79/650 [34:50<3:51:06, 24.28s/it]

 12%|█████████████▍                                                                                               | 80/650 [35:13<3:48:55, 24.10s/it]

 12%|█████████████▌                                                                                               | 81/650 [35:36<3:44:44, 23.70s/it]

 13%|█████████████▊                                                                                               | 82/650 [36:00<3:45:12, 23.79s/it]

 13%|█████████████▉                                                                                               | 83/650 [36:25<3:46:20, 23.95s/it]

 13%|██████████████                                                                                               | 84/650 [36:49<3:46:43, 24.04s/it]

 13%|██████████████▎                                                                                              | 85/650 [37:13<3:48:03, 24.22s/it]

 13%|██████████████▍                                                                                              | 86/650 [37:37<3:46:14, 24.07s/it]

 13%|██████████████▌                                                                                              | 87/650 [38:01<3:45:26, 24.03s/it]

 14%|██████████████▊                                                                                              | 88/650 [38:25<3:45:25, 24.07s/it]

 14%|██████████████▉                                                                                              | 89/650 [38:49<3:44:36, 24.02s/it]

 14%|███████████████                                                                                              | 90/650 [39:12<3:41:02, 23.68s/it]

 14%|███████████████▎                                                                                             | 91/650 [39:36<3:40:59, 23.72s/it]

 14%|███████████████▍                                                                                             | 92/650 [40:00<3:41:03, 23.77s/it]

 14%|███████████████▌                                                                                             | 93/650 [40:23<3:40:07, 23.71s/it]

 14%|███████████████▊                                                                                             | 94/650 [40:47<3:40:40, 23.81s/it]

 15%|███████████████▉                                                                                             | 95/650 [41:10<3:38:12, 23.59s/it]

 15%|████████████████                                                                                             | 96/650 [41:35<3:39:10, 23.74s/it]

 15%|████████████████▎                                                                                            | 97/650 [41:59<3:39:27, 23.81s/it]

 15%|████████████████▍                                                                                            | 98/650 [42:23<3:39:57, 23.91s/it]

 15%|████████████████▌                                                                                            | 99/650 [42:46<3:38:31, 23.80s/it]
{'loss': 2.2181, 'learning_rate': 4.751664519422778e-05, 'epoch': 1.52}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.50s/it]
{'eval_loss': 2.1995277404785156, 'eval_runtime': 103.0868, 'eval_samples_per_second': 4.598, 'eval_steps_per_second': 0.291, 'epoch': 1.52}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 15%|████████████████▌                                                                                           | 100/650 [44:54<8:23:45, 54.95s/it]

 16%|████████████████▊                                                                                           | 101/650 [45:17<6:56:42, 45.54s/it]

 16%|████████████████▉                                                                                           | 102/650 [45:41<5:56:09, 39.00s/it]

 16%|█████████████████                                                                                           | 103/650 [46:05<5:14:38, 34.51s/it]

 16%|█████████████████▎                                                                                          | 104/650 [46:29<4:45:21, 31.36s/it]

 16%|█████████████████▍                                                                                          | 105/650 [46:53<4:23:34, 29.02s/it]

 16%|█████████████████▌                                                                                          | 106/650 [47:17<4:09:15, 27.49s/it]

 16%|█████████████████▊                                                                                          | 107/650 [47:41<3:59:02, 26.41s/it]

 17%|█████████████████▉                                                                                          | 108/650 [48:05<3:51:50, 25.67s/it]

 17%|██████████████████                                                                                          | 109/650 [48:28<3:45:41, 25.03s/it]

 17%|██████████████████▎                                                                                         | 110/650 [48:52<3:42:33, 24.73s/it]

 17%|██████████████████▍                                                                                         | 111/650 [49:16<3:40:36, 24.56s/it]

 17%|██████████████████▌                                                                                         | 112/650 [49:40<3:38:34, 24.38s/it]

 17%|██████████████████▊                                                                                         | 113/650 [50:04<3:37:08, 24.26s/it]

 18%|██████████████████▉                                                                                         | 114/650 [50:28<3:36:27, 24.23s/it]

 18%|███████████████████                                                                                         | 115/650 [50:52<3:35:32, 24.17s/it]

 18%|███████████████████▎                                                                                        | 116/650 [51:16<3:34:50, 24.14s/it]

 18%|███████████████████▍                                                                                        | 117/650 [51:40<3:32:24, 23.91s/it]
{'loss': 2.2413, 'learning_rate': 4.6721534754996125e-05, 'epoch': 1.8}


 18%|███████████████████▊                                                                                        | 119/650 [52:27<3:30:50, 23.82s/it]
{'loss': 2.2426, 'learning_rate': 4.666080674062213e-05, 'epoch': 1.83}

 18%|███████████████████▉                                                                                        | 120/650 [52:51<3:30:39, 23.85s/it]

 19%|████████████████████                                                                                        | 121/650 [53:15<3:30:32, 23.88s/it]

 19%|████████████████████▎                                                                                       | 122/650 [53:39<3:30:23, 23.91s/it]

 19%|████████████████████▍                                                                                       | 123/650 [54:02<3:27:49, 23.66s/it]


 19%|████████████████████▊                                                                                       | 125/650 [54:49<3:26:09, 23.56s/it]

 19%|████████████████████▉                                                                                       | 126/650 [55:14<3:28:21, 23.86s/it]

 20%|█████████████████████                                                                                       | 127/650 [55:38<3:28:20, 23.90s/it]

 20%|█████████████████████▎                                                                                      | 128/650 [56:01<3:26:52, 23.78s/it]

 20%|█████████████████████▍                                                                                      | 129/650 [56:25<3:26:54, 23.83s/it]

 20%|█████████████████████▌                                                                                      | 130/650 [56:45<3:16:52, 22.72s/it]
{'loss': 2.3479, 'learning_rate': 4.5958999492978524e-05, 'epoch': 2.0}

 20%|█████████████████████▊                                                                                      | 131/650 [57:09<3:19:48, 23.10s/it]

 20%|█████████████████████▉                                                                                      | 132/650 [57:34<3:22:21, 23.44s/it]





























100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:41<00:00,  3.23s/it]

{'eval_loss': 2.137834072113037, 'eval_runtime': 104.559, 'eval_samples_per_second': 4.533, 'eval_steps_per_second': 0.287, 'epoch': 2.03}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0626, 'learning_rate': 4.575700381655979e-05, 'epoch': 2.05}
 20%|██████████████████████                                                                                      | 133/650 [59:43<7:55:20, 55.16s/it]

 21%|█████████████████████▊                                                                                    | 134/650 [1:00:07<6:33:45, 45.79s/it]

 21%|██████████████████████                                                                                    | 135/650 [1:00:30<5:35:58, 39.14s/it]

 21%|██████████████████████▏                                                                                   | 136/650 [1:00:54<4:56:48, 34.65s/it]

 21%|██████████████████████▎                                                                                   | 137/650 [1:01:18<4:28:37, 31.42s/it]

 21%|██████████████████████▌                                                                                   | 138/650 [1:01:42<4:07:40, 29.02s/it]

 21%|██████████████████████▋                                                                                   | 139/650 [1:02:06<3:54:16, 27.51s/it]

 22%|██████████████████████▊                                                                                   | 140/650 [1:02:30<3:44:37, 26.43s/it]


 22%|███████████████████████▏                                                                                  | 142/650 [1:03:16<3:30:14, 24.83s/it]
{'loss': 2.0459, 'learning_rate': 4.519666802851422e-05, 'epoch': 2.18}


 22%|███████████████████████▍                                                                                  | 144/650 [1:04:04<3:25:55, 24.42s/it]
{'loss': 1.9184, 'learning_rate': 4.505172898724018e-05, 'epoch': 2.22}

 22%|███████████████████████▋                                                                                  | 145/650 [1:04:28<3:23:54, 24.23s/it]

 22%|███████████████████████▊                                                                                  | 146/650 [1:04:52<3:22:42, 24.13s/it]

 23%|███████████████████████▉                                                                                  | 147/650 [1:05:16<3:20:41, 23.94s/it]

 23%|████████████████████████▏                                                                                 | 148/650 [1:05:40<3:21:00, 24.03s/it]

 23%|████████████████████████▎                                                                                 | 149/650 [1:06:04<3:20:58, 24.07s/it]

 23%|████████████████████████▍                                                                                 | 150/650 [1:06:28<3:20:25, 24.05s/it]

 23%|████████████████████████▌                                                                                 | 151/650 [1:06:52<3:18:47, 23.90s/it]

 23%|████████████████████████▊                                                                                 | 152/650 [1:07:16<3:18:35, 23.93s/it]

 24%|████████████████████████▉                                                                                 | 153/650 [1:07:40<3:18:39, 23.98s/it]

 24%|█████████████████████████                                                                                 | 154/650 [1:08:04<3:18:14, 23.98s/it]

 24%|█████████████████████████▎                                                                                | 155/650 [1:08:28<3:17:54, 23.99s/it]

 24%|█████████████████████████▍                                                                                | 156/650 [1:08:52<3:17:25, 23.98s/it]

 24%|█████████████████████████▌                                                                                | 157/650 [1:09:16<3:16:54, 23.96s/it]

 24%|█████████████████████████▊                                                                                | 158/650 [1:09:40<3:16:34, 23.97s/it]

 24%|█████████████████████████▉                                                                                | 159/650 [1:10:04<3:16:10, 23.97s/it]

 25%|██████████████████████████                                                                                | 160/650 [1:10:27<3:15:45, 23.97s/it]

 25%|██████████████████████████▎                                                                               | 161/650 [1:10:52<3:16:04, 24.06s/it]

 25%|██████████████████████████▍                                                                               | 162/650 [1:11:16<3:15:14, 24.00s/it]

 25%|██████████████████████████▌                                                                               | 163/650 [1:11:40<3:15:26, 24.08s/it]

 25%|██████████████████████████▋                                                                               | 164/650 [1:12:04<3:14:52, 24.06s/it]

 25%|██████████████████████████▉                                                                               | 165/650 [1:12:28<3:14:21, 24.04s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.51s/it]

{'eval_loss': 2.0372719764709473, 'eval_runtime': 103.4031, 'eval_samples_per_second': 4.584, 'eval_steps_per_second': 0.29, 'epoch': 2.54}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.1164, 'learning_rate': 4.3334207222307275e-05, 'epoch': 2.55}
 26%|███████████████████████████                                                                               | 166/650 [1:14:36<7:25:24, 55.22s/it]

 26%|███████████████████████████▏                                                                              | 167/650 [1:15:00<6:09:43, 45.93s/it]

 26%|███████████████████████████▍                                                                              | 168/650 [1:15:24<5:16:08, 39.35s/it]

 26%|███████████████████████████▌                                                                              | 169/650 [1:15:48<4:37:47, 34.65s/it]


 26%|███████████████████████████▉                                                                              | 171/650 [1:16:35<3:52:13, 29.09s/it]

 26%|████████████████████████████                                                                              | 172/650 [1:16:59<3:38:49, 27.47s/it]
{'loss': 2.0716, 'learning_rate': 4.2828169363714016e-05, 'epoch': 2.65}

 27%|████████████████████████████▏                                                                             | 173/650 [1:17:23<3:29:08, 26.31s/it]

 27%|████████████████████████████▍                                                                             | 174/650 [1:17:47<3:23:04, 25.60s/it]

 27%|████████████████████████████▌                                                                             | 175/650 [1:18:11<3:18:38, 25.09s/it]

 27%|████████████████████████████▋                                                                             | 176/650 [1:18:35<3:15:49, 24.79s/it]

 27%|████████████████████████████▊                                                                             | 177/650 [1:18:59<3:13:33, 24.55s/it]


 28%|█████████████████████████████▏                                                                            | 179/650 [1:19:45<3:07:46, 23.92s/it]

 28%|█████████████████████████████▎                                                                            | 180/650 [1:20:09<3:07:36, 23.95s/it]

 28%|█████████████████████████████▌                                                                            | 181/650 [1:20:34<3:07:45, 24.02s/it]
{'loss': 2.0607, 'learning_rate': 4.204052726053676e-05, 'epoch': 2.78}

 28%|█████████████████████████████▋                                                                            | 182/650 [1:20:57<3:06:08, 23.86s/it]


 28%|██████████████████████████████                                                                            | 184/650 [1:21:46<3:06:40, 24.04s/it]
{'loss': 2.0448, 'learning_rate': 4.1770581951752976e-05, 'epoch': 2.83}

 28%|██████████████████████████████▏                                                                           | 185/650 [1:22:09<3:05:05, 23.88s/it]


 29%|██████████████████████████████▍                                                                           | 187/650 [1:22:56<3:02:39, 23.67s/it]
{'loss': 2.1241, 'learning_rate': 4.1497033672796924e-05, 'epoch': 2.88}

 29%|██████████████████████████████▋                                                                           | 188/650 [1:23:19<3:01:57, 23.63s/it]

 29%|██████████████████████████████▊                                                                           | 189/650 [1:23:43<3:01:23, 23.61s/it]

 29%|██████████████████████████████▉                                                                           | 190/650 [1:24:07<3:01:46, 23.71s/it]

 29%|███████████████████████████████▏                                                                          | 191/650 [1:24:31<3:01:18, 23.70s/it]

 30%|███████████████████████████████▎                                                                          | 192/650 [1:24:54<2:59:19, 23.49s/it]

 30%|███████████████████████████████▍                                                                          | 193/650 [1:25:17<2:58:31, 23.44s/it]

 30%|███████████████████████████████▋                                                                          | 194/650 [1:25:41<2:58:43, 23.52s/it]

 30%|███████████████████████████████▊                                                                          | 195/650 [1:26:01<2:52:17, 22.72s/it]

 30%|███████████████████████████████▉                                                                          | 196/650 [1:26:25<2:53:57, 22.99s/it]

 30%|████████████████████████████████▏                                                                         | 197/650 [1:26:49<2:55:37, 23.26s/it]

 30%|████████████████████████████████▎                                                                         | 198/650 [1:27:13<2:56:42, 23.46s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:38<00:03,  3.56s/it]

{'eval_loss': 2.0331830978393555, 'eval_runtime': 104.298, 'eval_samples_per_second': 4.545, 'eval_steps_per_second': 0.288, 'epoch': 3.05}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0003, 'learning_rate': 4.036799757500856e-05, 'epoch': 3.06}
 31%|████████████████████████████████▍                                                                         | 199/650 [1:29:22<6:54:05, 55.09s/it]

 31%|████████████████████████████████▌                                                                         | 200/650 [1:29:46<5:42:50, 45.71s/it]


 31%|████████████████████████████████▉                                                                         | 202/650 [1:30:32<4:16:43, 34.38s/it]

 31%|█████████████████████████████████                                                                         | 203/650 [1:30:56<3:52:43, 31.24s/it]
{'loss': 2.1684, 'learning_rate': 3.997971923329426e-05, 'epoch': 3.12}

 31%|█████████████████████████████████▎                                                                        | 204/650 [1:31:19<3:33:35, 28.73s/it]

 32%|█████████████████████████████████▍                                                                        | 205/650 [1:31:43<3:22:38, 27.32s/it]

 32%|█████████████████████████████████▌                                                                        | 206/650 [1:32:07<3:14:47, 26.32s/it]

 32%|█████████████████████████████████▊                                                                        | 207/650 [1:32:31<3:09:03, 25.61s/it]

 32%|█████████████████████████████████▉                                                                        | 208/650 [1:32:55<3:05:41, 25.21s/it]

 32%|██████████████████████████████████                                                                        | 209/650 [1:33:19<3:02:13, 24.79s/it]

 32%|██████████████████████████████████▏                                                                       | 210/650 [1:33:43<2:59:17, 24.45s/it]

 32%|██████████████████████████████████▍                                                                       | 211/650 [1:34:07<2:57:27, 24.25s/it]

 33%|██████████████████████████████████▌                                                                       | 212/650 [1:34:31<2:56:52, 24.23s/it]

 33%|██████████████████████████████████▋                                                                       | 213/650 [1:34:55<2:56:23, 24.22s/it]


 33%|███████████████████████████████████                                                                       | 215/650 [1:35:43<2:53:54, 23.99s/it]

 33%|███████████████████████████████████▏                                                                      | 216/650 [1:36:07<2:53:24, 23.97s/it]
{'loss': 2.0253, 'learning_rate': 3.867908541966594e-05, 'epoch': 3.32}

 33%|███████████████████████████████████▍                                                                      | 217/650 [1:36:30<2:52:16, 23.87s/it]

 34%|███████████████████████████████████▌                                                                      | 218/650 [1:36:54<2:51:59, 23.89s/it]

 34%|███████████████████████████████████▋                                                                      | 219/650 [1:37:18<2:51:56, 23.94s/it]

 34%|███████████████████████████████████▉                                                                      | 220/650 [1:37:42<2:51:41, 23.96s/it]

 34%|████████████████████████████████████                                                                      | 221/650 [1:38:06<2:51:11, 23.94s/it]

 34%|████████████████████████████████████▏                                                                     | 222/650 [1:38:30<2:49:59, 23.83s/it]

 34%|████████████████████████████████████▎                                                                     | 223/650 [1:38:54<2:49:36, 23.83s/it]


 35%|████████████████████████████████████▋                                                                     | 225/650 [1:39:41<2:47:44, 23.68s/it]
{'loss': 2.0497, 'learning_rate': 3.7746005769000363e-05, 'epoch': 3.46}

 35%|████████████████████████████████████▊                                                                     | 226/650 [1:40:04<2:47:16, 23.67s/it]

 35%|█████████████████████████████████████                                                                     | 227/650 [1:40:28<2:48:08, 23.85s/it]


 35%|█████████████████████████████████████▎                                                                    | 229/650 [1:41:17<2:48:23, 24.00s/it]
{'loss': 2.1531, 'learning_rate': 3.7323285090222054e-05, 'epoch': 3.52}

 35%|█████████████████████████████████████▌                                                                    | 230/650 [1:41:40<2:47:18, 23.90s/it]

 36%|█████████████████████████████████████▋                                                                    | 231/650 [1:42:04<2:47:02, 23.92s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.50s/it]

{'eval_loss': 2.0329222679138184, 'eval_runtime': 103.1542, 'eval_samples_per_second': 4.595, 'eval_steps_per_second': 0.291, 'epoch': 3.55}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8307, 'learning_rate': 3.700314695492876e-05, 'epoch': 3.57}
 36%|█████████████████████████████████████▊                                                                    | 232/650 [1:44:12<6:23:31, 55.05s/it]

 36%|█████████████████████████████████████▉                                                                    | 233/650 [1:44:36<5:17:45, 45.72s/it]

 36%|██████████████████████████████████████▏                                                                   | 234/650 [1:45:00<4:32:20, 39.28s/it]

 36%|██████████████████████████████████████▎                                                                   | 235/650 [1:45:24<4:00:13, 34.73s/it]


 36%|██████████████████████████████████████▋                                                                   | 237/650 [1:46:11<3:19:09, 28.93s/it]
{'loss': 1.9571, 'learning_rate': 3.646388707716738e-05, 'epoch': 3.65}

 37%|██████████████████████████████████████▊                                                                   | 238/650 [1:46:35<3:07:31, 27.31s/it]

 37%|██████████████████████████████████████▉                                                                   | 239/650 [1:46:58<3:00:01, 26.28s/it]

 37%|███████████████████████████████████████▏                                                                  | 240/650 [1:47:22<2:54:48, 25.58s/it]

 37%|███████████████████████████████████████▎                                                                  | 241/650 [1:47:46<2:51:00, 25.09s/it]

 37%|███████████████████████████████████████▍                                                                  | 242/650 [1:48:09<2:44:52, 24.25s/it]

 37%|███████████████████████████████████████▋                                                                  | 243/650 [1:48:32<2:42:01, 23.88s/it]

 38%|███████████████████████████████████████▊                                                                  | 244/650 [1:48:56<2:42:21, 23.99s/it]

 38%|███████████████████████████████████████▉                                                                  | 245/650 [1:49:20<2:42:00, 24.00s/it]

 38%|████████████████████████████████████████                                                                  | 246/650 [1:49:44<2:41:57, 24.05s/it]

 38%|████████████████████████████████████████▎                                                                 | 247/650 [1:50:08<2:40:53, 23.95s/it]

 38%|████████████████████████████████████████▍                                                                 | 248/650 [1:50:32<2:41:14, 24.07s/it]

 38%|████████████████████████████████████████▌                                                                 | 249/650 [1:50:55<2:38:49, 23.77s/it]

 38%|████████████████████████████████████████▊                                                                 | 250/650 [1:51:19<2:38:40, 23.80s/it]

 39%|████████████████████████████████████████▉                                                                 | 251/650 [1:51:42<2:37:21, 23.66s/it]

 39%|█████████████████████████████████████████                                                                 | 252/650 [1:52:06<2:37:36, 23.76s/it]

 39%|█████████████████████████████████████████▎                                                                | 253/650 [1:52:30<2:37:45, 23.84s/it]

 39%|█████████████████████████████████████████▍                                                                | 254/650 [1:52:54<2:37:43, 23.90s/it]

 39%|█████████████████████████████████████████▌                                                                | 255/650 [1:53:18<2:37:33, 23.93s/it]

 39%|█████████████████████████████████████████▋                                                                | 256/650 [1:53:42<2:36:39, 23.86s/it]

 40%|█████████████████████████████████████████▉                                                                | 257/650 [1:54:06<2:36:23, 23.88s/it]

 40%|██████████████████████████████████████████                                                                | 258/650 [1:54:30<2:36:31, 23.96s/it]

 40%|██████████████████████████████████████████▏                                                               | 259/650 [1:54:54<2:36:36, 24.03s/it]

 40%|██████████████████████████████████████████▍                                                               | 260/650 [1:55:15<2:30:04, 23.09s/it]

 40%|██████████████████████████████████████████▌                                                               | 261/650 [1:55:39<2:31:24, 23.35s/it]

 40%|██████████████████████████████████████████▋                                                               | 262/650 [1:56:03<2:32:10, 23.53s/it]

 40%|██████████████████████████████████████████▉                                                               | 263/650 [1:56:27<2:32:33, 23.65s/it]

 41%|███████████████████████████████████████████                                                               | 264/650 [1:56:51<2:33:18, 23.83s/it]



























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.51s/it]

{'eval_loss': 2.0317959785461426, 'eval_runtime': 103.6347, 'eval_samples_per_second': 4.574, 'eval_steps_per_second': 0.289, 'epoch': 4.06}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0364, 'learning_rate': 3.332693851772331e-05, 'epoch': 4.08}
 41%|███████████████████████████████████████████▏                                                              | 265/650 [1:58:59<5:53:13, 55.05s/it]

 41%|███████████████████████████████████████████▍                                                              | 266/650 [1:59:23<4:52:40, 45.73s/it]

 41%|███████████████████████████████████████████▌                                                              | 267/650 [1:59:47<4:09:21, 39.07s/it]

 41%|███████████████████████████████████████████▋                                                              | 268/650 [2:00:11<3:39:44, 34.52s/it]


 42%|████████████████████████████████████████████                                                              | 270/650 [2:00:58<3:04:08, 29.07s/it]

 42%|████████████████████████████████████████████▏                                                             | 271/650 [2:01:22<2:53:15, 27.43s/it]
{'loss': 2.0657, 'learning_rate': 3.263243195632068e-05, 'epoch': 4.17}

 42%|████████████████████████████████████████████▎                                                             | 272/650 [2:01:46<2:45:56, 26.34s/it]

 42%|████████████████████████████████████████████▌                                                             | 273/650 [2:02:10<2:40:31, 25.55s/it]

 42%|████████████████████████████████████████████▋                                                             | 274/650 [2:02:33<2:36:29, 24.97s/it]

 42%|████████████████████████████████████████████▊                                                             | 275/650 [2:02:57<2:33:40, 24.59s/it]

 42%|█████████████████████████████████████████████                                                             | 276/650 [2:03:21<2:32:36, 24.48s/it]

 43%|█████████████████████████████████████████████▏                                                            | 277/650 [2:03:45<2:31:14, 24.33s/it]

 43%|█████████████████████████████████████████████▎                                                            | 278/650 [2:04:09<2:30:10, 24.22s/it]

 43%|█████████████████████████████████████████████▍                                                            | 279/650 [2:04:33<2:29:37, 24.20s/it]

 43%|█████████████████████████████████████████████▋                                                            | 280/650 [2:04:57<2:28:52, 24.14s/it]

 43%|█████████████████████████████████████████████▊                                                            | 281/650 [2:05:21<2:28:03, 24.08s/it]

 43%|█████████████████████████████████████████████▉                                                            | 282/650 [2:05:45<2:27:24, 24.03s/it]


 44%|██████████████████████████████████████████████▎                                                           | 284/650 [2:06:33<2:26:10, 23.96s/it]

 44%|██████████████████████████████████████████████▍                                                           | 285/650 [2:06:57<2:25:37, 23.94s/it]
{'loss': 2.097, 'learning_rate': 3.098745290791539e-05, 'epoch': 4.38}

 44%|██████████████████████████████████████████████▋                                                           | 286/650 [2:07:19<2:22:37, 23.51s/it]

 44%|██████████████████████████████████████████████▊                                                           | 287/650 [2:07:43<2:23:16, 23.68s/it]


 44%|███████████████████████████████████████████████▏                                                          | 289/650 [2:08:31<2:22:40, 23.71s/it]
{'loss': 2.1326, 'learning_rate': 3.051197542572203e-05, 'epoch': 4.45}

 45%|███████████████████████████████████████████████▎                                                          | 290/650 [2:08:54<2:21:54, 23.65s/it]

 45%|███████████████████████████████████████████████▍                                                          | 291/650 [2:09:18<2:21:28, 23.65s/it]

 45%|███████████████████████████████████████████████▌                                                          | 292/650 [2:09:42<2:21:38, 23.74s/it]

 45%|███████████████████████████████████████████████▊                                                          | 293/650 [2:10:06<2:21:59, 23.87s/it]

 45%|███████████████████████████████████████████████▉                                                          | 294/650 [2:10:30<2:22:19, 23.99s/it]

 45%|████████████████████████████████████████████████                                                          | 295/650 [2:10:54<2:22:12, 24.04s/it]

 46%|████████████████████████████████████████████████▎                                                         | 296/650 [2:11:19<2:22:09, 24.10s/it]

 46%|████████████████████████████████████████████████▍                                                         | 297/650 [2:11:43<2:21:54, 24.12s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.50s/it]

{'eval_loss': 2.032576322555542, 'eval_runtime': 103.2468, 'eval_samples_per_second': 4.591, 'eval_steps_per_second': 0.291, 'epoch': 4.57}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 46%|████████████████████████████████████████████████▌                                                         | 298/650 [2:13:51<5:24:28, 55.31s/it]
{'loss': 2.1111, 'learning_rate': 2.943473194186693e-05, 'epoch': 4.58}

 46%|████████████████████████████████████████████████▊                                                         | 299/650 [2:14:15<4:28:32, 45.91s/it]

 46%|████████████████████████████████████████████████▉                                                         | 300/650 [2:14:39<3:48:54, 39.24s/it]

 46%|█████████████████████████████████████████████████                                                         | 301/650 [2:15:02<3:21:05, 34.57s/it]

 46%|█████████████████████████████████████████████████▏                                                        | 302/650 [2:15:26<3:01:46, 31.34s/it]

 47%|█████████████████████████████████████████████████▍                                                        | 303/650 [2:15:50<2:48:44, 29.18s/it]

 47%|█████████████████████████████████████████████████▌                                                        | 304/650 [2:16:14<2:39:11, 27.61s/it]

 47%|█████████████████████████████████████████████████▋                                                        | 305/650 [2:16:37<2:30:30, 26.18s/it]

 47%|█████████████████████████████████████████████████▉                                                        | 306/650 [2:17:01<2:26:02, 25.47s/it]

 47%|██████████████████████████████████████████████████                                                        | 307/650 [2:17:25<2:22:53, 25.00s/it]

 47%|██████████████████████████████████████████████████▏                                                       | 308/650 [2:17:49<2:20:40, 24.68s/it]


 48%|██████████████████████████████████████████████████▌                                                       | 310/650 [2:18:36<2:16:28, 24.08s/it]

 48%|██████████████████████████████████████████████████▋                                                       | 311/650 [2:19:00<2:15:55, 24.06s/it]
{'loss': 1.9569, 'learning_rate': 2.7864126102502524e-05, 'epoch': 4.78}

 48%|██████████████████████████████████████████████████▉                                                       | 312/650 [2:19:24<2:15:47, 24.11s/it]

 48%|███████████████████████████████████████████████████                                                       | 313/650 [2:19:48<2:15:34, 24.14s/it]

 48%|███████████████████████████████████████████████████▏                                                      | 314/650 [2:20:12<2:14:55, 24.09s/it]

 48%|███████████████████████████████████████████████████▎                                                      | 315/650 [2:20:36<2:14:12, 24.04s/it]


 49%|███████████████████████████████████████████████████▋                                                      | 317/650 [2:21:24<2:13:01, 23.97s/it]
{'loss': 2.1227, 'learning_rate': 2.7134949802227073e-05, 'epoch': 4.88}


 49%|████████████████████████████████████████████████████                                                      | 319/650 [2:22:12<2:12:24, 24.00s/it]
{'loss': 2.0481, 'learning_rate': 2.689145235764035e-05, 'epoch': 4.91}

 49%|████████████████████████████████████████████████████▏                                                     | 320/650 [2:22:36<2:12:27, 24.08s/it]

 49%|████████████████████████████████████████████████████▎                                                     | 321/650 [2:23:00<2:12:07, 24.10s/it]


 50%|████████████████████████████████████████████████████▋                                                     | 323/650 [2:23:48<2:10:44, 23.99s/it]

 50%|████████████████████████████████████████████████████▊                                                     | 324/650 [2:24:12<2:10:25, 24.00s/it]
{'loss': 2.1333, 'learning_rate': 2.6281969501293734e-05, 'epoch': 4.98}

 50%|█████████████████████████████████████████████████████                                                     | 325/650 [2:24:33<2:05:06, 23.10s/it]


 50%|█████████████████████████████████████████████████████▎                                                    | 327/650 [2:25:20<2:05:37, 23.33s/it]

 50%|█████████████████████████████████████████████████████▍                                                    | 328/650 [2:25:44<2:06:12, 23.52s/it]
{'loss': 2.118, 'learning_rate': 2.579381492513347e-05, 'epoch': 5.05}

 51%|█████████████████████████████████████████████████████▋                                                    | 329/650 [2:26:08<2:05:59, 23.55s/it]

 51%|█████████████████████████████████████████████████████▊                                                    | 330/650 [2:26:31<2:05:32, 23.54s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:39<00:03,  3.74s/it]


 51%|█████████████████████████████████████████████████████▊                                                    | 330/650 [2:28:16<2:05:32, 23.54s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Data_Science_and_Machine_Learning_class-bs_16-lr_5e-05-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-330)... Done. 0.4s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9979, 'learning_rate': 2.5427489828901675e-05, 'epoch': 5.09}
 51%|█████████████████████████████████████████████████████▉                                                    | 331/650 [2:28:42<4:55:32, 55.59s/it]

 51%|██████████████████████████████████████████████████████▏                                                   | 332/650 [2:29:05<4:04:15, 46.09s/it]

 51%|██████████████████████████████████████████████████████▎                                                   | 333/650 [2:29:29<3:27:58, 39.36s/it]

 51%|██████████████████████████████████████████████████████▍                                                   | 334/650 [2:29:52<3:01:33, 34.47s/it]

 52%|██████████████████████████████████████████████████████▋                                                   | 335/650 [2:30:16<2:44:20, 31.30s/it]

 52%|██████████████████████████████████████████████████████▊                                                   | 336/650 [2:30:40<2:32:02, 29.05s/it]

 52%|██████████████████████████████████████████████████████▉                                                   | 337/650 [2:31:04<2:23:31, 27.51s/it]

 52%|███████████████████████████████████████████████████████                                                   | 338/650 [2:31:27<2:16:58, 26.34s/it]

 52%|███████████████████████████████████████████████████████▎                                                  | 339/650 [2:31:52<2:13:13, 25.70s/it]

 52%|███████████████████████████████████████████████████████▍                                                  | 340/650 [2:32:16<2:10:11, 25.20s/it]

 52%|███████████████████████████████████████████████████████▌                                                  | 341/650 [2:32:39<2:07:27, 24.75s/it]

 53%|███████████████████████████████████████████████████████▊                                                  | 342/650 [2:33:03<2:05:58, 24.54s/it]

 53%|███████████████████████████████████████████████████████▉                                                  | 343/650 [2:33:27<2:04:34, 24.35s/it]

 53%|████████████████████████████████████████████████████████                                                  | 344/650 [2:33:51<2:02:47, 24.08s/it]

 53%|████████████████████████████████████████████████████████▎                                                 | 345/650 [2:34:14<2:01:31, 23.91s/it]

 53%|████████████████████████████████████████████████████████▍                                                 | 346/650 [2:34:38<2:01:11, 23.92s/it]

 53%|████████████████████████████████████████████████████████▌                                                 | 347/650 [2:35:02<2:00:46, 23.92s/it]

 54%|████████████████████████████████████████████████████████▊                                                 | 348/650 [2:35:26<2:00:24, 23.92s/it]

 54%|████████████████████████████████████████████████████████▉                                                 | 349/650 [2:35:50<2:00:21, 23.99s/it]

 54%|█████████████████████████████████████████████████████████                                                 | 350/650 [2:36:15<2:00:23, 24.08s/it]

 54%|█████████████████████████████████████████████████████████▏                                                | 351/650 [2:36:39<1:59:51, 24.05s/it]


 54%|█████████████████████████████████████████████████████████▌                                                | 353/650 [2:37:27<1:59:30, 24.14s/it]

 54%|█████████████████████████████████████████████████████████▋                                                | 354/650 [2:37:51<1:58:56, 24.11s/it]
{'loss': 1.8654, 'learning_rate': 2.2621756608484445e-05, 'epoch': 5.45}

 55%|█████████████████████████████████████████████████████████▉                                                | 355/650 [2:38:15<1:57:54, 23.98s/it]

 55%|██████████████████████████████████████████████████████████                                                | 356/650 [2:38:38<1:56:08, 23.70s/it]

 55%|██████████████████████████████████████████████████████████▏                                               | 357/650 [2:39:01<1:54:36, 23.47s/it]

 55%|██████████████████████████████████████████████████████████▍                                               | 358/650 [2:39:24<1:54:08, 23.45s/it]

 55%|██████████████████████████████████████████████████████████▌                                               | 359/650 [2:39:48<1:54:30, 23.61s/it]

 55%|██████████████████████████████████████████████████████████▋                                               | 360/650 [2:40:12<1:54:03, 23.60s/it]

 56%|██████████████████████████████████████████████████████████▊                                               | 361/650 [2:40:35<1:52:53, 23.44s/it]

 56%|███████████████████████████████████████████████████████████                                               | 362/650 [2:40:59<1:53:11, 23.58s/it]

 56%|███████████████████████████████████████████████████████████▏                                              | 363/650 [2:41:23<1:53:33, 23.74s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:41<00:03,  3.50s/it]

{'eval_loss': 2.033623695373535, 'eval_runtime': 107.2861, 'eval_samples_per_second': 4.418, 'eval_steps_per_second': 0.28, 'epoch': 5.58}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9556, 'learning_rate': 2.140915876564113e-05, 'epoch': 5.6}

 56%|███████████████████████████████████████████████████████████▌                                              | 365/650 [2:43:59<3:41:15, 46.58s/it]
{'loss': 2.112, 'learning_rate': 2.1288322741176707e-05, 'epoch': 5.62}


 56%|███████████████████████████████████████████████████████████▊                                              | 367/650 [2:44:47<2:45:34, 35.10s/it]
{'loss': 2.0026, 'learning_rate': 2.1046919382864888e-05, 'epoch': 5.65}

 57%|████████████████████████████████████████████████████████████                                              | 368/650 [2:45:10<2:28:44, 31.65s/it]


 57%|████████████████████████████████████████████████████████████▎                                             | 370/650 [2:45:59<2:10:23, 27.94s/it]

 57%|████████████████████████████████████████████████████████████▌                                             | 371/650 [2:46:23<2:04:21, 26.74s/it]

 57%|████████████████████████████████████████████████████████████▋                                             | 372/650 [2:46:47<2:00:26, 26.00s/it]
{'loss': 2.1296, 'learning_rate': 2.044511270446935e-05, 'epoch': 5.72}

 57%|████████████████████████████████████████████████████████████▊                                             | 373/650 [2:47:13<1:59:16, 25.84s/it]

 58%|████████████████████████████████████████████████████████████▉                                             | 374/650 [2:47:37<1:56:15, 25.27s/it]

 58%|█████████████████████████████████████████████████████████████▏                                            | 375/650 [2:48:00<1:53:29, 24.76s/it]

 58%|█████████████████████████████████████████████████████████████▎                                            | 376/650 [2:48:24<1:51:56, 24.51s/it]

 58%|█████████████████████████████████████████████████████████████▍                                            | 377/650 [2:48:48<1:50:43, 24.34s/it]

 58%|█████████████████████████████████████████████████████████████▋                                            | 378/650 [2:49:12<1:49:50, 24.23s/it]


 58%|█████████████████████████████████████████████████████████████▉                                            | 380/650 [2:50:00<1:48:02, 24.01s/it]

 59%|██████████████████████████████████████████████████████████████▏                                           | 381/650 [2:50:24<1:47:35, 24.00s/it]
{'loss': 2.1068, 'learning_rate': 1.9368950734203544e-05, 'epoch': 5.86}

 59%|██████████████████████████████████████████████████████████████▎                                           | 382/650 [2:50:49<1:48:09, 24.22s/it]


 59%|██████████████████████████████████████████████████████████████▌                                           | 384/650 [2:51:37<1:47:54, 24.34s/it]

 59%|██████████████████████████████████████████████████████████████▊                                           | 385/650 [2:52:01<1:47:04, 24.24s/it]

 59%|██████████████████████████████████████████████████████████████▉                                           | 386/650 [2:52:24<1:44:08, 23.67s/it]

 60%|███████████████████████████████████████████████████████████████                                           | 387/650 [2:52:47<1:43:43, 23.66s/it]
{'loss': 1.9406, 'learning_rate': 1.8657429786617416e-05, 'epoch': 5.95}

 60%|███████████████████████████████████████████████████████████████▎                                          | 388/650 [2:53:11<1:43:39, 23.74s/it]

 60%|███████████████████████████████████████████████████████████████▍                                          | 389/650 [2:53:34<1:42:10, 23.49s/it]

 60%|███████████████████████████████████████████████████████████████▌                                          | 390/650 [2:53:55<1:38:25, 22.71s/it]

 60%|███████████████████████████████████████████████████████████████▊                                          | 391/650 [2:54:19<1:39:23, 23.02s/it]

 60%|███████████████████████████████████████████████████████████████▉                                          | 392/650 [2:54:43<1:40:08, 23.29s/it]

 60%|████████████████████████████████████████████████████████████████                                          | 393/650 [2:55:07<1:40:21, 23.43s/it]


 61%|████████████████████████████████████████████████████████████████▍                                         | 395/650 [2:55:54<1:40:26, 23.63s/it]

 61%|████████████████████████████████████████████████████████████████▌                                         | 396/650 [2:56:18<1:39:50, 23.58s/it]
{'loss': 2.028, 'learning_rate': 1.7600557339504142e-05, 'epoch': 6.09}




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.49s/it]

{'eval_loss': 2.0331733226776123, 'eval_runtime': 102.9793, 'eval_samples_per_second': 4.603, 'eval_steps_per_second': 0.291, 'epoch': 6.09}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0825, 'learning_rate': 1.7483972982745217e-05, 'epoch': 6.11}
 61%|████████████████████████████████████████████████████████████████▋                                         | 397/650 [2:58:25<3:50:12, 54.59s/it]


 61%|█████████████████████████████████████████████████████████████████                                         | 399/650 [2:59:13<2:42:57, 38.96s/it]

 62%|█████████████████████████████████████████████████████████████████▏                                        | 400/650 [2:59:36<2:23:05, 34.34s/it]
{'loss': 2.0661, 'learning_rate': 1.7135307529248348e-05, 'epoch': 6.15}

 62%|█████████████████████████████████████████████████████████████████▍                                        | 401/650 [3:00:00<2:09:06, 31.11s/it]

 62%|█████████████████████████████████████████████████████████████████▌                                        | 402/650 [3:00:23<1:59:31, 28.92s/it]

 62%|█████████████████████████████████████████████████████████████████▋                                        | 403/650 [3:00:47<1:52:38, 27.36s/it]


 62%|██████████████████████████████████████████████████████████████████                                        | 405/650 [3:01:34<1:43:47, 25.42s/it]

 62%|██████████████████████████████████████████████████████████████████▏                                       | 406/650 [3:01:58<1:41:43, 25.01s/it]
{'loss': 2.026, 'learning_rate': 1.6443120087216767e-05, 'epoch': 6.25}

 63%|██████████████████████████████████████████████████████████████████▎                                       | 407/650 [3:02:22<1:39:15, 24.51s/it]

 63%|██████████████████████████████████████████████████████████████████▌                                       | 408/650 [3:02:46<1:37:57, 24.29s/it]


 63%|██████████████████████████████████████████████████████████████████▊                                       | 410/650 [3:03:33<1:35:35, 23.90s/it]

 63%|███████████████████████████████████████████████████████████████████                                       | 411/650 [3:03:57<1:35:29, 23.97s/it]

 63%|███████████████████████████████████████████████████████████████████▏                                      | 412/650 [3:04:21<1:34:53, 23.92s/it]
{'loss': 1.9456, 'learning_rate': 1.5758285646363778e-05, 'epoch': 6.34}

 64%|███████████████████████████████████████████████████████████████████▎                                      | 413/650 [3:04:44<1:33:51, 23.76s/it]

 64%|███████████████████████████████████████████████████████████████████▌                                      | 414/650 [3:05:08<1:33:17, 23.72s/it]

 64%|███████████████████████████████████████████████████████████████████▋                                      | 415/650 [3:05:32<1:33:15, 23.81s/it]

 64%|███████████████████████████████████████████████████████████████████▊                                      | 416/650 [3:05:56<1:33:02, 23.86s/it]

 64%|████████████████████████████████████████████████████████████████████                                      | 417/650 [3:06:20<1:32:51, 23.91s/it]

 64%|████████████████████████████████████████████████████████████████████▏                                     | 418/650 [3:06:43<1:32:20, 23.88s/it]

 64%|████████████████████████████████████████████████████████████████████▎                                     | 419/650 [3:07:07<1:31:41, 23.82s/it]


 65%|████████████████████████████████████████████████████████████████████▋                                     | 421/650 [3:07:55<1:30:55, 23.82s/it]

 65%|████████████████████████████████████████████████████████████████████▊                                     | 422/650 [3:08:19<1:30:23, 23.79s/it]
{'loss': 1.9453, 'learning_rate': 1.4634830192306032e-05, 'epoch': 6.49}

 65%|████████████████████████████████████████████████████████████████████▉                                     | 423/650 [3:08:42<1:29:51, 23.75s/it]

 65%|█████████████████████████████████████████████████████████████████████▏                                    | 424/650 [3:09:06<1:29:49, 23.85s/it]

 65%|█████████████████████████████████████████████████████████████████████▎                                    | 425/650 [3:09:30<1:29:23, 23.84s/it]


 66%|█████████████████████████████████████████████████████████████████████▋                                    | 427/650 [3:10:17<1:27:49, 23.63s/it]

 66%|█████████████████████████████████████████████████████████████████████▊                                    | 428/650 [3:10:41<1:27:36, 23.68s/it]
{'loss': 2.0605, 'learning_rate': 1.3972462027085042e-05, 'epoch': 6.58}

 66%|█████████████████████████████████████████████████████████████████████▉                                    | 429/650 [3:11:04<1:26:46, 23.56s/it]




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:36<00:03,  3.47s/it]

{'eval_loss': 2.0338454246520996, 'eval_runtime': 102.4004, 'eval_samples_per_second': 4.629, 'eval_steps_per_second': 0.293, 'epoch': 6.6}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0701, 'learning_rate': 1.3753750551905003e-05, 'epoch': 6.62}
 66%|██████████████████████████████████████████████████████████████████████                                    | 430/650 [3:13:10<3:19:22, 54.38s/it]


 66%|██████████████████████████████████████████████████████████████████████▍                                   | 432/650 [3:13:58<2:20:25, 38.65s/it]

 67%|██████████████████████████████████████████████████████████████████████▌                                   | 433/650 [3:14:22<2:03:58, 34.28s/it]
{'loss': 1.9638, 'learning_rate': 1.3427703298737048e-05, 'epoch': 6.66}

 67%|██████████████████████████████████████████████████████████████████████▊                                   | 434/650 [3:14:44<1:51:00, 30.83s/it]

 67%|██████████████████████████████████████████████████████████████████████▉                                   | 435/650 [3:15:08<1:42:50, 28.70s/it]

 67%|███████████████████████████████████████████████████████████████████████                                   | 436/650 [3:15:32<1:37:15, 27.27s/it]

 67%|███████████████████████████████████████████████████████████████████████▎                                  | 437/650 [3:15:56<1:33:04, 26.22s/it]


 68%|███████████████████████████████████████████████████████████████████████▌                                  | 439/650 [3:16:42<1:26:18, 24.54s/it]

 68%|███████████████████████████████████████████████████████████████████████▊                                  | 440/650 [3:17:06<1:25:04, 24.31s/it]

 68%|███████████████████████████████████████████████████████████████████████▉                                  | 441/650 [3:17:30<1:24:23, 24.23s/it]

 68%|████████████████████████████████████████████████████████████████████████                                  | 442/650 [3:17:54<1:23:41, 24.14s/it]
{'loss': 2.0764, 'learning_rate': 1.2464756107633525e-05, 'epoch': 6.8}


 68%|████████████████████████████████████████████████████████████████████████▍                                 | 444/650 [3:18:40<1:21:04, 23.62s/it]

 68%|████████████████████████████████████████████████████████████████████████▌                                 | 445/650 [3:19:04<1:20:49, 23.66s/it]
{'loss': 1.9438, 'learning_rate': 1.2149068433191136e-05, 'epoch': 6.85}

 69%|████████████████████████████████████████████████████████████████████████▋                                 | 446/650 [3:19:27<1:20:31, 23.68s/it]


 69%|█████████████████████████████████████████████████████████████████████████                                 | 448/650 [3:20:14<1:19:26, 23.60s/it]

 69%|█████████████████████████████████████████████████████████████████████████▏                                | 449/650 [3:20:38<1:19:15, 23.66s/it]

 69%|█████████████████████████████████████████████████████████████████████████▍                                | 450/650 [3:21:02<1:19:00, 23.70s/it]
{'loss': 2.0182, 'learning_rate': 1.1629090816800916e-05, 'epoch': 6.92}


 70%|█████████████████████████████████████████████████████████████████████████▋                                | 452/650 [3:21:48<1:17:06, 23.37s/it]

 70%|█████████████████████████████████████████████████████████████████████████▊                                | 453/650 [3:22:12<1:17:17, 23.54s/it]

 70%|██████████████████████████████████████████████████████████████████████████                                | 454/650 [3:22:36<1:17:09, 23.62s/it]
{'loss': 2.0504, 'learning_rate': 1.1218838982701036e-05, 'epoch': 6.98}


 70%|██████████████████████████████████████████████████████████████████████████▎                               | 456/650 [3:23:21<1:14:32, 23.05s/it]

 70%|██████████████████████████████████████████████████████████████████████████▌                               | 457/650 [3:23:44<1:14:49, 23.26s/it]

 70%|██████████████████████████████████████████████████████████████████████████▋                               | 458/650 [3:24:08<1:15:10, 23.49s/it]

 71%|██████████████████████████████████████████████████████████████████████████▊                               | 459/650 [3:24:32<1:15:18, 23.66s/it]

 71%|███████████████████████████████████████████████████████████████████████████                               | 460/650 [3:24:56<1:15:00, 23.69s/it]
{'loss': 1.9914, 'learning_rate': 1.0613378600384612e-05, 'epoch': 7.08}


 71%|███████████████████████████████████████████████████████████████████████████▎                              | 462/650 [3:25:43<1:13:37, 23.50s/it]
{'loss': 1.9375, 'learning_rate': 1.0414280315041563e-05, 'epoch': 7.11}




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:36<00:03,  3.47s/it]

{'eval_loss': 2.034379720687866, 'eval_runtime': 102.5334, 'eval_samples_per_second': 4.623, 'eval_steps_per_second': 0.293, 'epoch': 7.11}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9453, 'learning_rate': 1.0315252257423185e-05, 'epoch': 7.12}
 71%|███████████████████████████████████████████████████████████████████████████▌                              | 463/650 [3:27:50<2:49:51, 54.50s/it]

 71%|███████████████████████████████████████████████████████████████████████████▋                              | 464/650 [3:28:13<2:20:24, 45.29s/it]


 72%|███████████████████████████████████████████████████████████████████████████▉                              | 466/650 [3:29:01<1:45:18, 34.34s/it]

 72%|████████████████████████████████████████████████████████████████████████████▏                             | 467/650 [3:29:25<1:35:07, 31.19s/it]

 72%|████████████████████████████████████████████████████████████████████████████▎                             | 468/650 [3:29:49<1:28:03, 29.03s/it]

 72%|████████████████████████████████████████████████████████████████████████████▍                             | 469/650 [3:30:13<1:22:44, 27.43s/it]
{'loss': 1.9928, 'learning_rate': 9.728527499526524e-06, 'epoch': 7.22}

 72%|████████████████████████████████████████████████████████████████████████████▋                             | 470/650 [3:30:36<1:18:53, 26.30s/it]


 73%|████████████████████████████████████████████████████████████████████████████▉                             | 472/650 [3:31:23<1:13:45, 24.86s/it]
{'loss': 1.9405, 'learning_rate': 9.440055135780706e-06, 'epoch': 7.26}

 73%|█████████████████████████████████████████████████████████████████████████████▏                            | 473/650 [3:31:46<1:11:33, 24.26s/it]

 73%|█████████████████████████████████████████████████████████████████████████████▎                            | 474/650 [3:32:10<1:10:46, 24.13s/it]

 73%|█████████████████████████████████████████████████████████████████████████████▍                            | 475/650 [3:32:34<1:10:17, 24.10s/it]


 73%|█████████████████████████████████████████████████████████████████████████████▊                            | 477/650 [3:33:21<1:08:58, 23.92s/it]

 74%|█████████████████████████████████████████████████████████████████████████████▉                            | 478/650 [3:33:45<1:07:59, 23.72s/it]
{'loss': 1.9976, 'learning_rate': 8.873200300530237e-06, 'epoch': 7.35}


 74%|██████████████████████████████████████████████████████████████████████████████▎                           | 480/650 [3:34:32<1:06:55, 23.62s/it]

 74%|██████████████████████████████████████████████████████████████████████████████▍                           | 481/650 [3:34:55<1:06:34, 23.64s/it]

 74%|██████████████████████████████████████████████████████████████████████████████▌                           | 482/650 [3:35:19<1:06:18, 23.68s/it]

 74%|██████████████████████████████████████████████████████████████████████████████▊                           | 483/650 [3:35:43<1:06:00, 23.72s/it]
{'loss': 1.9448, 'learning_rate': 8.411386849443844e-06, 'epoch': 7.43}

 74%|██████████████████████████████████████████████████████████████████████████████▉                           | 484/650 [3:36:06<1:05:13, 23.57s/it]

 75%|███████████████████████████████████████████████████████████████████████████████                           | 485/650 [3:36:30<1:05:12, 23.71s/it]


 75%|███████████████████████████████████████████████████████████████████████████████▍                          | 487/650 [3:37:17<1:04:16, 23.66s/it]

 75%|███████████████████████████████████████████████████████████████████████████████▌                          | 488/650 [3:37:41<1:03:53, 23.66s/it]
{'loss': 2.0947, 'learning_rate': 7.959472739463242e-06, 'epoch': 7.51}

 75%|███████████████████████████████████████████████████████████████████████████████▋                          | 489/650 [3:38:05<1:03:31, 23.68s/it]

 75%|███████████████████████████████████████████████████████████████████████████████▉                          | 490/650 [3:38:29<1:03:11, 23.70s/it]


 76%|████████████████████████████████████████████████████████████████████████████████▏                         | 492/650 [3:39:15<1:01:47, 23.47s/it]

 76%|████████████████████████████████████████████████████████████████████████████████▍                         | 493/650 [3:39:39<1:01:51, 23.64s/it]

 76%|████████████████████████████████████████████████████████████████████████████████▌                         | 494/650 [3:40:03<1:01:33, 23.68s/it]

 76%|████████████████████████████████████████████████████████████████████████████████▋                         | 495/650 [3:40:27<1:01:23, 23.77s/it]
{'loss': 2.0845, 'learning_rate': 7.343936201444495e-06, 'epoch': 7.62}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.48s/it]
{'eval_loss': 2.035114288330078, 'eval_runtime': 102.8201, 'eval_samples_per_second': 4.61, 'eval_steps_per_second': 0.292, 'epoch': 7.62}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 76%|████████████████████████████████████████████████████████████████████████████████▉                         | 496/650 [3:42:34<2:20:27, 54.72s/it]

 76%|█████████████████████████████████████████████████████████████████████████████████                         | 497/650 [3:42:58<1:55:52, 45.44s/it]

 77%|█████████████████████████████████████████████████████████████████████████████████▏                        | 498/650 [3:43:22<1:38:42, 38.96s/it]

 77%|█████████████████████████████████████████████████████████████████████████████████▍                        | 499/650 [3:43:45<1:26:36, 34.42s/it]
{'loss': 2.1005, 'learning_rate': 7.001427390675552e-06, 'epoch': 7.68}


 77%|█████████████████████████████████████████████████████████████████████████████████▋                        | 501/650 [3:44:32<1:11:18, 28.72s/it]

 77%|█████████████████████████████████████████████████████████████████████████████████▊                        | 502/650 [3:44:56<1:07:20, 27.30s/it]
{'loss': 1.9576, 'learning_rate': 6.749049919228473e-06, 'epoch': 7.72}

 77%|██████████████████████████████████████████████████████████████████████████████████                        | 503/650 [3:45:19<1:03:57, 26.11s/it]

 78%|██████████████████████████████████████████████████████████████████████████████████▏                       | 504/650 [3:45:43<1:01:41, 25.36s/it]

 78%|██████████████████████████████████████████████████████████████████████████████████▎                       | 505/650 [3:46:07<1:00:08, 24.89s/it]

 78%|████████████████████████████████████████████████████████████████████████████████████                        | 506/650 [3:46:31<59:05, 24.62s/it]

 78%|████████████████████████████████████████████████████████████████████████████████████▏                       | 507/650 [3:46:53<57:17, 24.04s/it]

 78%|████████████████████████████████████████████████████████████████████████████████████▍                       | 508/650 [3:47:17<56:42, 23.96s/it]


 78%|████████████████████████████████████████████████████████████████████████████████████▋                       | 510/650 [3:48:05<55:38, 23.85s/it]

 79%|████████████████████████████████████████████████████████████████████████████████████▉                       | 511/650 [3:48:28<55:12, 23.83s/it]

 79%|█████████████████████████████████████████████████████████████████████████████████████                       | 512/650 [3:48:52<54:44, 23.80s/it]

 79%|█████████████████████████████████████████████████████████████████████████████████████▏                      | 513/650 [3:49:16<54:26, 23.84s/it]
{'loss': 2.081, 'learning_rate': 5.857615507343464e-06, 'epoch': 7.89}

 79%|█████████████████████████████████████████████████████████████████████████████████████▍                      | 514/650 [3:49:39<53:40, 23.68s/it]


 79%|█████████████████████████████████████████████████████████████████████████████████████▋                      | 516/650 [3:50:26<52:28, 23.50s/it]
{'loss': 1.9684, 'learning_rate': 5.623986234956935e-06, 'epoch': 7.94}

 80%|█████████████████████████████████████████████████████████████████████████████████████▉                      | 517/650 [3:50:49<52:01, 23.47s/it]


 80%|██████████████████████████████████████████████████████████████████████████████████████▏                     | 519/650 [3:51:36<51:03, 23.39s/it]
{'loss': 2.1144, 'learning_rate': 5.3945196803492995e-06, 'epoch': 7.98}


 80%|██████████████████████████████████████████████████████████████████████████████████████▌                     | 521/650 [3:52:21<49:23, 22.97s/it]

 80%|██████████████████████████████████████████████████████████████████████████████████████▋                     | 522/650 [3:52:44<49:14, 23.09s/it]
{'loss': 2.0693, 'learning_rate': 5.169265141818852e-06, 'epoch': 8.03}

 80%|██████████████████████████████████████████████████████████████████████████████████████▉                     | 523/650 [3:53:08<49:11, 23.24s/it]

 81%|███████████████████████████████████████████████████████████████████████████████████████                     | 524/650 [3:53:32<49:18, 23.48s/it]


 81%|███████████████████████████████████████████████████████████████████████████████████████▍                    | 526/650 [3:54:19<48:34, 23.50s/it]

 81%|███████████████████████████████████████████████████████████████████████████████████████▌                    | 527/650 [3:54:43<48:28, 23.65s/it]

 81%|███████████████████████████████████████████████████████████████████████████████████████▋                    | 528/650 [3:55:06<47:51, 23.54s/it]
{'loss': 2.0857, 'learning_rate': 4.731584771265529e-06, 'epoch': 8.12}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:36<00:03,  3.47s/it]
{'eval_loss': 2.0351779460906982, 'eval_runtime': 102.512, 'eval_samples_per_second': 4.624, 'eval_steps_per_second': 0.293, 'epoch': 8.12}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 81%|██████████████████████████████████████████████████████████████████████████████████████▎                   | 529/650 [3:57:13<1:49:58, 54.53s/it]

 82%|██████████████████████████████████████████████████████████████████████████████████████▍                   | 530/650 [3:57:37<1:30:41, 45.34s/it]

 82%|██████████████████████████████████████████████████████████████████████████████████████▌                   | 531/650 [3:58:01<1:17:13, 38.94s/it]
{'loss': 2.0528, 'learning_rate': 4.519252969928306e-06, 'epoch': 8.17}

 82%|██████████████████████████████████████████████████████████████████████████████████████▊                   | 532/650 [3:58:24<1:07:09, 34.15s/it]

 82%|██████████████████████████████████████████████████████████████████████████████████████▉                   | 533/650 [3:58:48<1:00:30, 31.03s/it]

 82%|████████████████████████████████████████████████████████████████████████████████████████▋                   | 534/650 [3:59:12<55:54, 28.92s/it]


 82%|█████████████████████████████████████████████████████████████████████████████████████████                   | 536/650 [3:59:59<49:59, 26.31s/it]

 83%|█████████████████████████████████████████████████████████████████████████████████████████▏                  | 537/650 [4:00:23<48:06, 25.55s/it]
{'loss': 2.092, 'learning_rate': 4.107834210782638e-06, 'epoch': 8.26}

 83%|█████████████████████████████████████████████████████████████████████████████████████████▍                  | 538/650 [4:00:46<46:07, 24.71s/it]


 83%|█████████████████████████████████████████████████████████████████████████████████████████▋                  | 540/650 [4:01:33<44:27, 24.25s/it]

 83%|█████████████████████████████████████████████████████████████████████████████████████████▉                  | 541/650 [4:01:57<43:43, 24.07s/it]

 83%|██████████████████████████████████████████████████████████████████████████████████████████                  | 542/650 [4:02:21<43:07, 23.95s/it]
{'loss': 2.0662, 'learning_rate': 3.7786846623156515e-06, 'epoch': 8.34}


 84%|██████████████████████████████████████████████████████████████████████████████████████████▍                 | 544/650 [4:03:07<41:50, 23.68s/it]

 84%|██████████████████████████████████████████████████████████████████████████████████████████▌                 | 545/650 [4:03:31<41:19, 23.62s/it]
{'loss': 1.8686, 'learning_rate': 3.587261361796171e-06, 'epoch': 8.38}


 84%|██████████████████████████████████████████████████████████████████████████████████████████▉                 | 547/650 [4:04:18<40:21, 23.51s/it]

 84%|███████████████████████████████████████████████████████████████████████████████████████████                 | 548/650 [4:04:42<40:13, 23.67s/it]

 84%|███████████████████████████████████████████████████████████████████████████████████████████▏                | 549/650 [4:05:05<39:51, 23.67s/it]

 85%|███████████████████████████████████████████████████████████████████████████████████████████▍                | 550/650 [4:05:29<39:38, 23.78s/it]

 85%|███████████████████████████████████████████████████████████████████████████████████████████▌                | 551/650 [4:05:53<39:13, 23.77s/it]

 85%|███████████████████████████████████████████████████████████████████████████████████████████▋                | 552/650 [4:06:17<38:48, 23.76s/it]

 85%|███████████████████████████████████████████████████████████████████████████████████████████▉                | 553/650 [4:06:41<38:22, 23.74s/it]
{'loss': 1.9965, 'learning_rate': 3.099398072427584e-06, 'epoch': 8.51}

 85%|████████████████████████████████████████████████████████████████████████████████████████████                | 554/650 [4:07:05<38:06, 23.82s/it]

 85%|████████████████████████████████████████████████████████████████████████████████████████████▏               | 555/650 [4:07:28<37:41, 23.81s/it]

 86%|████████████████████████████████████████████████████████████████████████████████████████████▍               | 556/650 [4:07:52<37:23, 23.87s/it]


 86%|████████████████████████████████████████████████████████████████████████████████████████████▋               | 558/650 [4:08:40<36:21, 23.71s/it]

 86%|████████████████████████████████████████████████████████████████████████████████████████████▉               | 559/650 [4:09:03<36:00, 23.74s/it]
{'loss': 2.0647, 'learning_rate': 2.7554164781180863e-06, 'epoch': 8.6}


 86%|█████████████████████████████████████████████████████████████████████████████████████████████▏              | 561/650 [4:09:50<34:51, 23.50s/it]
{'loss': 2.0269, 'learning_rate': 2.644989590336172e-06, 'epoch': 8.63}




























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:36<00:03,  3.47s/it]

{'eval_loss': 2.035762310028076, 'eval_runtime': 102.4989, 'eval_samples_per_second': 4.624, 'eval_steps_per_second': 0.293, 'epoch': 8.63}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8086, 'learning_rate': 2.5905759569464478e-06, 'epoch': 8.65}

 87%|███████████████████████████████████████████████████████████████████████████████████████████▊              | 563/650 [4:12:20<1:05:11, 44.95s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████████████████▋              | 564/650 [4:12:44<55:26, 38.68s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████████████████▉              | 565/650 [4:13:07<48:25, 34.19s/it]
{'loss': 2.0417, 'learning_rate': 2.4305498475010792e-06, 'epoch': 8.69}

 87%|██████████████████████████████████████████████████████████████████████████████████████████████              | 566/650 [4:13:31<43:17, 30.92s/it]


 87%|██████████████████████████████████████████████████████████████████████████████████████████████▍             | 568/650 [4:14:18<37:19, 27.31s/it]

 88%|██████████████████████████████████████████████████████████████████████████████████████████████▌             | 569/650 [4:14:42<35:23, 26.22s/it]

 88%|██████████████████████████████████████████████████████████████████████████████████████████████▋             | 570/650 [4:15:06<34:03, 25.55s/it]

 88%|██████████████████████████████████████████████████████████████████████████████████████████████▊             | 571/650 [4:15:30<32:56, 25.01s/it]

 88%|███████████████████████████████████████████████████████████████████████████████████████████████             | 572/650 [4:15:53<31:59, 24.61s/it]
{'loss': 1.944, 'learning_rate': 2.076069474090844e-06, 'epoch': 8.8}

 88%|███████████████████████████████████████████████████████████████████████████████████████████████▏            | 573/650 [4:16:17<31:12, 24.32s/it]


 88%|███████████████████████████████████████████████████████████████████████████████████████████████▌            | 575/650 [4:17:04<29:56, 23.95s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████████████████▋            | 576/650 [4:17:28<29:33, 23.97s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████████████████▊            | 577/650 [4:17:52<29:04, 23.90s/it]

 89%|████████████████████████████████████████████████████████████████████████████████████████████████            | 578/650 [4:18:16<28:42, 23.92s/it]

 89%|████████████████████████████████████████████████████████████████████████████████████████████████▏           | 579/650 [4:18:40<28:15, 23.88s/it]
{'loss': 2.0773, 'learning_rate': 1.7484005709361122e-06, 'epoch': 8.91}

 89%|████████████████████████████████████████████████████████████████████████████████████████████████▎           | 580/650 [4:19:03<27:43, 23.76s/it]


 90%|████████████████████████████████████████████████████████████████████████████████████████████████▋           | 582/650 [4:19:51<26:55, 23.75s/it]

 90%|████████████████████████████████████████████████████████████████████████████████████████████████▊           | 583/650 [4:20:15<26:38, 23.86s/it]

 90%|█████████████████████████████████████████████████████████████████████████████████████████████████           | 584/650 [4:20:39<26:12, 23.83s/it]
{'loss': 2.1541, 'learning_rate': 1.5309800828191584e-06, 'epoch': 8.98}


 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▎          | 586/650 [4:21:23<24:39, 23.11s/it]

 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▌          | 587/650 [4:21:47<24:27, 23.29s/it]

 90%|█████████████████████████████████████████████████████████████████████████████████████████████████▋          | 588/650 [4:22:11<24:16, 23.50s/it]

 91%|█████████████████████████████████████████████████████████████████████████████████████████████████▊          | 589/650 [4:22:34<23:50, 23.46s/it]
{'loss': 1.9629, 'learning_rate': 1.3275648543022878e-06, 'epoch': 9.06}


 91%|██████████████████████████████████████████████████████████████████████████████████████████████████▏         | 591/650 [4:23:21<23:04, 23.47s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████████████████▎         | 592/650 [4:23:45<22:47, 23.58s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████████████████▌         | 593/650 [4:24:09<22:29, 23.68s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████████████████▋         | 594/650 [4:24:33<22:13, 23.81s/it]
{'loss': 1.9915, 'learning_rate': 1.138276274480235e-06, 'epoch': 9.14}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:36<00:03,  3.47s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████████████████▋         | 594/650 [4:26:15<22:13, 23.81s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Data_Science_and_Machine_Learning_class-bs_16-lr_5e-05-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-594)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████▊         | 595/650 [4:26:39<50:01, 54.58s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████         | 596/650 [4:27:03<40:48, 45.34s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▏        | 597/650 [4:27:27<34:18, 38.84s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▎        | 598/650 [4:27:50<29:44, 34.31s/it]
{'loss': 2.051, 'learning_rate': 9.97092791093296e-07, 'epoch': 9.2}

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▌        | 599/650 [4:28:14<26:22, 31.02s/it]

 92%|███████████████████████████████████████████████████████████████████████████████████████████████████▋        | 600/650 [4:28:38<24:04, 28.90s/it]


 93%|████████████████████████████████████████████████████████████████████████████████████████████████████        | 602/650 [4:29:25<21:00, 26.26s/it]

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 603/650 [4:29:49<19:53, 25.40s/it]
{'loss': 2.0809, 'learning_rate': 8.335111272354757e-07, 'epoch': 9.28}

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 604/650 [4:30:12<19:00, 24.80s/it]

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 605/650 [4:30:36<18:24, 24.55s/it]


 93%|████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 607/650 [4:31:23<17:14, 24.06s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████       | 608/650 [4:31:47<16:46, 23.96s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 609/650 [4:32:11<16:20, 23.90s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 610/650 [4:32:34<15:44, 23.61s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 611/650 [4:32:57<15:22, 23.65s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 612/650 [4:33:21<14:59, 23.67s/it]
{'loss': 2.0561, 'learning_rate': 5.754663630973045e-07, 'epoch': 9.42}


 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████      | 614/650 [4:34:08<14:08, 23.57s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 615/650 [4:34:31<13:46, 23.61s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎     | 616/650 [4:34:55<13:23, 23.64s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 617/650 [4:35:19<13:03, 23.76s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋     | 618/650 [4:35:43<12:37, 23.68s/it]
{'loss': 1.9622, 'learning_rate': 4.2964239186018273e-07, 'epoch': 9.51}


 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████     | 620/650 [4:36:30<11:48, 23.63s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 621/650 [4:36:54<11:25, 23.65s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 622/650 [4:37:17<11:03, 23.68s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 623/650 [4:37:40<10:32, 23.43s/it]
{'loss': 2.0126, 'learning_rate': 3.2424602720311225e-07, 'epoch': 9.58}


 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 625/650 [4:38:28<09:53, 23.72s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████    | 626/650 [4:38:51<09:26, 23.61s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏   | 627/650 [4:39:15<09:03, 23.65s/it]
{'loss': 2.0268, 'learning_rate': 2.5052848824173857e-07, 'epoch': 9.65}





























 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 29/30 [01:37<00:03,  3.49s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏   | 627/650 [4:40:58<09:03, 23.65s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-Data_Science_and_Machine_Learning_class-bs_16-lr_5e-05-m_l_1280-m_p_l_768-w_decay_0.2/checkpoint-627)... Done. 0.2s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9775, 'learning_rate': 2.3357504696500664e-07, 'epoch': 9.66}
 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 628/650 [4:41:23<20:05, 54.80s/it]


 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 630/650 [4:42:10<12:59, 39.00s/it]

 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 631/650 [4:42:34<10:52, 34.33s/it]

 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████   | 632/650 [4:42:58<09:21, 31.22s/it]

 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 633/650 [4:43:22<08:13, 29.05s/it]

 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎  | 634/650 [4:43:46<07:20, 27.55s/it]

 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 635/650 [4:44:09<06:35, 26.36s/it]

 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 636/650 [4:44:33<05:58, 25.63s/it]
{'loss': 1.9723, 'learning_rate': 1.1926203142463843e-07, 'epoch': 9.78}


 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████  | 638/650 [4:45:20<04:54, 24.57s/it]

 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 639/650 [4:45:44<04:28, 24.39s/it]

 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 640/650 [4:46:08<04:02, 24.26s/it]

 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 641/650 [4:46:32<03:36, 24.01s/it]
{'loss': 2.1425, 'learning_rate': 6.710825354243888e-08, 'epoch': 9.86}

 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 642/650 [4:46:55<03:10, 23.83s/it]


 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████ | 644/650 [4:47:43<02:22, 23.75s/it]

 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 645/650 [4:48:06<01:58, 23.75s/it]

 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 646/650 [4:48:30<01:35, 23.75s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 647/650 [4:48:54<01:11, 23.75s/it]
{'loss': 1.9151, 'learning_rate': 2.4165893188168864e-08, 'epoch': 9.95}

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▋| 648/650 [4:49:17<00:47, 23.65s/it]


100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [4:50:02<00:00, 22.86s/it]
{'loss': 2.099, 'learning_rate': 1.0741358493907006e-08, 'epoch': 10.0}

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [4:50:02<00:00, 26.77s/it]