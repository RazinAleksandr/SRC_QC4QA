  0%|                                                                                                            | 0/650 [00:00<?, ?it/s]/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▏                                                                                                 | 1/650 [00:27<4:55:50, 27.35s/it]

  0%|▎                                                                                                 | 2/650 [00:53<4:48:33, 26.72s/it]

  0%|▍                                                                                                 | 3/650 [01:19<4:46:01, 26.52s/it]
{'loss': 2.4727, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.05}

  1%|▌                                                                                                 | 4/650 [01:46<4:44:39, 26.44s/it]

  1%|▊                                                                                                 | 5/650 [02:12<4:43:48, 26.40s/it]

  1%|▉                                                                                                 | 6/650 [02:38<4:43:15, 26.39s/it]


  1%|█▏                                                                                                | 8/650 [03:31<4:42:10, 26.37s/it]

  1%|█▎                                                                                                | 9/650 [03:57<4:41:38, 26.36s/it]

  2%|█▍                                                                                               | 10/650 [04:23<4:39:18, 26.19s/it]

  2%|█▋                                                                                               | 11/650 [04:50<4:39:54, 26.28s/it]
{'loss': 2.4264, 'learning_rate': 2.999713552377097e-05, 'epoch': 0.17}

  2%|█▊                                                                                               | 12/650 [05:16<4:40:04, 26.34s/it]


  2%|██                                                                                               | 14/650 [06:09<4:40:07, 26.43s/it]

  2%|██▏                                                                                              | 15/650 [06:36<4:39:50, 26.44s/it]

  2%|██▍                                                                                              | 16/650 [07:02<4:39:41, 26.47s/it]
{'loss': 2.4827, 'learning_rate': 2.99855004640871e-05, 'epoch': 0.25}

  3%|██▌                                                                                              | 17/650 [07:29<4:39:14, 26.47s/it]


  3%|██▊                                                                                              | 19/650 [08:22<4:38:20, 26.47s/it]

  3%|██▉                                                                                              | 20/650 [08:48<4:37:59, 26.48s/it]
{'loss': 2.3451, 'learning_rate': 2.9969753177502497e-05, 'epoch': 0.31}

  3%|███▏                                                                                             | 21/650 [09:15<4:37:53, 26.51s/it]


  4%|███▍                                                                                             | 23/650 [10:08<4:37:21, 26.54s/it]

  4%|███▌                                                                                             | 24/650 [10:34<4:37:04, 26.56s/it]
{'loss': 2.3116, 'learning_rate': 2.9948288490633714e-05, 'epoch': 0.37}


  4%|███▉                                                                                             | 26/650 [11:28<4:36:45, 26.61s/it]

  4%|████                                                                                             | 27/650 [11:54<4:36:26, 26.62s/it]
{'loss': 2.4088, 'learning_rate': 2.9928442781145217e-05, 'epoch': 0.42}

  4%|████▏                                                                                            | 28/650 [12:21<4:36:08, 26.64s/it]


  5%|████▍                                                                                            | 30/650 [13:14<4:34:44, 26.59s/it]
{'loss': 2.3281, 'learning_rate': 2.9905389864285287e-05, 'epoch': 0.46}

  5%|████▋                                                                                            | 31/650 [13:41<4:34:31, 26.61s/it]

  5%|████▊                                                                                            | 32/650 [14:08<4:34:18, 26.63s/it]

  5%|████▉                                                                                            | 33/650 [14:34<4:32:00, 26.45s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.69s/it]

{'eval_loss': 2.412285804748535, 'eval_runtime': 100.2331, 'eval_samples_per_second': 8.68, 'eval_steps_per_second': 0.279, 'epoch': 0.51}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  5%|█████                                                                                            | 34/650 [16:41<9:42:16, 56.72s/it]
{'loss': 2.3684, 'learning_rate': 2.986967231182466e-05, 'epoch': 0.52}


  6%|█████▎                                                                                           | 36/650 [17:34<7:03:24, 41.38s/it]

  6%|█████▌                                                                                           | 37/650 [18:01<6:17:45, 36.97s/it]
{'loss': 2.4138, 'learning_rate': 2.9839156360347433e-05, 'epoch': 0.57}

  6%|█████▋                                                                                           | 38/650 [18:27<5:45:14, 33.85s/it]


  6%|█████▉                                                                                           | 40/650 [19:20<5:05:17, 30.03s/it]

  6%|██████                                                                                           | 41/650 [19:47<4:53:32, 28.92s/it]

  6%|██████▎                                                                                          | 42/650 [20:13<4:45:45, 28.20s/it]
{'loss': 2.4063, 'learning_rate': 2.978121548841459e-05, 'epoch': 0.65}

  7%|██████▍                                                                                          | 43/650 [20:39<4:39:48, 27.66s/it]


  7%|██████▋                                                                                          | 45/650 [21:32<4:32:47, 27.05s/it]

  7%|██████▊                                                                                          | 46/650 [21:59<4:30:32, 26.87s/it]

  7%|███████                                                                                          | 47/650 [22:25<4:28:59, 26.77s/it]

  7%|███████▏                                                                                         | 48/650 [22:51<4:25:49, 26.49s/it]
{'loss': 2.2932, 'learning_rate': 2.9700046443011285e-05, 'epoch': 0.74}

  8%|███████▎                                                                                         | 49/650 [23:18<4:25:24, 26.50s/it]


  8%|███████▌                                                                                         | 51/650 [24:11<4:24:04, 26.45s/it]

  8%|███████▊                                                                                         | 52/650 [24:37<4:23:42, 26.46s/it]

  8%|███████▉                                                                                         | 53/650 [25:03<4:23:12, 26.45s/it]
{'loss': 2.3265, 'learning_rate': 2.9622752934178467e-05, 'epoch': 0.82}

  8%|████████                                                                                         | 54/650 [25:30<4:22:41, 26.45s/it]

  8%|████████▏                                                                                        | 55/650 [25:56<4:22:05, 26.43s/it]


  9%|████████▌                                                                                        | 57/650 [26:49<4:20:37, 26.37s/it]

  9%|████████▋                                                                                        | 58/650 [27:15<4:20:07, 26.36s/it]

  9%|████████▊                                                                                        | 59/650 [27:42<4:19:35, 26.36s/it]
{'loss': 2.3285, 'learning_rate': 2.9518486560518666e-05, 'epoch': 0.91}

  9%|████████▉                                                                                        | 60/650 [28:08<4:19:15, 26.37s/it]

  9%|█████████                                                                                        | 61/650 [28:34<4:18:51, 26.37s/it]


 10%|█████████▍                                                                                       | 63/650 [29:27<4:17:59, 26.37s/it]

 10%|█████████▌                                                                                       | 64/650 [29:53<4:17:43, 26.39s/it]
{'loss': 2.2537, 'learning_rate': 2.9422063618621974e-05, 'epoch': 0.98}


 10%|█████████▊                                                                                       | 66/650 [30:36<3:55:11, 24.16s/it]
{'loss': 2.3763, 'learning_rate': 2.9381081243022145e-05, 'epoch': 1.02}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:31<00:07,  3.67s/it]
{'eval_loss': 2.3580095767974854, 'eval_runtime': 99.6869, 'eval_samples_per_second': 8.727, 'eval_steps_per_second': 0.281, 'epoch': 1.02}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.3608, 'learning_rate': 2.9360074864911965e-05, 'epoch': 1.03}
 10%|█████████▉                                                                                       | 67/650 [32:42<8:53:15, 54.88s/it]


 11%|██████████▎                                                                                      | 69/650 [33:35<6:30:55, 40.37s/it]

 11%|██████████▍                                                                                      | 70/650 [34:02<5:49:48, 36.19s/it]

 11%|██████████▌                                                                                      | 71/650 [34:28<5:20:48, 33.24s/it]
{'loss': 2.3239, 'learning_rate': 2.9272626546270493e-05, 'epoch': 1.09}

 11%|██████████▋                                                                                      | 72/650 [34:55<5:00:32, 31.20s/it]


 11%|███████████                                                                                      | 74/650 [35:47<4:36:08, 28.77s/it]

 12%|███████████▏                                                                                     | 75/650 [36:14<4:29:00, 28.07s/it]

 12%|███████████▎                                                                                     | 76/650 [36:40<4:24:08, 27.61s/it]
{'loss': 2.3352, 'learning_rate': 2.9155654585454936e-05, 'epoch': 1.17}


 12%|███████████▋                                                                                     | 78/650 [37:34<4:18:10, 27.08s/it]

 12%|███████████▊                                                                                     | 79/650 [38:00<4:16:15, 26.93s/it]
{'loss': 2.3504, 'learning_rate': 2.9081411950308506e-05, 'epoch': 1.22}

 12%|███████████▉                                                                                     | 80/650 [38:27<4:14:27, 26.78s/it]


 13%|████████████▏                                                                                    | 82/650 [39:19<4:11:54, 26.61s/it]

 13%|████████████▍                                                                                    | 83/650 [39:46<4:11:02, 26.56s/it]

 13%|████████████▌                                                                                    | 84/650 [40:12<4:10:10, 26.52s/it]
{'loss': 2.3745, 'learning_rate': 2.8950959657438333e-05, 'epoch': 1.29}

 13%|████████████▋                                                                                    | 85/650 [40:39<4:09:30, 26.50s/it]


 13%|████████████▉                                                                                    | 87/650 [41:32<4:08:15, 26.46s/it]

 14%|█████████████▏                                                                                   | 88/650 [41:58<4:07:53, 26.47s/it]

 14%|█████████████▎                                                                                   | 89/650 [42:25<4:08:01, 26.53s/it]
{'loss': 2.2355, 'learning_rate': 2.881218205689182e-05, 'epoch': 1.37}


 14%|█████████████▌                                                                                   | 91/650 [43:18<4:07:48, 26.60s/it]

 14%|█████████████▋                                                                                   | 92/650 [43:45<4:07:20, 26.60s/it]
{'loss': 2.3186, 'learning_rate': 2.872495359522484e-05, 'epoch': 1.42}


 14%|██████████████                                                                                   | 94/650 [44:38<4:06:46, 26.63s/it]

 15%|██████████████▏                                                                                  | 95/650 [45:05<4:06:17, 26.63s/it]
{'loss': 2.3067, 'learning_rate': 2.8634776482249183e-05, 'epoch': 1.46}

 15%|██████████████▎                                                                                  | 96/650 [45:31<4:05:51, 26.63s/it]


 15%|██████████████▌                                                                                  | 98/650 [46:24<4:04:53, 26.62s/it]
{'loss': 2.2018, 'learning_rate': 2.8541670091499358e-05, 'epoch': 1.51}

 15%|██████████████▊                                                                                  | 99/650 [46:56<4:17:44, 28.07s/it]


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.70s/it]
{'eval_loss': 2.325266122817993, 'eval_runtime': 102.6724, 'eval_samples_per_second': 8.474, 'eval_steps_per_second': 0.273, 'epoch': 1.52}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.3735, 'learning_rate': 2.8477981639976315e-05, 'epoch': 1.54}

 16%|██████████████▉                                                                                 | 101/650 [49:32<7:28:20, 49.00s/it]

 16%|███████████████                                                                                 | 102/650 [49:59<6:26:19, 42.30s/it]
{'loss': 2.297, 'learning_rate': 2.84130062457983e-05, 'epoch': 1.57}

 16%|███████████████▏                                                                                | 103/650 [50:26<5:42:38, 37.58s/it]


 16%|███████████████▌                                                                                | 105/650 [51:19<4:50:39, 32.00s/it]
{'loss': 2.3365, 'learning_rate': 2.831314374211701e-05, 'epoch': 1.62}

 16%|███████████████▋                                                                                | 106/650 [51:46<4:35:38, 30.40s/it]


 17%|███████████████▉                                                                                | 108/650 [52:39<4:16:56, 28.44s/it]

 17%|████████████████                                                                                | 109/650 [53:05<4:11:32, 27.90s/it]
{'loss': 2.2096, 'learning_rate': 2.817554836723083e-05, 'epoch': 1.68}


 17%|████████████████▍                                                                               | 111/650 [54:02<4:15:21, 28.43s/it]

 17%|████████████████▌                                                                               | 112/650 [54:30<4:13:50, 28.31s/it]

 17%|████████████████▋                                                                               | 113/650 [54:57<4:08:38, 27.78s/it]

 18%|████████████████▊                                                                               | 114/650 [55:24<4:05:10, 27.45s/it]

 18%|████████████████▉                                                                               | 115/650 [55:50<4:02:39, 27.21s/it]

 18%|█████████████████▏                                                                              | 116/650 [56:17<4:00:44, 27.05s/it]

 18%|█████████████████▎                                                                              | 117/650 [56:44<3:59:02, 26.91s/it]

 18%|█████████████████▍                                                                              | 118/650 [57:10<3:57:54, 26.83s/it]

 18%|█████████████████▌                                                                              | 119/650 [57:40<4:03:54, 27.56s/it]

 18%|█████████████████▋                                                                              | 120/650 [58:11<4:12:32, 28.59s/it]

 19%|█████████████████▊                                                                              | 121/650 [58:37<4:06:41, 27.98s/it]

 19%|██████████████████                                                                              | 122/650 [59:04<4:02:43, 27.58s/it]
{'loss': 2.2302, 'learning_rate': 2.7693895419005124e-05, 'epoch': 1.88}


 19%|██████████████████▎                                                                             | 124/650 [59:57<3:57:36, 27.10s/it]

 19%|██████████████████                                                                            | 125/650 [1:00:24<3:56:05, 26.98s/it]
{'loss': 2.3247, 'learning_rate': 2.7575399695787114e-05, 'epoch': 1.92}


 20%|██████████████████▎                                                                           | 127/650 [1:01:17<3:53:36, 26.80s/it]

 20%|██████████████████▌                                                                           | 128/650 [1:01:44<3:52:37, 26.74s/it]

 20%|██████████████████▋                                                                           | 129/650 [1:02:10<3:51:52, 26.70s/it]

 20%|██████████████████▊                                                                           | 130/650 [1:02:27<3:25:43, 23.74s/it]

 20%|██████████████████▉                                                                           | 131/650 [1:02:53<3:32:16, 24.54s/it]

 20%|███████████████████                                                                           | 132/650 [1:03:20<3:37:20, 25.17s/it]
{'loss': 2.2641, 'learning_rate': 2.7288448218043017e-05, 'epoch': 2.03}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.69s/it]

 20%|███████████████████                                                                           | 132/650 [1:05:00<3:37:20, 25.17s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-API_USAGE-bs_16-lr_3e-05-m_l_768-m_p_l_512-w_decay_0.2/checkpoint-132)... Done. 0.1s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 20%|███████████████████▏                                                                          | 133/650 [1:05:27<8:00:49, 55.80s/it]

 21%|███████████████████▍                                                                          | 134/650 [1:05:54<6:44:40, 47.06s/it]

 21%|███████████████████▌                                                                          | 135/650 [1:06:21<5:51:10, 40.91s/it]

 21%|███████████████████▋                                                                          | 136/650 [1:06:47<5:13:37, 36.61s/it]

 21%|███████████████████▊                                                                          | 137/650 [1:07:14<4:47:31, 33.63s/it]

 21%|███████████████████▉                                                                          | 138/650 [1:07:40<4:29:06, 31.54s/it]
{'loss': 2.1714, 'learning_rate': 2.703103739234411e-05, 'epoch': 2.12}


 22%|████████████████████▏                                                                         | 140/650 [1:08:33<4:06:16, 28.97s/it]

 22%|████████████████████▍                                                                         | 141/650 [1:09:00<3:59:51, 28.27s/it]

 22%|████████████████████▌                                                                         | 142/650 [1:09:27<3:55:12, 27.78s/it]

 22%|████████████████████▋                                                                         | 143/650 [1:09:53<3:51:48, 27.43s/it]

 22%|████████████████████▊                                                                         | 144/650 [1:10:20<3:49:14, 27.18s/it]

 22%|████████████████████▉                                                                         | 145/650 [1:10:47<3:47:08, 26.99s/it]
{'loss': 2.2414, 'learning_rate': 2.6717674232361516e-05, 'epoch': 2.23}


 23%|█████████████████████▎                                                                        | 147/650 [1:11:40<3:44:20, 26.76s/it]

 23%|█████████████████████▍                                                                        | 148/650 [1:12:06<3:43:30, 26.71s/it]

 23%|█████████████████████▌                                                                        | 149/650 [1:12:33<3:42:36, 26.66s/it]
{'loss': 2.3192, 'learning_rate': 2.6532432216574606e-05, 'epoch': 2.29}


 23%|█████████████████████▊                                                                        | 151/650 [1:13:26<3:40:50, 26.55s/it]

 23%|█████████████████████▉                                                                        | 152/650 [1:13:52<3:40:39, 26.59s/it]

 24%|██████████████████████▏                                                                       | 153/650 [1:14:19<3:40:08, 26.58s/it]
{'loss': 2.274, 'learning_rate': 2.6342785617061398e-05, 'epoch': 2.35}


 24%|██████████████████████▍                                                                       | 155/650 [1:15:12<3:39:28, 26.60s/it]

 24%|██████████████████████▌                                                                       | 156/650 [1:15:39<3:39:01, 26.60s/it]

 24%|██████████████████████▋                                                                       | 157/650 [1:16:05<3:38:14, 26.56s/it]

 24%|██████████████████████▊                                                                       | 158/650 [1:16:32<3:37:47, 26.56s/it]

 24%|██████████████████████▉                                                                       | 159/650 [1:16:58<3:37:34, 26.59s/it]

 25%|███████████████████████▏                                                                      | 160/650 [1:17:25<3:37:09, 26.59s/it]
{'loss': 2.1315, 'learning_rate': 2.6000524333384367e-05, 'epoch': 2.46}

 25%|███████████████████████▎                                                                      | 161/650 [1:17:52<3:36:47, 26.60s/it]


 25%|███████████████████████▌                                                                      | 163/650 [1:18:44<3:34:38, 26.45s/it]

 25%|███████████████████████▋                                                                      | 164/650 [1:19:11<3:34:21, 26.46s/it]

 25%|███████████████████████▊                                                                      | 165/650 [1:19:37<3:34:25, 26.53s/it]
{'loss': 2.1854, 'learning_rate': 2.5748150878334035e-05, 'epoch': 2.54}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:31<00:07,  3.68s/it]
{'eval_loss': 2.142740249633789, 'eval_runtime': 99.9903, 'eval_samples_per_second': 8.701, 'eval_steps_per_second': 0.28, 'epoch': 2.54}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 26%|████████████████████████                                                                      | 166/650 [1:21:44<7:37:20, 56.70s/it]

 26%|████████████████████████▏                                                                     | 167/650 [1:22:11<6:23:49, 47.68s/it]

 26%|████████████████████████▎                                                                     | 168/650 [1:22:37<5:31:43, 41.29s/it]

 26%|████████████████████████▍                                                                     | 169/650 [1:23:04<4:55:51, 36.91s/it]

 26%|████████████████████████▌                                                                     | 170/650 [1:23:31<4:30:33, 33.82s/it]

 26%|████████████████████████▋                                                                     | 171/650 [1:23:57<4:12:47, 31.66s/it]
{'loss': 2.0191, 'learning_rate': 2.5436849756363238e-05, 'epoch': 2.63}


 27%|█████████████████████████                                                                     | 173/650 [1:24:51<3:51:27, 29.12s/it]

 27%|█████████████████████████▏                                                                    | 174/650 [1:25:17<3:45:02, 28.37s/it]
{'loss': 2.1263, 'learning_rate': 2.527781899426762e-05, 'epoch': 2.68}


 27%|█████████████████████████▍                                                                    | 176/650 [1:26:11<3:37:04, 27.48s/it]

 27%|█████████████████████████▌                                                                    | 177/650 [1:26:37<3:34:25, 27.20s/it]

 27%|█████████████████████████▋                                                                    | 178/650 [1:27:04<3:32:20, 26.99s/it]
{'loss': 2.1583, 'learning_rate': 2.5062349171051785e-05, 'epoch': 2.74}


 28%|██████████████████████████                                                                    | 180/650 [1:27:57<3:29:47, 26.78s/it]

 28%|██████████████████████████▏                                                                   | 181/650 [1:28:23<3:28:50, 26.72s/it]

 28%|██████████████████████████▎                                                                   | 182/650 [1:28:50<3:28:22, 26.71s/it]

 28%|██████████████████████████▍                                                                   | 183/650 [1:29:17<3:27:43, 26.69s/it]

 28%|██████████████████████████▌                                                                   | 184/650 [1:29:43<3:27:02, 26.66s/it]

 28%|██████████████████████████▊                                                                   | 185/650 [1:30:10<3:26:22, 26.63s/it]
{'loss': 2.1715, 'learning_rate': 2.473196471545659e-05, 'epoch': 2.85}


 29%|███████████████████████████                                                                   | 187/650 [1:31:03<3:25:34, 26.64s/it]

 29%|███████████████████████████▏                                                                  | 188/650 [1:31:28<3:21:42, 26.19s/it]

 29%|███████████████████████████▎                                                                  | 189/650 [1:31:55<3:22:20, 26.34s/it]

 29%|███████████████████████████▍                                                                  | 190/650 [1:32:22<3:22:24, 26.40s/it]

 29%|███████████████████████████▌                                                                  | 191/650 [1:32:48<3:22:48, 26.51s/it]

 30%|███████████████████████████▊                                                                  | 192/650 [1:33:15<3:22:30, 26.53s/it]

 30%|███████████████████████████▉                                                                  | 193/650 [1:33:42<3:22:23, 26.57s/it]

 30%|████████████████████████████                                                                  | 194/650 [1:34:08<3:22:12, 26.61s/it]

 30%|████████████████████████████▏                                                                 | 195/650 [1:34:25<2:59:20, 23.65s/it]

 30%|████████████████████████████▎                                                                 | 196/650 [1:34:52<3:05:45, 24.55s/it]

 30%|████████████████████████████▍                                                                 | 197/650 [1:35:18<3:09:58, 25.16s/it]

 30%|████████████████████████████▋                                                                 | 198/650 [1:35:45<3:12:24, 25.54s/it]
{'loss': 2.1506, 'learning_rate': 2.4220798545005137e-05, 'epoch': 3.05}


























 96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 27/28 [01:35<00:03,  3.69s/it]

{'eval_loss': 2.216665506362915, 'eval_runtime': 100.367, 'eval_samples_per_second': 8.668, 'eval_steps_per_second': 0.279, 'epoch': 3.05}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 31%|████████████████████████████▊                                                                 | 199/650 [1:37:52<7:02:00, 56.14s/it]

 31%|████████████████████████████▉                                                                 | 200/650 [1:38:20<5:56:48, 47.57s/it]

 31%|█████████████████████████████                                                                 | 201/650 [1:38:49<5:14:12, 41.99s/it]

 31%|█████████████████████████████▏                                                                | 202/650 [1:39:18<4:44:32, 38.11s/it]

 31%|█████████████████████████████▎                                                                | 203/650 [1:39:46<4:21:46, 35.14s/it]

 31%|█████████████████████████████▌                                                                | 204/650 [1:40:13<4:02:00, 32.56s/it]
{'loss': 2.2054, 'learning_rate': 2.39290499208587e-05, 'epoch': 3.14}


 32%|█████████████████████████████▊                                                                | 206/650 [1:41:06<3:38:23, 29.51s/it]

 32%|█████████████████████████████▉                                                                | 207/650 [1:41:32<3:31:32, 28.65s/it]

 32%|██████████████████████████████                                                                | 208/650 [1:41:59<3:26:44, 28.06s/it]

 32%|██████████████████████████████▏                                                               | 209/650 [1:42:26<3:23:07, 27.64s/it]

 32%|██████████████████████████████▎                                                               | 210/650 [1:42:52<3:20:39, 27.36s/it]

 32%|██████████████████████████████▌                                                               | 211/650 [1:43:19<3:18:23, 27.11s/it]

 33%|██████████████████████████████▋                                                               | 212/650 [1:43:46<3:16:50, 26.97s/it]

 33%|██████████████████████████████▊                                                               | 213/650 [1:44:12<3:15:36, 26.86s/it]

 33%|██████████████████████████████▉                                                               | 214/650 [1:44:39<3:14:43, 26.80s/it]
{'loss': 2.2433, 'learning_rate': 2.3329744584028738e-05, 'epoch': 3.29}


 33%|███████████████████████████████▏                                                              | 216/650 [1:45:32<3:13:10, 26.71s/it]

 33%|███████████████████████████████▍                                                              | 217/650 [1:45:59<3:12:29, 26.67s/it]

 34%|███████████████████████████████▌                                                              | 218/650 [1:46:25<3:11:53, 26.65s/it]

 34%|███████████████████████████████▋                                                              | 219/650 [1:46:52<3:11:22, 26.64s/it]

 34%|███████████████████████████████▊                                                              | 220/650 [1:47:18<3:10:47, 26.62s/it]

 34%|███████████████████████████████▉                                                              | 221/650 [1:47:45<3:10:21, 26.62s/it]
{'loss': 2.0672, 'learning_rate': 2.2898315018223977e-05, 'epoch': 3.4}


 34%|████████████████████████████████▏                                                             | 223/650 [1:48:38<3:09:36, 26.64s/it]

 34%|████████████████████████████████▍                                                             | 224/650 [1:49:05<3:09:04, 26.63s/it]

 35%|████████████████████████████████▌                                                             | 225/650 [1:49:32<3:08:28, 26.61s/it]

 35%|████████████████████████████████▋                                                             | 226/650 [1:49:59<3:09:35, 26.83s/it]

 35%|████████████████████████████████▊                                                             | 227/650 [1:50:27<3:12:37, 27.32s/it]

 35%|████████████████████████████████▉                                                             | 228/650 [1:50:56<3:13:51, 27.56s/it]

 35%|█████████████████████████████████                                                             | 229/650 [1:51:24<3:15:28, 27.86s/it]

 35%|█████████████████████████████████▎                                                            | 230/650 [1:51:51<3:12:24, 27.49s/it]

 36%|█████████████████████████████████▍                                                            | 231/650 [1:52:17<3:10:03, 27.22s/it]
{'loss': 2.0183, 'learning_rate': 2.226608976084913e-05, 'epoch': 3.55}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.70s/it]
{'eval_loss': 2.101490020751953, 'eval_runtime': 100.482, 'eval_samples_per_second': 8.658, 'eval_steps_per_second': 0.279, 'epoch': 3.55}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 36%|█████████████████████████████████▌                                                            | 232/650 [1:54:25<6:39:35, 57.36s/it]

 36%|█████████████████████████████████▋                                                            | 233/650 [1:54:51<5:34:20, 48.11s/it]
{'loss': 2.0977, 'learning_rate': 2.2137514666289893e-05, 'epoch': 3.58}


 36%|█████████████████████████████████▉                                                            | 235/650 [1:55:45<4:17:05, 37.17s/it]

 36%|██████████████████████████████████▏                                                           | 236/650 [1:56:12<3:54:47, 34.03s/it]

 36%|██████████████████████████████████▎                                                           | 237/650 [1:56:38<3:38:59, 31.81s/it]

 37%|██████████████████████████████████▍                                                           | 238/650 [1:57:05<3:27:52, 30.27s/it]

 37%|██████████████████████████████████▌                                                           | 239/650 [1:57:32<3:20:10, 29.22s/it]

 37%|██████████████████████████████████▋                                                           | 240/650 [1:57:58<3:14:48, 28.51s/it]

 37%|██████████████████████████████████▊                                                           | 241/650 [1:58:27<3:14:31, 28.54s/it]

 37%|██████████████████████████████████▉                                                           | 242/650 [1:58:56<3:15:03, 28.68s/it]

 37%|███████████████████████████████████▏                                                          | 243/650 [1:59:25<3:15:53, 28.88s/it]

 38%|███████████████████████████████████▎                                                          | 244/650 [1:59:54<3:14:45, 28.78s/it]
{'loss': 2.0728, 'learning_rate': 2.141850180550283e-05, 'epoch': 3.75}


 38%|███████████████████████████████████▌                                                          | 246/650 [2:00:47<3:06:26, 27.69s/it]

 38%|███████████████████████████████████▋                                                          | 247/650 [2:01:14<3:03:54, 27.38s/it]
{'loss': 2.0627, 'learning_rate': 2.121910188461638e-05, 'epoch': 3.8}

 38%|███████████████████████████████████▊                                                          | 248/650 [2:01:41<3:02:10, 27.19s/it]


 38%|████████████████████████████████████▏                                                         | 250/650 [2:02:33<2:58:01, 26.70s/it]

 39%|████████████████████████████████████▎                                                         | 251/650 [2:03:00<2:57:14, 26.65s/it]

 39%|████████████████████████████████████▍                                                         | 252/650 [2:03:26<2:56:48, 26.66s/it]
{'loss': 2.0551, 'learning_rate': 2.0883820848733406e-05, 'epoch': 3.88}


 39%|████████████████████████████████████▋                                                         | 254/650 [2:04:19<2:55:10, 26.54s/it]

 39%|████████████████████████████████████▉                                                         | 255/650 [2:04:46<2:54:47, 26.55s/it]

 39%|█████████████████████████████████████                                                         | 256/650 [2:05:12<2:54:18, 26.54s/it]

 40%|█████████████████████████████████████▏                                                        | 257/650 [2:05:39<2:53:44, 26.53s/it]

 40%|█████████████████████████████████████▎                                                        | 258/650 [2:06:05<2:53:22, 26.54s/it]

 40%|█████████████████████████████████████▍                                                        | 259/650 [2:06:32<2:53:05, 26.56s/it]

 40%|█████████████████████████████████████▌                                                        | 260/650 [2:06:49<2:33:34, 23.63s/it]

 40%|█████████████████████████████████████▋                                                        | 261/650 [2:07:15<2:39:00, 24.53s/it]

 40%|█████████████████████████████████████▉                                                        | 262/650 [2:07:42<2:42:38, 25.15s/it]

 40%|██████████████████████████████████████                                                        | 263/650 [2:08:08<2:44:49, 25.55s/it]

 41%|██████████████████████████████████████▏                                                       | 264/650 [2:08:35<2:46:24, 25.87s/it]
{'loss': 2.0591, 'learning_rate': 2.006520598579818e-05, 'epoch': 4.06}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:31<00:07,  3.69s/it]

 41%|██████████████████████████████████████▏                                                       | 264/650 [2:10:15<2:46:24, 25.87s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-API_USAGE-bs_16-lr_3e-05-m_l_768-m_p_l_512-w_decay_0.2/checkpoint-264)... Done. 0.1s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 41%|██████████████████████████████████████▎                                                       | 265/650 [2:10:42<6:00:36, 56.20s/it]

 41%|██████████████████████████████████████▍                                                       | 266/650 [2:11:09<5:02:46, 47.31s/it]

 41%|██████████████████████████████████████▌                                                       | 267/650 [2:11:35<4:22:25, 41.11s/it]

 41%|██████████████████████████████████████▊                                                       | 268/650 [2:12:03<3:55:39, 37.02s/it]

 41%|██████████████████████████████████████▉                                                       | 269/650 [2:12:32<3:40:05, 34.66s/it]

 42%|███████████████████████████████████████                                                       | 270/650 [2:13:01<3:28:20, 32.90s/it]

 42%|███████████████████████████████████████▏                                                      | 271/650 [2:13:29<3:18:38, 31.45s/it]

 42%|███████████████████████████████████████▎                                                      | 272/650 [2:13:55<3:08:44, 29.96s/it]

 42%|███████████████████████████████████████▍                                                      | 273/650 [2:14:22<3:01:57, 28.96s/it]

 42%|███████████████████████████████████████▌                                                      | 274/650 [2:14:48<2:56:52, 28.22s/it]

 42%|███████████████████████████████████████▊                                                      | 275/650 [2:15:15<2:53:02, 27.69s/it]

 42%|███████████████████████████████████████▉                                                      | 276/650 [2:15:41<2:50:07, 27.29s/it]
{'loss': 2.0424, 'learning_rate': 1.9229184561200366e-05, 'epoch': 4.25}


 43%|████████████████████████████████████████▏                                                     | 278/650 [2:16:34<2:46:57, 26.93s/it]

 43%|████████████████████████████████████████▎                                                     | 279/650 [2:17:02<2:47:55, 27.16s/it]

 43%|████████████████████████████████████████▍                                                     | 280/650 [2:17:31<2:50:07, 27.59s/it]

 43%|████████████████████████████████████████▋                                                     | 281/650 [2:18:00<2:52:26, 28.04s/it]

 43%|████████████████████████████████████████▊                                                     | 282/650 [2:18:29<2:53:38, 28.31s/it]

 44%|████████████████████████████████████████▉                                                     | 283/650 [2:18:55<2:49:39, 27.74s/it]

 44%|█████████████████████████████████████████                                                     | 284/650 [2:19:21<2:46:52, 27.36s/it]

 44%|█████████████████████████████████████████▏                                                    | 285/650 [2:19:48<2:45:03, 27.13s/it]

 44%|█████████████████████████████████████████▎                                                    | 286/650 [2:20:15<2:43:26, 26.94s/it]

 44%|█████████████████████████████████████████▌                                                    | 287/650 [2:20:41<2:42:20, 26.83s/it]

 44%|█████████████████████████████████████████▋                                                    | 288/650 [2:21:08<2:41:24, 26.75s/it]

 44%|█████████████████████████████████████████▊                                                    | 289/650 [2:21:34<2:40:45, 26.72s/it]

 45%|█████████████████████████████████████████▉                                                    | 290/650 [2:22:01<2:40:13, 26.70s/it]

 45%|██████████████████████████████████████████                                                    | 291/650 [2:22:27<2:39:03, 26.58s/it]

 45%|██████████████████████████████████████████▏                                                   | 292/650 [2:22:55<2:40:33, 26.91s/it]

 45%|██████████████████████████████████████████▎                                                   | 293/650 [2:23:24<2:43:47, 27.53s/it]

 45%|██████████████████████████████████████████▌                                                   | 294/650 [2:23:53<2:46:41, 28.10s/it]

 45%|██████████████████████████████████████████▋                                                   | 295/650 [2:24:22<2:47:23, 28.29s/it]

 46%|██████████████████████████████████████████▊                                                   | 296/650 [2:24:49<2:43:55, 27.78s/it]

 46%|██████████████████████████████████████████▉                                                   | 297/650 [2:25:15<2:41:31, 27.46s/it]
{'loss': 1.9793, 'learning_rate': 1.7732932377318387e-05, 'epoch': 4.57}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.69s/it]
{'eval_loss': 2.097024917602539, 'eval_runtime': 100.3542, 'eval_samples_per_second': 8.669, 'eval_steps_per_second': 0.279, 'epoch': 4.57}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 46%|███████████████████████████████████████████                                                   | 298/650 [2:27:23<5:37:09, 57.47s/it]

 46%|███████████████████████████████████████████▏                                                  | 299/650 [2:27:50<4:42:17, 48.26s/it]

 46%|███████████████████████████████████████████▍                                                  | 300/650 [2:28:16<4:03:43, 41.78s/it]

 46%|███████████████████████████████████████████▌                                                  | 301/650 [2:28:43<3:36:44, 37.26s/it]

 46%|███████████████████████████████████████████▋                                                  | 302/650 [2:29:10<3:17:43, 34.09s/it]

 47%|███████████████████████████████████████████▊                                                  | 303/650 [2:29:36<3:03:25, 31.72s/it]
{'loss': 2.058, 'learning_rate': 1.7299454808329962e-05, 'epoch': 4.66}


 47%|████████████████████████████████████████████                                                  | 305/650 [2:30:29<2:47:50, 29.19s/it]

 47%|████████████████████████████████████████████▎                                                 | 306/650 [2:30:57<2:45:17, 28.83s/it]

 47%|████████████████████████████████████████████▍                                                 | 307/650 [2:31:27<2:45:28, 28.95s/it]

 47%|████████████████████████████████████████████▌                                                 | 308/650 [2:31:56<2:44:55, 28.94s/it]

 48%|████████████████████████████████████████████▋                                                 | 309/650 [2:32:24<2:43:12, 28.72s/it]

 48%|████████████████████████████████████████████▊                                                 | 310/650 [2:32:50<2:39:15, 28.10s/it]

 48%|████████████████████████████████████████████▉                                                 | 311/650 [2:33:17<2:36:21, 27.67s/it]

 48%|█████████████████████████████████████████████                                                 | 312/650 [2:33:44<2:34:14, 27.38s/it]

 48%|█████████████████████████████████████████████▎                                                | 313/650 [2:34:10<2:32:37, 27.17s/it]
{'loss': 2.0108, 'learning_rate': 1.6572785936796702e-05, 'epoch': 4.82}


 48%|█████████████████████████████████████████████▌                                                | 315/650 [2:35:04<2:30:29, 26.95s/it]

 49%|█████████████████████████████████████████████▋                                                | 316/650 [2:35:31<2:29:31, 26.86s/it]

 49%|█████████████████████████████████████████████▊                                                | 317/650 [2:35:57<2:28:51, 26.82s/it]

 49%|█████████████████████████████████████████████▉                                                | 318/650 [2:36:24<2:28:05, 26.76s/it]

 49%|██████████████████████████████████████████████▏                                               | 319/650 [2:36:51<2:27:38, 26.76s/it]

 49%|██████████████████████████████████████████████▎                                               | 320/650 [2:37:18<2:28:08, 26.93s/it]

 49%|██████████████████████████████████████████████▍                                               | 321/650 [2:37:46<2:29:51, 27.33s/it]

 50%|██████████████████████████████████████████████▌                                               | 322/650 [2:38:15<2:32:24, 27.88s/it]

 50%|██████████████████████████████████████████████▋                                               | 323/650 [2:38:45<2:33:50, 28.23s/it]

 50%|██████████████████████████████████████████████▊                                               | 324/650 [2:39:12<2:31:32, 27.89s/it]

 50%|███████████████████████████████████████████████                                               | 325/650 [2:39:29<2:13:12, 24.59s/it]

 50%|███████████████████████████████████████████████▏                                              | 326/650 [2:39:55<2:16:08, 25.21s/it]

 50%|███████████████████████████████████████████████▎                                              | 327/650 [2:40:22<2:18:10, 25.67s/it]

 50%|███████████████████████████████████████████████▍                                              | 328/650 [2:40:49<2:19:22, 25.97s/it]

 51%|███████████████████████████████████████████████▌                                              | 329/650 [2:41:15<2:20:13, 26.21s/it]

 51%|███████████████████████████████████████████████▋                                              | 330/650 [2:41:42<2:20:38, 26.37s/it]
{'loss': 2.0772, 'learning_rate': 1.5329767371561776e-05, 'epoch': 5.08}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.71s/it]
{'eval_loss': 2.0957531929016113, 'eval_runtime': 100.8275, 'eval_samples_per_second': 8.629, 'eval_steps_per_second': 0.278, 'epoch': 5.08}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 51%|███████████████████████████████████████████████▊                                              | 331/650 [2:43:50<5:02:30, 56.90s/it]

 51%|████████████████████████████████████████████████                                              | 332/650 [2:44:17<4:13:44, 47.87s/it]

 51%|████████████████████████████████████████████████▏                                             | 333/650 [2:44:45<3:40:42, 41.77s/it]

 51%|████████████████████████████████████████████████▎                                             | 334/650 [2:45:13<3:18:35, 37.71s/it]

 52%|████████████████████████████████████████████████▍                                             | 335/650 [2:45:41<3:03:16, 34.91s/it]

 52%|████████████████████████████████████████████████▌                                             | 336/650 [2:46:10<2:53:27, 33.15s/it]

 52%|████████████████████████████████████████████████▋                                             | 337/650 [2:46:37<2:43:19, 31.31s/it]

 52%|████████████████████████████████████████████████▉                                             | 338/650 [2:47:04<2:35:35, 29.92s/it]

 52%|█████████████████████████████████████████████████                                             | 339/650 [2:47:31<2:30:05, 28.96s/it]

 52%|█████████████████████████████████████████████████▏                                            | 340/650 [2:47:57<2:26:08, 28.29s/it]

 52%|█████████████████████████████████████████████████▎                                            | 341/650 [2:48:24<2:22:53, 27.75s/it]

 53%|█████████████████████████████████████████████████▍                                            | 342/650 [2:48:50<2:20:47, 27.43s/it]

 53%|█████████████████████████████████████████████████▌                                            | 343/650 [2:49:17<2:19:18, 27.23s/it]

 53%|█████████████████████████████████████████████████▋                                            | 344/650 [2:49:44<2:18:02, 27.07s/it]

 53%|█████████████████████████████████████████████████▉                                            | 345/650 [2:50:11<2:17:03, 26.96s/it]

 53%|██████████████████████████████████████████████████                                            | 346/650 [2:50:37<2:16:15, 26.89s/it]

 53%|██████████████████████████████████████████████████▏                                           | 347/650 [2:51:04<2:15:55, 26.92s/it]

 54%|██████████████████████████████████████████████████▎                                           | 348/650 [2:51:32<2:17:00, 27.22s/it]

 54%|██████████████████████████████████████████████████▍                                           | 349/650 [2:52:01<2:18:42, 27.65s/it]

 54%|██████████████████████████████████████████████████▌                                           | 350/650 [2:52:30<2:20:57, 28.19s/it]

 54%|██████████████████████████████████████████████████▊                                           | 351/650 [2:53:00<2:22:21, 28.57s/it]

 54%|██████████████████████████████████████████████████▉                                           | 352/650 [2:53:27<2:19:20, 28.05s/it]

 54%|███████████████████████████████████████████████████                                           | 353/650 [2:53:53<2:16:51, 27.65s/it]
{'loss': 2.0681, 'learning_rate': 1.3646025881283844e-05, 'epoch': 5.43}


 55%|███████████████████████████████████████████████████▎                                          | 355/650 [2:54:47<2:13:35, 27.17s/it]

 55%|███████████████████████████████████████████████████▍                                          | 356/650 [2:55:13<2:12:22, 27.01s/it]

 55%|███████████████████████████████████████████████████▋                                          | 357/650 [2:55:40<2:11:31, 26.93s/it]

 55%|███████████████████████████████████████████████████▊                                          | 358/650 [2:56:07<2:10:49, 26.88s/it]

 55%|███████████████████████████████████████████████████▉                                          | 359/650 [2:56:34<2:10:11, 26.84s/it]

 55%|████████████████████████████████████████████████████                                          | 360/650 [2:57:01<2:09:39, 26.83s/it]

 56%|████████████████████████████████████████████████████▏                                         | 361/650 [2:57:27<2:09:05, 26.80s/it]

 56%|████████████████████████████████████████████████████▎                                         | 362/650 [2:57:54<2:08:37, 26.80s/it]

 56%|████████████████████████████████████████████████████▍                                         | 363/650 [2:58:21<2:08:17, 26.82s/it]
{'loss': 2.095, 'learning_rate': 1.2918048304994762e-05, 'epoch': 5.58}


























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [02:00<00:08,  4.29s/it]
{'eval_loss': 2.095372438430786, 'eval_runtime': 128.6832, 'eval_samples_per_second': 6.761, 'eval_steps_per_second': 0.218, 'epoch': 5.58}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 56%|████████████████████████████████████████████████████▋                                         | 364/650 [3:00:57<5:12:29, 65.56s/it]

 56%|████████████████████████████████████████████████████▊                                         | 365/650 [3:01:24<4:16:11, 53.94s/it]

 56%|████████████████████████████████████████████████████▉                                         | 366/650 [3:01:50<3:36:42, 45.78s/it]

 56%|█████████████████████████████████████████████████████                                         | 367/650 [3:02:17<3:08:47, 40.02s/it]

 57%|█████████████████████████████████████████████████████▏                                        | 368/650 [3:02:44<2:49:22, 36.04s/it]

 57%|█████████████████████████████████████████████████████▎                                        | 369/650 [3:03:11<2:35:44, 33.25s/it]

 57%|█████████████████████████████████████████████████████▌                                        | 370/650 [3:03:37<2:26:06, 31.31s/it]

 57%|█████████████████████████████████████████████████████▋                                        | 371/650 [3:04:04<2:19:16, 29.95s/it]

 57%|█████████████████████████████████████████████████████▊                                        | 372/650 [3:04:30<2:13:29, 28.81s/it]

 57%|█████████████████████████████████████████████████████▉                                        | 373/650 [3:04:57<2:10:10, 28.20s/it]

 58%|██████████████████████████████████████████████████████                                        | 374/650 [3:05:24<2:07:59, 27.82s/it]

 58%|██████████████████████████████████████████████████████▏                                       | 375/650 [3:05:52<2:08:14, 27.98s/it]

 58%|██████████████████████████████████████████████████████▍                                       | 376/650 [3:06:21<2:09:09, 28.28s/it]

 58%|██████████████████████████████████████████████████████▌                                       | 377/650 [3:06:50<2:09:40, 28.50s/it]

 58%|██████████████████████████████████████████████████████▋                                       | 378/650 [3:07:19<2:09:33, 28.58s/it]

 58%|██████████████████████████████████████████████████████▊                                       | 379/650 [3:07:46<2:06:28, 28.00s/it]

 58%|██████████████████████████████████████████████████████▉                                       | 380/650 [3:08:12<2:04:11, 27.60s/it]
{'loss': 2.0317, 'learning_rate': 1.1692814744566788e-05, 'epoch': 5.85}


 59%|███████████████████████████████████████████████████████▏                                      | 382/650 [3:09:06<2:01:17, 27.15s/it]

 59%|███████████████████████████████████████████████████████▍                                      | 383/650 [3:09:32<2:00:11, 27.01s/it]

 59%|███████████████████████████████████████████████████████▌                                      | 384/650 [3:09:59<1:59:23, 26.93s/it]

 59%|███████████████████████████████████████████████████████▋                                      | 385/650 [3:10:26<1:58:45, 26.89s/it]

 59%|███████████████████████████████████████████████████████▊                                      | 386/650 [3:10:53<1:58:02, 26.83s/it]

 60%|███████████████████████████████████████████████████████▉                                      | 387/650 [3:11:19<1:57:29, 26.80s/it]

 60%|████████████████████████████████████████████████████████                                      | 388/650 [3:11:46<1:56:57, 26.78s/it]

 60%|████████████████████████████████████████████████████████▎                                     | 389/650 [3:12:13<1:56:27, 26.77s/it]

 60%|████████████████████████████████████████████████████████▍                                     | 390/650 [3:12:29<1:42:38, 23.69s/it]

 60%|████████████████████████████████████████████████████████▌                                     | 391/650 [3:12:56<1:46:31, 24.68s/it]

 60%|████████████████████████████████████████████████████████▋                                     | 392/650 [3:13:25<1:50:48, 25.77s/it]

 60%|████████████████████████████████████████████████████████▊                                     | 393/650 [3:13:54<1:55:13, 26.90s/it]

 61%|████████████████████████████████████████████████████████▉                                     | 394/650 [3:14:24<1:57:57, 27.65s/it]

 61%|█████████████████████████████████████████████████████████                                     | 395/650 [3:14:53<1:59:12, 28.05s/it]

 61%|█████████████████████████████████████████████████████████▎                                    | 396/650 [3:15:19<1:57:02, 27.65s/it]
{'loss': 2.0518, 'learning_rate': 1.0560334403702485e-05, 'epoch': 6.09}


























 96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 27/28 [01:36<00:03,  3.70s/it]

{'eval_loss': 2.0952682495117188, 'eval_runtime': 100.5192, 'eval_samples_per_second': 8.655, 'eval_steps_per_second': 0.279, 'epoch': 6.09}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0658, 'learning_rate': 1.049038378964713e-05, 'epoch': 6.11}
 61%|█████████████████████████████████████████████████████████▍                                    | 397/650 [3:17:27<4:03:02, 57.64s/it]

 61%|█████████████████████████████████████████████████████████▌                                    | 398/650 [3:17:54<3:23:07, 48.36s/it]

 61%|█████████████████████████████████████████████████████████▋                                    | 399/650 [3:18:20<2:55:08, 41.87s/it]

 62%|█████████████████████████████████████████████████████████▊                                    | 400/650 [3:18:48<2:36:13, 37.49s/it]

 62%|█████████████████████████████████████████████████████████▉                                    | 401/650 [3:19:17<2:24:49, 34.90s/it]

 62%|██████████████████████████████████████████████████████████▏                                   | 402/650 [3:19:46<2:17:09, 33.18s/it]

 62%|██████████████████████████████████████████████████████████▎                                   | 403/650 [3:20:14<2:11:00, 31.83s/it]

 62%|██████████████████████████████████████████████████████████▍                                   | 404/650 [3:20:41<2:04:12, 30.30s/it]

 62%|██████████████████████████████████████████████████████████▌                                   | 405/650 [3:21:08<1:59:18, 29.22s/it]

 62%|██████████████████████████████████████████████████████████▋                                   | 406/650 [3:21:34<1:55:39, 28.44s/it]

 63%|██████████████████████████████████████████████████████████▊                                   | 407/650 [3:22:01<1:52:58, 27.89s/it]

 63%|███████████████████████████████████████████████████████████                                   | 408/650 [3:22:27<1:50:43, 27.45s/it]

 63%|███████████████████████████████████████████████████████████▏                                  | 409/650 [3:22:54<1:49:13, 27.19s/it]

 63%|███████████████████████████████████████████████████████████▎                                  | 410/650 [3:23:21<1:48:25, 27.10s/it]

 63%|███████████████████████████████████████████████████████████▍                                  | 411/650 [3:23:48<1:47:42, 27.04s/it]

 63%|███████████████████████████████████████████████████████████▌                                  | 412/650 [3:24:15<1:47:00, 26.98s/it]

 64%|███████████████████████████████████████████████████████████▋                                  | 413/650 [3:24:44<1:49:18, 27.67s/it]

 64%|███████████████████████████████████████████████████████████▊                                  | 414/650 [3:25:14<1:52:02, 28.48s/it]

 64%|████████████████████████████████████████████████████████████                                  | 415/650 [3:25:58<2:09:42, 33.12s/it]

 64%|████████████████████████████████████████████████████████████▏                                 | 416/650 [3:26:27<2:04:32, 31.93s/it]

 64%|████████████████████████████████████████████████████████████▎                                 | 417/650 [3:26:54<1:57:48, 30.34s/it]

 64%|████████████████████████████████████████████████████████████▍                                 | 418/650 [3:27:21<1:53:05, 29.25s/it]

 64%|████████████████████████████████████████████████████████████▌                                 | 419/650 [3:27:47<1:49:33, 28.46s/it]

 65%|████████████████████████████████████████████████████████████▋                                 | 420/650 [3:28:14<1:47:03, 27.93s/it]

 65%|████████████████████████████████████████████████████████████▉                                 | 421/650 [3:28:41<1:44:59, 27.51s/it]

 65%|█████████████████████████████████████████████████████████████                                 | 422/650 [3:29:07<1:43:38, 27.27s/it]

 65%|█████████████████████████████████████████████████████████████▏                                | 423/650 [3:29:34<1:42:30, 27.10s/it]

 65%|█████████████████████████████████████████████████████████████▎                                | 424/650 [3:30:01<1:41:37, 26.98s/it]

 65%|█████████████████████████████████████████████████████████████▍                                | 425/650 [3:30:27<1:40:51, 26.90s/it]

 66%|█████████████████████████████████████████████████████████████▌                                | 426/650 [3:30:54<1:40:12, 26.84s/it]

 66%|█████████████████████████████████████████████████████████████▊                                | 427/650 [3:31:21<1:39:35, 26.80s/it]

 66%|█████████████████████████████████████████████████████████████▉                                | 428/650 [3:31:48<1:39:05, 26.78s/it]

 66%|██████████████████████████████████████████████████████████████                                | 429/650 [3:32:15<1:38:54, 26.85s/it]


























 96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 27/28 [01:57<00:03,  3.75s/it]

{'eval_loss': 2.0949039459228516, 'eval_runtime': 122.3685, 'eval_samples_per_second': 7.11, 'eval_steps_per_second': 0.229, 'epoch': 6.6}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0501, 'learning_rate': 8.252250331143001e-06, 'epoch': 6.62}
 66%|██████████████████████████████████████████████████████████████▏                               | 430/650 [3:34:44<3:53:23, 63.65s/it]

 66%|██████████████████████████████████████████████████████████████▎                               | 431/650 [3:35:10<3:11:12, 52.38s/it]

 66%|██████████████████████████████████████████████████████████████▍                               | 432/650 [3:35:37<2:42:21, 44.69s/it]

 67%|██████████████████████████████████████████████████████████████▌                               | 433/650 [3:36:04<2:22:05, 39.29s/it]

 67%|██████████████████████████████████████████████████████████████▊                               | 434/650 [3:36:30<2:07:32, 35.43s/it]

 67%|██████████████████████████████████████████████████████████████▉                               | 435/650 [3:36:57<1:57:37, 32.82s/it]

 67%|███████████████████████████████████████████████████████████████                               | 436/650 [3:37:23<1:50:29, 30.98s/it]

 67%|███████████████████████████████████████████████████████████████▏                              | 437/650 [3:37:50<1:45:32, 29.73s/it]

 67%|███████████████████████████████████████████████████████████████▎                              | 438/650 [3:38:18<1:43:12, 29.21s/it]

 68%|███████████████████████████████████████████████████████████████▍                              | 439/650 [3:38:47<1:41:59, 29.00s/it]

 68%|███████████████████████████████████████████████████████████████▋                              | 440/650 [3:39:16<1:41:43, 29.06s/it]


 68%|███████████████████████████████████████████████████████████████▉                              | 442/650 [3:40:09<1:36:01, 27.70s/it]
{'loss': 1.9483, 'learning_rate': 7.478853664580114e-06, 'epoch': 6.8}

 68%|████████████████████████████████████████████████████████████████                              | 443/650 [3:40:36<1:34:29, 27.39s/it]

 68%|████████████████████████████████████████████████████████████████▏                             | 444/650 [3:41:03<1:33:17, 27.17s/it]

 68%|████████████████████████████████████████████████████████████████▎                             | 445/650 [3:41:29<1:32:19, 27.02s/it]

 69%|████████████████████████████████████████████████████████████████▍                             | 446/650 [3:41:56<1:31:32, 26.92s/it]

 69%|████████████████████████████████████████████████████████████████▋                             | 447/650 [3:42:23<1:30:50, 26.85s/it]

 69%|████████████████████████████████████████████████████████████████▊                             | 448/650 [3:42:49<1:30:08, 26.78s/it]

 69%|████████████████████████████████████████████████████████████████▉                             | 449/650 [3:43:16<1:29:33, 26.73s/it]

 69%|█████████████████████████████████████████████████████████████████                             | 450/650 [3:43:43<1:29:03, 26.72s/it]

 69%|█████████████████████████████████████████████████████████████████▏                            | 451/650 [3:44:09<1:28:39, 26.73s/it]

 70%|█████████████████████████████████████████████████████████████████▎                            | 452/650 [3:44:37<1:28:38, 26.86s/it]

 70%|█████████████████████████████████████████████████████████████████▌                            | 453/650 [3:45:06<1:30:12, 27.47s/it]

 70%|█████████████████████████████████████████████████████████████████▋                            | 454/650 [3:45:35<1:31:41, 28.07s/it]

 70%|█████████████████████████████████████████████████████████████████▊                            | 455/650 [3:45:53<1:21:21, 25.03s/it]

 70%|█████████████████████████████████████████████████████████████████▉                            | 456/650 [3:46:20<1:22:31, 25.53s/it]

 70%|██████████████████████████████████████████████████████████████████                            | 457/650 [3:46:46<1:23:08, 25.85s/it]

 70%|██████████████████████████████████████████████████████████████████▏                           | 458/650 [3:47:13<1:23:29, 26.09s/it]

 71%|██████████████████████████████████████████████████████████████████▍                           | 459/650 [3:47:39<1:23:32, 26.24s/it]

 71%|██████████████████████████████████████████████████████████████████▌                           | 460/650 [3:48:06<1:23:31, 26.38s/it]

 71%|██████████████████████████████████████████████████████████████████▋                           | 461/650 [3:48:33<1:23:25, 26.48s/it]

 71%|██████████████████████████████████████████████████████████████████▊                           | 462/650 [3:49:00<1:23:11, 26.55s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:48<00:09,  4.88s/it]


 71%|██████████████████████████████████████████████████████████████████▊                           | 462/650 [3:50:58<1:23:11, 26.55s/it][34m[1mwandb[39m[22m: Adding directory to artifact (/home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-hf-Lora-API_USAGE-bs_16-lr_3e-05-m_l_768-m_p_l_512-w_decay_0.2/checkpoint-462)... Done. 0.4s
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0883, 'learning_rate': 6.189151354453912e-06, 'epoch': 7.12}
 71%|██████████████████████████████████████████████████████████████████▉                           | 463/650 [3:51:28<3:16:19, 62.99s/it]

 71%|███████████████████████████████████████████████████████████████████                           | 464/650 [3:51:54<2:41:25, 52.07s/it]

 72%|███████████████████████████████████████████████████████████████████▏                          | 465/650 [3:52:21<2:16:57, 44.42s/it]

 72%|███████████████████████████████████████████████████████████████████▍                          | 466/650 [3:52:47<1:59:52, 39.09s/it]

 72%|███████████████████████████████████████████████████████████████████▌                          | 467/650 [3:53:14<1:47:52, 35.37s/it]

 72%|███████████████████████████████████████████████████████████████████▋                          | 468/650 [3:53:41<1:39:18, 32.74s/it]

 72%|███████████████████████████████████████████████████████████████████▊                          | 469/650 [3:54:07<1:33:18, 30.93s/it]

 72%|███████████████████████████████████████████████████████████████████▉                          | 470/650 [3:54:34<1:28:55, 29.64s/it]

 72%|████████████████████████████████████████████████████████████████████                          | 471/650 [3:55:01<1:25:45, 28.74s/it]

 73%|████████████████████████████████████████████████████████████████████▎                         | 472/650 [3:55:27<1:23:26, 28.13s/it]


 73%|████████████████████████████████████████████████████████████████████▌                         | 474/650 [3:56:21<1:20:16, 27.36s/it]
{'loss': 2.0481, 'learning_rate': 5.549756151065584e-06, 'epoch': 7.29}

 73%|████████████████████████████████████████████████████████████████████▋                         | 475/650 [3:56:47<1:19:06, 27.13s/it]

 73%|████████████████████████████████████████████████████████████████████▊                         | 476/650 [3:57:14<1:18:19, 27.01s/it]

 73%|████████████████████████████████████████████████████████████████████▉                         | 477/650 [3:57:41<1:17:33, 26.90s/it]

 74%|█████████████████████████████████████████████████████████████████████▏                        | 478/650 [3:58:07<1:16:48, 26.79s/it]

 74%|█████████████████████████████████████████████████████████████████████▎                        | 479/650 [3:58:34<1:16:12, 26.74s/it]

 74%|█████████████████████████████████████████████████████████████████████▍                        | 480/650 [3:59:00<1:15:40, 26.71s/it]

 74%|█████████████████████████████████████████████████████████████████████▌                        | 481/650 [3:59:27<1:15:08, 26.68s/it]

 74%|█████████████████████████████████████████████████████████████████████▋                        | 482/650 [3:59:54<1:14:40, 26.67s/it]


 74%|█████████████████████████████████████████████████████████████████████▉                        | 484/650 [4:00:47<1:13:43, 26.65s/it]
{'loss': 2.0564, 'learning_rate': 4.9921220185405596e-06, 'epoch': 7.45}

 75%|██████████████████████████████████████████████████████████████████████▏                       | 485/650 [4:01:12<1:12:21, 26.31s/it]

 75%|██████████████████████████████████████████████████████████████████████▎                       | 486/650 [4:01:39<1:12:14, 26.43s/it]

 75%|██████████████████████████████████████████████████████████████████████▍                       | 487/650 [4:02:06<1:11:59, 26.50s/it]

 75%|██████████████████████████████████████████████████████████████████████▌                       | 488/650 [4:02:33<1:12:24, 26.82s/it]

 75%|██████████████████████████████████████████████████████████████████████▋                       | 489/650 [4:03:03<1:14:03, 27.60s/it]

 75%|██████████████████████████████████████████████████████████████████████▊                       | 490/650 [4:03:32<1:14:52, 28.08s/it]

 76%|███████████████████████████████████████████████████████████████████████                       | 491/650 [4:04:01<1:14:58, 28.29s/it]

 76%|███████████████████████████████████████████████████████████████████████▏                      | 492/650 [4:04:27<1:13:08, 27.77s/it]

 76%|███████████████████████████████████████████████████████████████████████▎                      | 493/650 [4:04:54<1:11:41, 27.40s/it]

 76%|███████████████████████████████████████████████████████████████████████▍                      | 494/650 [4:05:20<1:10:40, 27.18s/it]

 76%|███████████████████████████████████████████████████████████████████████▌                      | 495/650 [4:05:47<1:09:44, 27.00s/it]


























 96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 27/28 [01:35<00:03,  3.70s/it]

{'eval_loss': 2.094968795776367, 'eval_runtime': 100.4263, 'eval_samples_per_second': 8.663, 'eval_steps_per_second': 0.279, 'epoch': 7.62}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0831, 'learning_rate': 4.3546029913333135e-06, 'epoch': 7.63}
 76%|███████████████████████████████████████████████████████████████████████▋                      | 496/650 [4:07:54<2:26:39, 57.14s/it]

 76%|███████████████████████████████████████████████████████████████████████▊                      | 497/650 [4:08:21<2:02:17, 47.96s/it]

 77%|████████████████████████████████████████████████████████████████████████                      | 498/650 [4:08:46<1:44:01, 41.07s/it]

 77%|████████████████████████████████████████████████████████████████████████▏                     | 499/650 [4:09:13<1:32:29, 36.75s/it]

 77%|████████████████████████████████████████████████████████████████████████▎                     | 500/650 [4:09:39<1:24:07, 33.65s/it]

 77%|████████████████████████████████████████████████████████████████████████▍                     | 501/650 [4:10:06<1:18:21, 31.55s/it]

 77%|████████████████████████████████████████████████████████████████████████▌                     | 502/650 [4:10:32<1:14:12, 30.08s/it]


 78%|████████████████████████████████████████████████████████████████████████▉                     | 504/650 [4:11:26<1:08:52, 28.30s/it]
{'loss': 1.9755, 'learning_rate': 3.949783979033632e-06, 'epoch': 7.75}

 78%|█████████████████████████████████████████████████████████████████████████                     | 505/650 [4:11:52<1:07:14, 27.82s/it]

 78%|█████████████████████████████████████████████████████████████████████████▏                    | 506/650 [4:12:19<1:05:56, 27.47s/it]

 78%|█████████████████████████████████████████████████████████████████████████▎                    | 507/650 [4:12:45<1:04:47, 27.18s/it]

 78%|█████████████████████████████████████████████████████████████████████████▍                    | 508/650 [4:13:12<1:03:51, 26.98s/it]

 78%|█████████████████████████████████████████████████████████████████████████▌                    | 509/650 [4:13:39<1:03:08, 26.87s/it]

 78%|█████████████████████████████████████████████████████████████████████████▊                    | 510/650 [4:14:05<1:02:35, 26.83s/it]

 79%|█████████████████████████████████████████████████████████████████████████▉                    | 511/650 [4:14:31<1:01:37, 26.60s/it]

 79%|██████████████████████████████████████████████████████████████████████████                    | 512/650 [4:14:58<1:01:13, 26.62s/it]

 79%|██████████████████████████████████████████████████████████████████████████▏                   | 513/650 [4:15:25<1:01:16, 26.83s/it]

 79%|██████████████████████████████████████████████████████████████████████████▎                   | 514/650 [4:15:54<1:02:12, 27.45s/it]


 79%|██████████████████████████████████████████████████████████████████████████▌                   | 516/650 [4:16:52<1:02:46, 28.11s/it]
{'loss': 2.1207, 'learning_rate': 3.374391740974161e-06, 'epoch': 7.94}

 80%|██████████████████████████████████████████████████████████████████████████▊                   | 517/650 [4:17:19<1:01:21, 27.68s/it]

 80%|██████████████████████████████████████████████████████████████████████████▉                   | 518/650 [4:17:45<1:00:14, 27.38s/it]

 80%|████████████████████████████████████████████████████████████████████████████▋                   | 519/650 [4:18:12<59:16, 27.15s/it]

 80%|████████████████████████████████████████████████████████████████████████████▊                   | 520/650 [4:18:29<52:06, 24.05s/it]

 80%|████████████████████████████████████████████████████████████████████████████▉                   | 521/650 [4:18:55<53:24, 24.84s/it]

 80%|█████████████████████████████████████████████████████████████████████████████                   | 522/650 [4:19:22<54:09, 25.39s/it]

 80%|█████████████████████████████████████████████████████████████████████████████▏                  | 523/650 [4:19:49<54:34, 25.79s/it]

 81%|█████████████████████████████████████████████████████████████████████████████▍                  | 524/650 [4:20:15<54:38, 26.02s/it]

 81%|█████████████████████████████████████████████████████████████████████████████▌                  | 525/650 [4:20:41<54:13, 26.03s/it]

 81%|█████████████████████████████████████████████████████████████████████████████▋                  | 526/650 [4:21:08<54:12, 26.23s/it]

 81%|█████████████████████████████████████████████████████████████████████████████▊                  | 527/650 [4:21:35<54:00, 26.34s/it]

 81%|█████████████████████████████████████████████████████████████████████████████▉                  | 528/650 [4:22:02<53:58, 26.55s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:52<00:07,  3.76s/it]

{'eval_loss': 2.0945613384246826, 'eval_runtime': 122.3464, 'eval_samples_per_second': 7.111, 'eval_steps_per_second': 0.229, 'epoch': 8.12}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0682, 'learning_rate': 2.7961928432575264e-06, 'epoch': 8.14}
 81%|████████████████████████████████████████████████████████████████████████████▌                 | 529/650 [4:24:31<2:07:54, 63.43s/it]

 82%|████████████████████████████████████████████████████████████████████████████▋                 | 530/650 [4:24:58<1:44:47, 52.40s/it]

 82%|████████████████████████████████████████████████████████████████████████████▊                 | 531/650 [4:25:24<1:28:33, 44.65s/it]

 82%|████████████████████████████████████████████████████████████████████████████▉                 | 532/650 [4:25:51<1:17:09, 39.23s/it]

 82%|█████████████████████████████████████████████████████████████████████████████                 | 533/650 [4:26:18<1:09:05, 35.43s/it]

 82%|█████████████████████████████████████████████████████████████████████████████▏                | 534/650 [4:26:44<1:03:24, 32.79s/it]

 82%|███████████████████████████████████████████████████████████████████████████████                 | 535/650 [4:27:11<59:19, 30.95s/it]

 82%|███████████████████████████████████████████████████████████████████████████████▏                | 536/650 [4:27:37<56:19, 29.64s/it]

 83%|███████████████████████████████████████████████████████████████████████████████▎                | 537/650 [4:28:04<54:05, 28.72s/it]

 83%|███████████████████████████████████████████████████████████████████████████████▍                | 538/650 [4:28:31<52:26, 28.10s/it]

 83%|███████████████████████████████████████████████████████████████████████████████▌                | 539/650 [4:28:57<51:08, 27.64s/it]

 83%|███████████████████████████████████████████████████████████████████████████████▊                | 540/650 [4:29:24<50:07, 27.34s/it]

 83%|███████████████████████████████████████████████████████████████████████████████▉                | 541/650 [4:29:50<49:15, 27.11s/it]

 83%|████████████████████████████████████████████████████████████████████████████████                | 542/650 [4:30:17<48:32, 26.96s/it]

 84%|████████████████████████████████████████████████████████████████████████████████▏               | 543/650 [4:30:44<47:53, 26.86s/it]

 84%|████████████████████████████████████████████████████████████████████████████████▎               | 544/650 [4:31:10<47:18, 26.78s/it]

 84%|████████████████████████████████████████████████████████████████████████████████▍               | 545/650 [4:31:37<46:42, 26.69s/it]

 84%|████████████████████████████████████████████████████████████████████████████████▋               | 546/650 [4:32:03<46:14, 26.68s/it]

 84%|████████████████████████████████████████████████████████████████████████████████▊               | 547/650 [4:32:30<45:46, 26.66s/it]


 84%|█████████████████████████████████████████████████████████████████████████████████               | 549/650 [4:33:23<44:49, 26.63s/it]
{'loss': 2.1232, 'learning_rate': 2.0035159556267243e-06, 'epoch': 8.45}

 85%|█████████████████████████████████████████████████████████████████████████████████▏              | 550/650 [4:33:50<44:21, 26.62s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████▍              | 551/650 [4:34:16<43:54, 26.61s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████▌              | 552/650 [4:34:43<43:27, 26.61s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████▋              | 553/650 [4:35:10<43:02, 26.62s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████▊              | 554/650 [4:35:36<42:34, 26.61s/it]

 85%|█████████████████████████████████████████████████████████████████████████████████▉              | 555/650 [4:36:03<42:10, 26.64s/it]

 86%|██████████████████████████████████████████████████████████████████████████████████              | 556/650 [4:36:30<41:44, 26.65s/it]

 86%|██████████████████████████████████████████████████████████████████████████████████▎             | 557/650 [4:36:56<41:16, 26.63s/it]


 86%|██████████████████████████████████████████████████████████████████████████████████▌             | 559/650 [4:37:49<40:20, 26.60s/it]
{'loss': 2.0778, 'learning_rate': 1.6532498868708517e-06, 'epoch': 8.6}

 86%|██████████████████████████████████████████████████████████████████████████████████▋             | 560/650 [4:38:16<39:54, 26.61s/it]

 86%|██████████████████████████████████████████████████████████████████████████████████▊             | 561/650 [4:38:43<39:27, 26.61s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.70s/it]

{'eval_loss': 2.094677686691284, 'eval_runtime': 100.3596, 'eval_samples_per_second': 8.669, 'eval_steps_per_second': 0.279, 'epoch': 8.63}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.023, 'learning_rate': 1.5543455741678686e-06, 'epoch': 8.65}
 86%|█████████████████████████████████████████████████████████████████████████████████▎            | 562/650 [4:40:50<1:23:24, 56.87s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▍            | 563/650 [4:41:17<1:09:18, 47.80s/it]


 87%|███████████████████████████████████████████████████████████████████████████████████▍            | 565/650 [4:42:10<52:22, 36.97s/it]
{'loss': 2.0659, 'learning_rate': 1.4583299085006474e-06, 'epoch': 8.69}

 87%|███████████████████████████████████████████████████████████████████████████████████▌            | 566/650 [4:42:36<47:22, 33.83s/it]

 87%|███████████████████████████████████████████████████████████████████████████████████▋            | 567/650 [4:43:03<43:47, 31.66s/it]


 88%|████████████████████████████████████████████████████████████████████████████████████            | 569/650 [4:43:56<39:09, 29.00s/it]
{'loss': 2.1561, 'learning_rate': 1.3348380350767115e-06, 'epoch': 8.75}

 88%|████████████████████████████████████████████████████████████████████████████████████▏           | 570/650 [4:44:22<37:39, 28.24s/it]

 88%|████████████████████████████████████████████████████████████████████████████████████▎           | 571/650 [4:44:49<36:33, 27.76s/it]


 88%|████████████████████████████████████████████████████████████████████████████████████▋           | 573/650 [4:45:42<34:49, 27.14s/it]
{'loss': 2.0367, 'learning_rate': 1.2165652992013505e-06, 'epoch': 8.82}

 88%|████████████████████████████████████████████████████████████████████████████████████▊           | 574/650 [4:46:08<34:03, 26.89s/it]

 88%|████████████████████████████████████████████████████████████████████████████████████▉           | 575/650 [4:46:35<33:31, 26.82s/it]


 89%|█████████████████████████████████████████████████████████████████████████████████████▏          | 577/650 [4:47:28<32:26, 26.67s/it]
{'loss': 2.0851, 'learning_rate': 1.1035568727999595e-06, 'epoch': 8.88}

 89%|█████████████████████████████████████████████████████████████████████████████████████▎          | 578/650 [4:47:55<31:59, 26.66s/it]

 89%|█████████████████████████████████████████████████████████████████████████████████████▌          | 579/650 [4:48:21<31:32, 26.65s/it]

 89%|█████████████████████████████████████████████████████████████████████████████████████▋          | 580/650 [4:48:48<31:04, 26.64s/it]

 89%|█████████████████████████████████████████████████████████████████████████████████████▊          | 581/650 [4:49:13<30:12, 26.28s/it]

 90%|█████████████████████████████████████████████████████████████████████████████████████▉          | 582/650 [4:49:40<29:52, 26.36s/it]

 90%|██████████████████████████████████████████████████████████████████████████████████████          | 583/650 [4:50:07<29:31, 26.45s/it]

 90%|██████████████████████████████████████████████████████████████████████████████████████▎         | 584/650 [4:50:33<29:09, 26.51s/it]


 90%|██████████████████████████████████████████████████████████████████████████████████████▌         | 586/650 [4:51:16<26:05, 24.46s/it]
{'loss': 2.0261, 'learning_rate': 8.687558246736893e-07, 'epoch': 9.02}

 90%|██████████████████████████████████████████████████████████████████████████████████████▋         | 587/650 [4:51:43<26:22, 25.12s/it]

 90%|██████████████████████████████████████████████████████████████████████████████████████▊         | 588/650 [4:52:10<26:24, 25.55s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████▉         | 589/650 [4:52:36<26:16, 25.85s/it]

 91%|███████████████████████████████████████████████████████████████████████████████████████▏        | 590/650 [4:53:03<26:04, 26.08s/it]

 91%|███████████████████████████████████████████████████████████████████████████████████████▎        | 591/650 [4:53:29<25:46, 26.21s/it]


 91%|███████████████████████████████████████████████████████████████████████████████████████▌        | 593/650 [4:54:23<25:06, 26.42s/it]
{'loss': 1.9934, 'learning_rate': 7.049989887402363e-07, 'epoch': 9.12}

 91%|███████████████████████████████████████████████████████████████████████████████████████▋        | 594/650 [4:54:49<24:41, 26.46s/it]


























 96%|████████████████████████████████████████████████████████████████████████████████████████████████▍   | 27/28 [01:35<00:03,  3.69s/it]

{'eval_loss': 2.0946521759033203, 'eval_runtime': 100.3413, 'eval_samples_per_second': 8.67, 'eval_steps_per_second': 0.279, 'epoch': 9.14}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.9598, 'learning_rate': 6.612743075359712e-07, 'epoch': 9.15}
 92%|███████████████████████████████████████████████████████████████████████████████████████▉        | 595/650 [4:56:57<52:00, 56.73s/it]

 92%|████████████████████████████████████████████████████████████████████████████████████████        | 596/650 [4:57:23<42:53, 47.66s/it]

 92%|████████████████████████████████████████████████████████████████████████████████████████▏       | 597/650 [4:57:50<36:30, 41.34s/it]


 92%|████████████████████████████████████████████████████████████████████████████████████████▍       | 599/650 [4:58:43<28:46, 33.85s/it]
{'loss': 1.9873, 'learning_rate': 5.779363813780259e-07, 'epoch': 9.22}

 92%|████████████████████████████████████████████████████████████████████████████████████████▌       | 600/650 [4:59:09<26:22, 31.65s/it]

 92%|████████████████████████████████████████████████████████████████████████████████████████▊       | 601/650 [4:59:36<24:36, 30.14s/it]


 93%|█████████████████████████████████████████████████████████████████████████████████████████       | 603/650 [5:00:29<22:09, 28.29s/it]
{'loss': 2.0913, 'learning_rate': 5.001066763412854e-07, 'epoch': 9.28}

 93%|█████████████████████████████████████████████████████████████████████████████████████████▏      | 604/650 [5:00:56<21:17, 27.76s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████████████▎      | 605/650 [5:01:22<20:33, 27.42s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████████████▌      | 606/650 [5:01:49<19:55, 27.17s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████████████▋      | 607/650 [5:02:15<19:20, 26.99s/it]

 94%|█████████████████████████████████████████████████████████████████████████████████████████▊      | 608/650 [5:02:42<18:48, 26.88s/it]


 94%|██████████████████████████████████████████████████████████████████████████████████████████      | 610/650 [5:03:35<17:47, 26.69s/it]
{'loss': 2.1349, 'learning_rate': 3.772470658215299e-07, 'epoch': 9.38}

 94%|██████████████████████████████████████████████████████████████████████████████████████████▏     | 611/650 [5:04:02<17:21, 26.71s/it]

 94%|██████████████████████████████████████████████████████████████████████████████████████████▍     | 612/650 [5:04:28<16:53, 26.68s/it]

 94%|██████████████████████████████████████████████████████████████████████████████████████████▌     | 613/650 [5:04:54<16:20, 26.49s/it]

 94%|██████████████████████████████████████████████████████████████████████████████████████████▋     | 614/650 [5:05:21<15:55, 26.54s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████▊     | 615/650 [5:05:48<15:29, 26.56s/it]

 95%|██████████████████████████████████████████████████████████████████████████████████████████▉     | 616/650 [5:06:14<15:03, 26.58s/it]


 95%|███████████████████████████████████████████████████████████████████████████████████████████▎    | 618/650 [5:07:07<14:10, 26.57s/it]
{'loss': 2.1764, 'learning_rate': 2.5778543511610963e-07, 'epoch': 9.51}

 95%|███████████████████████████████████████████████████████████████████████████████████████████▍    | 619/650 [5:07:34<13:44, 26.59s/it]

 95%|███████████████████████████████████████████████████████████████████████████████████████████▌    | 620/650 [5:08:01<13:18, 26.60s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████▋    | 621/650 [5:08:27<12:51, 26.60s/it]

 96%|███████████████████████████████████████████████████████████████████████████████████████████▊    | 622/650 [5:08:54<12:24, 26.60s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████    | 623/650 [5:09:21<11:58, 26.61s/it]


 96%|████████████████████████████████████████████████████████████████████████████████████████████▎   | 625/650 [5:10:14<11:05, 26.64s/it]
{'loss': 1.9385, 'learning_rate': 1.71724417018716e-07, 'epoch': 9.62}

 96%|████████████████████████████████████████████████████████████████████████████████████████████▍   | 626/650 [5:10:41<10:39, 26.66s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████▌   | 627/650 [5:11:07<10:13, 26.66s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.70s/it]

{'eval_loss': 2.0947799682617188, 'eval_runtime': 100.3694, 'eval_samples_per_second': 8.668, 'eval_steps_per_second': 0.279, 'epoch': 9.65}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 97%|████████████████████████████████████████████████████████████████████████████████████████████▊   | 628/650 [5:13:15<20:51, 56.87s/it]
{'loss': 1.9999, 'learning_rate': 1.4014502817900399e-07, 'epoch': 9.66}


 97%|█████████████████████████████████████████████████████████████████████████████████████████████   | 630/650 [5:14:08<13:48, 41.44s/it]

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▏  | 631/650 [5:14:34<11:42, 37.00s/it]
{'loss': 2.0803, 'learning_rate': 1.1175811139433057e-07, 'epoch': 9.71}

 97%|█████████████████████████████████████████████████████████████████████████████████████████████▎  | 632/650 [5:15:01<10:10, 33.89s/it]


 98%|█████████████████████████████████████████████████████████████████████████████████████████████▋  | 634/650 [5:15:54<08:02, 30.17s/it]
{'loss': 2.1342, 'learning_rate': 8.656976527324389e-08, 'epoch': 9.75}

 98%|█████████████████████████████████████████████████████████████████████████████████████████████▊  | 635/650 [5:16:21<07:16, 29.10s/it]


 98%|██████████████████████████████████████████████████████████████████████████████████████████████  | 637/650 [5:17:14<06:02, 27.85s/it]

 98%|██████████████████████████████████████████████████████████████████████████████████████████████▏ | 638/650 [5:17:41<05:29, 27.48s/it]
{'loss': 2.15, 'learning_rate': 5.797011211271519e-08, 'epoch': 9.82}

 98%|██████████████████████████████████████████████████████████████████████████████████████████████▍ | 639/650 [5:18:07<04:58, 27.15s/it]


 99%|██████████████████████████████████████████████████████████████████████████████████████████████▋ | 641/650 [5:19:00<04:01, 26.87s/it]
{'loss': 2.1539, 'learning_rate': 4.026495212546333e-08, 'epoch': 9.86}

 99%|██████████████████████████████████████████████████████████████████████████████████████████████▊ | 642/650 [5:19:27<03:34, 26.80s/it]


 99%|███████████████████████████████████████████████████████████████████████████████████████████████ | 644/650 [5:20:20<02:40, 26.70s/it]

 99%|███████████████████████████████████████████████████████████████████████████████████████████████▎| 645/650 [5:20:47<02:13, 26.69s/it]
{'loss': 2.0931, 'learning_rate': 2.1658077061711412e-08, 'epoch': 9.92}

 99%|███████████████████████████████████████████████████████████████████████████████████████████████▍| 646/650 [5:21:13<01:46, 26.65s/it]


100%|███████████████████████████████████████████████████████████████████████████████████████████████▋| 648/650 [5:22:07<00:53, 26.63s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████████████▊| 649/650 [5:22:33<00:26, 26.60s/it]
{'loss': 2.1438, 'learning_rate': 8.771882583089586e-09, 'epoch': 9.98}
{'loss': 2.0932, 'learning_rate': 6.444815096344203e-09, 'epoch': 10.0}


100%|████████████████████████████████████████████████████████████████████████████████████████████████| 650/650 [5:22:50<00:00, 29.80s/it]