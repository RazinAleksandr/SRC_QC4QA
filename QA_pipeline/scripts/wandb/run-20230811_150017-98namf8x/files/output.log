  0%|                                                                                                            | 0/650 [00:00<?, ?it/s]/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▏                                                                                                 | 1/650 [00:27<4:55:50, 27.35s/it]

  0%|▎                                                                                                 | 2/650 [00:53<4:48:33, 26.72s/it]

  0%|▍                                                                                                 | 3/650 [01:19<4:46:01, 26.52s/it]
{'loss': 2.4727, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.05}

  1%|▌                                                                                                 | 4/650 [01:46<4:44:39, 26.44s/it]

  1%|▊                                                                                                 | 5/650 [02:12<4:43:48, 26.40s/it]

  1%|▉                                                                                                 | 6/650 [02:38<4:43:15, 26.39s/it]


  1%|█▏                                                                                                | 8/650 [03:31<4:42:10, 26.37s/it]

  1%|█▎                                                                                                | 9/650 [03:57<4:41:38, 26.36s/it]

  2%|█▍                                                                                               | 10/650 [04:23<4:39:18, 26.19s/it]

  2%|█▋                                                                                               | 11/650 [04:50<4:39:54, 26.28s/it]
{'loss': 2.4264, 'learning_rate': 2.999713552377097e-05, 'epoch': 0.17}

  2%|█▊                                                                                               | 12/650 [05:16<4:40:04, 26.34s/it]


  2%|██                                                                                               | 14/650 [06:09<4:40:07, 26.43s/it]

  2%|██▏                                                                                              | 15/650 [06:36<4:39:50, 26.44s/it]

  2%|██▍                                                                                              | 16/650 [07:02<4:39:41, 26.47s/it]
{'loss': 2.4827, 'learning_rate': 2.99855004640871e-05, 'epoch': 0.25}

  3%|██▌                                                                                              | 17/650 [07:29<4:39:14, 26.47s/it]


  3%|██▊                                                                                              | 19/650 [08:22<4:38:20, 26.47s/it]

  3%|██▉                                                                                              | 20/650 [08:48<4:37:59, 26.48s/it]
{'loss': 2.3451, 'learning_rate': 2.9969753177502497e-05, 'epoch': 0.31}

  3%|███▏                                                                                             | 21/650 [09:15<4:37:53, 26.51s/it]


  4%|███▍                                                                                             | 23/650 [10:08<4:37:21, 26.54s/it]

  4%|███▌                                                                                             | 24/650 [10:34<4:37:04, 26.56s/it]
{'loss': 2.3116, 'learning_rate': 2.9948288490633714e-05, 'epoch': 0.37}


  4%|███▉                                                                                             | 26/650 [11:28<4:36:45, 26.61s/it]

  4%|████                                                                                             | 27/650 [11:54<4:36:26, 26.62s/it]
{'loss': 2.4088, 'learning_rate': 2.9928442781145217e-05, 'epoch': 0.42}

  4%|████▏                                                                                            | 28/650 [12:21<4:36:08, 26.64s/it]


  5%|████▍                                                                                            | 30/650 [13:14<4:34:44, 26.59s/it]
{'loss': 2.3281, 'learning_rate': 2.9905389864285287e-05, 'epoch': 0.46}

  5%|████▋                                                                                            | 31/650 [13:41<4:34:31, 26.61s/it]

  5%|████▊                                                                                            | 32/650 [14:08<4:34:18, 26.63s/it]

  5%|████▉                                                                                            | 33/650 [14:34<4:32:00, 26.45s/it]

























 93%|████████████████████████████████████████████████████████████████████████████████████████████▊       | 26/28 [01:32<00:07,  3.69s/it]

{'eval_loss': 2.412285804748535, 'eval_runtime': 100.2331, 'eval_samples_per_second': 8.68, 'eval_steps_per_second': 0.279, 'epoch': 0.51}
/home/st-aleksandr-razin/.conda/envs/qc4qa/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  5%|█████                                                                                            | 34/650 [16:41<9:42:16, 56.72s/it]
{'loss': 2.3684, 'learning_rate': 2.986967231182466e-05, 'epoch': 0.52}


  6%|█████▎                                                                                           | 36/650 [17:34<7:03:24, 41.38s/it]

  6%|█████▌                                                                                           | 37/650 [18:01<6:17:45, 36.97s/it]
{'loss': 2.4138, 'learning_rate': 2.9839156360347433e-05, 'epoch': 0.57}

  6%|█████▋                                                                                           | 38/650 [18:27<5:45:14, 33.85s/it]


  6%|█████▉                                                                                           | 40/650 [19:20<5:05:17, 30.03s/it]

  6%|██████                                                                                           | 41/650 [19:47<4:53:32, 28.92s/it]

  6%|██████▎                                                                                          | 42/650 [20:13<4:45:45, 28.20s/it]
{'loss': 2.4063, 'learning_rate': 2.978121548841459e-05, 'epoch': 0.65}

  7%|██████▍                                                                                          | 43/650 [20:39<4:39:48, 27.66s/it]


  7%|██████▋                                                                                          | 45/650 [21:32<4:32:47, 27.05s/it]

  7%|██████▊                                                                                          | 46/650 [21:59<4:30:32, 26.87s/it]
