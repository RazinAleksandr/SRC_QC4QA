run_config:
  domain: Database_and_SQL_class
  adapter: Lora

eval:
  model:
    name: "/raid/models/llama-7b-hf"
    load_in_8bit: True
    peft_model_id: /home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/experiments/train-llama-7b-bs_32-lr_1e4-m_l_1280-m_p_l_768-filtred_no_code-no_0_scores/checkpoint-117
    torch_dtype: fp16 # fp16/null
    device_map: cuda:2
    padding_side: left
  seed: 42
  batch_size: 16
  compute_metrics: true
  data:
    dataset_name: RazinAleks/SO-Python_QA-
    max_prompt_length: 512
    split: "test"  
    use_title: true
    columns_to_save:
      - Question
      - Answer
      - Title # if use_title
      - Score
      - Users Score
  generate_config:
    do_sample: true
    max_new_tokens: 256
    no_repeat_ngram_size: 2
    top_k: 50
    temperature: 0.8
    top_p: 0.9
    use_cache: true
    num_return_sequences: 1
log_config:
  save_steps: 5
  dir: /home/st-aleksandr-razin/workspace/SRC_QC4QA/QA_pipeline/artifacts/tests/
  #file_name: test-p_tune_llama_filt_no_code-7b-max_prompt_length_768-Data_Science_and_Machine_Learning_class.csv
wandb_config:
  project: 'SRC_QC4QA'
  #name: test-p_tune_llama_filt_no_code-7b-max_prompt_length_768-Data_Science_and_Machine_Learning_class.csv
  tags:
    - "test"