# config.yaml
teacher_model_params:
  model_name: "intfloat/e5-base"
  num_classes: 8
  n_inputs: 768
 
  batch_norm: true #false
  dropout_rate: 0.3
  activation: "leaky_relu"

student_model_params:
  model_name: "intfloat/e5-small"
  num_classes: 8
  n_inputs: 384

  batch_norm: true # false
  dropout_rate: 0.3
  activation: "leaky_relu"

data_params:
  data_path: "/home/st-aleksandr-razin/workspace/SRC_QC4QA/QC_pipeline/dataset"
  tokenizer_name: "intfloat/e5-base"
  max_len: 512
  train_batch_size: 256 # 64
  val_batch_size: 128
  num_workers: 8
  pin_memory: False
  transform: False

exp_params:
  LR: 0.00003 # 1e-4
  weight_decay: 0.3
  scheduler_gamma: 0.95
  distill_weight: 0.5 ## 0.25
  temperature: 1
  manual_seed: 17
  freeze: true

trainer_params:
  #gradient_clip_val: 1
  #gradient_clip_algorithm: "value"
  devices: [0]  #[1]
  accelerator: "gpu"
  #gpus: "1"
  #devices: 2
  log_every_n_steps: 1
  val_check_interval: 400
  max_epochs: 20

logging_params:
  log_dir: "/home/st-aleksandr-razin/workspace/SRC_QC4QA/QC_pipeline/artifacts/logs"
  save_dir: "/home/st-aleksandr-razin/workspace/SRC_QC4QA/QC_pipeline/artifacts/saved_models"
  name: "E5_Knowledge_distillation_QC4QA"

seed: 17